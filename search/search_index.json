{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to spotforecast2-safe (Core)","text":"<p>spotforecast2-safe is a specialized, hardened Python package for time series forecasting in safety-critical production environments. It provides a minimal, auditable core for feature engineering and recursive forecasting.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udce6 GitHub Repository</li> <li>\ud83d\udcda API Reference</li> <li>\ufffd\ufe0f Safety &amp; Compliance</li> <li>\ud83d\udcca Model/Method Card</li> <li>\ud83d\ude80 Current Version: 0.8.2</li> </ul>"},{"location":"#software-identification-cpe","title":"Software Identification (CPE)","text":"<p>For vulnerability tracking, supply chain management, and SBOM (Software Bill of Materials) generation:</p> <pre><code>cpe:2.3:a:sequential_parameter_optimization:spotforecast2_safe:*:*:*:*:*:*:*:*\n</code></pre> <p>Current release identifier:</p> <pre><code>cpe:2.3:a:sequential_parameter_optimization:spotforecast2_safe:0.8.2:*:*:*:*:*:*:*\n</code></pre> <p>See Model/Method Card for additional compliance details.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/sequential-parameter-optimization/spotforecast2-safe.git\ncd spotforecast2-safe\nuv sync\n</code></pre>"},{"location":"#safety-critical-features","title":"Safety-Critical Features","text":"<ul> <li>Zero Dead Code: No GUI, plotting, or AutoML dependencies (No Plotly, No Optuna).</li> <li>Deterministic Transformations: Mathematical logic that ensures bit-level reproducibility.</li> <li>Fail-Safe Processing: Explicit failure on dirty or incomplete data (NaNs/Infs) instead of silent imputation.</li> <li>Minimal Footprint: Reduced attack surface for high-security deployment targets.</li> </ul>"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Data Service: Robust fetching of time series, weather, and holiday data.</li> <li>Preprocessing: Hardened tools for data curation, resampling, and temporal splitting.</li> <li>Forecasting Engine: Simplified recursive forecasting and seasonal baselines.</li> </ul>"},{"location":"#disclaimer-liability","title":"\u26a0\ufe0f Disclaimer &amp; Liability","text":"<p>IMPORTANT: This software is provided \"as is\" and any express or implied warranties are disclaimed. The use of this software in safety-critical systems is at the sole risk of the user. For full details, see the Disclaimer in the Model Card.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>spotforecast2</li> <li>spotoptim</li> </ul>"},{"location":"#attributions","title":"Attributions","text":"<p>Parts of the code are ported from <code>skforecast</code> to reduce external dependencies. Many thanks to the skforecast team for their great work!</p>"},{"location":"api_overview/","title":"API Overview &amp; Getting Started","text":"<p>This page provides a high-level introduction to spotforecast2-safe's public API and key concepts. For detailed API documentation, see the API Reference.</p>"},{"location":"api_overview/#main-entry-points","title":"Main Entry Points","text":"<p>The spotforecast2-safe library organizes functionality into six major modules:</p> <ul> <li>Data: Fetching and managing time series, weather, and holiday data</li> <li>Preprocessing: Feature engineering, data curation, and transformation</li> <li>Processing: Utilities for handling timestamps and temporal conversions</li> <li>Forecaster: Recursive forecasting models (ForecasterRecursive, ForecasterEquivalentDate)</li> <li>Utils: CPE generation, configuration, validation, and helper functions</li> <li>Weather: Climate data integration</li> </ul>"},{"location":"api_overview/#quick-start","title":"Quick Start","text":""},{"location":"api_overview/#1-import-core-components","title":"1. Import Core Components","text":"<pre><code>from spotforecast2_safe.data import Period\nfrom spotforecast2_safe.preprocessing import (\n    ForecasterRecursive,\n    ForecasterRecursiveLGBM,\n    ExogBuilder,\n    RollingFeatures,\n)\nfrom spotforecast2_safe.utils import generate_holiday\n</code></pre>"},{"location":"api_overview/#2-load-prepare-data","title":"2. Load &amp; Prepare Data","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Create sample time series data\ndates = pd.date_range('2020-01-01', periods=100, freq='D')\nvalues = np.random.randn(100).cumsum()\ndf = pd.DataFrame({'date': dates, 'value': values})\ndf.set_index('date', inplace=True)\n</code></pre>"},{"location":"api_overview/#3-define-forecasting-period","title":"3. Define Forecasting Period","text":"<pre><code># Define train/test split for temporal validation\nperiod = Period(\n    train_end='2022-12-31',\n    test_start='2023-01-01',\n    test_end='2023-12-31'\n)\n</code></pre>"},{"location":"api_overview/#4-create-rolling-features","title":"4. Create Rolling Features","text":"<pre><code># Transform time series into lag features for supervised learning\nrolling_features = RollingFeatures(\n    window_size=7,  # Use 7-day history\n    include_autocorrelated_features=True\n)\n\nX, y = rolling_features.fit_transform(df['value'])\n</code></pre>"},{"location":"api_overview/#5-train-recursive-forecaster","title":"5. Train Recursive Forecaster","text":"<pre><code># Initialize forecaster with LightGBM backend\nforecaster = ForecasterRecursiveLGBM(\n    steps_ahead=30,  # Forecast 30 days ahead\n    window_size=7,   # Use 7-day history\n)\n\n# Train on historical data\nforecaster.fit(X_train, y_train)\n\n# Forecast future values\ny_pred = forecaster.predict(steps=30)\n</code></pre>"},{"location":"api_overview/#key-concepts","title":"Key Concepts","text":""},{"location":"api_overview/#period-management","title":"Period Management","text":"<p>The <code>Period</code> class encapsulates temporal information for train/test splits:</p> <pre><code>from spotforecast2_safe.data import Period\n\nperiod = Period(\n    train_end='2023-06-30',\n    test_start='2023-07-01',\n    test_end='2023-12-31'\n)\n</code></pre> <p>See Period API for details.</p>"},{"location":"api_overview/#recursive-forecasting","title":"Recursive Forecasting","text":"<p>Recursive forecasting predicts multiple steps ahead by feeding model predictions back as inputs. The main classes are:</p> <ul> <li><code>ForecasterRecursive</code>: Base class for recursive forecasters</li> <li><code>ForecasterRecursiveLGBM</code>: LightGBM implementation (recommended for most use cases)</li> <li><code>ForecasterRecursiveXGB</code>: XGBoost implementation</li> </ul> <pre><code>from spotforecast2_safe.preprocessing import ForecasterRecursiveLGBM\n\nforecaster = ForecasterRecursiveLGBM(\n    steps_ahead=30,\n    window_size=7,\n    verbose=True\n)\nforecaster.fit(X_train, y_train)\nforecast = forecaster.predict(steps=30)\n</code></pre> <p>See ForecasterRecursive Guide for detailed examples.</p>"},{"location":"api_overview/#feature-engineering","title":"Feature Engineering","text":"<p>The <code>ExogBuilder</code> class constructs exogenous (external) features:</p> <pre><code>from spotforecast2_safe.preprocessing import ExogBuilder\n\nbuilder = ExogBuilder(\n    df_exog=weather_data,  # External variables (e.g., temperature)\n    include_lag_features=True\n)\n\nX_with_exog = builder.transform(X)\n</code></pre>"},{"location":"api_overview/#holiday-integration","title":"Holiday Integration","text":"<p>Generate holiday calendars for demand forecasting:</p> <pre><code>from spotforecast2_safe.utils import generate_holiday\n\nholidays = generate_holiday(\n    country='US',\n    year=2023\n)\n</code></pre> <p>See Holiday Generation API for details.</p>"},{"location":"api_overview/#model-persistence-savingloading","title":"Model Persistence (Saving/Loading)","text":"<p>Save trained models for production deployment:</p> <pre><code>from spotforecast2_safe.manager.persistence import save_model, load_model\n\n# Save model and metadata\nsave_model(forecaster, 'my_forecaster.pkl')\n\n# Load in production\nforecaster = load_model('my_forecaster.pkl')\n</code></pre> <p>See Model Persistence Guide for details.</p>"},{"location":"api_overview/#safety-critical-properties","title":"Safety-Critical Properties","text":"<p>All spotforecast2-safe operations maintain these critical properties:</p>"},{"location":"api_overview/#determinism","title":"Determinism","text":"<p>Same input always produces identical output (bit-level reproducible):</p> <pre><code>X, y1 = rolling_features.fit_transform(df['value'])\nX, y2 = rolling_features.fit_transform(df['value'])\nassert np.array_equal(y1, y2)  # True\n</code></pre>"},{"location":"api_overview/#fail-safe-operation","title":"Fail-Safe Operation","text":"<p>Invalid data raises explicit errors instead of silent failures:</p> <pre><code># This raises ValueError, not NaN propagation\ndf_with_nans = pd.DataFrame({'value': [1, np.nan, 3]})\ntry:\n    X, y = rolling_features.fit_transform(df_with_nans['value'])\nexcept ValueError as e:\n    print(f\"Data validation error: {e}\")\n</code></pre>"},{"location":"api_overview/#auditability","title":"Auditability","text":"<p>All transformations are traceable with clear, white-box code:</p> <pre><code># Source code is visible via:\n# 1. Docstrings (in editor)\n# 2. Automatic API documentation (mkdocs)\n# 3. GitHub repository\n</code></pre>"},{"location":"api_overview/#complete-example-end-to-end-forecasting","title":"Complete Example: End-to-End Forecasting","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing import ForecasterRecursiveLGBM, RollingFeatures\nfrom spotforecast2_safe.data import Period\nfrom spotforecast2_safe.utils import generate_holiday\n\n# 1. Prepare data\ndates = pd.date_range('2020-01-01', periods=365*3, freq='D')\nvalues = np.sin(np.arange(len(dates))*2*np.pi/365) + np.random.randn(len(dates))*0.1\ndf = pd.DataFrame({'date': dates, 'value': values})\ndf.set_index('date', inplace=True)\n\n# 2. Define periods\nperiod = Period(\n    train_end='2022-12-31',\n    test_start='2023-01-01',\n    test_end='2023-12-31'\n)\n\n# 3. Create features\nrolling = RollingFeatures(window_size=14)\nX, y = rolling.fit_transform(df['value'])\n\n# 4. Split data\ntrain_mask = df.index &lt;= period.train_end\nX_train, y_train = X[train_mask], y[train_mask]\nX_test, y_test = X[~train_mask], y[~train_mask]\n\n# 5. Train forecaster\nforecaster = ForecasterRecursiveLGBM(steps_ahead=30, window_size=14)\nforecaster.fit(X_train, y_train)\n\n# 6. Generate forecast\nforecast = forecaster.predict(steps=30)\nprint(f\"30-day forecast: {forecast}\")\n</code></pre>"},{"location":"api_overview/#documentation-organization","title":"Documentation Organization","text":"<p>The complete documentation is organized as follows:</p> <ul> <li>Home (this page): High-level overview</li> <li>API Reference: Detailed API documentation by module</li> <li>Data Module: Data fetching and Period management</li> <li>Preprocessing Module: Feature engineering and forecasters</li> <li>Processing Module: Utilities for timestamps and conversions</li> <li>Utils Module: Helper functions and CPE generation</li> <li>Weather Module: Climate data integration</li> <li>Exceptions: Error types and documentation</li> <li>Guides: Practical examples and workflows</li> <li>ForecasterRecursive Guide: Advanced forecasting techniques</li> <li>Model Persistence: Production deployment</li> <li>Safety &amp; Compliance: Documentation for auditors and compliance</li> <li>Model/Method Card: Compliance and safety design</li> <li>Contributing Guide: How to contribute to the project</li> </ul>"},{"location":"api_overview/#next-steps","title":"Next Steps","text":"<ol> <li>Quick Start: Follow the Quick Start example above</li> <li>Learn Core Concepts: Read about Period Management and Recursive Forecasting</li> <li>Explore Examples: Check out ForecasterRecursive Guide</li> <li>API Reference: Dive into specific modules in API Documentation</li> <li>Contribute: See Contributing Guide to contribute improvements</li> </ol>"},{"location":"api_overview/#troubleshooting","title":"Troubleshooting","text":"<p>For common issues and solutions:</p> <ul> <li>Data validation errors: Ensure all input data is clean (no NaNs or Infs)</li> <li>Import errors: Verify the package is installed with <code>uv sync</code></li> <li>Version compatibility: Check you're using Python 3.13 or later</li> </ul> <p>Before reporting a new issue, search the publicly available archives:</p> <ul> <li>Issues Archive: Browse all reported bugs, feature requests, and their resolutions. Use the search feature to find similar problems.</li> <li>Discussions Archive: Search community questions and answers for help with common tasks.</li> </ul> <p>If you don't find a solution, see the Reporting Issues guide to submit a new bug report. For more help, see the project's GitHub Issues.</p>"},{"location":"api_overview/#see-also","title":"See Also","text":"<ul> <li>Complete API Reference</li> <li>Model/Method Card</li> <li>Security Policy - Vulnerability reporting and security best practices</li> <li>Contributing Guide</li> <li>GitHub Repository</li> </ul>"},{"location":"contributing/","title":"Contributing to spotforecast2-safe","text":"<p>Thank you for your interest in contributing to spotforecast2-safe! This document provides guidelines and requirements for contributing to the project.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment for all contributors. Please treat all community members with respect.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13 or later</li> <li>uv package manager (install with <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code>)</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/sequential-parameter-optimization/spotforecast2-safe.git\ncd spotforecast2-safe\n</code></pre> <ol> <li>Create and activate the virtual environment:</li> </ol> <pre><code>uv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <ol> <li>Install the project with development dependencies:</li> </ol> <pre><code>uv sync\n</code></pre> <ol> <li>Run the test suite to verify setup:</li> </ol> <pre><code>uv run pytest tests/ -v\n</code></pre>"},{"location":"contributing/#building-the-package","title":"Building the Package","text":"<p>To build the source and binary distributions (wheels):</p> <pre><code># Using the standard build tool\nuv run python -m build\n\n# The artifacts will be in the dist/ directory\nls -lah dist/\n</code></pre>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":"<p>All contributions must adhere to the following standards:</p>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<ul> <li>Code style: Black (enforced)</li> <li>Import sorting: isort</li> <li>Linting: flake8</li> </ul> <p>Run formatting tools before committing:</p> <pre><code>black src/ tests/\nisort src/ tests/\nflake8 src/ tests/ --max-line-length=100\n</code></pre>"},{"location":"contributing/#documentation-style","title":"Documentation Style","text":"<ul> <li>Docstrings: Google style format</li> <li>All public functions, classes, and modules must have comprehensive docstrings</li> <li>Include type hints in function signatures</li> <li>Include usage examples in docstrings where applicable</li> </ul> <p>Example:</p> <pre><code>def get_cpe_identifier(version: str = \"*\") -&gt; str:\n    \"\"\"Generates the CPE 2.3 identifier for the spotforecast2-safe project.\n\n    This function constructs a Common Platform Enumeration (CPE) 2.3 formatted\n    string that uniquely identifies the spotforecast2-safe software.\n\n    Args:\n        version: The specific version of the software. Use wildcard \"*\" to match\n            all versions, or provide a semantic version string. Defaults to \"*\".\n\n    Returns:\n        str: The formatted Common Platform Enumeration 2.3 string.\n\n    Raises:\n        TypeError: If version is not a string.\n\n    Examples:\n        Generate a CPE identifier for all versions:\n\n        &gt;&gt;&gt; get_cpe_identifier()\n        'cpe:2.3:a:sequential_parameter_optimization:spotforecast2_safe:*:*:*:*:*:*:*:*'\n\n    See Also:\n        https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-188.pdf\n    \"\"\"\n</code></pre>"},{"location":"contributing/#spdx-license-headers","title":"SPDX License Headers","text":"<p>All source files (Python, YAML, etc.) must include SPDX headers at the top:</p> <pre><code># SPDX-FileCopyrightText: &lt;year&gt; &lt;your name&gt;\n# SPDX-License-Identifier: AGPL-3.0-or-later\n\n\"\"\"Module docstring.\"\"\"\n</code></pre> <p>The project uses REUSE compliance for license tracking. Run the REUSE check:</p> <pre><code>uv run reuse lint\n</code></pre>"},{"location":"contributing/#testing-requirements","title":"Testing Requirements","text":"<p>All contributions must include tests covering new functionality:</p> <ul> <li>Write tests in <code>tests/</code> directory following the existing naming convention: <code>test_*.py</code></li> <li>Use pytest as the testing framework</li> <li>Aim for high code coverage (minimum 80% for new code)</li> <li>Run tests before submitting a pull request:</li> </ul> <pre><code>uv run pytest tests/ -v --cov=src/spotforecast2_safe\n</code></pre> <p>Test files should also include SPDX headers and follow the same style guidelines.</p>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>Python 3.13+ features optional type hints are encouraged for all new code. Use precise types to improve IDE support and catch errors early:</p> <pre><code>from typing import Optional\n\ndef process_data(values: list[float], threshold: Optional[float] = None) -&gt; dict[str, int]:\n    \"\"\"Process a list of values.\"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>This project uses Semantic Versioning and Conventional Commits for automatic changelog generation.</p> <p>Commit message format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre> <p>Types (required): - feat: A new feature - fix: A bug fix - docs: Documentation changes only - style: Changes that do not affect code meaning (formatting, SPDX headers) - refactor: Code change that neither fixes a bug nor adds a feature - perf: Performance improvements - test: Test additions or changes - chore: Changes to build system, dependencies, or other non-code changes</p> <p>Example:</p> <pre><code>feat(cpe): add CPE identifier generation for compliance tracking\n\nImplement get_cpe_identifier() function to generate NIST CPE 2.3\nformatted strings for vulnerability tracking and SBOM management.\n\nCloses #42\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a feature branch from <code>develop</code>:</li> </ol> <pre><code>git checkout -b feat/your-feature-name\n</code></pre> <ol> <li>Make your changes and commit with conventional commit messages</li> <li>Ensure all tests pass and code standards are met</li> <li>Create a Pull Request against the <code>develop</code> branch</li> <li>PR description should clearly explain:</li> <li>What problem it solves</li> <li>How the solution works</li> <li>Any breaking changes</li> </ol> <p>Pull Request title should follow the conventional commit format.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>For larger features, update or create documentation:</p> <ul> <li>API documentation goes in <code>docs/api/</code></li> <li>Processing guides go in <code>docs/processing/</code></li> <li>Preprocessing guides go in <code>docs/preprocessing/</code></li> <li>Use Markdown with Google-style docstring conventions</li> </ul> <p>Build documentation locally:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>Visit http://localhost:8000 to preview changes.</p>"},{"location":"contributing/#safety-critical-standards","title":"Safety-Critical Standards","text":"<p>This is a safety-critical library. Contributions must maintain:</p> <ul> <li>Deterministic behavior (same input = same output, bit-level reproducible)</li> <li>Fail-safe operation (explicit errors, no silent failures)</li> <li>Auditability (white-box code, clear logic, comprehensive tests)</li> <li>Minimal dependencies (no unnecessary external packages)</li> </ul> <p>Any changes that affect these properties must be clearly documented and justified in the PR.</p>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Before reporting a bug:</p> <ol> <li>Check existing issues to avoid duplicates</li> <li>Run the latest version to confirm the bug persists</li> <li>Include reproduction steps, expected behavior, and actual behavior</li> </ol> <p>Security issues should not be reported in public issues. Email security concerns directly to bartzbeielstein.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to spotforecast2-safe, you agree that your contributions are licensed under AGPL-3.0-or-later. Include the SPDX header in all new files you create.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Check existing issues and discussions on GitHub</li> <li>Read the Model/Method Card for system design details</li> <li>Review the Safety Documentation</li> </ul> <p>Thank you for contributing to spotforecast2-safe!</p>"},{"location":"security/","title":"Security Policy &amp; Vulnerability Reporting","text":"<p>This page documents the security measures and vulnerability reporting process for spotforecast2-safe.</p>"},{"location":"security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>If you discover a security vulnerability, DO NOT create a public GitHub issue. This puts all users at risk.</p>"},{"location":"security/#primary-private-security-advisory","title":"Primary: Private Security Advisory","text":"<p>Use GitHub's Private Security Advisory feature:</p> <ol> <li>Go to: https://github.com/sequential-parameter-optimization/spotforecast2-safe/security/advisories</li> <li>Click \"Report a vulnerability\"</li> <li>Provide details about the vulnerability</li> </ol>"},{"location":"security/#alternative-email-reporting","title":"Alternative: Email Reporting","text":"<p>Email your findings to:</p> <pre><code>bartzbeielstein@users.noreply.github.com\n</code></pre> <p>Subject line: <code>[SECURITY] spotforecast2-safe Vulnerability Report</code></p>"},{"location":"security/#what-to-include","title":"What to Include","text":"<p>Provide as much detail as possible:</p> <ul> <li>Description of the vulnerability</li> <li>Affected version(s)</li> <li>Steps to reproduce (if applicable)</li> <li>Potential impact and severity</li> <li>Suggested fix (if available)</li> </ul>"},{"location":"security/#response-timeline","title":"Response Timeline","text":"<p>We aim to respond to all vulnerability reports promptly:</p> <ul> <li>Acknowledgment: Within 24 hours</li> <li>Initial assessment: Within 3 business days</li> <li>Fix and patch: Varies based on severity (critical issues prioritized)</li> <li>Public disclosure: Coordinated after patch is available</li> </ul>"},{"location":"security/#security-advisories","title":"Security Advisories","text":"<p>Published security advisories are available at:</p> <p>https://github.com/sequential-parameter-optimization/spotforecast2-safe/security/advisories</p>"},{"location":"security/#supported-versions","title":"Supported Versions","text":"<p>The following versions receive security updates:</p> Version Status End of Support 0.3.x Supported (Current) October 2027 0.2.x Limited Support February 2026 &lt; 0.2.0 Unsupported N/A <p>We recommend using the latest version. Check PyPI for the current release.</p>"},{"location":"security/#security-features-design-goals","title":"Security Features &amp; Design Goals","text":"<p>spotforecast2-safe is designed with security in mind:</p> <p>Zero Dead Code - No GUI components, plotting libraries, or AutoML frameworks - Minimal external dependencies (see below) - Reduced attack surface for supply chain security</p> <p>Deterministic Operations - All transformations are bit-level reproducible - Predictable behavior enables auditing - No hidden randomness or stochastic operations</p> <p>Fail-Safe Processing - All transformations validate input data - Invalid data raises explicit errors - No silent failures or data imputation - NaNs and Infs are rejected immediately</p> <p>Minimal Dependencies</p> <p>Core dependencies are carefully selected to minimize the CVE surface:</p> <ul> <li>astral - Solar position calculations</li> <li>feature-engine - Feature preprocessing</li> <li>flake8 - Code linting</li> <li>holidays - Holiday calendars</li> <li>lightgbm - Gradient boosting (optional)</li> <li>numba - JIT compilation</li> <li>pandas - Data handling</li> <li>pyarrow - Parquet/Arrow support</li> <li>requests - HTTP client</li> <li>scikit-learn - ML utilities</li> <li>tqdm - Progress bars</li> </ul> <p>Supply Chain Security Measures</p> <ul> <li>\u2705 Dependencies pinned with compatible release specifiers</li> <li>\u2705 Dependabot enabled for automated dependency updates</li> <li>\u2705 GitHub Actions pinned to specific commit hashes</li> <li>\u2705 REUSE compliance for license tracking of all code</li> <li>\u2705 Regular security scanning via bandit and Safety</li> <li>\u2705 CPE identifiers for vulnerability tracking</li> </ul>"},{"location":"security/#security-best-practices-for-users","title":"Security Best Practices for Users","text":""},{"location":"security/#using-current-version","title":"Using Current Version","text":"<ol> <li>Always use the latest available version from PyPI</li> <li>Review CHANGELOG.md for security patches (in repository)</li> <li>Monitor GitHub Releases for updates</li> <li>Subscribe to security advisories at the GitHub link above</li> </ol>"},{"location":"security/#production-deployment","title":"Production Deployment","text":"<ol> <li>Pin exact versions in <code>requirements.txt</code> or <code>pyproject.toml</code></li> <li>Use virtual environments (venv or conda)</li> <li>Keep dependencies updated</li> <li>Review MODEL_CARD.md for safety-critical considerations</li> <li>Enable automatic dependency updates (Dependabot)</li> </ol>"},{"location":"security/#development","title":"Development","text":"<ol> <li>Clone only from the official GitHub repository</li> <li>Verify GPG signatures on releases (recommended)</li> <li>Use <code>pre-commit</code> hooks for code quality</li> <li>Run security checks locally (bandit, Safety)</li> <li>Follow contribution guidelines in CONTRIBUTING.md</li> </ol>"},{"location":"security/#dependency-management","title":"Dependency Management","text":""},{"location":"security/#monitoring-dependencies","title":"Monitoring Dependencies","text":"<p>To check for known vulnerabilities in your environment:</p> <pre><code># Install Safety\npip install safety\n\n# Check installed packages\nsafety check\n</code></pre> <p>This project uses: - Dependabot: Automated checks for outdated and vulnerable dependencies - bandit: Code security analysis - Safety: Dependency vulnerability scanning</p>"},{"location":"security/#pinned-versions","title":"Pinned Versions","text":"<p>All dependencies use compatible release specifiers to allow patch updates while preventing breaking changes:</p> <pre><code>dependency&gt;=1.2.3,&lt;2.0\n</code></pre> <p>This approach ensures: - Security patches are automatically available - Breaking changes are avoided - Supply chain integrity is maintained</p>"},{"location":"security/#cicd-security","title":"CI/CD Security","text":"<p>All commits to main and develop branches undergo automated security checks:</p> <ol> <li>REUSE Compliance: License header verification for all files</li> <li>Code Quality: Black, isort, ruff, mypy formatting and linting</li> <li>Security Scanning: bandit for code vulnerabilities, Safety for dependencies</li> <li>Test Coverage: pytest with minimum coverage thresholds</li> <li>Dependency Analysis: Dependabot for outdated packages</li> <li>Build Artifacts: Verified and scanned before deployment</li> </ol>"},{"location":"security/#compliance-standards","title":"Compliance Standards","text":"<p>spotforecast2-safe follows these compliance standards:</p> <ul> <li>REUSE Compliant: All files have SPDX license identifiers</li> <li>SPDX: Standards-based license tracking</li> <li>EU AI Act Ready: Support for compliance via MODEL_CARD.md</li> <li>OpenSSF Scorecard: Monitored for security practices</li> <li>Python 3.13+: Requires latest Python version for security patches</li> </ul>"},{"location":"security/#contact","title":"Contact","text":"<p>For security inquiries:</p> <ul> <li>Vulnerability Reports: Use Private Security Advisory</li> <li>Alternative Email: bartzbeielstein@users.noreply.github.com</li> <li>General Inquiries: https://github.com/sequential-parameter-optimization</li> <li>Security Advisories: https://github.com/sequential-parameter-optimization/spotforecast2-safe/security/advisories</li> </ul>"},{"location":"security/#see-also","title":"See Also","text":"<ul> <li>Model/Method Card - System design and compliance documentation</li> <li>Contributing Guide - How to contribute safely</li> <li>CHANGELOG.md - Release notes and security patches (in repository)</li> <li>GitHub Security Advisories - Published advisories</li> </ul> <p>Last Updated: February 2026</p>"},{"location":"tasks/","title":"Task Scripts","text":"<p><code>spotforecast2-safe</code> provides command-line task scripts for safety-critical forecasting workflows. These scripts are registered as console entry points and can be invoked directly via <code>uv run</code> or after package installation.</p>"},{"location":"tasks/#available-commands","title":"Available Commands","text":"Command Description <code>spotforecast-safe-demo</code> Demo task comparing baseline, covariate, and custom LightGBM forecasts <code>spotforecast-safe-n2o1-cov-df</code> N-to-1 forecasting with exogenous covariates and DataFrame input"},{"location":"tasks/#demo-task","title":"Demo Task","text":"<p>The <code>spotforecast-safe-demo</code> command runs a comprehensive comparison of three forecasting approaches:</p> <ol> <li>Baseline: Standard N-to-1 recursive forecaster</li> <li>Covariate-enhanced: Includes weather, holidays, and cyclical features</li> <li>Custom LightGBM: Optimized hyperparameters with safety-critical configuration</li> </ol>"},{"location":"tasks/#usage","title":"Usage","text":"<pre><code># Run with default settings (force training)\nuv run spotforecast-safe-demo\n\n# Skip training (use cached models if available)\nuv run spotforecast-safe-demo --force_train false\n\n# Specify custom data path\nuv run spotforecast-safe-demo --data_path /path/to/data.csv\n\n# Enable logging\nuv run spotforecast-safe-demo --logging true\n</code></pre>"},{"location":"tasks/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General failure 2 Data loading error 3 Model training error <p>Safety-Critical Consideration</p> <p>The demo task logs all execution steps and errors. In production, always enable logging with <code>--logging true</code> for auditability.</p>"},{"location":"tasks/#n-to-1-with-covariates-and-dataframe","title":"N-to-1 with Covariates and DataFrame","text":"<p>The <code>spotforecast-safe-n2o1-cov-df</code> command executes the full N-to-1 forecasting pipeline with exogenous covariates and flexible data input.</p>"},{"location":"tasks/#features","title":"Features","text":"<ul> <li>Automatic feature engineering: Weather, holidays, cyclical time features</li> <li>Weighted aggregation: Combine multiple forecasts with configurable weights</li> <li>DataFrame input: Pass custom data or use default data fetcher</li> <li>Comprehensive logging: Safety-critical execution tracing</li> </ul>"},{"location":"tasks/#usage_1","title":"Usage","text":"<pre><code># Run with default settings\nuv run spotforecast-safe-n2o1-cov-df\n\n# Custom forecast horizon\nuv run spotforecast-safe-n2o1-cov-df --forecast_horizon 48\n\n# Enable verbose output\nuv run spotforecast-safe-n2o1-cov-df --verbose true\n\n# Enable logging to file\nuv run spotforecast-safe-n2o1-cov-df --logging true --log_dir ~/logs\n</code></pre>"},{"location":"tasks/#parameters","title":"Parameters","text":"Parameter Default Description <code>--forecast_horizon</code> 24 Number of steps ahead to forecast <code>--contamination</code> 0.01 Outlier detection threshold <code>--window_size</code> 72 Rolling window size for features <code>--lags</code> 24 Number of lag features <code>--train_ratio</code> 0.8 Train/validation split ratio <code>--verbose</code> False Enable detailed output <code>--logging</code> False Enable file logging"},{"location":"tasks/#configuration","title":"Configuration","text":"<p>All tasks use sensible defaults but can be customized via:</p> <ul> <li>Command-line arguments (use <code>--help</code> for details)</li> <li>Environment variables for API keys and paths</li> <li>Configuration files stored in <code>~/spotforecast2_data/</code></li> </ul> <pre><code># View available options for any command\nuv run spotforecast-safe-demo --help\nuv run spotforecast-safe-n2o1-cov-df --help\n</code></pre>"},{"location":"tasks/#model-persistence","title":"Model Persistence","text":"<p>Trained models are saved to <code>~/spotforecast2_models/&lt;task_name&gt;/</code> by default. This allows:</p> <ul> <li>Incremental retraining: Only retrain when models are stale</li> <li>Reproducibility: Models are versioned by task and timestamp</li> <li>Auditability: Full training logs are stored alongside models</li> </ul> <p>Best Practice</p> <p>For production deployments, always verify model checksums and training timestamps before using cached models.</p>"},{"location":"tasks/#logging","title":"Logging","text":"<p>Safety-critical tasks support comprehensive logging:</p> <pre><code># Enable logging to default directory\nuv run spotforecast-safe-demo --logging true\n\n# Specify custom log directory\nuv run spotforecast-safe-n2o1-cov-df --logging true --log_dir /var/log/spotforecast\n</code></pre> <p>Log files include:</p> <ul> <li>Execution timestamps</li> <li>Parameter configurations</li> <li>Model training metrics</li> <li>Error tracebacks (if any)</li> </ul>"},{"location":"api/data/","title":"Data Module","text":"<p>This module provides utilities for fetching and loading time series data.</p>"},{"location":"api/data/#spotforecast2_safe.data","title":"<code>spotforecast2_safe.data</code>","text":""},{"location":"api/data/#spotforecast2_safe.data.Data","title":"<code>Data</code>  <code>dataclass</code>","text":"<p>Container for input time series data.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>pandas DataFrame containing the input time series data.</p> Source code in <code>src/spotforecast2_safe/data/data.py</code> <pre><code>@dataclass\nclass Data:\n    \"\"\"Container for input time series data.\n\n    Attributes:\n        data: pandas DataFrame containing the input time series data.\n    \"\"\"\n\n    data: pd.DataFrame\n\n    @classmethod\n    def from_csv(\n        cls,\n        csv_path: Path,\n        timezone: Optional[str],\n        columns: Optional[List[str]] = None,\n        parse_dates=True,\n        index_col=0,\n        **kwargs,\n    ) -&gt; \"Data\":\n        \"\"\"Load data from a CSV file.\n\n        The CSV must contain a datetime column that becomes the DataFrame index.\n        The index is localized to the provided timezone if it is naive, and then\n        converted to UTC.\n\n        Args:\n            csv_path (Path): Path to the CSV file.\n            timezone (Optional[str]): Timezone to assign if the index has no\n                timezone. Must be provided if the index is naive.\n            columns (Optional[List[str]]): List of column names to include. If\n                provided, only these columns will be loaded from the CSV\n                (optimizes reading speed). If None, all columns are loaded.\n            parse_dates (bool or list, optional): Passed to ``pd.read_csv``.\n                Defaults to True.\n            index_col (int or str, optional): Column to use as index. Defaults to 0.\n            **kwargs (Any): Additional keyword arguments forwarded to ``pd.read_csv``.\n\n        Returns:\n            Data: Instance containing the loaded DataFrame.\n\n        Raises:\n            ValueError: If the CSV does not yield a DatetimeIndex.\n            ValueError: If the index is timezone-naive and no timezone is provided.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.data import Data\n            &gt;&gt;&gt; data = Data.from_csv(\n            ...     Path(\"data.csv\"),\n            ...     timezone=\"UTC\",\n            ...     columns=[\"target_col\"]\n            ... )\n        \"\"\"\n        # If columns specified, add index column to usecols for efficient reading\n        usecols = None\n        if columns is not None:\n            # Get the index column name/number\n            if isinstance(index_col, int):\n                # Read header first to get column names\n                header_df = pd.read_csv(csv_path, nrows=0)\n                index_col_name = header_df.columns[index_col]\n            else:\n                index_col_name = index_col\n            usecols = [index_col_name] + columns\n\n        df = pd.read_csv(\n            csv_path,\n            parse_dates=parse_dates,\n            index_col=index_col,\n            usecols=usecols,\n            **kwargs,\n        )\n        df = convert_to_utc(df, timezone)\n        if df.index.freq is None:\n            try:\n                df.index.freq = pd.infer_freq(df.index)\n            except (ValueError, TypeError) as e:\n                logger.debug(\n                    \"Could not infer frequency for index in from_csv: %s. \"\n                    \"Frequency will remain None.\",\n                    e,\n                )\n        return cls(data=df)\n\n    @classmethod\n    def from_dataframe(\n        cls,\n        df: pd.DataFrame,\n        timezone: Optional[str],\n        columns: Optional[List[str]] = None,\n    ) -&gt; \"Data\":\n        \"\"\"Create a new Data instance from an existing DataFrame.\n\n        The DataFrame must have a datetime index. The index is localized to the\n        provided timezone if it is naive, and then converted to UTC.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing data.\n            timezone (Optional[str]): Timezone to assign if the index is naive.\n                Must be provided if the index has no timezone.\n            columns (Optional[List[str]]): List of column names to include.\n                If provided, only these columns will be selected from the\n                DataFrame. If None, all columns are used.\n\n        Returns:\n            Data: Instance containing the provided DataFrame.\n\n        Raises:\n            ValueError: If the DataFrame index is not a DatetimeIndex.\n            ValueError: If the index is timezone-naive and no timezone is provided.\n        \"\"\"\n        df = convert_to_utc(df, timezone)\n\n        # Select columns if specified\n        if columns is not None:\n            df = df[columns].copy()\n\n        if df.index.freq is None:\n            try:\n                df.index.freq = pd.infer_freq(df.index)\n            except (ValueError, TypeError) as e:\n                logger.debug(\n                    \"Could not infer frequency for index in from_dataframe: %s. \"\n                    \"Frequency will remain None.\",\n                    e,\n                )\n\n        return cls(data=df)\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.Data.from_csv","title":"<code>from_csv(csv_path, timezone, columns=None, parse_dates=True, index_col=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load data from a CSV file.</p> <p>The CSV must contain a datetime column that becomes the DataFrame index. The index is localized to the provided timezone if it is naive, and then converted to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>Path</code> <p>Path to the CSV file.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Timezone to assign if the index has no timezone. Must be provided if the index is naive.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to include. If provided, only these columns will be loaded from the CSV (optimizes reading speed). If None, all columns are loaded.</p> <code>None</code> <code>parse_dates</code> <code>bool or list</code> <p>Passed to <code>pd.read_csv</code>. Defaults to True.</p> <code>True</code> <code>index_col</code> <code>int or str</code> <p>Column to use as index. Defaults to 0.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to <code>pd.read_csv</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>Instance containing the loaded DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the CSV does not yield a DatetimeIndex.</p> <code>ValueError</code> <p>If the index is timezone-naive and no timezone is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data import Data\n&gt;&gt;&gt; data = Data.from_csv(\n...     Path(\"data.csv\"),\n...     timezone=\"UTC\",\n...     columns=[\"target_col\"]\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/data/data.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    csv_path: Path,\n    timezone: Optional[str],\n    columns: Optional[List[str]] = None,\n    parse_dates=True,\n    index_col=0,\n    **kwargs,\n) -&gt; \"Data\":\n    \"\"\"Load data from a CSV file.\n\n    The CSV must contain a datetime column that becomes the DataFrame index.\n    The index is localized to the provided timezone if it is naive, and then\n    converted to UTC.\n\n    Args:\n        csv_path (Path): Path to the CSV file.\n        timezone (Optional[str]): Timezone to assign if the index has no\n            timezone. Must be provided if the index is naive.\n        columns (Optional[List[str]]): List of column names to include. If\n            provided, only these columns will be loaded from the CSV\n            (optimizes reading speed). If None, all columns are loaded.\n        parse_dates (bool or list, optional): Passed to ``pd.read_csv``.\n            Defaults to True.\n        index_col (int or str, optional): Column to use as index. Defaults to 0.\n        **kwargs (Any): Additional keyword arguments forwarded to ``pd.read_csv``.\n\n    Returns:\n        Data: Instance containing the loaded DataFrame.\n\n    Raises:\n        ValueError: If the CSV does not yield a DatetimeIndex.\n        ValueError: If the index is timezone-naive and no timezone is provided.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data import Data\n        &gt;&gt;&gt; data = Data.from_csv(\n        ...     Path(\"data.csv\"),\n        ...     timezone=\"UTC\",\n        ...     columns=[\"target_col\"]\n        ... )\n    \"\"\"\n    # If columns specified, add index column to usecols for efficient reading\n    usecols = None\n    if columns is not None:\n        # Get the index column name/number\n        if isinstance(index_col, int):\n            # Read header first to get column names\n            header_df = pd.read_csv(csv_path, nrows=0)\n            index_col_name = header_df.columns[index_col]\n        else:\n            index_col_name = index_col\n        usecols = [index_col_name] + columns\n\n    df = pd.read_csv(\n        csv_path,\n        parse_dates=parse_dates,\n        index_col=index_col,\n        usecols=usecols,\n        **kwargs,\n    )\n    df = convert_to_utc(df, timezone)\n    if df.index.freq is None:\n        try:\n            df.index.freq = pd.infer_freq(df.index)\n        except (ValueError, TypeError) as e:\n            logger.debug(\n                \"Could not infer frequency for index in from_csv: %s. \"\n                \"Frequency will remain None.\",\n                e,\n            )\n    return cls(data=df)\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.Data.from_dataframe","title":"<code>from_dataframe(df, timezone, columns=None)</code>  <code>classmethod</code>","text":"<p>Create a new Data instance from an existing DataFrame.</p> <p>The DataFrame must have a datetime index. The index is localized to the provided timezone if it is naive, and then converted to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing data.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Timezone to assign if the index is naive. Must be provided if the index has no timezone.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to include. If provided, only these columns will be selected from the DataFrame. If None, all columns are used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>Instance containing the provided DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame index is not a DatetimeIndex.</p> <code>ValueError</code> <p>If the index is timezone-naive and no timezone is provided.</p> Source code in <code>src/spotforecast2_safe/data/data.py</code> <pre><code>@classmethod\ndef from_dataframe(\n    cls,\n    df: pd.DataFrame,\n    timezone: Optional[str],\n    columns: Optional[List[str]] = None,\n) -&gt; \"Data\":\n    \"\"\"Create a new Data instance from an existing DataFrame.\n\n    The DataFrame must have a datetime index. The index is localized to the\n    provided timezone if it is naive, and then converted to UTC.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data.\n        timezone (Optional[str]): Timezone to assign if the index is naive.\n            Must be provided if the index has no timezone.\n        columns (Optional[List[str]]): List of column names to include.\n            If provided, only these columns will be selected from the\n            DataFrame. If None, all columns are used.\n\n    Returns:\n        Data: Instance containing the provided DataFrame.\n\n    Raises:\n        ValueError: If the DataFrame index is not a DatetimeIndex.\n        ValueError: If the index is timezone-naive and no timezone is provided.\n    \"\"\"\n    df = convert_to_utc(df, timezone)\n\n    # Select columns if specified\n    if columns is not None:\n        df = df[columns].copy()\n\n    if df.index.freq is None:\n        try:\n            df.index.freq = pd.infer_freq(df.index)\n        except (ValueError, TypeError) as e:\n            logger.debug(\n                \"Could not infer frequency for index in from_dataframe: %s. \"\n                \"Frequency will remain None.\",\n                e,\n            )\n\n    return cls(data=df)\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.Period","title":"<code>Period</code>  <code>dataclass</code>","text":"<p>Class abstraction for the information required to encode a period using RBF.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the period (e.g., 'hour', 'day').</p> <code>n_periods</code> <code>int</code> <p>Number of periods to encode (e.g., 24 for hours).</p> <code>column</code> <code>str</code> <p>Name of the column in the DataFrame containing the period information.</p> <code>input_range</code> <code>Tuple[int, int]</code> <p>Tuple of (min, max) values for the period (e.g., (0, 23) for hours).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data import Period\n&gt;&gt;&gt; period = Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))\n&gt;&gt;&gt; period.name\n'hour'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/data.py</code> <pre><code>@dataclass\nclass Period:\n    \"\"\"Class abstraction for the information required to encode a period using RBF.\n\n    Attributes:\n        name: Name of the period (e.g., 'hour', 'day').\n        n_periods: Number of periods to encode (e.g., 24 for hours).\n        column: Name of the column in the DataFrame containing the period information.\n        input_range: Tuple of (min, max) values for the period (e.g., (0, 23) for hours).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data import Period\n        &gt;&gt;&gt; period = Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))\n        &gt;&gt;&gt; period.name\n        'hour'\n    \"\"\"\n\n    name: str\n    n_periods: int\n    column: str\n    input_range: Tuple[int, int]\n\n    def __post_init__(self):\n        \"\"\"Validate the period configuration.\"\"\"\n        if self.n_periods &lt;= 0:\n            raise ValueError(f\"n_periods must be positive, got {self.n_periods}\")\n        if self.input_range[0] &gt;= self.input_range[1]:\n            raise ValueError(\n                f\"input_range[0] must be less than input_range[1], got {self.input_range}\"\n            )\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.Period.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the period configuration.</p> Source code in <code>src/spotforecast2_safe/data/data.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the period configuration.\"\"\"\n    if self.n_periods &lt;= 0:\n        raise ValueError(f\"n_periods must be positive, got {self.n_periods}\")\n    if self.input_range[0] &gt;= self.input_range[1]:\n        raise ValueError(\n            f\"input_range[0] must be less than input_range[1], got {self.input_range}\"\n        )\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_holiday_data","title":"<code>fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_weather_data","title":"<code>fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.get_cache_home","title":"<code>get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/data/#data-fetching-functions","title":"Data Fetching Functions","text":""},{"location":"api/data/#fetch_data","title":"fetch_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data","title":"<code>spotforecast2_safe.data.fetch_data</code>","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_data","title":"<code>fetch_data(filename=None, dataframe=None, columns=None, index_col=0, parse_dates=True, dayfirst=False, timezone='UTC')</code>","text":"<p>Fetches the integrated raw dataset from a CSV file or processes a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename of the CSV file containing the dataset. Must be located in the data home directory. This is required if dataframe is None.</p> <code>None</code> <code>dataframe</code> <code>DataFrame</code> <p>A pandas DataFrame to process. If provided, it will be processed with proper timezone handling. Mutually exclusive with filename.</p> <code>None</code> <code>columns</code> <code>list</code> <p>List of columns to be included in the dataset. If None, all columns are included. If an empty list is provided, a ValueError is raised.</p> <code>None</code> <code>index_col</code> <code>int</code> <p>Column index to be used as the index (only used when loading from CSV).</p> <code>0</code> <code>parse_dates</code> <code>bool</code> <p>Whether to parse dates in the index column (only used when loading from CSV).</p> <code>True</code> <code>dayfirst</code> <code>bool</code> <p>Whether the day comes first in date parsing (only used when loading from CSV).</p> <code>False</code> <code>timezone</code> <code>str</code> <p>Timezone to set for the datetime index. If a DataFrame with naive index is provided, it will be localized to this timezone then converted to UTC. Default: \"UTC\".</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The integrated raw dataset with UTC timezone.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If columns is an empty list, if both filename and dataframe are provided, or if neither filename nor dataframe is provided.</p> <code>FileNotFoundError</code> <p>If CSV file does not exist.</p> <p>Examples:</p> <p>Load from CSV (default):</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; data = fetch_data(columns=[\"col1\", \"col2\"])\n&gt;&gt;&gt; data.head()\n                Header1  Header2  Header3\n</code></pre> <p>Load from specific CSV:</p> <pre><code>&gt;&gt;&gt; data = fetch_data(filename=\"custom_data.csv\")\n</code></pre> <p>Process a DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]},\n...                   index=pd.date_range(\"2024-01-01\", periods=3, freq=\"h\"))\n&gt;&gt;&gt; data = fetch_data(dataframe=df, timezone=\"Europe/Berlin\")\n&gt;&gt;&gt; data.index.tz\n&lt;UTC&gt;\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_data(\n    filename: Optional[str] = None,\n    dataframe: Optional[pd.DataFrame] = None,\n    columns: Optional[list] = None,\n    index_col: int = 0,\n    parse_dates: bool = True,\n    dayfirst: bool = False,\n    timezone: str = \"UTC\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches the integrated raw dataset from a CSV file or processes a DataFrame.\n\n    Args:\n        filename (str, optional):\n            Filename of the CSV file containing the dataset. Must be located in the\n            data home directory. This is required if dataframe is None.\n        dataframe (pd.DataFrame, optional):\n            A pandas DataFrame to process. If provided, it will be processed with\n            proper timezone handling. Mutually exclusive with filename.\n        columns (list, optional):\n            List of columns to be included in the dataset. If None, all columns are included.\n            If an empty list is provided, a ValueError is raised.\n        index_col (int):\n            Column index to be used as the index (only used when loading from CSV).\n        parse_dates (bool):\n            Whether to parse dates in the index column (only used when loading from CSV).\n        dayfirst (bool):\n            Whether the day comes first in date parsing (only used when loading from CSV).\n        timezone (str):\n            Timezone to set for the datetime index. If a DataFrame with naive index is provided,\n            it will be localized to this timezone then converted to UTC. Default: \"UTC\".\n\n    Returns:\n        pd.DataFrame: The integrated raw dataset with UTC timezone.\n\n    Raises:\n        ValueError: If columns is an empty list, if both filename and dataframe are provided,\n            or if neither filename nor dataframe is provided.\n        FileNotFoundError: If CSV file does not exist.\n\n    Examples:\n        Load from CSV (default):\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; data = fetch_data(columns=[\"col1\", \"col2\"])\n        &gt;&gt;&gt; data.head()\n                        Header1  Header2  Header3\n\n        Load from specific CSV:\n        &gt;&gt;&gt; data = fetch_data(filename=\"custom_data.csv\")\n\n        Process a DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]},\n        ...                   index=pd.date_range(\"2024-01-01\", periods=3, freq=\"h\"))\n        &gt;&gt;&gt; data = fetch_data(dataframe=df, timezone=\"Europe/Berlin\")\n        &gt;&gt;&gt; data.index.tz\n        &lt;UTC&gt;\n    \"\"\"\n    if columns is not None and len(columns) == 0:\n        raise ValueError(\"columns must be specified and cannot be empty.\")\n\n    if filename is not None and dataframe is not None:\n        raise ValueError(\n            \"Cannot specify both filename and dataframe. Please provide only one.\"\n        )\n\n    # Process DataFrame if provided\n    if dataframe is not None:\n        dataset = Data.from_dataframe(\n            df=dataframe,\n            timezone=timezone,\n            columns=columns,\n        )\n    else:\n        # Load from CSV file\n        if filename is None:\n            raise ValueError(\n                \"filename must be specified when dataframe is None. \"\n                \"Explicitly provide a filename (e.g., 'data_in.csv') or a DataFrame.\"\n            )\n        csv_path = get_data_home() / filename\n        if not Path(csv_path).is_file():\n            raise FileNotFoundError(f\"The file {csv_path} does not exist.\")\n\n        dataset = Data.from_csv(\n            csv_path=csv_path,\n            index_col=index_col,\n            parse_dates=parse_dates,\n            dayfirst=dayfirst,\n            timezone=timezone,\n            columns=columns,\n        )\n\n    return dataset.data\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_holiday_data","title":"<code>fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_weather_data","title":"<code>fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_cache_home","title":"<code>get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/data/#fetch_holiday_data","title":"fetch_holiday_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_holiday_data","title":"<code>spotforecast2_safe.data.fetch_data.fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#fetch_weather_data","title":"fetch_weather_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_weather_data","title":"<code>spotforecast2_safe.data.fetch_data.fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#data-utilities","title":"Data Utilities","text":""},{"location":"api/data/#get_cache_home","title":"get_cache_home","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_cache_home","title":"<code>spotforecast2_safe.data.fetch_data.get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#get_data_home","title":"get_data_home","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_data_home","title":"<code>spotforecast2_safe.data.fetch_data.get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions Module","text":"<p>Custom exceptions and error handling.</p>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions","title":"<code>spotforecast2_safe.exceptions</code>","text":"<p>Custom exceptions and warnings for spotforecast2.</p> <p>This module contains all the custom warnings and error classes used across spotforecast2.</p> <p>Examples:</p> <p>Using custom warnings::</p> <pre><code>import warnings\nfrom spotforecast2_safe.exceptions import MissingValuesWarning\n\n# Raise a warning\nwarnings.warn(\n    \"Missing values detected in input data.\",\n    MissingValuesWarning\n)\n\n# Suppress a specific warning\nwarnings.simplefilter('ignore', category=MissingValuesWarning)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.DataTransformationWarning","title":"<code>DataTransformationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for output data in transformed space.</p> <p>Used to notify that the output data is in the transformed space.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Output is in transformed space.\",\n...     DataTransformationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTransformationWarning(UserWarning):\n    \"\"\"Warning for output data in transformed space.\n\n    Used to notify that the output data is in the transformed space.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Output is in transformed space.\",\n        ...     DataTransformationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTransformationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.ExogenousInterpretationWarning","title":"<code>ExogenousInterpretationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning about implications when using exogenous variables.</p> <p>Used to notify about important implications when using exogenous variables with models that use a two-step approach (e.g., regression + ARAR).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous variables may not be used as expected.\",\n...     ExogenousInterpretationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class ExogenousInterpretationWarning(UserWarning):\n    \"\"\"Warning about implications when using exogenous variables.\n\n    Used to notify about important implications when using exogenous\n    variables with models that use a two-step approach (e.g., regression + ARAR).\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous variables may not be used as expected.\",\n        ...     ExogenousInterpretationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ExogenousInterpretationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.FeatureOutOfRangeWarning","title":"<code>FeatureOutOfRangeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for features out of training range.</p> <p>Used to notify that a feature is out of the range seen during training.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Feature value exceeds training range.\",\n...     FeatureOutOfRangeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class FeatureOutOfRangeWarning(UserWarning):\n    \"\"\"Warning for features out of training range.\n\n    Used to notify that a feature is out of the range seen during training.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Feature value exceeds training range.\",\n        ...     FeatureOutOfRangeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=FeatureOutOfRangeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for ignored arguments.</p> <p>Used to notify that an argument is ignored when using a method or a function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Argument 'x' is ignored in this context.\",\n...     IgnoredArgumentWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class IgnoredArgumentWarning(UserWarning):\n    \"\"\"Warning for ignored arguments.\n\n    Used to notify that an argument is ignored when using a method\n    or a function.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Argument 'x' is ignored in this context.\",\n        ...     IgnoredArgumentWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.InputTypeWarning","title":"<code>InputTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for inefficient input format.</p> <p>Used to notify that input format is not the most efficient or recommended for the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Input format is not optimal for this forecaster.\",\n...     InputTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class InputTypeWarning(UserWarning):\n    \"\"\"Warning for inefficient input format.\n\n    Used to notify that input format is not the most efficient or\n    recommended for the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Input format is not optimal for this forecaster.\",\n        ...     InputTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=InputTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for potentially long training processes.</p> <p>Used to notify that a large number of models will be trained and the the process may take a while to run.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Training may take a long time.\",\n...     LongTrainingWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class LongTrainingWarning(UserWarning):\n    \"\"\"Warning for potentially long training processes.\n\n    Used to notify that a large number of models will be trained and the\n    the process may take a while to run.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Training may take a long time.\",\n        ...     LongTrainingWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=LongTrainingWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.MissingExogWarning","title":"<code>MissingExogWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing exogenous variables.</p> <p>Used to indicate that there are missing exogenous variables in the data. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing exogenous variables detected.\",\n...     MissingExogWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingExogWarning(UserWarning):\n    \"\"\"Warning for missing exogenous variables.\n\n    Used to indicate that there are missing exogenous variables in the\n    data. Most machine learning models do not accept missing values, so the\n    Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing exogenous variables detected.\",\n        ...     MissingExogWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingExogWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; from spotforecast2_safe.exceptions import MissingValuesWarning\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Args:\n        message (str): The message to display.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; from spotforecast2_safe.exceptions import MissingValuesWarning\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        self.message = message\n\n    def __str__(self) -&gt; str:\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.NotFittedError","title":"<code>NotFittedError</code>","text":"<p>               Bases: <code>ValueError</code>, <code>AttributeError</code></p> <p>Exception class to raise if estimator is used before fitting.</p> <p>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.exceptions import NotFittedError\n&gt;&gt;&gt; try:\n...     raise NotFittedError(\"Forecaster not fitted\")\n... except NotFittedError as e:\n...     print(e)\nForecaster not fitted\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class NotFittedError(ValueError, AttributeError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    This class inherits from both ValueError and AttributeError to help with\n    exception handling and backward compatibility.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.exceptions import NotFittedError\n        &gt;&gt;&gt; try:\n        ...     raise NotFittedError(\"Forecaster not fitted\")\n        ... except NotFittedError as e:\n        ...     print(e)\n        Forecaster not fitted\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.OneStepAheadValidationWarning","title":"<code>OneStepAheadValidationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for one-step-ahead validation usage.</p> <p>Used to notify that the one-step-ahead validation is being used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Using one-step-ahead validation.\",\n...     OneStepAheadValidationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class OneStepAheadValidationWarning(UserWarning):\n    \"\"\"Warning for one-step-ahead validation usage.\n\n    Used to notify that the one-step-ahead validation is being used.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Using one-step-ahead validation.\",\n        ...     OneStepAheadValidationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.ResidualsUsageWarning","title":"<code>ResidualsUsageWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incorrect residuals usage.</p> <p>Used to notify that a residual are not correctly used in the probabilistic forecasting process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Residuals are not properly used.\",\n...     ResidualsUsageWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class ResidualsUsageWarning(UserWarning):\n    \"\"\"Warning for incorrect residuals usage.\n\n    Used to notify that a residual are not correctly used in the\n    probabilistic forecasting process.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Residuals are not properly used.\",\n        ...     ResidualsUsageWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ResidualsUsageWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.SaveLoadSkforecastWarning","title":"<code>SaveLoadSkforecastWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for save/load operations.</p> <p>Used to notify any issues that may arise when saving or loading a forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Issues detected when saving forecaster.\",\n...     SaveLoadSkforecastWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class SaveLoadSkforecastWarning(UserWarning):\n    \"\"\"Warning for save/load operations.\n\n    Used to notify any issues that may arise when saving or loading\n    a forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Issues detected when saving forecaster.\",\n        ...     SaveLoadSkforecastWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SaveLoadSkforecastWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.SpotforecastVersionWarning","title":"<code>SpotforecastVersionWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for version mismatch.</p> <p>Used to notify that the version installed in the environment differs from the version used to initialize the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Version mismatch detected.\",\n...     SpotforecastVersionWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class SpotforecastVersionWarning(UserWarning):\n    \"\"\"Warning for version mismatch.\n\n    Used to notify that the version installed in the\n    environment differs from the version used to initialize the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Version mismatch detected.\",\n        ...     SpotforecastVersionWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SpotforecastVersionWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.UnknownLevelWarning","title":"<code>UnknownLevelWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for unknown levels in prediction.</p> <p>Used to notify that a level being predicted was not part of the training data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Predicting for an unknown level.\",\n...     UnknownLevelWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class UnknownLevelWarning(UserWarning):\n    \"\"\"Warning for unknown levels in prediction.\n\n    Used to notify that a level being predicted was not part of the\n    training data.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Predicting for an unknown level.\",\n        ...     UnknownLevelWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=UnknownLevelWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.format_warning_handler","title":"<code>format_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom warning handler to format warnings in a box.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is used internally by the warnings module\n&gt;&gt;&gt; set_warnings_style('skforecast')\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def format_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom warning handler to format warnings in a box.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; # This is used internally by the warnings module\n        &gt;&gt;&gt; set_warnings_style('skforecast')  # doctest: +SKIP\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        width = 88\n        title = type(message).__name__\n        output_text = [\"\\\\n\"]\n\n        wrapped_message = textwrap.fill(\n            str(message), width=width - 2, expand_tabs=True, replace_whitespace=True\n        )\n        title_top_border = f\"\u256d{'\u2500' * ((width - len(title) - 2) // 2)} {title} {'\u2500' * ((width - len(title) - 2) // 2)}\u256e\"\n        if len(title) % 2 != 0:\n            title_top_border = title_top_border[:-1] + \"\u2500\" + \"\u256e\"\n        bottom_border = f\"\u2570{'\u2500' * width}\u256f\"\n        output_text.append(title_top_border)\n\n        for line in wrapped_message.split(\"\\\\n\"):\n            output_text.append(f\"\u2502 {line.ljust(width - 2)} \u2502\")\n\n        output_text.append(bottom_border)\n        output_text = \"\\\\n\".join(output_text)\n        color = \"\\\\033[38;5;208m\"\n        reset = \"\\\\033[0m\"\n        output_text = f\"{color}{output_text}{reset}\"\n        print(output_text)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.rich_warning_handler","title":"<code>rich_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom handler for warnings that uses rich to display formatted panels.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is used internally when rich is available\n&gt;&gt;&gt; set_warnings_style('skforecast')\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def rich_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom handler for warnings that uses rich to display formatted panels.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; # This is used internally when rich is available\n        &gt;&gt;&gt; set_warnings_style('skforecast')  # doctest: +SKIP\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        if not HAS_RICH:\n            # Fallback to format_warning_handler if rich is not available\n            format_warning_handler(message, category, filename, lineno, file, line)\n            return\n\n        console = Console()\n\n        category_name = category.__name__\n        text = (\n            f\"{message.message}\\\\n\\\\n\"\n            f\"Category : spotforecast2.exceptions.{category_name}\\\\n\"\n            f\"Location : {filename}:{lineno}\\\\n\"\n            f\"Suppress : warnings.simplefilter('ignore', category={category_name})\"\n        )\n\n        panel = Panel(\n            Text(text, justify=\"left\"),\n            title=category_name,\n            title_align=\"center\",\n            border_style=\"color(214)\",\n            width=88,\n        )\n\n        console.print(panel)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.runtime_deprecated","title":"<code>runtime_deprecated(replacement=None, version=None, removal=None, category=FutureWarning)</code>","text":"<p>Decorator to mark functions or classes as deprecated.</p> <p>Works for both function and class targets, and ensures warnings are visible even inside Jupyter notebooks.</p> <p>Parameters:</p> Name Type Description Default <code>replacement</code> <code>str</code> <p>Name of the replacement function/class to use instead.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version in which the function/class was deprecated.</p> <code>None</code> <code>removal</code> <code>str</code> <p>Version in which the function/class will be removed.</p> <code>None</code> <code>category</code> <code>type[Warning]</code> <p>Warning category to use. Default is FutureWarning.</p> <code>FutureWarning</code> <p>Returns:</p> Type Description <code>object</code> <p>Decorator function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @runtime_deprecated(replacement='new_function', version='0.5', removal='1.0')\n... def old_function():\n...     pass\n&gt;&gt;&gt; old_function()\nFutureWarning: old_function() is deprecated since version 0.5; use new_function instead...\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def runtime_deprecated(\n    replacement: str = None,\n    version: str = None,\n    removal: str = None,\n    category: type[Warning] = FutureWarning,\n) -&gt; object:\n    \"\"\"Decorator to mark functions or classes as deprecated.\n\n    Works for both function and class targets, and ensures warnings are visible\n    even inside Jupyter notebooks.\n\n    Args:\n        replacement: Name of the replacement function/class to use instead.\n        version: Version in which the function/class was deprecated.\n        removal: Version in which the function/class will be removed.\n        category: Warning category to use. Default is FutureWarning.\n\n    Returns:\n        Decorator function.\n\n    Examples:\n        &gt;&gt;&gt; @runtime_deprecated(replacement='new_function', version='0.5', removal='1.0')\n        ... def old_function():\n        ...     pass\n        &gt;&gt;&gt; old_function()  # doctest: +SKIP\n        FutureWarning: old_function() is deprecated since version 0.5; use new_function instead...\n    \"\"\"\n\n    def decorator(obj):\n        is_function = inspect.isfunction(obj) or inspect.ismethod(obj)\n        is_class = inspect.isclass(obj)\n\n        if not (is_function or is_class):\n            raise TypeError(\n                \"@runtime_deprecated can only be used on functions or classes\"\n            )\n\n        # ----- Build warning message -----\n        name = obj.__name__\n        message = (\n            f\"{name}() is deprecated\" if is_function else f\"{name} class is deprecated\"\n        )\n        if version:\n            message += f\" since version {version}\"\n        if replacement:\n            message += f\"; use {replacement} instead\"\n        if removal:\n            message += f\". It will be removed in version {removal}.\"\n        else:\n            message += \".\"\n\n        def issue_warning():\n            \"\"\"Emit warning in a way that always shows in notebooks.\"\"\"\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"always\", category)\n                warnings.warn(message, category, stacklevel=3)\n\n        # ----- Case 1: decorating a function -----\n        if is_function:\n\n            @wraps(obj)\n            def wrapper(*args, **kwargs):\n                issue_warning()\n                return obj(*args, **kwargs)\n\n            # Add metadata\n            wrapper.__deprecated__ = True\n            wrapper.__replacement__ = replacement\n            wrapper.__version__ = version\n            wrapper.__removal__ = removal\n            return wrapper\n\n        # ----- Case 2: decorating a class -----\n        else:  # is_class must be True due to earlier check\n            orig_init = getattr(obj, \"__init__\", None)\n            orig_new = getattr(obj, \"__new__\", None)\n\n            # Only wrap whichever exists (some classes use __new__, others __init__)\n            if orig_new and (orig_new is not object.__new__):\n\n                @wraps(orig_new)\n                def wrapped_new(cls, *args, **kwargs):\n                    issue_warning()\n                    return orig_new(cls, *args, **kwargs)\n\n                obj.__new__ = staticmethod(wrapped_new)\n\n            elif orig_init:\n\n                @wraps(orig_init)\n                def wrapped_init(self, *args, **kwargs):\n                    issue_warning()\n                    return orig_init(self, *args, **kwargs)\n\n                obj.__init__ = wrapped_init\n\n            # Add metadata\n            obj.__deprecated__ = True\n            obj.__replacement__ = replacement\n            obj.__version__ = version\n            obj.__removal__ = removal\n\n            return obj\n\n    return decorator\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.set_warnings_style","title":"<code>set_warnings_style(style='skforecast')</code>","text":"<p>Set the warning handler based on the provided style.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>The style of the warning handler. Either 'skforecast' or 'default'.</p> <code>'skforecast'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_warnings_style('skforecast')\n&gt;&gt;&gt; # Now warnings will be displayed with formatting\n&gt;&gt;&gt; set_warnings_style('default')\n&gt;&gt;&gt; # Back to default Python warning format\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_warnings_style(style: str = \"skforecast\") -&gt; None:\n    \"\"\"Set the warning handler based on the provided style.\n\n    Args:\n        style: The style of the warning handler. Either 'skforecast' or 'default'.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; set_warnings_style('skforecast')\n        &gt;&gt;&gt; # Now warnings will be displayed with formatting\n        &gt;&gt;&gt; set_warnings_style('default')\n        &gt;&gt;&gt; # Back to default Python warning format\n    \"\"\"\n    if style == \"skforecast\":\n        if not hasattr(warnings, \"_original_showwarning\"):\n            warnings._original_showwarning = warnings.showwarning\n        if HAS_RICH:\n            warnings.showwarning = rich_warning_handler\n        else:\n            warnings.showwarning = format_warning_handler\n    else:\n        if hasattr(warnings, \"_original_showwarning\"):\n            warnings.showwarning = warnings._original_showwarning\n</code></pre>"},{"location":"api/forecaster/","title":"Forecaster Module","text":"<p>Core forecasting classes and utilities.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster","title":"<code>spotforecast2_safe.forecaster</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase","title":"<code>ForecasterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all forecasters in spotforecast2.</p> <p>All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Attributes:</p> Name Type Description <code>__spotforecast_tags__</code> <p>Dictionary with forecaster tags that characterize the behavior of the forecaster.</p> <p>Examples:</p> <p>To see all abstract methods that need to be implemented:</p> <pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.base import ForecasterBase\n&gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n['create_train_X_y', 'fit', 'predict', 'set_params']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>class ForecasterBase(ABC):\n    \"\"\"Base class for all forecasters in spotforecast2.\n\n    All forecasters should specify all the parameters that can be set at\n    the class level in their __init__.\n\n    Attributes:\n        __spotforecast_tags__: Dictionary with forecaster tags that characterize\n            the behavior of the forecaster.\n\n    Examples:\n        To see all abstract methods that need to be implemented:\n\n        &gt;&gt;&gt; import inspect\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.base import ForecasterBase\n        &gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n        ['create_train_X_y', 'fit', 'predict', 'set_params']\n    \"\"\"\n\n    def _preprocess_repr(\n        self,\n        estimator: object | None = None,\n        training_range_: dict[str, str] | None = None,\n        series_names_in_: list[str] | None = None,\n        exog_names_in_: list[str] | None = None,\n        transformer_series: object | dict[str, object] | None = None,\n    ) -&gt; tuple[str, str | None, str | None, str | None, str | None]:\n        \"\"\"Prepare the information to be displayed when a Forecaster object is printed.\n\n        Args:\n            estimator: Estimator object. Default is None.\n            training_range_: Training range. Only used for ForecasterRecursiveMultiSeries.\n                Default is None.\n            series_names_in_: Names of the series used in the forecaster.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n            exog_names_in_: Names of the exogenous variables used in the forecaster.\n                Default is None.\n            transformer_series: Transformer used in the series.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n\n        Returns:\n            Tuple containing params (estimator parameters string), training_range_\n            (training range string representation), series_names_in_ (series names\n            string representation), exog_names_in_ (exogenous variable names string\n            representation), and transformer_series (transformer string representation).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; estimator = Ridge(alpha=0.5)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=estimator, lags=3)\n            &gt;&gt;&gt; params, tr, sn, en, ts = forecaster._preprocess_repr(estimator=estimator)\n            &gt;&gt;&gt; params\n            \"{'alpha': 0.5, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\"\n        \"\"\"\n\n        if estimator is not None:\n            if isinstance(estimator, Pipeline):\n                name_pipe_steps = tuple(\n                    name + \"__\" for name in estimator.named_steps.keys()\n                )\n                params = {\n                    key: value\n                    for key, value in estimator.get_params().items()\n                    if key.startswith(name_pipe_steps)\n                }\n            else:\n                params = estimator.get_params()\n            params = str(params)\n        else:\n            params = None\n\n        if training_range_ is not None:\n            training_range_ = [\n                f\"'{k}': {v.astype(str).to_list()}\" for k, v in training_range_.items()\n            ]\n            if len(training_range_) &gt; 10:\n                training_range_ = training_range_[:5] + [\"...\"] + training_range_[-5:]\n            training_range_ = \", \".join(training_range_)\n\n        if series_names_in_ is not None:\n            if len(series_names_in_) &gt; 50:\n                series_names_in_ = (\n                    series_names_in_[:25] + [\"...\"] + series_names_in_[-25:]\n                )\n            series_names_in_ = \", \".join(series_names_in_)\n\n        if exog_names_in_ is not None:\n            if len(exog_names_in_) &gt; 50:\n                exog_names_in_ = exog_names_in_[:25] + [\"...\"] + exog_names_in_[-25:]\n            exog_names_in_ = \", \".join(exog_names_in_)\n\n        if transformer_series is not None:\n            if isinstance(transformer_series, dict):\n                transformer_series = [\n                    f\"'{k}': {v}\" for k, v in transformer_series.items()\n                ]\n                if len(transformer_series) &gt; 10:\n                    transformer_series = (\n                        transformer_series[:5] + [\"...\"] + transformer_series[-5:]\n                    )\n                transformer_series = \", \".join(transformer_series)\n            else:\n                transformer_series = str(transformer_series)\n\n        return (\n            params,\n            training_range_,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        )\n\n    def _format_text_repr(\n        self,\n        text: str,\n        max_text_length: int = 58,\n        width: int = 80,\n        indent: str = \"    \",\n    ) -&gt; str:\n        \"\"\"Format text for __repr__ method.\n\n        Args:\n            text: Text to format.\n            max_text_length: Maximum length of the text before wrapping. Default is 58.\n            width: Maximum width of the text. Default is 80.\n            indent: Indentation of the text. Default is four spaces.\n\n        Returns:\n            Formatted text string with proper wrapping and indentation.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster._format_text_repr(\"Short text\")\n            'Short text'\n        \"\"\"\n\n        if text is not None and len(text) &gt; max_text_length:\n            text = \"\\n    \" + textwrap.fill(\n                str(text), width=width, subsequent_indent=indent\n            )\n\n        return text\n\n    @abstractmethod\n    def create_train_X_y(\n        self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned.\n                Default is None.\n\n        Returns:\n            Tuple containing X_train (training values/predictors with shape\n            (len(y) - max_lag, len(lags))) and y_train (target values of the\n            time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n            &gt;&gt;&gt; X_train.head(2)\n               lag_1  lag_2  lag_3\n            3    2.0    1.0    0.0\n            4    3.0    2.0    1.0\n            &gt;&gt;&gt; y_train.head(2)\n            3    3\n            4    4\n            Name: y, dtype: int64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n        \"\"\"Training Forecaster.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned so\n                that y[i] is regressed on exog[i]. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.is_fitted\n            True\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n    ) -&gt; pd.Series:\n        \"\"\"Predict n steps ahead.\n\n        Args:\n            steps: Number of steps to predict.\n            last_window: Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1). If None, the values stored in\n                last_window are used to calculate the initial predictors, and the\n                predictions start right after training data. Default is None.\n            exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n        Returns:\n            Predicted values as a pandas Series.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            10    9.5\n            11    9.0\n            12    8.5\n            Name: pred, dtype: float64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def set_params(self, params: dict[str, object]) -&gt; None:\n        \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n        Args:\n            params: Parameters values dictionary.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n            &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n            &gt;&gt;&gt; forecaster.estimator.alpha\n            0.5\n        \"\"\"\n\n        pass\n\n    def set_lags(\n        self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute lags.\n\n        Attributes max_lag and window_size are also updated.\n\n        Args:\n            lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n                If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n                or range: include only lags present in lags, all elements must be int.\n                If None: no lags are included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.set_lags(lags=5)\n            &gt;&gt;&gt; forecaster.lags\n            array([1, 2, 3, 4, 5])\n        \"\"\"\n\n        pass\n\n    def set_window_features(\n        self, window_features: object | list[object] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute window_features.\n\n        Attributes max_size_window_features, window_features_names,\n        window_features_class_names and window_size are also updated.\n\n        Args:\n            window_features: Instance or list of instances used to create window features.\n                Window features are created from the original time series and are\n                included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n            &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n            &gt;&gt;&gt; forecaster.window_features\n            [RollingFeatures(stats=['mean'], window_sizes=[3])]\n        \"\"\"\n\n        pass\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            Dictionary with forecaster tags describing behavior and capabilities.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; tags['forecaster_task']\n            'regression'\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.summary()\n            ForecasterRecursive\n            ===================\n            Estimator: Ridge()\n            Lags: [1 2 3]\n            ...\n        \"\"\"\n\n        print(self.__repr__())\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n        This method is called when an object is unpickled (deserialized).\n        It handles the migration of deprecated attributes to their new names.\n\n        Args:\n            state: The state dictionary from the pickled object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pickle\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n            &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n        \"\"\"\n\n        if \"regressor\" in state and \"estimator\" not in state:\n            state[\"estimator\"] = state.pop(\"regressor\")\n\n        self.__dict__.update(state)\n\n    @property\n    def regressor(self) -&gt; Any:\n        \"\"\"Deprecated property. Use estimator instead.\n\n        Returns:\n            The estimator object.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\n            Ridge()\n        \"\"\"\n        warnings.warn(\n            \"The `regressor` attribute is deprecated and will be removed in future \"\n            \"versions. Use `estimator` instead.\",\n            FutureWarning,\n        )\n        return self.estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.regressor","title":"<code>regressor</code>  <code>property</code>","text":"<p>Deprecated property. Use estimator instead.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The estimator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\nRidge()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> <p>This method is called when an object is unpickled (deserialized). It handles the migration of deprecated attributes to their new names.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The state dictionary from the pickled object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n&gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n    This method is called when an object is unpickled (deserialized).\n    It handles the migration of deprecated attributes to their new names.\n\n    Args:\n        state: The state dictionary from the pickled object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n        &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n    \"\"\"\n\n    if \"regressor\" in state and \"estimator\" not in state:\n        state[\"estimator\"] = state.pop(\"regressor\")\n\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing X_train (training values/predictors with shape</p> <code>Series</code> <p>(len(y) - max_lag, len(lags))) and y_train (target values of the</p> <code>tuple[DataFrame, Series]</code> <p>time series related to each row of X_train with shape (len(y) - max_lag,)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n&gt;&gt;&gt; X_train.head(2)\n   lag_1  lag_2  lag_3\n3    2.0    1.0    0.0\n4    3.0    2.0    1.0\n&gt;&gt;&gt; y_train.head(2)\n3    3\n4    4\nName: y, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef create_train_X_y(\n    self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned.\n            Default is None.\n\n    Returns:\n        Tuple containing X_train (training values/predictors with shape\n        (len(y) - max_lag, len(lags))) and y_train (target values of the\n        time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n        &gt;&gt;&gt; X_train.head(2)\n           lag_1  lag_2  lag_3\n        3    2.0    1.0    0.0\n        4    3.0    2.0    1.0\n        &gt;&gt;&gt; y_train.head(2)\n        3    3\n        4    4\n        Name: y, dtype: int64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.fit","title":"<code>fit(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.is_fitted\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n    \"\"\"Training Forecaster.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned so\n            that y[i] is regressed on exog[i]. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.is_fitted\n        True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with forecaster tags describing behavior and capabilities.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; tags['forecaster_task']\n'regression'\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        Dictionary with forecaster tags describing behavior and capabilities.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; tags['forecaster_task']\n        'regression'\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If None, the values stored in last_window are used to calculate the initial predictors, and the predictions start right after training data. Default is None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Predicted values as a pandas Series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n10    9.5\n11    9.0\n12    8.5\nName: pred, dtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; pd.Series:\n    \"\"\"Predict n steps ahead.\n\n    Args:\n        steps: Number of steps to predict.\n        last_window: Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1). If None, the values stored in\n            last_window are used to calculate the initial predictors, and the\n            predictions start right after training data. Default is None.\n        exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n    Returns:\n        Predicted values as a pandas Series.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        10    9.5\n        11    9.0\n        12    8.5\n        Name: pred, dtype: float64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute lags.</p> <p>Attributes max_lag and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int | list[int] | ndarray[int] | range[int] | None</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. If int: include lags from 1 to lags (included). If list, 1d numpy ndarray, or range: include only lags present in lags, all elements must be int. If None: no lags are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.set_lags(lags=5)\n&gt;&gt;&gt; forecaster.lags\narray([1, 2, 3, 4, 5])\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_lags(\n    self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute lags.\n\n    Attributes max_lag and window_size are also updated.\n\n    Args:\n        lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n            If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n            or range: include only lags present in lags, all elements must be int.\n            If None: no lags are included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.set_lags(lags=5)\n        &gt;&gt;&gt; forecaster.lags\n        array([1, 2, 3, 4, 5])\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_params","title":"<code>set_params(params)</code>  <code>abstractmethod</code>","text":"<p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, object]</code> <p>Parameters values dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n&gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n&gt;&gt;&gt; forecaster.estimator.alpha\n0.5\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef set_params(self, params: dict[str, object]) -&gt; None:\n    \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n    Args:\n        params: Parameters values dictionary.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n        &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n        &gt;&gt;&gt; forecaster.estimator.alpha\n        0.5\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute window_features.</p> <p>Attributes max_size_window_features, window_features_names, window_features_class_names and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>object | list[object] | None</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n&gt;&gt;&gt; forecaster.window_features\n[RollingFeatures(stats=['mean'], window_sizes=[3])]\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_window_features(\n    self, window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute window_features.\n\n    Attributes max_size_window_features, window_features_names,\n    window_features_class_names and window_size are also updated.\n\n    Args:\n        window_features: Instance or list of instances used to create window features.\n            Window features are created from the original time series and are\n            included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n        &gt;&gt;&gt; forecaster.window_features\n        [RollingFeatures(stats=['mean'], window_sizes=[3])]\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.summary()\nForecasterRecursive\n===================\nEstimator: Ridge()\nLags: [1 2 3]\n...\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.summary()\n        ForecasterRecursive\n        ===================\n        Estimator: Ridge()\n        Lags: [1 2 3]\n        ...\n    \"\"\"\n\n    print(self.__repr__())\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive","title":"<code>ForecasterRecursive</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>Recursive autoregressive forecaster for scikit-learn compatible estimators.</p> <p>This class turns any estimator compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. The forecaster learns to predict future values by using lagged values of the target variable and optional exogenous features. Predictions are made iteratively, where each step uses previous predictions as input for the next step (recursive strategy).</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object</code> <p>Scikit-learn compatible estimator for regression. If None, a default estimator will be initialized. Can also be passed via regressor parameter.</p> <code>None</code> <code>lags</code> <code>Union[int, List[int], ndarray, range, None]</code> <p>Lagged values of the target variable to use as predictors. Can be an integer (uses lags from 1 to lags), list of integers, numpy array, or range. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>window_features</code> <code>Union[object, List[object], None]</code> <p>List of window feature objects to compute features from the target variable. Each object must implement transform_batch() method. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>transformer_y</code> <code>Optional[object]</code> <p>Transformer object for the target variable. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>transformer_exog</code> <code>Optional[object]</code> <p>Transformer object for exogenous variables. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>weight_func</code> <code>Optional[Callable]</code> <p>Function to compute sample weights for training. Must accept an index and return an array of weights. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>Optional[int]</code> <p>Order of differencing to apply to the target variable. Must be a positive integer. Differencing is applied before creating lags. Defaults to None.</p> <code>None</code> <code>fit_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of additional keyword arguments to pass to the estimator's fit() method. Defaults to None.</p> <code>None</code> <code>binner_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of keyword arguments for QuantileBinner used in probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.</p> <code>None</code> <code>forecaster_id</code> <code>Union[str, int, None]</code> <p>Identifier for the forecaster instance. Can be a string or integer. Used for tracking and logging purposes. Defaults to None.</p> <code>None</code> <code>regressor</code> <code>object</code> <p>Alternative parameter name for estimator. If provided, used instead of estimator. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator</code> <p>Fitted scikit-learn estimator.</p> <code>lags</code> <p>Lag indices used in the model.</p> <code>lags_names</code> <p>Names of lag features (e.g., ['lag_1', 'lag_2']).</p> <code>window_features</code> <p>List of window feature transformers.</p> <code>window_features_names</code> <p>Names of window features.</p> <code>window_size</code> <p>Maximum window size needed (max of lags and window features).</p> <code>transformer_y</code> <p>Transformer for target variable.</p> <code>transformer_exog</code> <p>Transformer for exogenous variables.</p> <code>weight_func</code> <p>Function for sample weighting.</p> <code>differentiation</code> <p>Order of differencing applied.</p> <code>differentiator</code> <p>TimeSeriesDifferentiator instance if differencing is used.</p> <code>is_fitted</code> <p>Boolean indicating if forecaster has been fitted.</p> <code>fit_date</code> <p>Timestamp of the last fit operation.</p> <code>last_window_</code> <p>Last window_size observations from training data.</p> <code>index_type_</code> <p>Type of index in training data (RangeIndex or DatetimeIndex).</p> <code>index_freq_</code> <p>Frequency of DatetimeIndex if applicable.</p> <code>training_range_</code> <p>First and last index values of training data.</p> <code>series_name_in_</code> <p>Name of the target series.</p> <code>exog_in_</code> <p>Boolean indicating if exogenous variables were used in training.</p> <code>exog_names_in_</code> <p>Names of exogenous variables.</p> <code>exog_type_in_</code> <p>Type of exogenous input (Series or DataFrame).</p> <code>X_train_features_names_out_</code> <p>Names of all training features.</p> <code>in_sample_residuals_</code> <p>Residuals from training set.</p> <code>in_sample_residuals_by_bin_</code> <p>Residuals grouped by bins for probabilistic pred.</p> <code>forecaster_id</code> <p>Identifier for the forecaster instance.</p> Note <ul> <li>Either lags or window_features (or both) must be provided during initialization.</li> <li>The forecaster uses a recursive strategy where each multi-step prediction   depends on previous predictions within the same forecast horizon.</li> <li>Exogenous variables must have the same index as the target variable and must   be available for the entire prediction horizon.</li> <li>The forecaster supports point predictions, prediction intervals, bootstrapping,   quantile predictions, and probabilistic forecasts via conformal methods.</li> </ul> <p>Examples:</p> <p>Create a basic forecaster with lags:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=10\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> <p>Create a forecaster with window features and transformations:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(n_estimators=100),\n...     lags=[1, 7, 30],\n...     window_features=[RollingFeatures(stats='mean', window_sizes=7)],\n...     transformer_y=StandardScaler(),\n...     differentiation=1\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n</code></pre> <p>Create a forecaster with exogenous variables:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=Ridge(),\n...     lags=7,\n...     forecaster_id='my_forecaster'\n... )\n&gt;&gt;&gt; forecaster.fit(y, exog)\n&gt;&gt;&gt; exog_future = pd.DataFrame(\n...     {'temp': np.random.randn(5)},\n...     index=pd.RangeIndex(start=100, stop=105)\n... )\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n</code></pre> <p>Create a forecaster with probabilistic prediction configuration:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=GradientBoostingRegressor(),\n...     lags=14,\n...     binner_kwargs={'n_bins': 15, 'method': 'linear'}\n... )\n&gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>class ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    Recursive autoregressive forecaster for scikit-learn compatible estimators.\n\n    This class turns any estimator compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster. The forecaster learns to predict\n    future values by using lagged values of the target variable and optional exogenous\n    features. Predictions are made iteratively, where each step uses previous predictions\n    as input for the next step (recursive strategy).\n\n    Args:\n        estimator: Scikit-learn compatible estimator for regression. If None, a default\n            estimator will be initialized. Can also be passed via regressor parameter.\n        lags: Lagged values of the target variable to use as predictors. Can be an\n            integer (uses lags from 1 to lags), list of integers, numpy array, or range.\n            At least one of lags or window_features must be provided. Defaults to None.\n        window_features: List of window feature objects to compute features from the\n            target variable. Each object must implement transform_batch() method.\n            At least one of lags or window_features must be provided. Defaults to None.\n        transformer_y: Transformer object for the target variable. Must implement fit()\n            and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        transformer_exog: Transformer object for exogenous variables. Must implement\n            fit() and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        weight_func: Function to compute sample weights for training. Must accept an\n            index and return an array of weights. Defaults to None.\n        differentiation: Order of differencing to apply to the target variable.\n            Must be a positive integer. Differencing is applied before creating lags.\n            Defaults to None.\n        fit_kwargs: Dictionary of additional keyword arguments to pass to the estimator's\n            fit() method. Defaults to None.\n        binner_kwargs: Dictionary of keyword arguments for QuantileBinner used in\n            probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.\n        forecaster_id: Identifier for the forecaster instance. Can be a string or\n            integer. Used for tracking and logging purposes. Defaults to None.\n        regressor: Alternative parameter name for estimator. If provided, used instead\n            of estimator. Defaults to None.\n\n    Attributes:\n        estimator: Fitted scikit-learn estimator.\n        lags: Lag indices used in the model.\n        lags_names: Names of lag features (e.g., ['lag_1', 'lag_2']).\n        window_features: List of window feature transformers.\n        window_features_names: Names of window features.\n        window_size: Maximum window size needed (max of lags and window features).\n        transformer_y: Transformer for target variable.\n        transformer_exog: Transformer for exogenous variables.\n        weight_func: Function for sample weighting.\n        differentiation: Order of differencing applied.\n        differentiator: TimeSeriesDifferentiator instance if differencing is used.\n        is_fitted: Boolean indicating if forecaster has been fitted.\n        fit_date: Timestamp of the last fit operation.\n        last_window_: Last window_size observations from training data.\n        index_type_: Type of index in training data (RangeIndex or DatetimeIndex).\n        index_freq_: Frequency of DatetimeIndex if applicable.\n        training_range_: First and last index values of training data.\n        series_name_in_: Name of the target series.\n        exog_in_: Boolean indicating if exogenous variables were used in training.\n        exog_names_in_: Names of exogenous variables.\n        exog_type_in_: Type of exogenous input (Series or DataFrame).\n        X_train_features_names_out_: Names of all training features.\n        in_sample_residuals_: Residuals from training set.\n        in_sample_residuals_by_bin_: Residuals grouped by bins for probabilistic pred.\n        forecaster_id: Identifier for the forecaster instance.\n\n    Note:\n        - Either lags or window_features (or both) must be provided during initialization.\n        - The forecaster uses a recursive strategy where each multi-step prediction\n          depends on previous predictions within the same forecast horizon.\n        - Exogenous variables must have the same index as the target variable and must\n          be available for the entire prediction horizon.\n        - The forecaster supports point predictions, prediction intervals, bootstrapping,\n          quantile predictions, and probabilistic forecasts via conformal methods.\n\n    Examples:\n        Create a basic forecaster with lags:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=10\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n\n        Create a forecaster with window features and transformations:\n\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(n_estimators=100),\n        ...     lags=[1, 7, 30],\n        ...     window_features=[RollingFeatures(stats='mean', window_sizes=7)],\n        ...     transformer_y=StandardScaler(),\n        ...     differentiation=1\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n\n        Create a forecaster with exogenous variables:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)}, index=y.index)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=Ridge(),\n        ...     lags=7,\n        ...     forecaster_id='my_forecaster'\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, exog)\n        &gt;&gt;&gt; exog_future = pd.DataFrame(\n        ...     {'temp': np.random.randn(5)},\n        ...     index=pd.RangeIndex(start=100, stop=105)\n        ... )\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n\n        Create a forecaster with probabilistic prediction configuration:\n\n        &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=GradientBoostingRegressor(),\n        ...     lags=14,\n        ...     binner_kwargs={'n_bins': 15, 'method': 'linear'}\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: object = None,\n        lags: Union[int, List[int], np.ndarray, range, None] = None,\n        window_features: Union[object, List[object], None] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        binner_kwargs: Optional[Dict[str, object]] = None,\n        forecaster_id: Union[str, int, None] = None,\n        regressor: object = None,\n    ) -&gt; None:\n\n        self.estimator = copy(initialize_estimator(estimator, regressor))\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiation_max = None\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.exog_dtypes_out_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        try:\n            from spotforecast2_safe import __version__\n\n            self.spotforecast_version = __version__\n        except ImportError:\n            self.spotforecast_version = \"unknown\"\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n\n        (\n            self.lags,\n            self.lags_names,\n            self.max_lag,\n        ) = initialize_lags(type(self).__name__, lags)\n        (\n            self.window_features,\n            self.window_features_names,\n            self.max_size_window_features,\n        ) = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        self.window_size = max(\n            [\n                ws\n                for ws in [self.max_lag, self.max_size_window_features]\n                if ws is not None\n            ]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=estimator,\n            weight_func=weight_func,\n            series_weights=None,\n        )\n\n        if differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation &lt; 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation  # , window_size=self.window_size # TODO: TimeSeriesDifferentiator in preprocessing created only takes order, add window_size if needed\n            )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n            estimator=estimator, fit_kwargs=fit_kwargs\n        )\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n\n        Returns:\n            str: String representation of the forecaster with key information about its configuration and state.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; print(forecaster)  # doctest: +ELLIPSIS\n            =========================\n            ForecasterRecursive\n            =========================\n            Estimator: LinearRegression\n            Lags: [1, 2, 3]\n            Window features: []\n            Window size: 3\n            Series name: None\n            Exogenous included: False\n            Exogenous names: None\n            Transformer for y: None\n            Transformer for exog: None\n            Weight function included: False\n            Differentiation order: None\n            Training range: None\n            Training index type: None\n            Training index frequency: None\n            Estimator parameters: {...}\n            fit_kwargs: {...}\n            Creation date: ...\n            Last fit date: None\n            spotforecast version: ...\n            Python version: ...\n            Forecaster id: None\n\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Estimator: {type(self.estimator).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Estimator parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"spotforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n\n        Returns:\n            HTML string representation of the forecaster.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; forecaster._repr_html_()  # doctest: +ELLIPSIS\n            '&lt;div class=\"container-...\"&gt;...&lt;/div&gt;'\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n                &lt;ul&gt;\n                    {exog_names_in_}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Data Transformations&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Estimator Parameters&lt;/summary&gt;\n                &lt;ul&gt;\n                    {params}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n                &lt;ul&gt;\n                    {self.fit_kwargs}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"\n        Custom __setstate__ to ensure backward compatibility when unpickling.\n        Only sets __spotforecast_tags__ if not present, preserving custom tags.\n        \"\"\"\n        super().__setstate__(state)\n        if not hasattr(self, \"__spotforecast_tags__\"):\n            self.__spotforecast_tags__ = {\n                \"library\": \"spotforecast\",\n                \"forecaster_name\": \"ForecasterRecursive\",\n                \"forecaster_task\": \"regression\",\n                \"forecasting_scope\": \"single-series\",\n                \"forecasting_strategy\": \"recursive\",\n                \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n                \"requires_index_frequency\": True,\n                \"allowed_input_types_series\": [\"pandas.Series\"],\n                \"supports_exog\": True,\n                \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n                \"handles_missing_values_series\": False,\n                \"handles_missing_values_exog\": True,\n                \"supports_lags\": True,\n                \"supports_window_features\": True,\n                \"supports_transformer_series\": True,\n                \"supports_transformer_exog\": True,\n                \"supports_weight_func\": True,\n                \"supports_differentiation\": True,\n                \"prediction_types\": [\n                    \"point\",\n                    \"interval\",\n                    \"bootstrapping\",\n                    \"quantiles\",\n                    \"distribution\",\n                ],\n                \"supports_probabilistic\": True,\n                \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n                \"handles_binned_residuals\": True,\n            }\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None,\n    ) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create lagged predictors and aligned target values.\n\n        Args:\n            y: Target values used to build lag features. Expected shape is\n                (n_samples,) or (n_samples, 1).\n            X_as_pandas: If True, returns lagged features as a pandas DataFrame.\n            train_index: Index to use for the lagged feature DataFrame when\n                `X_as_pandas` is True.\n\n        Returns:\n            Tuple containing:\n                - X_data: Lagged predictors with shape (n_rows, n_lags) or None\n                  if no lags are configured.\n                - y_data: Target values aligned to the lagged predictors with\n                  shape (n_rows,).\n\n        Raises:\n            ValueError: If `X_as_pandas` is True but `train_index` is not provided.\n            ValueError: If the length of `y` is not sufficient to create the\n                specified lags.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(lags=3)\n            &gt;&gt;&gt; y = np.arange(10)\n            &gt;&gt;&gt; train_index = pd.RangeIndex(start=3, stop=10)\n            &gt;&gt;&gt; X_data, y_data = forecaster._create_lags(y=y, X_as_pandas=True, train_index=train_index)\n            &gt;&gt;&gt; isinstance(X_data, pd.DataFrame)\n            True\n            &gt;&gt;&gt; X_data.shape\n            (7, 3)\n            &gt;&gt;&gt; y_data.shape\n            (7,)\n        \"\"\"\n        if X_as_pandas and train_index is None:\n            raise ValueError(\n                \"If `X_as_pandas` is True, `train_index` must be provided.\"\n            )\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\"\n            )\n\n        X_data = None\n        if self.lags is not None:\n            # y = y.ravel() # Assuming y is already raveled\n            # Using stride_tricks for sliding window\n            y_strided = np.lib.stride_tricks.sliding_window_view(y, self.window_size)[\n                :-1\n            ]\n            X_data = y_strided[:, self.window_size - self.lags]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                    data=X_data, columns=self.lags_names, index=train_index\n                )\n\n        y_data = y[self.window_size :]\n\n        return X_data, y_data\n\n    def _create_window_features(\n        self,\n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -&gt; Tuple[List[Union[np.ndarray, pd.DataFrame]], List[str]]:\n        \"\"\"\n        Generate window features from the target series.\n\n        Args:\n            y: Target series used to compute window features. Must be a pandas\n                Series with an index aligned to `train_index` after trimming.\n            train_index: Index for the training rows to align the window features.\n            X_as_pandas: If True, keeps each window feature matrix as a pandas\n                DataFrame; otherwise converts to NumPy arrays.\n\n        Returns:\n            Tuple containing:\n                - X_train_window_features: List of window feature matrices, one\n                  per window feature transformer.\n                - X_train_window_features_names_out_: List of feature names for\n                  all generated window features.\n\n        Raises:\n            TypeError: If any window feature's `transform_batch` method does not\n                return a pandas DataFrame.\n            ValueError: If the output DataFrame from any window feature does not\n                have the same number of rows as `train_index` or if the index\n                does not match `train_index`.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; train_index = y.index[3:]  # Assuming window_size is 3\n            &gt;&gt;&gt; X_train_window_features, feature_names = forecaster._create_window_features(\n            ...     y=y,\n            ...     train_index=train_index,\n            ...     X_as_pandas=True\n            ... )\n            &gt;&gt;&gt; isinstance(X_train_window_features[0], pd.DataFrame)\n            True\n            &gt;&gt;&gt; X_train_window_features[0].shape[0] == len(train_index)\n            True\n            &gt;&gt;&gt; (X_train_window_features[0].index == train_index).all()\n            True\n\n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a pandas DataFrame.\"\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same number of rows as \"\n                    f\"the input time series - `window_size`: {len_train_index}.\"\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same index as \"\n                    f\"the input time series - `window_size`.\"\n                )\n\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        \"\"\"Create training predictors and target values.\n\n        Args:\n            y: Target series for training. Must be a pandas Series.\n            exog:\n                Optional exogenous variables for training. Can be a pandas Series or DataFrame.\n                Must have the same index as `y` and cover the same time range.\n\n        Returns:\n            Tuple containing:\n                - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided).\n                - y_train: Series of target values aligned with the predictors.\n                - X_train_features_names_out_: List of all predictor feature names.\n                - lags_names: List of lag feature names.\n                - window_features_names: List of window feature names.\n                - exog_names_in_: List of exogenous variable names (if exogenous variables are used).\n                - exog_dtypes_in_: Dictionary of input data types for exogenous variables.\n                - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).\n\n        Raises:\n            ValueError: If the length of `y` is not sufficient to create the specified lags and window features.\n            ValueError: If `exog` is provided but does not have the same index as `y` or does not cover the same time range.\n            ValueError: If `exog` is provided but contains data types that are not supported after transformation.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n            ...  exog_names_out, feature_names, exog_dtypes_in_,\n            ...  exog_dtypes_out_) = forecaster._create_train_X_y(y=y, exog=exog)\n            &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_train, pd.Series)\n            True\n            &gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\n            True\n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name=\"y\")\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n            df=y,\n            transformer=self.transformer_y,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        y_values, y_index = check_extract_values_and_index(data=y, data_label=\"`y`\")\n        if y_values.ndim == 2 and y_values.shape[1] == 1:\n            y_values = y_values.ravel()\n        train_index = y_index[self.window_size :]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                self.differentiator.fit(y_values)  # Differentiator requires fit first\n                y_values = self.differentiator.transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        X_as_pandas = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            _, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\", ignore_freq=True, return_values=False\n            )\n\n            len_y_original = len(y)\n            len_exog = len(exog)\n            len_train = len(train_index)\n\n            # Safety-critical validation: exog must be either full-length or pre-aligned\n            if len_exog == len_y_original:\n                # Standard case: exog covers full y range, trim by window_size\n                exog = exog.iloc[self.window_size :, :]\n            elif len_exog == len_train:\n                # Alternative case: exog already aligned to training index\n                pass\n            else:\n                raise ValueError(\n                    f\"Length mismatch for exogenous variables. Expected either:\\n\"\n                    f\"  - Full length matching `y`: {len_y_original} observations, OR\\n\"\n                    f\"  - Pre-aligned length: {len_train} observations (y length - window_size)\\n\"\n                    f\"Got: {len_exog} observations.\\n\"\n                    f\"Window size: {self.window_size}\"\n                )\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=fit_transformer,\n                inverse_transform=False,\n            )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n            X_as_pandas = any(\n                not pd.api.types.is_numeric_dtype(dtype)\n                or pd.api.types.is_bool_dtype(dtype)\n                for dtype in set(exog.dtypes)\n            )\n\n        X_train = []\n        X_train_features_names_out_ = []\n\n        # Create lags\n        # Note: y_values might have NaNs from differentiation.\n        # TODO: check if _create_lags handles this!\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            if isinstance(y_values, pd.Series):\n                y_vals_for_wf = y_values.iloc[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n            else:\n                y_vals_for_wf = y_values[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n\n            y_window_features = pd.Series(y_vals_for_wf, index=y_index_for_wf)\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features,\n                    X_as_pandas=X_as_pandas,\n                    train_index=train_index,\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                data=X_train, index=train_index, columns=X_train_features_names_out_\n            )\n\n        y_train = pd.Series(data=y_train, index=train_index, name=\"y\")\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        )\n\n    def create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        \"\"\"Public method to create training predictors and target values.\n\n        This method is a public wrapper around the internal method `_create_train_X_y`,\n        which generates the training predictors and target values based on the provided time series and exogenous variables.\n        It ensures that the necessary transformations and feature engineering steps are applied to prepare the data for training the forecaster.\n\n        Args:\n            y: Target series for training. Must be a pandas Series.\n            exog: Optional exogenous variables for training. Can be a pandas Series or DataFrame. Must have the same index as `y` and cover the same time range. Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided).\n                - y_train: Series of target values aligned with the predictors.\n                - X_train_features_names_out_: List of all predictor feature names.\n                - lags_names: List of lag feature names.\n                - window_features_names: List of window feature names.\n                - exog_names_in_: List of exogenous variable names (if exogenous variables are used).\n                - exog_dtypes_in_: Dictionary of input data types for exogenous variables.\n                - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n            ...  exog_names_out, feature_names, exog_dtypes_in_,\n            ...  exog_dtypes_out_) = forecaster.create_train_X_y(y=y, exog=exog)\n            &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_train, pd.Series)\n            True\n            &gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\n            True\n\n        \"\"\"\n        return self._create_train_X_y(y=y, exog=exog)\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Args:\n            y: Training time series.\n            initial_train_size: Initial size of the training set. It is the number of\n                observations used to train the forecaster before making the first\n                prediction.\n            exog: Exogenous variable/s included as predictor/s. Must have the same\n                number of observations as y and their indexes must be aligned.\n                Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: Predictor values used to train the model as pandas DataFrame.\n                - y_train: Values of the time series related to each row of X_train for\n                    each step in the form {step: y_step_[i]} as dict.\n                - X_test: Predictor values used to test the model as pandas DataFrame.\n                - y_test: Values of the time series related to each row of X_test for\n                    each step in the form {step: y_step_[i]} as dict.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; X_train, y_train, X_test, y_test = forecaster._train_test_split_one_step_ahead(y=y, initial_train_size=20, exog=exog)\n            &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_train, pd.Series)\n            True\n            &gt;&gt;&gt; isinstance(X_test, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_test, pd.Series)\n            True\n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y=y.iloc[:initial_train_size],\n            exog=exog.iloc[:initial_train_size] if exog is not None else None,\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y=y.iloc[test_init:],\n            exog=exog.iloc[test_init:] if exog is not None else None,\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def get_params(self, deep: bool = True) -&gt; Dict[str, object]:\n        \"\"\"\n        Get parameters for this forecaster.\n\n        Args:\n            deep: If True, will return the parameters for this forecaster and\n                contained sub-objects that are estimators.\n\n        Returns:\n            params: Dictionary of parameter names mapped to their values.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; forecaster.get_params()  # doctest: +ELLIPSIS\n            {\n                'estimator': LinearRegression(), 'lags': 3, 'window_features': None,\n                'transformer_y': None, 'transformer_exog': None, 'weight_func': None,\n                'differentiation': None, 'fit_kwargs': {}, 'binner_kwargs': None, 'forecaster_id': '...'}\n        \"\"\"\n        params = {}\n        for key in [\n            \"estimator\",\n            \"lags\",\n            \"window_features\",\n            \"transformer_y\",\n            \"transformer_exog\",\n            \"weight_func\",\n            \"differentiation\",\n            \"fit_kwargs\",\n            \"binner_kwargs\",\n            \"forecaster_id\",\n        ]:\n            if hasattr(self, key):\n                params[key] = getattr(self, key)\n\n        if not deep:\n            return params\n\n        if hasattr(self, \"estimator\") and self.estimator is not None:\n            if hasattr(self.estimator, \"get_params\"):\n                for key, value in self.estimator.get_params(deep=True).items():\n                    params[f\"estimator__{key}\"] = value\n\n        return params\n\n    def set_params(\n        self, params: Dict[str, object] = None, **kwargs: object\n    ) -&gt; \"ForecasterRecursive\":\n        \"\"\"\n        Set the parameters of this forecaster.\n\n        Args:\n            params: Optional dictionary of parameter names mapped to their new values.\n                If provided, these parameters are set first.\n            **kwargs: Dictionary of parameter names mapped to their new values.\n                Parameters can be for the forecaster itself or for the contained estimator (using the `estimator__` prefix).\n\n        Returns:\n            self: The forecaster instance with updated parameters.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; forecaster.set_params(estimator__fit_intercept=False)\n            &gt;&gt;&gt; forecaster.estimator.get_params()[\"fit_intercept\"]\n            False\n        \"\"\"\n\n        # Merge params dict and kwargs\n        all_params = {}\n        if params is not None:\n            all_params.update(params)\n        all_params.update(kwargs)\n\n        if not all_params:\n            return self\n\n        valid_params = self.get_params(deep=True)\n        nested_params = {}\n\n        for key, value in all_params.items():\n            if key not in valid_params and \"__\" not in key:\n                # Relaxed check for now\n                pass\n\n            if \"__\" in key:\n                obj_name, param_name = key.split(\"__\", 1)\n                if obj_name not in nested_params:\n                    nested_params[obj_name] = {}\n                nested_params[obj_name][param_name] = value\n            else:\n                setattr(self, key, value)\n\n        for obj_name, obj_params in nested_params.items():\n            if hasattr(self, obj_name):\n                obj = getattr(self, obj_name)\n                if hasattr(obj, \"set_params\"):\n                    obj.set_params(**obj_params)\n                else:\n                    for param_name, value in obj_params.items():\n                        setattr(obj, param_name, value)\n\n        return self\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        suppress_warnings: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Fit the forecaster to the training data.\n\n        Args:\n            y:\n                  Target series for training. Must be a pandas Series.\n            exog:\n                  Optional exogenous variables for training. Can be a pandas Series or DataFrame.Must have the same index as `y` and cover the same time range. Defaults to None.\n            store_last_window:\n                  Whether to store the last window of the training series for use in prediction. Defaults to True.\n            store_in_sample_residuals:\n                  Whether to store in-sample residuals after fitting, which can be used for certain probabilistic prediction methods. Defaults to False.\n            random_state:\n                  Random seed for reproducibility when sampling residuals if `store_in_sample_residuals` is True. Defaults to 123.\n            suppress_warnings:\n                  Whether to suppress warnings during fitting, such as those related to insufficient data length for lags or window features. Defaults to False.\n\n        Returns:\n            None\n\n        Examples:\n                 &gt;&gt;&gt; import numpy as np\n                 &gt;&gt;&gt; import pandas as pd\n                 &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n                 &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n                 &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n                 &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n                 &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n                 &gt;&gt;&gt; forecaster = ForecasterRecursive(\n                 ...     estimator=LinearRegression(),\n                 ...     lags=3,\n                 ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n                 ... )\n                 &gt;&gt;&gt; forecaster.fit(y=y, exog=exog, store_in_sample_residuals=True)\n        \"\"\"\n\n        # Reset values\n        self.is_fitted = False\n        self.fit_date = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        SAMPLE_WEIGHT_NAME = \"sample_weight\"\n        if self.weight_func is not None:\n            sample_weight, _, _ = initialize_weights(\n                forecaster_name=type(self).__name__,\n                estimator=self.estimator,\n                weight_func=self.weight_func,\n                series_weights=None,\n            )\n            sample_weight = sample_weight(y.index[self.window_size :])\n            self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n        self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n            del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n        # Store attributes\n        self.last_window_ = y.iloc[-self.window_size :].copy()\n        self.index_type_ = type(y.index)\n        if isinstance(y.index, pd.DatetimeIndex):\n            self.index_freq_ = y.index.freqstr\n        else:\n            try:\n                self.index_freq_ = y.index.step\n            except AttributeError:\n                self.index_freq_ = None\n\n        self.training_range_ = y.index[[0, -1]]\n        self.series_name_in_ = y.name\n        self.exog_in_ = exog is not None\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog) if exog is not None else None\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        self.in_sample_residuals_ = None\n        self.binner_intervals_ = None\n        self.in_sample_residuals_by_bin_ = None\n\n        y_pred = self.estimator.predict(X_train)\n        self._binning_in_sample_residuals(\n            y_true=y_train.to_numpy(),\n            y_pred=y_pred,\n            store_in_sample_residuals=store_in_sample_residuals,\n            random_state=random_state,\n        )\n\n    def _create_predict_inputs(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        predict_probabilistic: bool = False,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        check_inputs: bool = True,\n    ) -&gt; Tuple[np.ndarray, Union[np.ndarray, None], pd.Index, pd.Index]:\n        \"\"\"\n        Create and validate inputs needed for prediction.\n\n        Args:\n            steps:\n                Number of future steps to predict. Can be an integer or a date (str/pd.Timestamp).\n            last_window:\n                Optional last window of observed values to use for prediction.\n                If None, uses the last window from training.\n                Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.\n            exog:\n                Optional exogenous variables for prediction.\n                Can be a pandas Series or DataFrame.\n                Must have the same structure as the exogenous variables used in training. Defaults to None.\n            check_inputs:\n                Whether to perform input validation checks. Defaults to True.\n\n        Returns:\n            Tuple containing:\n                - last_window_values:\n                    Numpy array of the last window values to use for prediction, transformed and ready for input into the prediction method.\n                - exog_values:\n                    Numpy array of exogenous variable values for prediction, transformed and ready for input into the prediction method,\n                    or None if no exogenous variables are used.\n                - prediction_index:\n                    Pandas Index for the predicted values, constructed based on the last window index and the number of steps to predict.\n                - exog_index:\n                    Pandas Index for the exogenous variable values, if exogenous variables are used; otherwise None.\n\n        Raises:\n            ValueError:\n                If input validation checks fail when `check_inputs` is True, such as if `last_window` does not have\n                the correct structure or if `exog` is not compatible with the training exogenous variables.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n            &gt;&gt;&gt; last_window = y.iloc[-3:]\n            &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n            &gt;&gt;&gt; last_window_values, exog_values, prediction_index, exog_index = forecaster._create_predict_inputs(\n            ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n            ... )\n            &gt;&gt;&gt; isinstance(last_window_values, np.ndarray)\n            True\n            &gt;&gt;&gt; isinstance(exog_values, np.ndarray)\n            True\n            &gt;&gt;&gt; isinstance(prediction_index, pd.Index)\n            True\n            &gt;&gt;&gt; isinstance(exog_index, pd.Index)\n            True\n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        # Transform steps to integer if it is a date\n        if not isinstance(steps, (int, np.integer)):\n            steps = date_to_index_position(\n                index=last_window.index, date_input=steps, method=\"prediction\"\n            )\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=self.exog_in_,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n                last_window_exog=None,\n                exog=exog,\n                exog_names_in_=self.exog_names_in_,\n                interval=None,\n                # alpha=None, # Removed alpha check for now\n            )\n\n            if predict_probabilistic:\n                check_residuals_input(\n                    forecaster_name=type(self).__name__,\n                    use_in_sample_residuals=use_in_sample_residuals,\n                    in_sample_residuals_=self.in_sample_residuals_,\n                    out_sample_residuals_=self.out_sample_residuals_,\n                    use_binned_residuals=use_binned_residuals,\n                    in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n                    out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n                )\n\n        last_window = input_to_frame(data=last_window, input_name=\"last_window\")\n        _, last_window_index = check_extract_values_and_index(\n            data=last_window,\n            data_label=\"`last_window`\",\n            ignore_freq=True,\n            return_values=False,\n        )\n\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n\n        last_window = transform_dataframe(\n            df=last_window,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, _ = check_extract_values_and_index(\n            data=last_window, data_label=\"`last_window`\"\n        )\n        last_window_values = last_window_values.ravel()\n\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        exog_values = None\n        exog_index = None\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=False,\n                inverse_transform=False,\n            )\n\n            exog_values, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\"\n            )\n\n            exog_values = (\n                exog_values if isinstance(exog, pd.Series) else exog.to_numpy()\n            )\n\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning,\n            )\n\n        return last_window_values, exog_values, prediction_index, exog_index\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Union[np.ndarray, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Create predictions recursively for the specified number of steps.\n\n        Args:\n            steps:\n                Number of future steps to predict.\n            last_window_values:\n                Numpy array of the last window values to use for prediction, transformed and ready for input into the prediction method.\n            exog_values:\n                Numpy array of exogenous variable values for prediction, transformed and ready for input into the prediction method.\n\n        Returns:\n            Numpy array of predicted values for the specified number of steps.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n            &gt;&gt;&gt; last_window = y.iloc[-3:]\n            &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n            &gt;&gt;&gt; last_window_values, exog_values, prediction_index, exog_index = forecaster._create_predict_inputs(\n            ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n            ... )\n            &gt;&gt;&gt; predictions = forecaster._recursive_predict(\n            ...     steps=5, last_window_values=last_window_values, exog_values=exog_values\n            ... )\n            &gt;&gt;&gt; isinstance(predictions, np.ndarray)\n            True\n        \"\"\"\n\n        predictions = np.full(shape=steps, fill_value=np.nan)\n\n        for step in range(steps):\n\n            X_gen = []\n\n            if self.lags is not None:\n                X_lags = last_window_values[-self.lags]\n                if X_lags.ndim == 1:\n                    X_lags = X_lags.reshape(1, -1)\n                X_gen.append(X_lags)\n\n            if self.window_features is not None:\n                X_window_features = []\n                for wf in self.window_features:\n                    wf_values = wf.transform(last_window_values)\n                    X_window_features.append(wf_values[-1:])\n\n                X_window_features = np.concatenate(X_window_features, axis=1)\n                X_gen.append(X_window_features)\n\n            if self.exog_in_:\n                X_exog = exog_values[step]\n                if X_exog.ndim &lt; 2:\n                    X_exog = X_exog.reshape(1, -1)\n                X_gen.append(X_exog)\n\n            X_gen = np.concatenate(X_gen, axis=1)\n\n            # Convert to DataFrame with feature names to avoid sklearn warning\n            if self.X_train_features_names_out_ is not None:\n                X_gen = pd.DataFrame(X_gen, columns=self.X_train_features_names_out_)\n\n            pred = self.estimator.predict(X_gen)\n            predictions[step] = pred[0]\n\n            last_window_values = np.append(last_window_values, pred)\n\n        return predictions\n\n    def _recursive_predict_bootstrapping(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        sampled_residuals: np.ndarray,\n        use_binned_residuals: bool,\n        n_boot: int,\n        exog_values: np.ndarray | None = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Vectorized bootstrap prediction - predict all n_boot iterations per step.\n        Instead of running n_boot sequential predictions, this method predicts\n        all bootstrap samples at once per step, significantly reducing overhead.\n\n        Args:\n            steps:\n                Number of steps to predict.\n            last_window_values:\n                Series values used to create the predictors needed in the first\n                iteration of the prediction (t + 1).\n            sampled_residuals:\n                Pre-sampled residuals for all bootstrap iterations.\n                - If `use_binned_residuals=True`: 3D array of shape (n_bins, steps, n_boot)\n                - If `use_binned_residuals=False`: 2D array of shape (steps, n_boot)\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values.\n                If `False`, residuals are selected randomly.\n            n_boot:\n                Number of bootstrap iterations.\n            exog_values:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n\n        Returns:\n            Numpy ndarray with the predicted values. Shape (steps, n_boot).\n\n        Raises:\n            ValueError:\n                If `sampled_residuals` does not match the expected shape/dimensions.\n            IndexError:\n                If `last_window_values` or `exog_values` are not of expected lengths.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n            &gt;&gt;&gt; _ = forecaster.fit(y=pd.Series(np.arange(10)))\n            &gt;&gt;&gt; last_window = np.array([8, 9])\n            &gt;&gt;&gt; residuals = np.random.normal(size=(3, 5)) # 3 steps, 5 boots\n            &gt;&gt;&gt; preds = forecaster._recursive_predict_bootstrapping(\n            ...     steps=3,\n            ...     last_window_values=last_window,\n            ...     sampled_residuals=residuals,\n            ...     use_binned_residuals=False,\n            ...     n_boot=5\n            ... )\n            &gt;&gt;&gt; preds.shape\n            (3, 5)\n        \"\"\"\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_)\n            if self.window_features is not None\n            else 0\n        )\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n        n_features = n_lags + n_window_features + n_exog\n\n        # Input matrix for prediction: shape (n_boot, n_features)\n        X = np.full((n_boot, n_features), fill_value=np.nan, dtype=float)\n\n        # Output predictions: shape (steps, n_boot)\n        predictions = np.full((steps, n_boot), fill_value=np.nan, dtype=float)\n\n        # Expand last_window to 2D: (window_size + steps, n_boot)\n        # Each column represents a separate bootstrap trajectory\n        last_window = np.tile(last_window_values[:, np.newaxis], (1, n_boot))\n        last_window = np.vstack([last_window, np.full((steps, n_boot), np.nan)])\n\n        estimator_name = type(self.estimator).__name__\n        is_linear = isinstance(self.estimator, LinearModel)\n        is_lightgbm = estimator_name == \"LGBMRegressor\"\n        is_xgboost = estimator_name == \"XGBRegressor\"\n\n        if is_linear:\n            coef = self.estimator.coef_\n            intercept = self.estimator.intercept_\n        elif is_lightgbm:\n            booster = self.estimator.booster_\n        elif is_xgboost:\n            booster = self.estimator.get_booster()\n\n        has_lags = self.lags is not None\n        has_window_features = self.window_features is not None\n        has_exog = exog_values is not None\n\n        for i in range(steps):\n\n            if has_lags:\n                for j, lag in enumerate(self.lags):\n                    X[:, j] = last_window[-(lag + steps - i), :]\n\n            if has_window_features:\n                window_data = last_window[: -(steps - i), :]\n                # transform accepts 2D: (window_length, n_boot) -&gt; (n_boot, n_stats)\n                # and concatenate along axis=1: (n_boot, total_window_features)\n                X[:, n_lags : n_lags + n_window_features] = np.concatenate(\n                    [wf.transform(window_data) for wf in self.window_features], axis=1\n                )\n\n            if has_exog:\n                X[:, n_lags + n_window_features :] = exog_values[i]\n\n            if is_linear:\n                pred = np.dot(X, coef) + intercept\n            elif is_lightgbm:\n                pred = booster.predict(X)\n            elif is_xgboost:\n                pred = booster.inplace_predict(X)\n            else:\n                pred = self.estimator.predict(X).ravel()\n\n            if use_binned_residuals:\n                # sampled_residuals is a 3D array: (n_bins, steps, n_boot)\n                boot_indices = np.arange(n_boot)\n                pred_bins = self.binner.transform(pred).astype(int)\n                pred += sampled_residuals[pred_bins, i, boot_indices]\n            else:\n                pred += sampled_residuals[i, :]\n\n            predictions[i, :] = pred\n            last_window[-(steps - i), :] = pred\n\n        return predictions\n\n    def predict(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; pd.Series:\n        \"\"\"\n        Predict future values recursively for the specified number of steps.\n\n        Args:\n            steps:\n                Number of future steps to predict.\n            last_window:\n                Optional last window of observed values to use for prediction. If None, uses the last window from training.\n                Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.\n            exog:\n                Optional exogenous variables for prediction. Can be a pandas Series or DataFrame.\n                Must have the same structure as the exogenous variables used in training. Defaults to None.\n            check_inputs:\n                Whether to perform input validation checks. Defaults to True.\n\n        Returns:\n            Pandas Series of predicted values for the specified number of steps,\n            indexed according to the prediction index constructed from the last window and the number of steps.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n            &gt;&gt;&gt; last_window = y.iloc[-3:]\n            &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n            &gt;&gt;&gt; predictions = forecaster.predict(\n            ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n            ... )\n            &gt;&gt;&gt; isinstance(predictions, pd.Series)\n            True\n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, _ = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        predictions = self._recursive_predict(\n            steps=steps, last_window_values=last_window_values, exog_values=exog_values\n        )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_dataframe(\n            df=pd.Series(predictions, name=\"pred\").to_frame(),\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n        predictions = predictions.iloc[:, 0]\n        predictions.index = prediction_index\n\n        return predictions\n\n    def predict_bootstrapping(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n        n_boot: int = 250,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: int = 123,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process.\n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions.\n        See the References section for more information.\n\n        Args:\n            steps:\n                Number of steps to predict.\n                - If steps is int, number of steps to predict.\n                - If str or pandas Datetime, the prediction will be up to that date.\n            last_window:\n                Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1).\n                If `last_window = None`, the values stored in `self.last_window_` are\n                used to calculate the initial predictors, and the predictions start\n                right after training data. Defaults to None.\n            exog:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n            n_boot:\n                Number of bootstrapping iterations to perform when estimating prediction\n                intervals. Defaults to 250.\n            use_in_sample_residuals:\n                If `True`, residuals from the training data are used as proxy of\n                prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values\n                (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state:\n                Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n        Returns:\n            Pandas DataFrame with predictions generated by bootstrapping. Shape: (steps, n_boot).\n\n        Raises:\n            ValueError:\n                If `steps` is not an integer or a valid date.\n            ValueError:\n                If `exog` is missing or has invalid shape.\n            ValueError:\n                If `n_boot` is not a positive integer.\n            ValueError:\n                If `use_in_sample_residuals=True` and `in_sample_residuals_` are not available.\n            ValueError:\n                If `use_in_sample_residuals=False` and `out_sample_residuals_` are not available.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; rng = np.random.default_rng(123)\n            &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; _ = forecaster.fit(y=y)\n            &gt;&gt;&gt; boot_preds = forecaster.predict_bootstrapping(steps=3, n_boot=5)\n            &gt;&gt;&gt; boot_preds.shape\n            (3, 5)\n\n        References:\n            .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n                   https://otexts.com/fpp3/prediction-intervals.html\n        \"\"\"\n\n        (\n            last_window_values,\n            exog_values,\n            prediction_index,\n            exog_index,  # Added missing exog_index and ignored steps return which is not there\n        ) = self._create_predict_inputs(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            predict_probabilistic=True,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n            check_inputs=True,\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            # Create 3D array with sampled residuals: (n_bins, steps, n_boot)\n            n_bins = len(residuals_by_bin)\n            sampled_residuals = np.stack(\n                [\n                    residuals_by_bin[k][\n                        rng.integers(\n                            low=0, high=len(residuals_by_bin[k]), size=(steps, n_boot)\n                        )\n                    ]\n                    for k in range(n_bins)\n                ],\n                axis=0,\n            )\n        else:\n            sampled_residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n            ]\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"X does not have valid feature names\",\n                category=UserWarning,\n            )\n            boot_predictions = self._recursive_predict_bootstrapping(\n                steps=steps,\n                last_window_values=last_window_values,\n                exog_values=exog_values,\n                sampled_residuals=sampled_residuals,\n                use_binned_residuals=use_binned_residuals,\n                n_boot=n_boot,\n            )\n\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(\n                boot_predictions\n            )\n\n        if self.transformer_y:\n            boot_predictions = transform_numpy(\n                array=boot_predictions,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=True,\n            )\n\n        boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n        boot_predictions = pd.DataFrame(\n            data=boot_predictions, index=prediction_index, columns=boot_columns\n        )\n\n        return boot_predictions\n\n    def _predict_interval_conformal(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n        nominal_coverage: float = 0.95,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate prediction intervals using the conformal prediction\n        split method [1]_.\n\n        Args:\n            steps:\n                Number of steps to predict.\n                - If steps is int, number of steps to predict.\n                - If str or pandas Datetime, the prediction will be up to that date.\n            last_window:\n                Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1).\n                If `last_window = None`, the values stored in` self.last_window_` are\n                used to calculate the initial predictors, and the predictions start\n                right after training data. Defaults to None.\n            exog:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n            nominal_coverage:\n                Nominal coverage, also known as expected coverage, of the prediction\n                intervals. Must be between 0 and 1. Defaults to 0.95.\n            use_in_sample_residuals:\n                If `True`, residuals from the training data are used as proxy of\n                prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values\n                (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n\n        Returns:\n            Pandas DataFrame with values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Raises:\n            ValueError:\n                If `nominal_coverage` is not between 0 and 1.\n            ValueError:\n                If inputs are invalid (checked by `_create_predict_inputs`).\n\n        Examples:\n            &gt;&gt;&gt; # Internal method, typically used via predict_interval(method='conformal')\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; rng = np.random.default_rng(123)\n            &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; _ = forecaster.fit(y=y)\n            &gt;&gt;&gt; preds = forecaster._predict_interval_conformal(steps=3, nominal_coverage=0.9)\n            &gt;&gt;&gt; preds.columns.tolist()\n            ['pred', 'lower_bound', 'upper_bound']\n\n        References:\n            .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n                   https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, exog_index = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                predict_probabilistic=True,\n                use_in_sample_residuals=use_in_sample_residuals,\n                use_binned_residuals=use_binned_residuals,\n                check_inputs=True,\n            )\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"X does not have valid feature names\",\n                category=UserWarning,\n            )\n            predictions = self._recursive_predict(\n                steps=steps,\n                last_window_values=last_window_values,\n                exog_values=exog_values,\n            )\n\n        if use_binned_residuals:\n            # Fallback to global residuals if bin is empty\n            if len(residuals) &gt; 0:\n                global_cf = np.quantile(np.abs(residuals), nominal_coverage)\n            else:\n                global_cf = np.nan\n\n            correction_factor_by_bin = {}\n            for k, v in residuals_by_bin.items():\n                if len(v) &gt; 0:\n                    correction_factor_by_bin[k] = np.quantile(\n                        np.abs(v), nominal_coverage\n                    )\n                else:\n                    correction_factor_by_bin[k] = global_cf\n\n            replace_func = np.vectorize(\n                lambda x: correction_factor_by_bin.get(x, global_cf)\n            )\n\n            predictions_bin = self.binner.transform(predictions)\n            correction_factor = replace_func(predictions_bin)\n        else:\n            correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n        lower_bound = predictions - correction_factor\n        upper_bound = predictions + correction_factor\n        predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        if self.transformer_y:\n            predictions = transform_numpy(\n                array=predictions,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=True,\n            )\n\n        predictions = pd.DataFrame(\n            data=predictions,\n            index=prediction_index,\n            columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n        )\n\n        return predictions\n\n    def predict_interval(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n        method: str = \"bootstrapping\",\n        interval: float | list[float] | tuple[float] = [5, 95],\n        n_boot: int = 250,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: int = 123,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead and estimate prediction intervals using either\n        bootstrapping or conformal prediction methods. Refer to the References\n        section for additional details on these methods.\n\n        Args:\n            steps:\n                Number of steps to predict.\n                - If steps is int, number of steps to predict.\n                - If str or pandas Datetime, the prediction will be up to that date.\n            last_window:\n                Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1).\n                If `last_window = None`, the values stored in `self.last_window_` are\n                used to calculate the initial predictors, and the predictions start\n                right after training data. Defaults to None.\n            exog:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n            method:\n                Technique used to estimate prediction intervals. Available options:\n                - 'bootstrapping': Bootstrapping is used to generate prediction\n                  intervals [1]_.\n                - 'conformal': Employs the conformal prediction split method for\n                  interval estimation [2]_.\n                Defaults to 'bootstrapping'.\n            interval:\n                Confidence level of the prediction interval. Interpretation depends\n                on the method used:\n                - If `float`, represents the nominal (expected) coverage (between 0\n                  and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n                  percentiles.\n                - If `list` or `tuple`, defines the exact percentiles to compute, which\n                  must be between 0 and 100 inclusive. For example, interval\n                  of 95% should be as `interval = [2.5, 97.5]`.\n                - When using `method='conformal'`, the interval must be a float or\n                  a list/tuple defining a symmetric interval.\n                Defaults to [5, 95].\n            n_boot:\n                Number of bootstrapping iterations to perform when estimating prediction\n                intervals. Defaults to 250.\n            use_in_sample_residuals:\n                If `True`, residuals from the training data are used as proxy of\n                prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values\n                (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state:\n                Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n        Returns:\n            Pandas DataFrame with values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Raises:\n            ValueError:\n                If `method` is not 'bootstrapping' or 'conformal'.\n            ValueError:\n                 If `interval` is invalid or not compatible with the chosen method.\n            ValueError:\n                If inputs (`steps`, `exog`, etc.) are invalid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; rng = np.random.default_rng(123)\n            &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; _ = forecaster.fit(y=y)\n            &gt;&gt;&gt; # Bootstrapping method\n            &gt;&gt;&gt; intervals_boot = forecaster.predict_interval(\n            ...     steps=3, method='bootstrapping', interval=[5, 95]\n            ... )\n            &gt;&gt;&gt; intervals_boot.columns.tolist()\n            ['pred', 'lower_bound', 'upper_bound']\n\n            &gt;&gt;&gt; # Conformal method\n            &gt;&gt;&gt; intervals_conf = forecaster.predict_interval(\n            ...     steps=3, method='conformal', interval=0.95\n            ... )\n            &gt;&gt;&gt; intervals_conf.columns.tolist()\n            ['pred', 'lower_bound', 'upper_bound']\n\n        References:\n            .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n                   https://otexts.com/fpp3/prediction-intervals.html\n            .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n                   https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n        \"\"\"\n\n        if method == \"bootstrapping\":\n\n            if isinstance(interval, (list, tuple)):\n                check_interval(interval=interval, ensure_symmetric_intervals=False)\n                interval = np.array(interval) / 100\n            else:\n                check_interval(alpha=interval, alpha_literal=\"interval\")\n                interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n            boot_predictions = self.predict_bootstrapping(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                n_boot=n_boot,\n                random_state=random_state,\n                use_in_sample_residuals=use_in_sample_residuals,\n                use_binned_residuals=use_binned_residuals,\n            )\n\n            predictions = self.predict(\n                steps=steps, last_window=last_window, exog=exog, check_inputs=False\n            )\n\n            predictions_interval = boot_predictions.quantile(\n                q=interval, axis=1\n            ).transpose()\n            predictions_interval.columns = [\"lower_bound\", \"upper_bound\"]\n            predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        elif method == \"conformal\":\n\n            if isinstance(interval, (list, tuple)):\n                check_interval(interval=interval, ensure_symmetric_intervals=True)\n                nominal_coverage = (interval[1] - interval[0]) / 100\n            else:\n                check_interval(alpha=interval, alpha_literal=\"interval\")\n                nominal_coverage = interval\n\n            predictions = self._predict_interval_conformal(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                nominal_coverage=nominal_coverage,\n                use_in_sample_residuals=use_in_sample_residuals,\n                use_binned_residuals=use_binned_residuals,\n            )\n        else:\n            raise ValueError(\n                f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n            )\n\n        return predictions\n\n    def _binning_in_sample_residuals(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Bin residuals according to the predicted value each residual is\n        associated with.\n        \"\"\"\n        residuals = y_true - y_pred\n\n        if self.binner_kwargs is not None:\n            self.binner.fit(y_pred)\n            if hasattr(self.binner, \"intervals_\"):\n                self.binner_intervals_ = self.binner.intervals_\n            else:\n                self.binner_intervals_ = {\n                    i: (self.binner.bins_[i - 1], self.binner.bins_[i])\n                    for i in range(1, len(self.binner.bins_))\n                }\n\n        if store_in_sample_residuals:\n            rng = np.random.default_rng(seed=random_state)\n\n            if self.binner_kwargs is not None:\n                data = pd.DataFrame({\"prediction\": y_pred, \"residuals\": residuals})\n                data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n                self.in_sample_residuals_by_bin_ = (\n                    data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n                )\n\n                max_sample = 10_000 // self.binner.n_bins\n                for k, v in self.in_sample_residuals_by_bin_.items():\n                    if len(v) &gt; max_sample:\n                        self.in_sample_residuals_by_bin_[k] = rng.choice(\n                            v, size=max_sample, replace=False\n                        )\n\n            if len(residuals) &gt; 10_000:\n                residuals = rng.choice(residuals, size=10_000, replace=False)\n\n            self.in_sample_residuals_ = residuals\n\n    def set_in_sample_residuals(\n        self,\n        y: pd.Series,\n        exog: pd.Series | pd.DataFrame | None = None,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set in-sample residuals in case they were not calculated during the\n        training process.\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_in_sample_residuals()`.\"\n            )\n\n        check_y(y=y)\n        y_index_range = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )[1][[0, -1]]\n\n        if not y_index_range.equals(self.training_range_):\n            raise IndexError(\n                f\"The index range of `y` does not match the range \"\n                f\"used during training. Please ensure the index is aligned \"\n                f\"with the training data.\\n\"\n                f\"    Expected : {self.training_range_}\\n\"\n                f\"    Received : {y_index_range}\"\n            )\n\n        (\n            X_train,\n            y_train,\n            _,\n            _,\n            _,\n            X_train_features_names_out_,\n            *_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        if not X_train_features_names_out_ == self.X_train_features_names_out_:\n            raise ValueError(\n                f\"Feature mismatch detected after matrix creation. The features \"\n                f\"generated from the provided data do not match those used during \"\n                f\"the training process. To correctly set in-sample residuals, \"\n                f\"ensure that the same data and preprocessing steps are applied.\\n\"\n                f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n                f\"    Current output  : {X_train_features_names_out_}\"\n            )\n\n        self._binning_in_sample_residuals(\n            y_true=y_train.to_numpy(),\n            y_pred=self.estimator.predict(X_train).ravel(),\n            store_in_sample_residuals=True,\n            random_state=random_state,\n        )\n\n    def set_out_sample_residuals(\n        self,\n        y_true: np.ndarray | pd.Series,\n        y_pred: np.ndarray | pd.Series,\n        append: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`.\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n\n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        if self.transformer_y:\n            y_true = transform_numpy(\n                array=y_true,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=False,\n            )\n            y_pred = transform_numpy(\n                array=y_pred,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=False,\n            )\n\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation :]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation :]\n\n        data = pd.DataFrame(\n            {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n        ).dropna()\n        y_pred = data[\"prediction\"].to_numpy()\n        residuals = data[\"residuals\"].to_numpy()\n\n        if self.binner is not None:\n            data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n            residuals_by_bin = (\n                data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n            )\n        else:\n            residuals_by_bin = {}\n\n        out_sample_residuals = (\n            np.array([])\n            if self.out_sample_residuals_ is None\n            else self.out_sample_residuals_\n        )\n        out_sample_residuals_by_bin = (\n            {}\n            if self.out_sample_residuals_by_bin_ is None\n            else self.out_sample_residuals_by_bin_\n        )\n        if append:\n            out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n            for k, v in residuals_by_bin.items():\n                if k in out_sample_residuals_by_bin:\n                    out_sample_residuals_by_bin[k] = np.concatenate(\n                        (out_sample_residuals_by_bin[k], v)\n                    )\n                else:\n                    out_sample_residuals_by_bin[k] = v\n        else:\n            out_sample_residuals = residuals\n            out_sample_residuals_by_bin = residuals_by_bin\n\n        if self.binner is not None:\n            max_samples = 10_000 // self.binner.n_bins\n            rng = np.random.default_rng(seed=random_state)\n\n            for k, v in out_sample_residuals_by_bin.items():\n                if len(v) &gt; max_samples:\n                    out_sample_residuals_by_bin[k] = rng.choice(\n                        v, size=max_samples, replace=False\n                    )\n\n            bin_keys = (\n                [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n            )\n            empty_bins = [\n                k\n                for k in bin_keys\n                if k not in out_sample_residuals_by_bin\n                or len(out_sample_residuals_by_bin[k]) == 0\n            ]\n\n            if empty_bins:\n                warnings.warn(\n                    f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                    f\"No predicted values fall in the interval \"\n                    f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                    f\"Empty bins will be filled with a random sample of residuals.\",\n                    ResidualsUsageWarning,\n                )\n                empty_bin_size = min(max_samples, len(out_sample_residuals))\n                for k in empty_bins:\n                    out_sample_residuals_by_bin[k] = rng.choice(\n                        a=out_sample_residuals, size=empty_bin_size, replace=False\n                    )\n\n        self.out_sample_residuals_ = out_sample_residuals\n        self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a ForecasterRecursive object is printed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the forecaster with key information about its configuration and state.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; print(forecaster)\n=========================\nForecasterRecursive\n=========================\nEstimator: LinearRegression\nLags: [1, 2, 3]\nWindow features: []\nWindow size: 3\nSeries name: None\nExogenous included: False\nExogenous names: None\nTransformer for y: None\nTransformer for exog: None\nWeight function included: False\nDifferentiation order: None\nTraining range: None\nTraining index type: None\nTraining index frequency: None\nEstimator parameters: {...}\nfit_kwargs: {...}\nCreation date: ...\nLast fit date: None\nspotforecast version: ...\nPython version: ...\nForecaster id: None\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a ForecasterRecursive object is printed.\n\n    Returns:\n        str: String representation of the forecaster with key information about its configuration and state.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; print(forecaster)  # doctest: +ELLIPSIS\n        =========================\n        ForecasterRecursive\n        =========================\n        Estimator: LinearRegression\n        Lags: [1, 2, 3]\n        Window features: []\n        Window size: 3\n        Series name: None\n        Exogenous included: False\n        Exogenous names: None\n        Transformer for y: None\n        Transformer for exog: None\n        Weight function included: False\n        Differentiation order: None\n        Training range: None\n        Training index type: None\n        Training index frequency: None\n        Estimator parameters: {...}\n        fit_kwargs: {...}\n        Creation date: ...\n        Last fit date: None\n        spotforecast version: ...\n        Python version: ...\n        Forecaster id: None\n\n    \"\"\"\n\n    params = (\n        self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n    )\n    exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Estimator: {type(self.estimator).__name__} \\n\"\n        f\"Lags: {self.lags} \\n\"\n        f\"Window features: {self.window_features_names} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Exogenous included: {self.exog_in_} \\n\"\n        f\"Exogenous names: {exog_names_in_} \\n\"\n        f\"Transformer for y: {self.transformer_y} \\n\"\n        f\"Transformer for exog: {self.transformer_exog} \\n\"\n        f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n        f\"Differentiation order: {self.differentiation} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Estimator parameters: {params} \\n\"\n        f\"fit_kwargs: {self.fit_kwargs} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"spotforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling. Only sets spotforecast_tags if not present, preserving custom tags.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"\n    Custom __setstate__ to ensure backward compatibility when unpickling.\n    Only sets __spotforecast_tags__ if not present, preserving custom tags.\n    \"\"\"\n    super().__setstate__(state)\n    if not hasattr(self, \"__spotforecast_tags__\"):\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",\n            \"forecasting_strategy\": \"recursive\",\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Public method to create training predictors and target values.</p> <p>This method is a public wrapper around the internal method <code>_create_train_X_y</code>, which generates the training predictors and target values based on the provided time series and exogenous variables. It ensures that the necessary transformations and feature engineering steps are applied to prepare the data for training the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Target series for training. Must be a pandas Series.</p> required <code>exog</code> <code>Union[Series, DataFrame, None]</code> <p>Optional exogenous variables for training. Can be a pandas Series or DataFrame. Must have the same index as <code>y</code> and cover the same time range. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Series, List[str], List[str], List[str], List[str], Dict[str, type], Dict[str, type]]</code> <p>Tuple containing: - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided). - y_train: Series of target values aligned with the predictors. - X_train_features_names_out_: List of all predictor feature names. - lags_names: List of lag feature names. - window_features_names: List of window feature names. - exog_names_in_: List of exogenous variable names (if exogenous variables are used). - exog_dtypes_in_: Dictionary of input data types for exogenous variables. - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=3,\n...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n... )\n&gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n...  exog_names_out, feature_names, exog_dtypes_in_,\n...  exog_dtypes_out_) = forecaster.create_train_X_y(y=y, exog=exog)\n&gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\nTrue\n&gt;&gt;&gt; isinstance(y_train, pd.Series)\nTrue\n&gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def create_train_X_y(\n    self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n) -&gt; Tuple[\n    pd.DataFrame,\n    pd.Series,\n    List[str],\n    List[str],\n    List[str],\n    List[str],\n    Dict[str, type],\n    Dict[str, type],\n]:\n    \"\"\"Public method to create training predictors and target values.\n\n    This method is a public wrapper around the internal method `_create_train_X_y`,\n    which generates the training predictors and target values based on the provided time series and exogenous variables.\n    It ensures that the necessary transformations and feature engineering steps are applied to prepare the data for training the forecaster.\n\n    Args:\n        y: Target series for training. Must be a pandas Series.\n        exog: Optional exogenous variables for training. Can be a pandas Series or DataFrame. Must have the same index as `y` and cover the same time range. Defaults to None.\n\n    Returns:\n        Tuple containing:\n            - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided).\n            - y_train: Series of target values aligned with the predictors.\n            - X_train_features_names_out_: List of all predictor feature names.\n            - lags_names: List of lag feature names.\n            - window_features_names: List of window feature names.\n            - exog_names_in_: List of exogenous variable names (if exogenous variables are used).\n            - exog_dtypes_in_: Dictionary of input data types for exogenous variables.\n            - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=3,\n        ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n        ... )\n        &gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n        ...  exog_names_out, feature_names, exog_dtypes_in_,\n        ...  exog_dtypes_out_) = forecaster.create_train_X_y(y=y, exog=exog)\n        &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n        True\n        &gt;&gt;&gt; isinstance(y_train, pd.Series)\n        True\n        &gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\n        True\n\n    \"\"\"\n    return self._create_train_X_y(y=y, exog=exog)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.fit","title":"<code>fit(y, exog=None, store_last_window=True, store_in_sample_residuals=False, random_state=123, suppress_warnings=False)</code>","text":"<p>Fit the forecaster to the training data.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Target series for training. Must be a pandas Series.</p> required <code>exog</code> <code>Union[Series, DataFrame, None]</code> <p>Optional exogenous variables for training. Can be a pandas Series or DataFrame.Must have the same index as <code>y</code> and cover the same time range. Defaults to None.</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether to store the last window of the training series for use in prediction. Defaults to True.</p> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>Whether to store in-sample residuals after fitting, which can be used for certain probabilistic prediction methods. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility when sampling residuals if <code>store_in_sample_residuals</code> is True. Defaults to 123.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>Whether to suppress warnings during fitting, such as those related to insufficient data length for lags or window features. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=3,\n...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n... )\n&gt;&gt;&gt; forecaster.fit(y=y, exog=exog, store_in_sample_residuals=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Union[pd.Series, pd.DataFrame, None] = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    suppress_warnings: bool = False,\n) -&gt; None:\n    \"\"\"\n    Fit the forecaster to the training data.\n\n    Args:\n        y:\n              Target series for training. Must be a pandas Series.\n        exog:\n              Optional exogenous variables for training. Can be a pandas Series or DataFrame.Must have the same index as `y` and cover the same time range. Defaults to None.\n        store_last_window:\n              Whether to store the last window of the training series for use in prediction. Defaults to True.\n        store_in_sample_residuals:\n              Whether to store in-sample residuals after fitting, which can be used for certain probabilistic prediction methods. Defaults to False.\n        random_state:\n              Random seed for reproducibility when sampling residuals if `store_in_sample_residuals` is True. Defaults to 123.\n        suppress_warnings:\n              Whether to suppress warnings during fitting, such as those related to insufficient data length for lags or window features. Defaults to False.\n\n    Returns:\n        None\n\n    Examples:\n             &gt;&gt;&gt; import numpy as np\n             &gt;&gt;&gt; import pandas as pd\n             &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n             &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n             &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n             &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n             &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n             &gt;&gt;&gt; forecaster = ForecasterRecursive(\n             ...     estimator=LinearRegression(),\n             ...     lags=3,\n             ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n             ... )\n             &gt;&gt;&gt; forecaster.fit(y=y, exog=exog, store_in_sample_residuals=True)\n    \"\"\"\n\n    # Reset values\n    self.is_fitted = False\n    self.fit_date = None\n\n    (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_,\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    SAMPLE_WEIGHT_NAME = \"sample_weight\"\n    if self.weight_func is not None:\n        sample_weight, _, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=self.estimator,\n            weight_func=self.weight_func,\n            series_weights=None,\n        )\n        sample_weight = sample_weight(y.index[self.window_size :])\n        self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n    self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n        del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n    # Store attributes\n    self.last_window_ = y.iloc[-self.window_size :].copy()\n    self.index_type_ = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq_ = y.index.freqstr\n    else:\n        try:\n            self.index_freq_ = y.index.step\n        except AttributeError:\n            self.index_freq_ = None\n\n    self.training_range_ = y.index[[0, -1]]\n    self.series_name_in_ = y.name\n    self.exog_in_ = exog is not None\n    self.exog_names_in_ = exog_names_in_\n    self.exog_type_in_ = type(exog) if exog is not None else None\n    self.exog_dtypes_in_ = exog_dtypes_in_\n    self.exog_dtypes_out_ = exog_dtypes_out_\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n    self.X_train_exog_names_out_ = X_train_exog_names_out_\n    self.X_train_features_names_out_ = X_train_features_names_out_\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    self.in_sample_residuals_ = None\n    self.binner_intervals_ = None\n    self.in_sample_residuals_by_bin_ = None\n\n    y_pred = self.estimator.predict(X_train)\n    self._binning_in_sample_residuals(\n        y_true=y_train.to_numpy(),\n        y_pred=y_pred,\n        store_in_sample_residuals=store_in_sample_residuals,\n        random_state=random_state,\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters for this forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, will return the parameters for this forecaster and contained sub-objects that are estimators.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>params</code> <code>Dict[str, object]</code> <p>Dictionary of parameter names mapped to their values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; forecaster.get_params()\n{\n    'estimator': LinearRegression(), 'lags': 3, 'window_features': None,\n    'transformer_y': None, 'transformer_exog': None, 'weight_func': None,\n    'differentiation': None, 'fit_kwargs': {}, 'binner_kwargs': None, 'forecaster_id': '...'}\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; Dict[str, object]:\n    \"\"\"\n    Get parameters for this forecaster.\n\n    Args:\n        deep: If True, will return the parameters for this forecaster and\n            contained sub-objects that are estimators.\n\n    Returns:\n        params: Dictionary of parameter names mapped to their values.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; forecaster.get_params()  # doctest: +ELLIPSIS\n        {\n            'estimator': LinearRegression(), 'lags': 3, 'window_features': None,\n            'transformer_y': None, 'transformer_exog': None, 'weight_func': None,\n            'differentiation': None, 'fit_kwargs': {}, 'binner_kwargs': None, 'forecaster_id': '...'}\n    \"\"\"\n    params = {}\n    for key in [\n        \"estimator\",\n        \"lags\",\n        \"window_features\",\n        \"transformer_y\",\n        \"transformer_exog\",\n        \"weight_func\",\n        \"differentiation\",\n        \"fit_kwargs\",\n        \"binner_kwargs\",\n        \"forecaster_id\",\n    ]:\n        if hasattr(self, key):\n            params[key] = getattr(self, key)\n\n    if not deep:\n        return params\n\n    if hasattr(self, \"estimator\") and self.estimator is not None:\n        if hasattr(self.estimator, \"get_params\"):\n            for key, value in self.estimator.get_params(deep=True).items():\n                params[f\"estimator__{key}\"] = value\n\n    return params\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.predict","title":"<code>predict(steps, last_window=None, exog=None, check_inputs=True)</code>","text":"<p>Predict future values recursively for the specified number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int | str | Timestamp</code> <p>Number of future steps to predict.</p> required <code>last_window</code> <code>Union[Series, DataFrame, None]</code> <p>Optional last window of observed values to use for prediction. If None, uses the last window from training. Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.</p> <code>None</code> <code>exog</code> <code>Union[Series, DataFrame, None]</code> <p>Optional exogenous variables for prediction. Can be a pandas Series or DataFrame. Must have the same structure as the exogenous variables used in training. Defaults to None.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>Whether to perform input validation checks. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Series</code> <p>Pandas Series of predicted values for the specified number of steps,</p> <code>Series</code> <p>indexed according to the prediction index constructed from the last window and the number of steps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=3,\n...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n... )\n&gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n&gt;&gt;&gt; last_window = y.iloc[-3:]\n&gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n&gt;&gt;&gt; predictions = forecaster.predict(\n...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n... )\n&gt;&gt;&gt; isinstance(predictions, pd.Series)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def predict(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: Union[pd.Series, pd.DataFrame, None] = None,\n    exog: Union[pd.Series, pd.DataFrame, None] = None,\n    check_inputs: bool = True,\n) -&gt; pd.Series:\n    \"\"\"\n    Predict future values recursively for the specified number of steps.\n\n    Args:\n        steps:\n            Number of future steps to predict.\n        last_window:\n            Optional last window of observed values to use for prediction. If None, uses the last window from training.\n            Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.\n        exog:\n            Optional exogenous variables for prediction. Can be a pandas Series or DataFrame.\n            Must have the same structure as the exogenous variables used in training. Defaults to None.\n        check_inputs:\n            Whether to perform input validation checks. Defaults to True.\n\n    Returns:\n        Pandas Series of predicted values for the specified number of steps,\n        indexed according to the prediction index constructed from the last window and the number of steps.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=3,\n        ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n        &gt;&gt;&gt; last_window = y.iloc[-3:]\n        &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n        &gt;&gt;&gt; predictions = forecaster.predict(\n        ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n        ... )\n        &gt;&gt;&gt; isinstance(predictions, pd.Series)\n        True\n    \"\"\"\n\n    last_window_values, exog_values, prediction_index, _ = (\n        self._create_predict_inputs(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            check_inputs=check_inputs,\n        )\n    )\n\n    predictions = self._recursive_predict(\n        steps=steps, last_window_values=last_window_values, exog_values=exog_values\n    )\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = transform_dataframe(\n        df=pd.Series(predictions, name=\"pred\").to_frame(),\n        transformer=self.transformer_y,\n        fit=False,\n        inverse_transform=True,\n    )\n\n    predictions = predictions.iloc[:, 0]\n    predictions.index = prediction_index\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions. See the References section for more information.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int | str | Timestamp</code> <p>Number of steps to predict. - If steps is int, number of steps to predict. - If str or pandas Datetime, the prediction will be up to that date.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data. Defaults to None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable/s included as predictor/s. Defaults to None.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with predictions generated by bootstrapping. Shape: (steps, n_boot).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>steps</code> is not an integer or a valid date.</p> <code>ValueError</code> <p>If <code>exog</code> is missing or has invalid shape.</p> <code>ValueError</code> <p>If <code>n_boot</code> is not a positive integer.</p> <code>ValueError</code> <p>If <code>use_in_sample_residuals=True</code> and <code>in_sample_residuals_</code> are not available.</p> <code>ValueError</code> <p>If <code>use_in_sample_residuals=False</code> and <code>out_sample_residuals_</code> are not available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; rng = np.random.default_rng(123)\n&gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; _ = forecaster.fit(y=y)\n&gt;&gt;&gt; boot_preds = forecaster.predict_bootstrapping(steps=3, n_boot=5)\n&gt;&gt;&gt; boot_preds.shape\n(3, 5)\n</code></pre> References <p>.. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process.\n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions.\n    See the References section for more information.\n\n    Args:\n        steps:\n            Number of steps to predict.\n            - If steps is int, number of steps to predict.\n            - If str or pandas Datetime, the prediction will be up to that date.\n        last_window:\n            Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data. Defaults to None.\n        exog:\n            Exogenous variable/s included as predictor/s. Defaults to None.\n        n_boot:\n            Number of bootstrapping iterations to perform when estimating prediction\n            intervals. Defaults to 250.\n        use_in_sample_residuals:\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals:\n            If `True`, residuals are selected based on the predicted values\n            (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state:\n            Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n    Returns:\n        Pandas DataFrame with predictions generated by bootstrapping. Shape: (steps, n_boot).\n\n    Raises:\n        ValueError:\n            If `steps` is not an integer or a valid date.\n        ValueError:\n            If `exog` is missing or has invalid shape.\n        ValueError:\n            If `n_boot` is not a positive integer.\n        ValueError:\n            If `use_in_sample_residuals=True` and `in_sample_residuals_` are not available.\n        ValueError:\n            If `use_in_sample_residuals=False` and `out_sample_residuals_` are not available.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; rng = np.random.default_rng(123)\n        &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; _ = forecaster.fit(y=y)\n        &gt;&gt;&gt; boot_preds = forecaster.predict_bootstrapping(steps=3, n_boot=5)\n        &gt;&gt;&gt; boot_preds.shape\n        (3, 5)\n\n    References:\n        .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n               https://otexts.com/fpp3/prediction-intervals.html\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index,\n        exog_index,  # Added missing exog_index and ignored steps return which is not there\n    ) = self._create_predict_inputs(\n        steps=steps,\n        last_window=last_window,\n        exog=exog,\n        predict_probabilistic=True,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        check_inputs=True,\n    )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    rng = np.random.default_rng(seed=random_state)\n    if use_binned_residuals:\n        # Create 3D array with sampled residuals: (n_bins, steps, n_boot)\n        n_bins = len(residuals_by_bin)\n        sampled_residuals = np.stack(\n            [\n                residuals_by_bin[k][\n                    rng.integers(\n                        low=0, high=len(residuals_by_bin[k]), size=(steps, n_boot)\n                    )\n                ]\n                for k in range(n_bins)\n            ],\n            axis=0,\n        )\n    else:\n        sampled_residuals = residuals[\n            rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n        ]\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n        )\n        boot_predictions = self._recursive_predict_bootstrapping(\n            steps=steps,\n            last_window_values=last_window_values,\n            exog_values=exog_values,\n            sampled_residuals=sampled_residuals,\n            use_binned_residuals=use_binned_residuals,\n            n_boot=n_boot,\n        )\n\n    if self.differentiation is not None:\n        boot_predictions = self.differentiator.inverse_transform_next_window(\n            boot_predictions\n        )\n\n    if self.transformer_y:\n        boot_predictions = transform_numpy(\n            array=boot_predictions,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n    boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n    boot_predictions = pd.DataFrame(\n        data=boot_predictions, index=prediction_index, columns=boot_columns\n    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, method='bootstrapping', interval=[5, 95], n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123)</code>","text":"<p>Predict n steps ahead and estimate prediction intervals using either bootstrapping or conformal prediction methods. Refer to the References section for additional details on these methods.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int | str | Timestamp</code> <p>Number of steps to predict. - If steps is int, number of steps to predict. - If str or pandas Datetime, the prediction will be up to that date.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data. Defaults to None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable/s included as predictor/s. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options: - 'bootstrapping': Bootstrapping is used to generate prediction   intervals [1]. - 'conformal': Employs the conformal prediction split method for   interval estimation [2]. Defaults to 'bootstrapping'.</p> <code>'bootstrapping'</code> <code>interval</code> <code>float | list[float] | tuple[float]</code> <p>Confidence level of the prediction interval. Interpretation depends on the method used: - If <code>float</code>, represents the nominal (expected) coverage (between 0   and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>   percentiles. - If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which   must be between 0 and 100 inclusive. For example, interval   of 95% should be as <code>interval = [2.5, 97.5]</code>. - When using <code>method='conformal'</code>, the interval must be a float or   a list/tuple defining a symmetric interval. Defaults to [5, 95].</p> <code>[5, 95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with values predicted by the forecaster and their estimated interval.</p> <code>DataFrame</code> <ul> <li>pred: predictions.</li> </ul> <code>DataFrame</code> <ul> <li>lower_bound: lower bound of the interval.</li> </ul> <code>DataFrame</code> <ul> <li>upper_bound: upper bound of the interval.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not 'bootstrapping' or 'conformal'.</p> <code>ValueError</code> <p>If <code>interval</code> is invalid or not compatible with the chosen method.</p> <code>ValueError</code> <p>If inputs (<code>steps</code>, <code>exog</code>, etc.) are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; rng = np.random.default_rng(123)\n&gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; _ = forecaster.fit(y=y)\n&gt;&gt;&gt; # Bootstrapping method\n&gt;&gt;&gt; intervals_boot = forecaster.predict_interval(\n...     steps=3, method='bootstrapping', interval=[5, 95]\n... )\n&gt;&gt;&gt; intervals_boot.columns.tolist()\n['pred', 'lower_bound', 'upper_bound']\n</code></pre> <pre><code>&gt;&gt;&gt; # Conformal method\n&gt;&gt;&gt; intervals_conf = forecaster.predict_interval(\n...     steps=3, method='conformal', interval=0.95\n... )\n&gt;&gt;&gt; intervals_conf.columns.tolist()\n['pred', 'lower_bound', 'upper_bound']\n</code></pre> References <p>.. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    method: str = \"bootstrapping\",\n    interval: float | list[float] | tuple[float] = [5, 95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using either\n    bootstrapping or conformal prediction methods. Refer to the References\n    section for additional details on these methods.\n\n    Args:\n        steps:\n            Number of steps to predict.\n            - If steps is int, number of steps to predict.\n            - If str or pandas Datetime, the prediction will be up to that date.\n        last_window:\n            Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data. Defaults to None.\n        exog:\n            Exogenous variable/s included as predictor/s. Defaults to None.\n        method:\n            Technique used to estimate prediction intervals. Available options:\n            - 'bootstrapping': Bootstrapping is used to generate prediction\n              intervals [1]_.\n            - 'conformal': Employs the conformal prediction split method for\n              interval estimation [2]_.\n            Defaults to 'bootstrapping'.\n        interval:\n            Confidence level of the prediction interval. Interpretation depends\n            on the method used:\n            - If `float`, represents the nominal (expected) coverage (between 0\n              and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n              percentiles.\n            - If `list` or `tuple`, defines the exact percentiles to compute, which\n              must be between 0 and 100 inclusive. For example, interval\n              of 95% should be as `interval = [2.5, 97.5]`.\n            - When using `method='conformal'`, the interval must be a float or\n              a list/tuple defining a symmetric interval.\n            Defaults to [5, 95].\n        n_boot:\n            Number of bootstrapping iterations to perform when estimating prediction\n            intervals. Defaults to 250.\n        use_in_sample_residuals:\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals:\n            If `True`, residuals are selected based on the predicted values\n            (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state:\n            Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n    Returns:\n        Pandas DataFrame with values predicted by the forecaster and their estimated interval.\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    Raises:\n        ValueError:\n            If `method` is not 'bootstrapping' or 'conformal'.\n        ValueError:\n             If `interval` is invalid or not compatible with the chosen method.\n        ValueError:\n            If inputs (`steps`, `exog`, etc.) are invalid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; rng = np.random.default_rng(123)\n        &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; _ = forecaster.fit(y=y)\n        &gt;&gt;&gt; # Bootstrapping method\n        &gt;&gt;&gt; intervals_boot = forecaster.predict_interval(\n        ...     steps=3, method='bootstrapping', interval=[5, 95]\n        ... )\n        &gt;&gt;&gt; intervals_boot.columns.tolist()\n        ['pred', 'lower_bound', 'upper_bound']\n\n        &gt;&gt;&gt; # Conformal method\n        &gt;&gt;&gt; intervals_conf = forecaster.predict_interval(\n        ...     steps=3, method='conformal', interval=0.95\n        ... )\n        &gt;&gt;&gt; intervals_conf.columns.tolist()\n        ['pred', 'lower_bound', 'upper_bound']\n\n    References:\n        .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n               https://otexts.com/fpp3/prediction-intervals.html\n        .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n               https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n    \"\"\"\n\n    if method == \"bootstrapping\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=False)\n            interval = np.array(interval) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n        boot_predictions = self.predict_bootstrapping(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            n_boot=n_boot,\n            random_state=random_state,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n        )\n\n        predictions = self.predict(\n            steps=steps, last_window=last_window, exog=exog, check_inputs=False\n        )\n\n        predictions_interval = boot_predictions.quantile(\n            q=interval, axis=1\n        ).transpose()\n        predictions_interval.columns = [\"lower_bound\", \"upper_bound\"]\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    elif method == \"conformal\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            nominal_coverage=nominal_coverage,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n        )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n        )\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.set_in_sample_residuals","title":"<code>set_in_sample_residuals(y, exog=None, random_state=123)</code>","text":"<p>Set in-sample residuals in case they were not calculated during the training process.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )[1][[0, -1]]\n\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    (\n        X_train,\n        y_train,\n        _,\n        _,\n        _,\n        X_train_features_names_out_,\n        *_,\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y_true=y_train.to_numpy(),\n        y_pred=self.estimator.predict(X_train).ravel(),\n        store_in_sample_residuals=True,\n        random_state=random_state,\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.set_out_sample_residuals","title":"<code>set_out_sample_residuals(y_true, y_pred, append=False, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    if self.transformer_y:\n        y_true = transform_numpy(\n            array=y_true,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        y_pred = transform_numpy(\n            array=y_pred,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n\n    if self.differentiation is not None:\n        differentiator = copy(self.differentiator)\n        differentiator.set_params(window_size=None)\n        y_true = differentiator.fit_transform(y_true)[self.differentiation :]\n        y_pred = differentiator.fit_transform(y_pred)[self.differentiation :]\n\n    data = pd.DataFrame(\n        {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n    ).dropna()\n    y_pred = data[\"prediction\"].to_numpy()\n    residuals = data[\"residuals\"].to_numpy()\n\n    if self.binner is not None:\n        data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = (\n            data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n        )\n    else:\n        residuals_by_bin = {}\n\n    out_sample_residuals = (\n        np.array([])\n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {}\n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    if self.binner is not None:\n        max_samples = 10_000 // self.binner.n_bins\n        rng = np.random.default_rng(seed=random_state)\n\n        for k, v in out_sample_residuals_by_bin.items():\n            if len(v) &gt; max_samples:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    v, size=max_samples, replace=False\n                )\n\n        bin_keys = (\n            [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n        )\n        empty_bins = [\n            k\n            for k in bin_keys\n            if k not in out_sample_residuals_by_bin\n            or len(out_sample_residuals_by_bin[k]) == 0\n        ]\n\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\",\n                ResidualsUsageWarning,\n            )\n            empty_bin_size = min(max_samples, len(out_sample_residuals))\n            for k in empty_bins:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    a=out_sample_residuals, size=empty_bin_size, replace=False\n                )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.set_params","title":"<code>set_params(params=None, **kwargs)</code>","text":"<p>Set the parameters of this forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, object]</code> <p>Optional dictionary of parameter names mapped to their new values. If provided, these parameters are set first.</p> <code>None</code> <code>**kwargs</code> <code>object</code> <p>Dictionary of parameter names mapped to their new values. Parameters can be for the forecaster itself or for the contained estimator (using the <code>estimator__</code> prefix).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'ForecasterRecursive'</code> <p>The forecaster instance with updated parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; forecaster.set_params(estimator__fit_intercept=False)\n&gt;&gt;&gt; forecaster.estimator.get_params()[\"fit_intercept\"]\nFalse\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def set_params(\n    self, params: Dict[str, object] = None, **kwargs: object\n) -&gt; \"ForecasterRecursive\":\n    \"\"\"\n    Set the parameters of this forecaster.\n\n    Args:\n        params: Optional dictionary of parameter names mapped to their new values.\n            If provided, these parameters are set first.\n        **kwargs: Dictionary of parameter names mapped to their new values.\n            Parameters can be for the forecaster itself or for the contained estimator (using the `estimator__` prefix).\n\n    Returns:\n        self: The forecaster instance with updated parameters.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; forecaster.set_params(estimator__fit_intercept=False)\n        &gt;&gt;&gt; forecaster.estimator.get_params()[\"fit_intercept\"]\n        False\n    \"\"\"\n\n    # Merge params dict and kwargs\n    all_params = {}\n    if params is not None:\n        all_params.update(params)\n    all_params.update(kwargs)\n\n    if not all_params:\n        return self\n\n    valid_params = self.get_params(deep=True)\n    nested_params = {}\n\n    for key, value in all_params.items():\n        if key not in valid_params and \"__\" not in key:\n            # Relaxed check for now\n            pass\n\n        if \"__\" in key:\n            obj_name, param_name = key.split(\"__\", 1)\n            if obj_name not in nested_params:\n                nested_params[obj_name] = {}\n            nested_params[obj_name][param_name] = value\n        else:\n            setattr(self, key, value)\n\n    for obj_name, obj_params in nested_params.items():\n        if hasattr(self, obj_name):\n            obj = getattr(self, obj_name)\n            if hasattr(obj, \"set_params\"):\n                obj.set_params(**obj_params)\n            else:\n                for param_name, value in obj_params.items():\n                    setattr(obj, param_name, value)\n\n    return self\n</code></pre>"},{"location":"api/forecaster/#base-forecaster","title":"Base Forecaster","text":""},{"location":"api/forecaster/#base","title":"base","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base","title":"<code>spotforecast2_safe.forecaster.base</code>","text":"<p>ForecasterBase class.</p> <p>This module contains the base class for all forecasters in spotforecast2_safe and spotforecast. All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Examples:</p> <p>Create a custom forecaster inheriting from ForecasterBase:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.base import ForecasterBase\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; class MyForecaster(ForecasterBase):\n...     def __init__(self, estimator):\n...         self.estimator = estimator\n...         self.__spotforecast_tags__ = {'hide_lags': True}\n...     def create_train_X_y(self, y, exog=None):\n...         return pd.DataFrame(), pd.Series(dtype=float)\n...     def fit(self, y, exog=None):\n...         pass\n...     def predict(self, steps, last_window=None, exog=None):\n...         return pd.Series(np.zeros(steps))\n...     def set_params(self, params):\n...         pass\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = MyForecaster(estimator=Ridge())\n&gt;&gt;&gt; forecaster\nMyForecaster(estimator=Ridge())\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase","title":"<code>ForecasterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all forecasters in spotforecast2.</p> <p>All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Attributes:</p> Name Type Description <code>__spotforecast_tags__</code> <p>Dictionary with forecaster tags that characterize the behavior of the forecaster.</p> <p>Examples:</p> <p>To see all abstract methods that need to be implemented:</p> <pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.base import ForecasterBase\n&gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n['create_train_X_y', 'fit', 'predict', 'set_params']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>class ForecasterBase(ABC):\n    \"\"\"Base class for all forecasters in spotforecast2.\n\n    All forecasters should specify all the parameters that can be set at\n    the class level in their __init__.\n\n    Attributes:\n        __spotforecast_tags__: Dictionary with forecaster tags that characterize\n            the behavior of the forecaster.\n\n    Examples:\n        To see all abstract methods that need to be implemented:\n\n        &gt;&gt;&gt; import inspect\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.base import ForecasterBase\n        &gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n        ['create_train_X_y', 'fit', 'predict', 'set_params']\n    \"\"\"\n\n    def _preprocess_repr(\n        self,\n        estimator: object | None = None,\n        training_range_: dict[str, str] | None = None,\n        series_names_in_: list[str] | None = None,\n        exog_names_in_: list[str] | None = None,\n        transformer_series: object | dict[str, object] | None = None,\n    ) -&gt; tuple[str, str | None, str | None, str | None, str | None]:\n        \"\"\"Prepare the information to be displayed when a Forecaster object is printed.\n\n        Args:\n            estimator: Estimator object. Default is None.\n            training_range_: Training range. Only used for ForecasterRecursiveMultiSeries.\n                Default is None.\n            series_names_in_: Names of the series used in the forecaster.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n            exog_names_in_: Names of the exogenous variables used in the forecaster.\n                Default is None.\n            transformer_series: Transformer used in the series.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n\n        Returns:\n            Tuple containing params (estimator parameters string), training_range_\n            (training range string representation), series_names_in_ (series names\n            string representation), exog_names_in_ (exogenous variable names string\n            representation), and transformer_series (transformer string representation).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; estimator = Ridge(alpha=0.5)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=estimator, lags=3)\n            &gt;&gt;&gt; params, tr, sn, en, ts = forecaster._preprocess_repr(estimator=estimator)\n            &gt;&gt;&gt; params\n            \"{'alpha': 0.5, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\"\n        \"\"\"\n\n        if estimator is not None:\n            if isinstance(estimator, Pipeline):\n                name_pipe_steps = tuple(\n                    name + \"__\" for name in estimator.named_steps.keys()\n                )\n                params = {\n                    key: value\n                    for key, value in estimator.get_params().items()\n                    if key.startswith(name_pipe_steps)\n                }\n            else:\n                params = estimator.get_params()\n            params = str(params)\n        else:\n            params = None\n\n        if training_range_ is not None:\n            training_range_ = [\n                f\"'{k}': {v.astype(str).to_list()}\" for k, v in training_range_.items()\n            ]\n            if len(training_range_) &gt; 10:\n                training_range_ = training_range_[:5] + [\"...\"] + training_range_[-5:]\n            training_range_ = \", \".join(training_range_)\n\n        if series_names_in_ is not None:\n            if len(series_names_in_) &gt; 50:\n                series_names_in_ = (\n                    series_names_in_[:25] + [\"...\"] + series_names_in_[-25:]\n                )\n            series_names_in_ = \", \".join(series_names_in_)\n\n        if exog_names_in_ is not None:\n            if len(exog_names_in_) &gt; 50:\n                exog_names_in_ = exog_names_in_[:25] + [\"...\"] + exog_names_in_[-25:]\n            exog_names_in_ = \", \".join(exog_names_in_)\n\n        if transformer_series is not None:\n            if isinstance(transformer_series, dict):\n                transformer_series = [\n                    f\"'{k}': {v}\" for k, v in transformer_series.items()\n                ]\n                if len(transformer_series) &gt; 10:\n                    transformer_series = (\n                        transformer_series[:5] + [\"...\"] + transformer_series[-5:]\n                    )\n                transformer_series = \", \".join(transformer_series)\n            else:\n                transformer_series = str(transformer_series)\n\n        return (\n            params,\n            training_range_,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        )\n\n    def _format_text_repr(\n        self,\n        text: str,\n        max_text_length: int = 58,\n        width: int = 80,\n        indent: str = \"    \",\n    ) -&gt; str:\n        \"\"\"Format text for __repr__ method.\n\n        Args:\n            text: Text to format.\n            max_text_length: Maximum length of the text before wrapping. Default is 58.\n            width: Maximum width of the text. Default is 80.\n            indent: Indentation of the text. Default is four spaces.\n\n        Returns:\n            Formatted text string with proper wrapping and indentation.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster._format_text_repr(\"Short text\")\n            'Short text'\n        \"\"\"\n\n        if text is not None and len(text) &gt; max_text_length:\n            text = \"\\n    \" + textwrap.fill(\n                str(text), width=width, subsequent_indent=indent\n            )\n\n        return text\n\n    @abstractmethod\n    def create_train_X_y(\n        self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned.\n                Default is None.\n\n        Returns:\n            Tuple containing X_train (training values/predictors with shape\n            (len(y) - max_lag, len(lags))) and y_train (target values of the\n            time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n            &gt;&gt;&gt; X_train.head(2)\n               lag_1  lag_2  lag_3\n            3    2.0    1.0    0.0\n            4    3.0    2.0    1.0\n            &gt;&gt;&gt; y_train.head(2)\n            3    3\n            4    4\n            Name: y, dtype: int64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n        \"\"\"Training Forecaster.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned so\n                that y[i] is regressed on exog[i]. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.is_fitted\n            True\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n    ) -&gt; pd.Series:\n        \"\"\"Predict n steps ahead.\n\n        Args:\n            steps: Number of steps to predict.\n            last_window: Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1). If None, the values stored in\n                last_window are used to calculate the initial predictors, and the\n                predictions start right after training data. Default is None.\n            exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n        Returns:\n            Predicted values as a pandas Series.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            10    9.5\n            11    9.0\n            12    8.5\n            Name: pred, dtype: float64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def set_params(self, params: dict[str, object]) -&gt; None:\n        \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n        Args:\n            params: Parameters values dictionary.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n            &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n            &gt;&gt;&gt; forecaster.estimator.alpha\n            0.5\n        \"\"\"\n\n        pass\n\n    def set_lags(\n        self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute lags.\n\n        Attributes max_lag and window_size are also updated.\n\n        Args:\n            lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n                If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n                or range: include only lags present in lags, all elements must be int.\n                If None: no lags are included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.set_lags(lags=5)\n            &gt;&gt;&gt; forecaster.lags\n            array([1, 2, 3, 4, 5])\n        \"\"\"\n\n        pass\n\n    def set_window_features(\n        self, window_features: object | list[object] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute window_features.\n\n        Attributes max_size_window_features, window_features_names,\n        window_features_class_names and window_size are also updated.\n\n        Args:\n            window_features: Instance or list of instances used to create window features.\n                Window features are created from the original time series and are\n                included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n            &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n            &gt;&gt;&gt; forecaster.window_features\n            [RollingFeatures(stats=['mean'], window_sizes=[3])]\n        \"\"\"\n\n        pass\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            Dictionary with forecaster tags describing behavior and capabilities.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; tags['forecaster_task']\n            'regression'\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.summary()\n            ForecasterRecursive\n            ===================\n            Estimator: Ridge()\n            Lags: [1 2 3]\n            ...\n        \"\"\"\n\n        print(self.__repr__())\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n        This method is called when an object is unpickled (deserialized).\n        It handles the migration of deprecated attributes to their new names.\n\n        Args:\n            state: The state dictionary from the pickled object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pickle\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n            &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n        \"\"\"\n\n        if \"regressor\" in state and \"estimator\" not in state:\n            state[\"estimator\"] = state.pop(\"regressor\")\n\n        self.__dict__.update(state)\n\n    @property\n    def regressor(self) -&gt; Any:\n        \"\"\"Deprecated property. Use estimator instead.\n\n        Returns:\n            The estimator object.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\n            Ridge()\n        \"\"\"\n        warnings.warn(\n            \"The `regressor` attribute is deprecated and will be removed in future \"\n            \"versions. Use `estimator` instead.\",\n            FutureWarning,\n        )\n        return self.estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.regressor","title":"<code>regressor</code>  <code>property</code>","text":"<p>Deprecated property. Use estimator instead.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The estimator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\nRidge()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> <p>This method is called when an object is unpickled (deserialized). It handles the migration of deprecated attributes to their new names.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The state dictionary from the pickled object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n&gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n    This method is called when an object is unpickled (deserialized).\n    It handles the migration of deprecated attributes to their new names.\n\n    Args:\n        state: The state dictionary from the pickled object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n        &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n    \"\"\"\n\n    if \"regressor\" in state and \"estimator\" not in state:\n        state[\"estimator\"] = state.pop(\"regressor\")\n\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing X_train (training values/predictors with shape</p> <code>Series</code> <p>(len(y) - max_lag, len(lags))) and y_train (target values of the</p> <code>tuple[DataFrame, Series]</code> <p>time series related to each row of X_train with shape (len(y) - max_lag,)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n&gt;&gt;&gt; X_train.head(2)\n   lag_1  lag_2  lag_3\n3    2.0    1.0    0.0\n4    3.0    2.0    1.0\n&gt;&gt;&gt; y_train.head(2)\n3    3\n4    4\nName: y, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef create_train_X_y(\n    self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned.\n            Default is None.\n\n    Returns:\n        Tuple containing X_train (training values/predictors with shape\n        (len(y) - max_lag, len(lags))) and y_train (target values of the\n        time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n        &gt;&gt;&gt; X_train.head(2)\n           lag_1  lag_2  lag_3\n        3    2.0    1.0    0.0\n        4    3.0    2.0    1.0\n        &gt;&gt;&gt; y_train.head(2)\n        3    3\n        4    4\n        Name: y, dtype: int64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.fit","title":"<code>fit(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.is_fitted\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n    \"\"\"Training Forecaster.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned so\n            that y[i] is regressed on exog[i]. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.is_fitted\n        True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with forecaster tags describing behavior and capabilities.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; tags['forecaster_task']\n'regression'\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        Dictionary with forecaster tags describing behavior and capabilities.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; tags['forecaster_task']\n        'regression'\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If None, the values stored in last_window are used to calculate the initial predictors, and the predictions start right after training data. Default is None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Predicted values as a pandas Series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n10    9.5\n11    9.0\n12    8.5\nName: pred, dtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; pd.Series:\n    \"\"\"Predict n steps ahead.\n\n    Args:\n        steps: Number of steps to predict.\n        last_window: Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1). If None, the values stored in\n            last_window are used to calculate the initial predictors, and the\n            predictions start right after training data. Default is None.\n        exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n    Returns:\n        Predicted values as a pandas Series.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        10    9.5\n        11    9.0\n        12    8.5\n        Name: pred, dtype: float64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute lags.</p> <p>Attributes max_lag and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int | list[int] | ndarray[int] | range[int] | None</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. If int: include lags from 1 to lags (included). If list, 1d numpy ndarray, or range: include only lags present in lags, all elements must be int. If None: no lags are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.set_lags(lags=5)\n&gt;&gt;&gt; forecaster.lags\narray([1, 2, 3, 4, 5])\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_lags(\n    self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute lags.\n\n    Attributes max_lag and window_size are also updated.\n\n    Args:\n        lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n            If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n            or range: include only lags present in lags, all elements must be int.\n            If None: no lags are included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.set_lags(lags=5)\n        &gt;&gt;&gt; forecaster.lags\n        array([1, 2, 3, 4, 5])\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_params","title":"<code>set_params(params)</code>  <code>abstractmethod</code>","text":"<p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, object]</code> <p>Parameters values dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n&gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n&gt;&gt;&gt; forecaster.estimator.alpha\n0.5\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef set_params(self, params: dict[str, object]) -&gt; None:\n    \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n    Args:\n        params: Parameters values dictionary.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n        &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n        &gt;&gt;&gt; forecaster.estimator.alpha\n        0.5\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute window_features.</p> <p>Attributes max_size_window_features, window_features_names, window_features_class_names and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>object | list[object] | None</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n&gt;&gt;&gt; forecaster.window_features\n[RollingFeatures(stats=['mean'], window_sizes=[3])]\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_window_features(\n    self, window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute window_features.\n\n    Attributes max_size_window_features, window_features_names,\n    window_features_class_names and window_size are also updated.\n\n    Args:\n        window_features: Instance or list of instances used to create window features.\n            Window features are created from the original time series and are\n            included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n        &gt;&gt;&gt; forecaster.window_features\n        [RollingFeatures(stats=['mean'], window_sizes=[3])]\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.summary()\nForecasterRecursive\n===================\nEstimator: Ridge()\nLags: [1 2 3]\n...\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.summary()\n        ForecasterRecursive\n        ===================\n        Estimator: Ridge()\n        Lags: [1 2 3]\n        ...\n    \"\"\"\n\n    print(self.__repr__())\n</code></pre>"},{"location":"api/forecaster/#recursive-forecasting","title":"Recursive Forecasting","text":""},{"location":"api/forecaster/#recursive","title":"recursive","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive","title":"<code>spotforecast2_safe.forecaster.recursive</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate","title":"<code>ForecasterEquivalentDate</code>","text":"<p>This forecaster predicts future values based on the most recent equivalent date. It also allows to aggregate multiple past values of the equivalent date using a function (e.g. mean, median, max, min, etc.). The equivalent date is calculated by moving back in time a specified number of steps (offset). The offset can be defined as an integer or as a pandas DateOffset. This approach is useful as a baseline, but it is a simplistic method and may not capture complex underlying patterns.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily, <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of valid dates. For example, Bday(2) can be used to move back two business days. If the date does not start on a valid date, it is first moved to a valid date. For example, if the date is a Saturday, it is moved to the previous Friday. Then, the offset is applied. If the result is a non-valid date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday. For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> required <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. Defaults to 1. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>1</code> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1. Defaults to np.mean.</p> <code>mean</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize the residuals into k bins according to the predicted values associated with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. Defaults to None.</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period.</p> <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction.</p> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>window_size</code> <code>int</code> <p>Number of past values needed to include the last equivalent dates according to the <code>offset</code> and <code>n_offsets</code>.</p> <code>last_window_</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the past values needed to include the last equivalent date according the <code>offset</code> and <code>n_offsets</code>.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 10_000 values.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>.</p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non-training data. Only stored up to 10_000 values. Use <code>set_out_sample_residuals()</code> method to set values.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>.</p> <code>binner</code> <code>QuantileBinner</code> <p><code>QuantileBinner</code> used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the estimator has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>spotforecast_version</code> <code>str</code> <p>Version of spotforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; # Series with daily frequency\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; # Forecast based on the value 7 days ago\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n2022-01-15    7\n2022-01-16    8\n2022-01-17    9\nFreq: D, Name: pred, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>class ForecasterEquivalentDate:\n    \"\"\"\n    This forecaster predicts future values based on the most recent equivalent\n    date. It also allows to aggregate multiple past values of the equivalent\n    date using a function (e.g. mean, median, max, min, etc.). The equivalent\n    date is calculated by moving back in time a specified number of steps (offset).\n    The offset can be defined as an integer or as a pandas DateOffset. This\n    approach is useful as a baseline, but it is a simplistic method and may not\n    capture complex underlying patterns.\n\n    Args:\n        offset (int, pandas.tseries.offsets.DateOffset): Number of steps to go back\n            in time to find the most recent equivalent date to the target period.\n            If `offset` is an integer, it represents the number of steps to go back\n            in time. For example, if the frequency of the time series is daily,\n            `offset = 7` means that the most recent data similar to the target\n            period is the value observed 7 days ago.\n            Pandas DateOffsets can also be used to move forward a given number of\n            valid dates. For example, Bday(2) can be used to move back two business\n            days. If the date does not start on a valid date, it is first moved to a\n            valid date. For example, if the date is a Saturday, it is moved to the\n            previous Friday. Then, the offset is applied. If the result is a non-valid\n            date, it is moved to the next valid date. For example, if the date\n            is a Sunday, it is moved to the next Monday.\n            For more information about offsets, see\n            https://pandas.pydata.org/docs/reference/offset_frequency.html.\n        n_offsets (int, optional): Number of equivalent dates (multiple of offset)\n            used in the prediction. Defaults to 1.\n            If `n_offsets` is greater than 1, the values at the equivalent dates are\n            aggregated using the `agg_func` function. For example, if the frequency\n            of the time series is daily, `offset = 7`, `n_offsets = 2` and\n            `agg_func = np.mean`, the predicted value will be the mean of the values\n            observed 7 and 14 days ago.\n        agg_func (Callable, optional): Function used to aggregate the values of the\n            equivalent dates when the number of equivalent dates (`n_offsets`) is\n            greater than 1. Defaults to np.mean.\n        binner_kwargs (dict, optional): Additional arguments to pass to the\n            `QuantileBinner` used to discretize the residuals into k bins according\n            to the predicted values associated with each residual. Available arguments\n            are: `n_bins`, `method`, `subsample`, `random_state` and `dtype`.\n            Argument `method` is passed internally to the function `numpy.percentile`.\n            Defaults to None.\n        forecaster_id (str, int, optional): Name used as an identifier of the\n            forecaster. Defaults to None.\n\n    Attributes:\n        offset (int, pandas.tseries.offsets.DateOffset): Number of steps to go back\n            in time to find the most recent equivalent date to the target period.\n        n_offsets (int): Number of equivalent dates (multiple of offset) used in\n            the prediction.\n        agg_func (Callable): Function used to aggregate the values of the equivalent\n            dates when the number of equivalent dates (`n_offsets`) is greater than 1.\n        window_size (int): Number of past values needed to include the last\n            equivalent dates according to the `offset` and `n_offsets`.\n        last_window_ (pandas Series): This window represents the most recent data\n            observed by the predictor during its training phase. It contains the\n            past values needed to include the last equivalent date according the\n            `offset` and `n_offsets`.\n        index_type_ (type): Type of index of the input used in training.\n        index_freq_ (str): Frequency of Index of the input used in training.\n        training_range_ (pandas Index): First and last values of index of the data\n            used during training.\n        series_name_in_ (str): Names of the series provided by the user during training.\n        in_sample_residuals_ (numpy ndarray): Residuals of the model when predicting\n            training data. Only stored up to 10_000 values.\n        in_sample_residuals_by_bin_ (dict): In sample residuals binned according to\n            the predicted value each residual is associated with. The number of\n            residuals stored per bin is limited to `10_000 // self.binner.n_bins_`\n            in the form `{bin: residuals}`.\n        out_sample_residuals_ (numpy ndarray): Residuals of the model when predicting\n            non-training data. Only stored up to 10_000 values. Use\n            `set_out_sample_residuals()` method to set values.\n        out_sample_residuals_by_bin_ (dict): Out of sample residuals binned\n            according to the predicted value each residual is associated with.\n            The number of residuals stored per bin is limited to\n            `10_000 // self.binner.n_bins_` in the form `{bin: residuals}`.\n        binner (spotforecast.preprocessing.QuantileBinner): `QuantileBinner` used to\n            discretize residuals into k bins according to the predicted values\n            associated with each residual.\n        binner_intervals_ (dict): Intervals used to discretize residuals into k bins\n            according to the predicted values associated with each residual.\n        binner_kwargs (dict): Additional arguments to pass to the `QuantileBinner`.\n        creation_date (str): Date of creation.\n        is_fitted (bool): Tag to identify if the estimator has been fitted (trained).\n        fit_date (str): Date of last fit.\n        spotforecast_version (str): Version of spotforecast library used to create\n            the forecaster.\n        python_version (str): Version of python used to create the forecaster.\n        forecaster_id (str, int): Name used as an identifier of the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; # Series with daily frequency\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; # Forecast based on the value 7 days ago\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        2022-01-15    7\n        2022-01-16    8\n        2022-01-17    9\n        Freq: D, Name: pred, dtype: int64\n    \"\"\"\n\n    def __init__(\n        self,\n        offset: int | pd.tseries.offsets.DateOffset,\n        n_offsets: int = 1,\n        agg_func: Callable = np.mean,\n        binner_kwargs: dict[str, object] | None = None,\n        forecaster_id: str | int | None = None,\n    ) -&gt; None:\n\n        self.offset = offset\n        self.n_offsets = n_offsets\n        self.agg_func = agg_func\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        try:\n            from spotforecast2_safe import __version__\n\n            self.spotforecast_version = __version__\n        except ImportError:\n            self.spotforecast_version = \"unknown\"\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n        self.estimator = None\n        self.differentiation = None\n        self.differentiation_max = None\n        self.window_size = None  # Defaults to None, validated later\n\n        if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n            raise TypeError(\n                \"`offset` must be an integer greater than 0 or a \"\n                \"pandas.tseries.offsets. Find more information about offsets in \"\n                \"https://pandas.pydata.org/docs/reference/offset_frequency.html\"\n            )\n\n        if isinstance(self.offset, int):\n            self.window_size = self.offset * self.n_offsets\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n                \"subsample\": 200000,\n                \"random_state\": 789654,\n                \"dtype\": np.float64,\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterEquivalentDate\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": False,\n            \"allowed_input_types_exog\": [],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": False,\n            \"supports_lags\": False,\n            \"supports_window_features\": False,\n            \"supports_transformer_series\": False,\n            \"supports_transformer_exog\": False,\n            \"supports_weight_func\": False,\n            \"supports_differentiation\": False,\n            \"prediction_types\": [\"point\", \"interval\"],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a Forecaster object is printed.\n\n        Returns:\n            str: Information about the forecaster. It contains the following information:\n            - Offset: Value of the `offset` argument.\n            - Number of offsets: Value of the `n_offsets` argument.\n            - Aggregation function: Name of the `agg_func` function.\n            - Window size: Value of the `window_size` attribute.\n            - Series name: Name of the series provided by the user during training.\n            - Training range: First and last values of index of the data used during training.\n            - Training index type: Type of index of the data used during training.\n            - Training index frequency: Frequency of index of the data used during training.\n            - Creation date: Date of creation of the forecaster object.\n            - Last fit date: Date of last fit of the forecaster object.\n            - spotforecast version: Version of spotforecast library used to create the forecaster.\n            - Python version: Version of python used to create the forecaster.\n            - Forecaster id: Name used as an identifier of the forecaster.\n\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data = np.arange(14),\n            ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n            &gt;&gt;&gt; print(forecaster)\n            ============================\n            ForecasterEquivalentDate\n            ============================\n            Offset: 7\n            Number of offsets: 1\n            Aggregation function: mean\n            Window size: 7\n            Series name: y\n            Training range: [Timestamp('2022-01-01 00:00:00'), Timestamp('2022-01-14 00:00:00')]\n            Training index type: DatetimeIndex\n            Training index frequency: D\n            Creation date: 2023-11-19 12:00:00\n            Last fit date: 2023-11-19 12:00:00\n            spotforecast version: 1.0.0\n            Python version: 3.8.10\n            Forecaster id: None\n\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Offset: {self.offset} \\n\"\n            f\"Number of offsets: {self.n_offsets} \\n\"\n            f\"Aggregation function: {self.agg_func.__name__} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"spotforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n\n        Returns:\n            str: HTML representation of the forecaster object. It contains the same\n            information as the `__repr__` method, but in a more structured way.\n            In detail, it contains the following information:\n            - Offset: Value of the `offset` argument.\n            - Number of offsets: Value of the `n_offsets` argument.\n            - Aggregation function: Name of the `agg_func` function.\n            - Window size: Value of the `window_size` attribute.\n            - Series name: Name of the series provided by the user during training.\n            - Training range: First and last values of index of the data used during training.\n            - Training index type: Type of index of the data used during training.\n            - Training index frequency: Frequency of index of the data used during training.\n            - Creation date: Date of creation of the forecaster object.\n            - Last fit date: Date of last fit of the forecaster object.\n            - spotforecast version: Version of spotforecast library used to create the forecaster.\n            - Python version: Version of python used to create the forecaster.\n            - Forecaster id: Name used as an identifier of the forecaster.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data = np.arange(14),\n            ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n            &gt;&gt;&gt; forecaster._repr_html_()  # doctest: +ELLIPSIS\n            '&lt;style&gt;...&lt;/style&gt;&lt;div class=\"container-...\"&gt;...&lt;/div&gt;'\n\n        \"\"\"\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Offset:&lt;/strong&gt; {self.offset}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Number of offsets:&lt;/strong&gt; {self.n_offsets}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Aggregation function:&lt;/strong&gt; {self.agg_func.__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def fit(\n        self,\n        y: pd.Series,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        exog: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Training Forecaster.\n\n        Args:\n            y (pandas Series): Training time series.\n            store_in_sample_residuals (bool, optional): If `True`, in-sample\n                residuals will be stored in the forecaster object after fitting\n                (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n                If `False`, only the intervals of the bins are stored. Defaults to False.\n            random_state (int, optional): Set a seed for the random generator so\n                that the stored sample residuals are always deterministic. Defaults to 123.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data = np.arange(14),\n            ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n        \"\"\"\n\n        if not isinstance(y, pd.Series):\n            raise TypeError(\n                f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n                f\"Found {type(y)}.\"\n            )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            if not isinstance(y.index, pd.DatetimeIndex):\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n            elif y.index.freq is None:\n                try:\n                    y.index.freq = pd.infer_freq(y.index)\n                except (ValueError, TypeError):\n                    raise TypeError(\n                        \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                        \"pandas DatetimeIndex with frequency.\"\n                    )\n                if y.index.freq is None:\n                    raise TypeError(\n                        \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                        \"pandas DatetimeIndex with frequency.\"\n                    )\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.is_fitted = False\n\n        _, y_index = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            # Calculate the window_size in steps for compatibility with the\n            # check_predict_input function. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance\n            # between two dates may not be constant.\n            first_valid_index = y_index[-1] - self.offset * self.n_offsets\n\n            try:\n                window_size_idx_start = y_index.get_loc(first_valid_index)\n                window_size_idx_end = y_index.get_loc(y_index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). This is because  \"\n                    f\"the offset ({self.offset}) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `y`.\"\n                )\n        else:\n            if len(y) &lt;= self.window_size:\n                raise ValueError(\n                    f\"Length of `y` must be greater than the maximum window size \"\n                    f\"needed by the forecaster. This is because  \"\n                    f\"the offset ({self.offset}) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `y`.\\n\"\n                    f\"    Length `y`: {len(y)}.\\n\"\n                    f\"    Max window size: {self.window_size}.\\n\"\n                )\n\n        self.is_fitted = True\n        self.series_name_in_ = y.name if y.name is not None else \"y\"\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.training_range_ = y_index[[0, -1]]\n        self.index_type_ = type(y_index)\n        self.index_freq_ = (\n            y_index.freq if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n        )\n\n        # NOTE: This is done to save time during fit in functions such as backtesting()\n        if self._probabilistic_mode is not False:\n            self._binning_in_sample_residuals(\n                y=y,\n                store_in_sample_residuals=store_in_sample_residuals,\n                random_state=random_state,\n            )\n\n        # The last time window of training data is stored so that equivalent\n        # dates are available when calling the `predict` method.\n        # Store the whole series to avoid errors when the offset is larger\n        # than the data available.\n        self.last_window_ = y.copy()\n\n    def _binning_in_sample_residuals(\n        self,\n        y: pd.Series,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Bin residuals according to the predicted value each residual is\n        associated with. First a `spotforecast.preprocessing.QuantileBinner` object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n\n        The number of residuals stored per bin is limited to\n        `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n        `10_000`.\n\n        Args:\n            y (pandas Series): Training time series.\n            store_in_sample_residuals (bool, optional): If `True`, in-sample\n                residuals will be stored in the forecaster object after fitting\n                (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n                If `False`, only the intervals of the bins are stored. Defaults to False.\n            random_state (int, optional): Set a seed for the random generator so\n                that the stored sample residuals are always deterministic. Defaults to 123.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data = np.arange(21, dtype=float),\n            ...     index = pd.date_range(start='2022-01-01', periods=21, freq='D')\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(\n            ...     offset=7,\n            ...     binner_kwargs={'n_bins': 2, 'random_state': 123}\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=data, store_in_sample_residuals=True)\n            &gt;&gt;&gt; forecaster.in_sample_residuals_\n            array([7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.])\n            &gt;&gt;&gt; forecaster.in_sample_residuals_by_bin_\n            {0: array([7., 7., 7., 7., 7., 7., 7.]), 1: array([7., 7., 7., 7., 7., 7., 7.])}\n\n\n        \"\"\"\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            y_preds = []\n            for n_off in range(1, self.n_offsets + 1):\n                idx = y.index - self.offset * n_off\n                mask = idx &gt;= y.index[0]\n                y_pred = y.loc[idx[mask]]\n                y_pred.index = y.index[-mask.sum() :]\n                y_preds.append(y_pred)\n\n            y_preds = pd.concat(y_preds, axis=1).to_numpy()\n            y_true = y.to_numpy()[-len(y_preds) :]\n\n        else:\n            y_preds = [\n                y.shift(self.offset * n_off)[self.window_size :]\n                for n_off in range(1, self.n_offsets + 1)\n            ]\n            y_preds = np.column_stack(y_preds)\n            y_true = y.to_numpy()[self.window_size :]\n\n        y_pred = np.apply_along_axis(self.agg_func, axis=1, arr=y_preds)\n\n        residuals = y_true - y_pred\n\n        if self._probabilistic_mode == \"binned\":\n            data = pd.DataFrame({\"prediction\": y_pred, \"residuals\": residuals}).dropna()\n            y_pred = data[\"prediction\"].to_numpy()\n            residuals = data[\"residuals\"].to_numpy()\n\n            self.binner.fit(y_pred)\n            self.binner_intervals_ = self.binner.intervals_\n\n        if store_in_sample_residuals:\n            rng = np.random.default_rng(seed=random_state)\n            if self._probabilistic_mode == \"binned\":\n                data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n                self.in_sample_residuals_by_bin_ = (\n                    data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n                )\n\n                max_sample = 10_000 // self.binner.n_bins_\n                for k, v in self.in_sample_residuals_by_bin_.items():\n                    if len(v) &gt; max_sample:\n                        sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                        self.in_sample_residuals_by_bin_[k] = sample\n\n                for k in self.binner_intervals_.keys():\n                    if k not in self.in_sample_residuals_by_bin_:\n                        self.in_sample_residuals_by_bin_[k] = np.array([])\n\n                empty_bins = [\n                    k\n                    for k, v in self.in_sample_residuals_by_bin_.items()\n                    if v.size == 0\n                ]\n                if empty_bins:\n                    empty_bin_size = min(max_sample, len(residuals))\n                    for k in empty_bins:\n                        self.in_sample_residuals_by_bin_[k] = rng.choice(\n                            a=residuals, size=empty_bin_size, replace=False\n                        )\n\n            if len(residuals) &gt; 10_000:\n                residuals = residuals[\n                    rng.integers(low=0, high=len(residuals), size=10_000)\n                ]\n\n            self.in_sample_residuals_ = residuals\n\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | None = None,\n        check_inputs: bool = True,\n        exog: Any = None,\n    ) -&gt; pd.Series:\n        \"\"\"\n        Predict n steps ahead.\n\n        Args:\n            steps (int): Number of steps to predict.\n            last_window (pandas Series, optional): Past values needed to select the\n                last equivalent dates according to the offset. If `last_window = None`,\n                the values stored in `self.last_window_` are used and the predictions\n                start immediately after the training data. Defaults to None.\n            check_inputs (bool, optional): If `True`, the input is checked for\n                possible warnings and errors with the `check_predict_input` function.\n                This argument is created for internal use and is not recommended to\n                be changed. Defaults to True.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            pd.Series: Predicted values.\n\n        Raises:\n            ValueError:\n                If all equivalent values are missing when using a pandas DateOffset as offset.\n                This can be caused by using an offset larger than the available data.\n                To avoid this, try to decrease the size of the offset, the number of `n_offsets` or increase the size of `last_window`.\n                In backtesting, this error may be caused by using an `initial_train_size` too small.\n            Warning:\n                If some equivalent values are missing when using a pandas DateOffset as offset.\n                This can be caused by using an offset larger than the available data or by using an `initial_train_size` too small in backtesting.\n                To avoid this, increase the `last_window` size or decrease the number of `n_offsets`.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data = np.arange(14),\n            ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            2022-01-15    7\n            2022-01-16    8\n            2022-01-17    9\n            Freq: D, Name: pred, dtype: int64\n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=False,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n            )\n\n        prediction_index = expand_index(index=last_window.index, steps=steps)\n\n        # Initialize to prevent use-before-initialization warnings\n        predictions = None\n\n        if isinstance(self.offset, int):\n\n            last_window_values = last_window.to_numpy(copy=True).ravel()\n            equivalent_indexes = np.tile(\n                np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n            )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets &gt; 1:\n                equivalent_indexes = [\n                    equivalent_indexes - n * self.offset\n                    for n in np.arange(self.n_offsets)\n                ]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                    self.agg_func, axis=0, arr=equivalent_values\n                )\n\n            predictions = pd.Series(\n                data=predictions, index=prediction_index, name=\"pred\"\n            )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            last_window = last_window.copy()\n            max_allowed_date = last_window.index[-1]\n\n            # For every date in prediction_index, calculate the n offsets\n            offset_dates = []\n            for date in prediction_index:\n                selected_offsets = []\n                while len(selected_offsets) &lt; self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date &lt;= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n\n            offset_dates = np.array(offset_dates)\n\n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                data=equivalent_values,\n                index=prediction_index,\n                columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n            )\n\n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    f\"All equivalent values are missing. This is caused by using \"\n                    f\"an offset ({self.offset}) larger than the available data. \"\n                    f\"Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `last_window`. In backtesting, this error may be \"\n                    f\"caused by using an `initial_train_size` too small.\"\n                )\n\n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                    f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                    f\"To avoid this, increase the `last_window` size or decrease \"\n                    f\"the number of `n_offsets`. The current configuration requires \"\n                    f\"a total offset of {self.offset * self.n_offsets}.\",\n                    MissingValuesWarning,\n                )\n\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.rename(\"pred\")\n\n        return predictions\n\n    def predict_interval(\n        self,\n        steps: int,\n        last_window: pd.Series | None = None,\n        method: str = \"conformal\",\n        interval: float | list[float] | tuple[float] = [5, 95],\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: Any = None,\n        exog: Any = None,\n        n_boot: Any = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead and estimate prediction intervals using conformal\n        prediction method. Refer to the References section for additional\n        details on this method.\n\n        Args:\n            steps (int): Number of steps to predict.\n            last_window (pandas Series, optional): Past values needed to select the\n                last equivalent dates according to the offset. If `last_window = None`,\n                the values stored in `self.last_window_` are used and the predictions\n                start immediately after the training data. Defaults to None.\n            method (str, optional): Technique used to estimate prediction intervals.\n                Available options:\n                - 'conformal': Employs the conformal prediction split method for\n                interval estimation [1]_. Defaults to 'conformal'.\n            interval (float, list, tuple, optional): Confidence level of the\n                prediction interval. Interpretation depends on the method used:\n                - If `float`, represents the nominal (expected) coverage (between 0\n                and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n                percentiles.\n                - If `list` or `tuple`, defines the exact percentiles to compute,\n                which must be between 0 and 100 inclusive. For example, interval\n                of 95% should be as `interval = [2.5, 97.5]`.\n                - When using `method='conformal'`, the interval must be a float or\n                a list/tuple defining a symmetric interval. Defaults to [5, 95].\n            use_in_sample_residuals (bool, optional): If `True`, residuals from the\n                training data are used as proxy of prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals (bool, optional): If `True`, residuals are selected\n                based on the predicted values (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state (Ignored): Not used, present here for API consistency by convention.\n            exog (Ignored): Not used, present here for API consistency by convention.\n            n_boot (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            pd.DataFrame: Values predicted by the forecaster and their estimated interval.\n                - pred: predictions.\n                - lower_bound: lower bound of the interval.\n                - upper_bound: upper bound of the interval.\n\n        Raises:\n            ValueError: If `method` is not 'conformal'.\n            ValueError: If `interval` is not a float or a list/tuple defining a symmetric interval when using `method='conformal'`.\n\n        References:\n            .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n                https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data = np.arange(14, dtype=float),\n            ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data, store_in_sample_residuals=True)\n            &gt;&gt;&gt; forecaster.predict_interval(steps=3, interval=0.8)\n                        pred  lower_bound  upper_bound\n            2022-01-15   7.0          6.0          8.0\n            2022-01-16   8.0          7.0          9.0\n            2022-01-17   9.0          8.0         10.0\n        \"\"\"\n\n        if method != \"conformal\":\n            raise ValueError(\n                f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n            )\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=False,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n        )\n\n        check_residuals_input(\n            forecaster_name=type(self).__name__,\n            use_in_sample_residuals=use_in_sample_residuals,\n            in_sample_residuals_=self.in_sample_residuals_,\n            out_sample_residuals_=self.out_sample_residuals_,\n            use_binned_residuals=use_binned_residuals,\n            in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n            out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n        )\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            nominal_coverage = interval\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        prediction_index = expand_index(index=last_window.index, steps=steps)\n\n        # Initialize to prevent use-before-initialization warnings\n        predictions = None\n\n        if isinstance(self.offset, int):\n\n            last_window_values = last_window.to_numpy(copy=True).ravel()\n            equivalent_indexes = np.tile(\n                np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n            )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets &gt; 1:\n                equivalent_indexes = [\n                    equivalent_indexes - n * self.offset\n                    for n in np.arange(self.n_offsets)\n                ]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                    self.agg_func, axis=0, arr=equivalent_values\n                )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            last_window = last_window.copy()\n            max_allowed_date = last_window.index[-1]\n\n            # For every date in prediction_index, calculate the n offsets\n            offset_dates = []\n            for date in prediction_index:\n                selected_offsets = []\n                while len(selected_offsets) &lt; self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date &lt;= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n\n            offset_dates = np.array(offset_dates)\n\n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                data=equivalent_values,\n                index=prediction_index,\n                columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n            )\n\n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    f\"All equivalent values are missing. This is caused by using \"\n                    f\"an offset ({self.offset}) larger than the available data. \"\n                    f\"Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `last_window`. In backtesting, this error may be \"\n                    f\"caused by using an `initial_train_size` too small.\"\n                )\n\n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                    f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                    f\"To avoid this, increase the `last_window` size or decrease \"\n                    f\"the number of `n_offsets`. The current configuration requires \"\n                    f\"a total offset of {self.offset * self.n_offsets}.\",\n                    MissingValuesWarning,\n                )\n\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.to_numpy()\n\n        if use_binned_residuals:\n            correction_factor_by_bin = {\n                k: np.quantile(np.abs(v), nominal_coverage)\n                for k, v in residuals_by_bin.items()\n            }\n            replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n            predictions_bin = self.binner.transform(predictions)\n            correction_factor = replace_func(predictions_bin)\n        else:\n            correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n        lower_bound = predictions - correction_factor\n        upper_bound = predictions + correction_factor\n        predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n        predictions = pd.DataFrame(\n            data=predictions,\n            index=prediction_index,\n            columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n        )\n\n        return predictions\n\n    def set_in_sample_residuals(\n        self, y: pd.Series, random_state: int = 123, exog: Any = None\n    ) -&gt; None:\n        \"\"\"\n        Set in-sample residuals in case they were not calculated during the\n        training process.\n\n        In-sample residuals are calculated as the difference between the true\n        values and the predictions made by the forecaster using the training\n        data. The following internal attributes are updated:\n\n        + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `binner_intervals_`: intervals used to bin the residuals are calculated\n        using the quantiles of the predicted values.\n        + `in_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the intervals of the predicted values and the values are\n        the residuals associated with that range.\n\n        A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n\n        Args:\n            y (pandas Series): Training time series.\n            random_state (int, optional): Sets a seed to the random sampling for\n                reproducible output. Defaults to 123.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            None\n\n        Raises:\n            NotFittedError: If the forecaster has not been fitted.\n            IndexError: If the index range of `y` does not match the training range.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data=np.arange(14, dtype=float),\n            ...     index=pd.date_range(start=\"2022-01-01\", periods=14, freq=\"D\"),\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n            &gt;&gt;&gt; # Recompute and store residuals if needed\n            &gt;&gt;&gt; forecaster.set_in_sample_residuals(y=data, random_state=123)\n            &gt;&gt;&gt; forecaster.in_sample_residuals_.shape\n            (7,)\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_in_sample_residuals()`.\"\n            )\n\n        check_y(y=y)\n        y_index_range = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )[1][[0, -1]]\n        if not y_index_range.equals(self.training_range_):\n            raise IndexError(\n                f\"The index range of `y` does not match the range \"\n                f\"used during training. Please ensure the index is aligned \"\n                f\"with the training data.\\n\"\n                f\"    Expected : {self.training_range_}\\n\"\n                f\"    Received : {y_index_range}\"\n            )\n\n        self._binning_in_sample_residuals(\n            y=y, store_in_sample_residuals=True, random_state=random_state\n        )\n\n    def set_out_sample_residuals(\n        self,\n        y_true: np.ndarray | pd.Series,\n        y_pred: np.ndarray | pd.Series,\n        append: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n\n        Args:\n            y_true (numpy ndarray, pandas Series): True values of the time series\n                from which the residuals have been calculated.\n            y_pred (numpy ndarray, pandas Series): Predicted values of the time series.\n            append (bool, optional): If `True`, new residuals are added to the once\n                already stored in the forecaster. If after appending the new\n                residuals, the limit of `10_000 // self.binner.n_bins_` values per\n                bin is reached, a random sample of residuals is stored. Defaults\n                to False.\n            random_state (int, optional): Sets a seed to the random sampling for\n                reproducible output. Defaults to 123.\n\n        Returns:\n            None\n\n        Raises:\n            NotFittedError: If the forecaster has not been fitted.\n            TypeError: If `y_true` or `y_pred` are not numpy arrays or pandas Series.\n            ValueError: If `y_true` and `y_pred` have different lengths.\n            ValueError: If `y_true` and `y_pred` are pandas Series with different indexes.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data=np.arange(21, dtype=float),\n            ...     index=pd.date_range(start=\"2022-01-01\", periods=21, freq=\"D\"),\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n            &gt;&gt;&gt; preds = forecaster.predict(steps=7)\n            &gt;&gt;&gt; y_true = pd.Series(data[-7:].to_numpy(), index=preds.index)\n            &gt;&gt;&gt; forecaster.set_out_sample_residuals(y_true=y_true, y_pred=preds)\n            &gt;&gt;&gt; forecaster.out_sample_residuals_.shape\n            (7,)\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n\n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        data = pd.DataFrame(\n            {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n        ).dropna()\n        y_pred = data[\"prediction\"].to_numpy()\n        residuals = data[\"residuals\"].to_numpy()\n\n        data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n\n        out_sample_residuals = (\n            np.array([])\n            if self.out_sample_residuals_ is None\n            else self.out_sample_residuals_\n        )\n        out_sample_residuals_by_bin = (\n            {}\n            if self.out_sample_residuals_by_bin_ is None\n            else self.out_sample_residuals_by_bin_\n        )\n        if append:\n            out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n            for k, v in residuals_by_bin.items():\n                if k in out_sample_residuals_by_bin:\n                    out_sample_residuals_by_bin[k] = np.concatenate(\n                        (out_sample_residuals_by_bin[k], v)\n                    )\n                else:\n                    out_sample_residuals_by_bin[k] = v\n        else:\n            out_sample_residuals = residuals\n            out_sample_residuals_by_bin = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in out_sample_residuals_by_bin.items():\n            if len(v) &gt; max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                out_sample_residuals_by_bin[k] = sample\n\n        bin_keys = (\n            [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n        )\n        for k in bin_keys:\n            if k not in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.array([])\n\n        empty_bins = [k for k, v in out_sample_residuals_by_bin.items() if v.size == 0]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\",\n                ResidualsUsageWarning,\n            )\n            empty_bin_size = min(max_samples, len(out_sample_residuals))\n            for k in empty_bins:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    a=out_sample_residuals, size=empty_bin_size, replace=False\n                )\n\n        if len(out_sample_residuals) &gt; 10_000:\n            out_sample_residuals = rng.choice(\n                a=out_sample_residuals, size=10_000, replace=False\n            )\n\n        self.out_sample_residuals_ = out_sample_residuals\n        self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            dict: Dictionary with forecaster tags.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; sorted(tags.keys())[:3]\n            ['allowed_input_types_exog', 'allowed_input_types_series', 'forecaster_name']\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"\n        Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n            &gt;&gt;&gt; data = pd.Series(\n            ...     data=np.arange(14, dtype=float),\n            ...     index=pd.date_range(start=\"2022-01-01\", periods=14, freq=\"D\"),\n            ... )\n            &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n            &gt;&gt;&gt; forecaster.fit(y=data)\n            &gt;&gt;&gt; forecaster.summary()  # doctest: +ELLIPSIS\n            ============================\n        \"\"\"\n\n        print(self)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a Forecaster object is printed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Information about the forecaster. It contains the following information:</p> <code>str</code> <ul> <li>Offset: Value of the <code>offset</code> argument.</li> </ul> <code>str</code> <ul> <li>Number of offsets: Value of the <code>n_offsets</code> argument.</li> </ul> <code>str</code> <ul> <li>Aggregation function: Name of the <code>agg_func</code> function.</li> </ul> <code>str</code> <ul> <li>Window size: Value of the <code>window_size</code> attribute.</li> </ul> <code>str</code> <ul> <li>Series name: Name of the series provided by the user during training.</li> </ul> <code>str</code> <ul> <li>Training range: First and last values of index of the data used during training.</li> </ul> <code>str</code> <ul> <li>Training index type: Type of index of the data used during training.</li> </ul> <code>str</code> <ul> <li>Training index frequency: Frequency of index of the data used during training.</li> </ul> <code>str</code> <ul> <li>Creation date: Date of creation of the forecaster object.</li> </ul> <code>str</code> <ul> <li>Last fit date: Date of last fit of the forecaster object.</li> </ul> <code>str</code> <ul> <li>spotforecast version: Version of spotforecast library used to create the forecaster.</li> </ul> <code>str</code> <ul> <li>Python version: Version of python used to create the forecaster.</li> </ul> <code>str</code> <ul> <li>Forecaster id: Name used as an identifier of the forecaster.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; print(forecaster)\n============================\nForecasterEquivalentDate\n============================\nOffset: 7\nNumber of offsets: 1\nAggregation function: mean\nWindow size: 7\nSeries name: y\nTraining range: [Timestamp('2022-01-01 00:00:00'), Timestamp('2022-01-14 00:00:00')]\nTraining index type: DatetimeIndex\nTraining index frequency: D\nCreation date: 2023-11-19 12:00:00\nLast fit date: 2023-11-19 12:00:00\nspotforecast version: 1.0.0\nPython version: 3.8.10\nForecaster id: None\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a Forecaster object is printed.\n\n    Returns:\n        str: Information about the forecaster. It contains the following information:\n        - Offset: Value of the `offset` argument.\n        - Number of offsets: Value of the `n_offsets` argument.\n        - Aggregation function: Name of the `agg_func` function.\n        - Window size: Value of the `window_size` attribute.\n        - Series name: Name of the series provided by the user during training.\n        - Training range: First and last values of index of the data used during training.\n        - Training index type: Type of index of the data used during training.\n        - Training index frequency: Frequency of index of the data used during training.\n        - Creation date: Date of creation of the forecaster object.\n        - Last fit date: Date of last fit of the forecaster object.\n        - spotforecast version: Version of spotforecast library used to create the forecaster.\n        - Python version: Version of python used to create the forecaster.\n        - Forecaster id: Name used as an identifier of the forecaster.\n\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; print(forecaster)\n        ============================\n        ForecasterEquivalentDate\n        ============================\n        Offset: 7\n        Number of offsets: 1\n        Aggregation function: mean\n        Window size: 7\n        Series name: y\n        Training range: [Timestamp('2022-01-01 00:00:00'), Timestamp('2022-01-14 00:00:00')]\n        Training index type: DatetimeIndex\n        Training index frequency: D\n        Creation date: 2023-11-19 12:00:00\n        Last fit date: 2023-11-19 12:00:00\n        spotforecast version: 1.0.0\n        Python version: 3.8.10\n        Forecaster id: None\n\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Offset: {self.offset} \\n\"\n        f\"Number of offsets: {self.n_offsets} \\n\"\n        f\"Aggregation function: {self.agg_func.__name__} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"spotforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.fit","title":"<code>fit(y, store_in_sample_residuals=False, random_state=123, exog=None)</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample residuals are always deterministic. Defaults to 123.</p> <code>123</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    exog: Any = None,\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Args:\n        y (pandas Series): Training time series.\n        store_in_sample_residuals (bool, optional): If `True`, in-sample\n            residuals will be stored in the forecaster object after fitting\n            (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n            If `False`, only the intervals of the bins are stored. Defaults to False.\n        random_state (int, optional): Set a seed for the random generator so\n            that the stored sample residuals are always deterministic. Defaults to 123.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        if not isinstance(y.index, pd.DatetimeIndex):\n            raise TypeError(\n                \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                \"pandas DatetimeIndex with frequency.\"\n            )\n        elif y.index.freq is None:\n            try:\n                y.index.freq = pd.infer_freq(y.index)\n            except (ValueError, TypeError):\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n            if y.index.freq is None:\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_ = None\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.training_range_ = None\n    self.series_name_in_ = None\n    self.is_fitted = False\n\n    _, y_index = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        # Calculate the window_size in steps for compatibility with the\n        # check_predict_input function. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance\n        # between two dates may not be constant.\n        first_valid_index = y_index[-1] - self.offset * self.n_offsets\n\n        try:\n            window_size_idx_start = y_index.get_loc(first_valid_index)\n            window_size_idx_end = y_index.get_loc(y_index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\"\n            )\n    else:\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster. This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n            )\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else \"y\"\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    self.training_range_ = y_index[[0, -1]]\n    self.index_type_ = type(y_index)\n    self.index_freq_ = (\n        y_index.freq if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n    )\n\n    # NOTE: This is done to save time during fit in functions such as backtesting()\n    if self._probabilistic_mode is not False:\n        self._binning_in_sample_residuals(\n            y=y,\n            store_in_sample_residuals=store_in_sample_residuals,\n            random_state=random_state,\n        )\n\n    # The last time window of training data is stored so that equivalent\n    # dates are available when calling the `predict` method.\n    # Store the whole series to avoid errors when the offset is larger\n    # than the data available.\n    self.last_window_ = y.copy()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Dictionary with forecaster tags.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; sorted(tags.keys())[:3]\n['allowed_input_types_exog', 'allowed_input_types_series', 'forecaster_name']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        dict: Dictionary with forecaster tags.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; sorted(tags.keys())[:3]\n        ['allowed_input_types_exog', 'allowed_input_types_series', 'forecaster_name']\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.predict","title":"<code>predict(steps, last_window=None, check_inputs=True, exog=None)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to the offset. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used and the predictions start immediately after the training data. Defaults to None.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors with the <code>check_predict_input</code> function. This argument is created for internal use and is not recommended to be changed. Defaults to True.</p> <code>True</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Predicted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all equivalent values are missing when using a pandas DateOffset as offset. This can be caused by using an offset larger than the available data. To avoid this, try to decrease the size of the offset, the number of <code>n_offsets</code> or increase the size of <code>last_window</code>. In backtesting, this error may be caused by using an <code>initial_train_size</code> too small.</p> <code>Warning</code> <p>If some equivalent values are missing when using a pandas DateOffset as offset. This can be caused by using an offset larger than the available data or by using an <code>initial_train_size</code> too small in backtesting. To avoid this, increase the <code>last_window</code> size or decrease the number of <code>n_offsets</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n2022-01-15    7\n2022-01-16    8\n2022-01-17    9\nFreq: D, Name: pred, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    check_inputs: bool = True,\n    exog: Any = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Args:\n        steps (int): Number of steps to predict.\n        last_window (pandas Series, optional): Past values needed to select the\n            last equivalent dates according to the offset. If `last_window = None`,\n            the values stored in `self.last_window_` are used and the predictions\n            start immediately after the training data. Defaults to None.\n        check_inputs (bool, optional): If `True`, the input is checked for\n            possible warnings and errors with the `check_predict_input` function.\n            This argument is created for internal use and is not recommended to\n            be changed. Defaults to True.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        pd.Series: Predicted values.\n\n    Raises:\n        ValueError:\n            If all equivalent values are missing when using a pandas DateOffset as offset.\n            This can be caused by using an offset larger than the available data.\n            To avoid this, try to decrease the size of the offset, the number of `n_offsets` or increase the size of `last_window`.\n            In backtesting, this error may be caused by using an `initial_train_size` too small.\n        Warning:\n            If some equivalent values are missing when using a pandas DateOffset as offset.\n            This can be caused by using an offset larger than the available data or by using an `initial_train_size` too small in backtesting.\n            To avoid this, increase the `last_window` size or decrease the number of `n_offsets`.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        2022-01-15    7\n        2022-01-16    8\n        2022-01-17    9\n        Freq: D, Name: pred, dtype: int64\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=False,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n        )\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    # Initialize to prevent use-before-initialization warnings\n    predictions = None\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n            np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n        )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset\n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                self.agg_func, axis=0, arr=equivalent_values\n            )\n\n        predictions = pd.Series(\n            data=predictions, index=prediction_index, name=\"pred\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n            data=equivalent_values,\n            index=prediction_index,\n            columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n        )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \"\n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning,\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.rename(\"pred\")\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.predict_interval","title":"<code>predict_interval(steps, last_window=None, method='conformal', interval=[5, 95], use_in_sample_residuals=True, use_binned_residuals=True, random_state=None, exog=None, n_boot=None)</code>","text":"<p>Predict n steps ahead and estimate prediction intervals using conformal prediction method. Refer to the References section for additional details on this method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to the offset. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used and the predictions start immediately after the training data. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options: - 'conformal': Employs the conformal prediction split method for interval estimation [1]_. Defaults to 'conformal'.</p> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends on the method used: - If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles. - If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>. - When using <code>method='conformal'</code>, the interval must be a float or a list/tuple defining a symmetric interval. Defaults to [5, 95].</p> <code>[5, 95]</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>n_boot</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Values predicted by the forecaster and their estimated interval. - pred: predictions. - lower_bound: lower bound of the interval. - upper_bound: upper bound of the interval.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not 'conformal'.</p> <code>ValueError</code> <p>If <code>interval</code> is not a float or a list/tuple defining a symmetric interval when using <code>method='conformal'</code>.</p> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.     https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14, dtype=float),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data, store_in_sample_residuals=True)\n&gt;&gt;&gt; forecaster.predict_interval(steps=3, interval=0.8)\n            pred  lower_bound  upper_bound\n2022-01-15   7.0          6.0          8.0\n2022-01-16   8.0          7.0          9.0\n2022-01-17   9.0          8.0         10.0\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    method: str = \"conformal\",\n    interval: float | list[float] | tuple[float] = [5, 95],\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: Any = None,\n    exog: Any = None,\n    n_boot: Any = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using conformal\n    prediction method. Refer to the References section for additional\n    details on this method.\n\n    Args:\n        steps (int): Number of steps to predict.\n        last_window (pandas Series, optional): Past values needed to select the\n            last equivalent dates according to the offset. If `last_window = None`,\n            the values stored in `self.last_window_` are used and the predictions\n            start immediately after the training data. Defaults to None.\n        method (str, optional): Technique used to estimate prediction intervals.\n            Available options:\n            - 'conformal': Employs the conformal prediction split method for\n            interval estimation [1]_. Defaults to 'conformal'.\n        interval (float, list, tuple, optional): Confidence level of the\n            prediction interval. Interpretation depends on the method used:\n            - If `float`, represents the nominal (expected) coverage (between 0\n            and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n            percentiles.\n            - If `list` or `tuple`, defines the exact percentiles to compute,\n            which must be between 0 and 100 inclusive. For example, interval\n            of 95% should be as `interval = [2.5, 97.5]`.\n            - When using `method='conformal'`, the interval must be a float or\n            a list/tuple defining a symmetric interval. Defaults to [5, 95].\n        use_in_sample_residuals (bool, optional): If `True`, residuals from the\n            training data are used as proxy of prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals (bool, optional): If `True`, residuals are selected\n            based on the predicted values (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state (Ignored): Not used, present here for API consistency by convention.\n        exog (Ignored): Not used, present here for API consistency by convention.\n        n_boot (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        pd.DataFrame: Values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Raises:\n        ValueError: If `method` is not 'conformal'.\n        ValueError: If `interval` is not a float or a list/tuple defining a symmetric interval when using `method='conformal'`.\n\n    References:\n        .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n            https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14, dtype=float),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data, store_in_sample_residuals=True)\n        &gt;&gt;&gt; forecaster.predict_interval(steps=3, interval=0.8)\n                    pred  lower_bound  upper_bound\n        2022-01-15   7.0          6.0          8.0\n        2022-01-16   8.0          7.0          9.0\n        2022-01-17   9.0          8.0         10.0\n    \"\"\"\n\n    if method != \"conformal\":\n        raise ValueError(\n            f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n        )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    check_predict_input(\n        forecaster_name=type(self).__name__,\n        steps=steps,\n        is_fitted=self.is_fitted,\n        exog_in_=False,\n        index_type_=self.index_type_,\n        index_freq_=self.index_freq_,\n        window_size=self.window_size,\n        last_window=last_window,\n    )\n\n    check_residuals_input(\n        forecaster_name=type(self).__name__,\n        use_in_sample_residuals=use_in_sample_residuals,\n        in_sample_residuals_=self.in_sample_residuals_,\n        out_sample_residuals_=self.out_sample_residuals_,\n        use_binned_residuals=use_binned_residuals,\n        in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n        out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n    )\n\n    if isinstance(interval, (list, tuple)):\n        check_interval(interval=interval, ensure_symmetric_intervals=True)\n        nominal_coverage = (interval[1] - interval[0]) / 100\n    else:\n        check_interval(alpha=interval, alpha_literal=\"interval\")\n        nominal_coverage = interval\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    # Initialize to prevent use-before-initialization warnings\n    predictions = None\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n            np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n        )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset\n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                self.agg_func, axis=0, arr=equivalent_values\n            )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n            data=equivalent_values,\n            index=prediction_index,\n            columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n        )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \"\n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning,\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.to_numpy()\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner.transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    predictions = pd.DataFrame(\n        data=predictions,\n        index=prediction_index,\n        columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n    )\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals","title":"<code>set_in_sample_residuals(y, random_state=123, exog=None)</code>","text":"<p>Set in-sample residuals in case they were not calculated during the training process.</p> <p>In-sample residuals are calculated as the difference between the true values and the predictions made by the forecaster using the training data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output. Defaults to 123.</p> <code>123</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the forecaster has not been fitted.</p> <code>IndexError</code> <p>If the index range of <code>y</code> does not match the training range.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data=np.arange(14, dtype=float),\n...     index=pd.date_range(start=\"2022-01-01\", periods=14, freq=\"D\"),\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; # Recompute and store residuals if needed\n&gt;&gt;&gt; forecaster.set_in_sample_residuals(y=data, random_state=123)\n&gt;&gt;&gt; forecaster.in_sample_residuals_.shape\n(7,)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def set_in_sample_residuals(\n    self, y: pd.Series, random_state: int = 123, exog: Any = None\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process.\n\n    In-sample residuals are calculated as the difference between the true\n    values and the predictions made by the forecaster using the training\n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range.\n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Args:\n        y (pandas Series): Training time series.\n        random_state (int, optional): Sets a seed to the random sampling for\n            reproducible output. Defaults to 123.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        None\n\n    Raises:\n        NotFittedError: If the forecaster has not been fitted.\n        IndexError: If the index range of `y` does not match the training range.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data=np.arange(14, dtype=float),\n        ...     index=pd.date_range(start=\"2022-01-01\", periods=14, freq=\"D\"),\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; # Recompute and store residuals if needed\n        &gt;&gt;&gt; forecaster.set_in_sample_residuals(y=data, random_state=123)\n        &gt;&gt;&gt; forecaster.in_sample_residuals_.shape\n        (7,)\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )[1][[0, -1]]\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y=y, store_in_sample_residuals=True, random_state=random_state\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals","title":"<code>set_out_sample_residuals(y_true, y_pred, append=False, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Two internal attributes are updated:</p> <ul> <li><code>out_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>out_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the  intervals of the predicted values and the values are the residuals associated with that range. If a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray, pandas Series</code> <p>True values of the time series from which the residuals have been calculated.</p> required <code>y_pred</code> <code>numpy ndarray, pandas Series</code> <p>Predicted values of the time series.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the forecaster. If after appending the new residuals, the limit of <code>10_000 // self.binner.n_bins_</code> values per bin is reached, a random sample of residuals is stored. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the forecaster has not been fitted.</p> <code>TypeError</code> <p>If <code>y_true</code> or <code>y_pred</code> are not numpy arrays or pandas Series.</p> <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have different lengths.</p> <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> are pandas Series with different indexes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data=np.arange(21, dtype=float),\n...     index=pd.date_range(start=\"2022-01-01\", periods=21, freq=\"D\"),\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; preds = forecaster.predict(steps=7)\n&gt;&gt;&gt; y_true = pd.Series(data[-7:].to_numpy(), index=preds.index)\n&gt;&gt;&gt; forecaster.set_out_sample_residuals(y_true=y_true, y_pred=preds)\n&gt;&gt;&gt; forecaster.out_sample_residuals_.shape\n(7,)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. Two internal attributes are updated:\n\n    + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `out_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the  intervals of the predicted values and the values are\n    the residuals associated with that range. If a bin binning is empty, it\n    is filled with a random sample of residuals from other bins. This is done\n    to ensure that all bins have at least one residual and can be used in the\n    prediction process.\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Args:\n        y_true (numpy ndarray, pandas Series): True values of the time series\n            from which the residuals have been calculated.\n        y_pred (numpy ndarray, pandas Series): Predicted values of the time series.\n        append (bool, optional): If `True`, new residuals are added to the once\n            already stored in the forecaster. If after appending the new\n            residuals, the limit of `10_000 // self.binner.n_bins_` values per\n            bin is reached, a random sample of residuals is stored. Defaults\n            to False.\n        random_state (int, optional): Sets a seed to the random sampling for\n            reproducible output. Defaults to 123.\n\n    Returns:\n        None\n\n    Raises:\n        NotFittedError: If the forecaster has not been fitted.\n        TypeError: If `y_true` or `y_pred` are not numpy arrays or pandas Series.\n        ValueError: If `y_true` and `y_pred` have different lengths.\n        ValueError: If `y_true` and `y_pred` are pandas Series with different indexes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data=np.arange(21, dtype=float),\n        ...     index=pd.date_range(start=\"2022-01-01\", periods=21, freq=\"D\"),\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; preds = forecaster.predict(steps=7)\n        &gt;&gt;&gt; y_true = pd.Series(data[-7:].to_numpy(), index=preds.index)\n        &gt;&gt;&gt; forecaster.set_out_sample_residuals(y_true=y_true, y_pred=preds)\n        &gt;&gt;&gt; forecaster.out_sample_residuals_.shape\n        (7,)\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    data = pd.DataFrame(\n        {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n    ).dropna()\n    y_pred = data[\"prediction\"].to_numpy()\n    residuals = data[\"residuals\"].to_numpy()\n\n    data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n\n    out_sample_residuals = (\n        np.array([])\n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {}\n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    bin_keys = (\n        [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n    )\n    for k in bin_keys:\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [k for k, v in out_sample_residuals_by_bin.items() if v.size == 0]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins have no out of sample residuals: {empty_bins}. \"\n            f\"No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\",\n            ResidualsUsageWarning,\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a=out_sample_residuals, size=empty_bin_size, replace=False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a=out_sample_residuals, size=10_000, replace=False\n        )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; data = pd.Series(\n...     data=np.arange(14, dtype=float),\n...     index=pd.date_range(start=\"2022-01-01\", periods=14, freq=\"D\"),\n... )\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; forecaster.summary()\n============================\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data=np.arange(14, dtype=float),\n        ...     index=pd.date_range(start=\"2022-01-01\", periods=14, freq=\"D\"),\n        ... )\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; forecaster.summary()  # doctest: +ELLIPSIS\n        ============================\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive","title":"<code>ForecasterRecursive</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>Recursive autoregressive forecaster for scikit-learn compatible estimators.</p> <p>This class turns any estimator compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. The forecaster learns to predict future values by using lagged values of the target variable and optional exogenous features. Predictions are made iteratively, where each step uses previous predictions as input for the next step (recursive strategy).</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object</code> <p>Scikit-learn compatible estimator for regression. If None, a default estimator will be initialized. Can also be passed via regressor parameter.</p> <code>None</code> <code>lags</code> <code>Union[int, List[int], ndarray, range, None]</code> <p>Lagged values of the target variable to use as predictors. Can be an integer (uses lags from 1 to lags), list of integers, numpy array, or range. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>window_features</code> <code>Union[object, List[object], None]</code> <p>List of window feature objects to compute features from the target variable. Each object must implement transform_batch() method. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>transformer_y</code> <code>Optional[object]</code> <p>Transformer object for the target variable. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>transformer_exog</code> <code>Optional[object]</code> <p>Transformer object for exogenous variables. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>weight_func</code> <code>Optional[Callable]</code> <p>Function to compute sample weights for training. Must accept an index and return an array of weights. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>Optional[int]</code> <p>Order of differencing to apply to the target variable. Must be a positive integer. Differencing is applied before creating lags. Defaults to None.</p> <code>None</code> <code>fit_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of additional keyword arguments to pass to the estimator's fit() method. Defaults to None.</p> <code>None</code> <code>binner_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of keyword arguments for QuantileBinner used in probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.</p> <code>None</code> <code>forecaster_id</code> <code>Union[str, int, None]</code> <p>Identifier for the forecaster instance. Can be a string or integer. Used for tracking and logging purposes. Defaults to None.</p> <code>None</code> <code>regressor</code> <code>object</code> <p>Alternative parameter name for estimator. If provided, used instead of estimator. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator</code> <p>Fitted scikit-learn estimator.</p> <code>lags</code> <p>Lag indices used in the model.</p> <code>lags_names</code> <p>Names of lag features (e.g., ['lag_1', 'lag_2']).</p> <code>window_features</code> <p>List of window feature transformers.</p> <code>window_features_names</code> <p>Names of window features.</p> <code>window_size</code> <p>Maximum window size needed (max of lags and window features).</p> <code>transformer_y</code> <p>Transformer for target variable.</p> <code>transformer_exog</code> <p>Transformer for exogenous variables.</p> <code>weight_func</code> <p>Function for sample weighting.</p> <code>differentiation</code> <p>Order of differencing applied.</p> <code>differentiator</code> <p>TimeSeriesDifferentiator instance if differencing is used.</p> <code>is_fitted</code> <p>Boolean indicating if forecaster has been fitted.</p> <code>fit_date</code> <p>Timestamp of the last fit operation.</p> <code>last_window_</code> <p>Last window_size observations from training data.</p> <code>index_type_</code> <p>Type of index in training data (RangeIndex or DatetimeIndex).</p> <code>index_freq_</code> <p>Frequency of DatetimeIndex if applicable.</p> <code>training_range_</code> <p>First and last index values of training data.</p> <code>series_name_in_</code> <p>Name of the target series.</p> <code>exog_in_</code> <p>Boolean indicating if exogenous variables were used in training.</p> <code>exog_names_in_</code> <p>Names of exogenous variables.</p> <code>exog_type_in_</code> <p>Type of exogenous input (Series or DataFrame).</p> <code>X_train_features_names_out_</code> <p>Names of all training features.</p> <code>in_sample_residuals_</code> <p>Residuals from training set.</p> <code>in_sample_residuals_by_bin_</code> <p>Residuals grouped by bins for probabilistic pred.</p> <code>forecaster_id</code> <p>Identifier for the forecaster instance.</p> Note <ul> <li>Either lags or window_features (or both) must be provided during initialization.</li> <li>The forecaster uses a recursive strategy where each multi-step prediction   depends on previous predictions within the same forecast horizon.</li> <li>Exogenous variables must have the same index as the target variable and must   be available for the entire prediction horizon.</li> <li>The forecaster supports point predictions, prediction intervals, bootstrapping,   quantile predictions, and probabilistic forecasts via conformal methods.</li> </ul> <p>Examples:</p> <p>Create a basic forecaster with lags:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=10\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> <p>Create a forecaster with window features and transformations:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(n_estimators=100),\n...     lags=[1, 7, 30],\n...     window_features=[RollingFeatures(stats='mean', window_sizes=7)],\n...     transformer_y=StandardScaler(),\n...     differentiation=1\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n</code></pre> <p>Create a forecaster with exogenous variables:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=Ridge(),\n...     lags=7,\n...     forecaster_id='my_forecaster'\n... )\n&gt;&gt;&gt; forecaster.fit(y, exog)\n&gt;&gt;&gt; exog_future = pd.DataFrame(\n...     {'temp': np.random.randn(5)},\n...     index=pd.RangeIndex(start=100, stop=105)\n... )\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n</code></pre> <p>Create a forecaster with probabilistic prediction configuration:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=GradientBoostingRegressor(),\n...     lags=14,\n...     binner_kwargs={'n_bins': 15, 'method': 'linear'}\n... )\n&gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>class ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    Recursive autoregressive forecaster for scikit-learn compatible estimators.\n\n    This class turns any estimator compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster. The forecaster learns to predict\n    future values by using lagged values of the target variable and optional exogenous\n    features. Predictions are made iteratively, where each step uses previous predictions\n    as input for the next step (recursive strategy).\n\n    Args:\n        estimator: Scikit-learn compatible estimator for regression. If None, a default\n            estimator will be initialized. Can also be passed via regressor parameter.\n        lags: Lagged values of the target variable to use as predictors. Can be an\n            integer (uses lags from 1 to lags), list of integers, numpy array, or range.\n            At least one of lags or window_features must be provided. Defaults to None.\n        window_features: List of window feature objects to compute features from the\n            target variable. Each object must implement transform_batch() method.\n            At least one of lags or window_features must be provided. Defaults to None.\n        transformer_y: Transformer object for the target variable. Must implement fit()\n            and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        transformer_exog: Transformer object for exogenous variables. Must implement\n            fit() and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        weight_func: Function to compute sample weights for training. Must accept an\n            index and return an array of weights. Defaults to None.\n        differentiation: Order of differencing to apply to the target variable.\n            Must be a positive integer. Differencing is applied before creating lags.\n            Defaults to None.\n        fit_kwargs: Dictionary of additional keyword arguments to pass to the estimator's\n            fit() method. Defaults to None.\n        binner_kwargs: Dictionary of keyword arguments for QuantileBinner used in\n            probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.\n        forecaster_id: Identifier for the forecaster instance. Can be a string or\n            integer. Used for tracking and logging purposes. Defaults to None.\n        regressor: Alternative parameter name for estimator. If provided, used instead\n            of estimator. Defaults to None.\n\n    Attributes:\n        estimator: Fitted scikit-learn estimator.\n        lags: Lag indices used in the model.\n        lags_names: Names of lag features (e.g., ['lag_1', 'lag_2']).\n        window_features: List of window feature transformers.\n        window_features_names: Names of window features.\n        window_size: Maximum window size needed (max of lags and window features).\n        transformer_y: Transformer for target variable.\n        transformer_exog: Transformer for exogenous variables.\n        weight_func: Function for sample weighting.\n        differentiation: Order of differencing applied.\n        differentiator: TimeSeriesDifferentiator instance if differencing is used.\n        is_fitted: Boolean indicating if forecaster has been fitted.\n        fit_date: Timestamp of the last fit operation.\n        last_window_: Last window_size observations from training data.\n        index_type_: Type of index in training data (RangeIndex or DatetimeIndex).\n        index_freq_: Frequency of DatetimeIndex if applicable.\n        training_range_: First and last index values of training data.\n        series_name_in_: Name of the target series.\n        exog_in_: Boolean indicating if exogenous variables were used in training.\n        exog_names_in_: Names of exogenous variables.\n        exog_type_in_: Type of exogenous input (Series or DataFrame).\n        X_train_features_names_out_: Names of all training features.\n        in_sample_residuals_: Residuals from training set.\n        in_sample_residuals_by_bin_: Residuals grouped by bins for probabilistic pred.\n        forecaster_id: Identifier for the forecaster instance.\n\n    Note:\n        - Either lags or window_features (or both) must be provided during initialization.\n        - The forecaster uses a recursive strategy where each multi-step prediction\n          depends on previous predictions within the same forecast horizon.\n        - Exogenous variables must have the same index as the target variable and must\n          be available for the entire prediction horizon.\n        - The forecaster supports point predictions, prediction intervals, bootstrapping,\n          quantile predictions, and probabilistic forecasts via conformal methods.\n\n    Examples:\n        Create a basic forecaster with lags:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=10\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n\n        Create a forecaster with window features and transformations:\n\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(n_estimators=100),\n        ...     lags=[1, 7, 30],\n        ...     window_features=[RollingFeatures(stats='mean', window_sizes=7)],\n        ...     transformer_y=StandardScaler(),\n        ...     differentiation=1\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n\n        Create a forecaster with exogenous variables:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)}, index=y.index)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=Ridge(),\n        ...     lags=7,\n        ...     forecaster_id='my_forecaster'\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, exog)\n        &gt;&gt;&gt; exog_future = pd.DataFrame(\n        ...     {'temp': np.random.randn(5)},\n        ...     index=pd.RangeIndex(start=100, stop=105)\n        ... )\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n\n        Create a forecaster with probabilistic prediction configuration:\n\n        &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=GradientBoostingRegressor(),\n        ...     lags=14,\n        ...     binner_kwargs={'n_bins': 15, 'method': 'linear'}\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: object = None,\n        lags: Union[int, List[int], np.ndarray, range, None] = None,\n        window_features: Union[object, List[object], None] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        binner_kwargs: Optional[Dict[str, object]] = None,\n        forecaster_id: Union[str, int, None] = None,\n        regressor: object = None,\n    ) -&gt; None:\n\n        self.estimator = copy(initialize_estimator(estimator, regressor))\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiation_max = None\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.exog_dtypes_out_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        try:\n            from spotforecast2_safe import __version__\n\n            self.spotforecast_version = __version__\n        except ImportError:\n            self.spotforecast_version = \"unknown\"\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n\n        (\n            self.lags,\n            self.lags_names,\n            self.max_lag,\n        ) = initialize_lags(type(self).__name__, lags)\n        (\n            self.window_features,\n            self.window_features_names,\n            self.max_size_window_features,\n        ) = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        self.window_size = max(\n            [\n                ws\n                for ws in [self.max_lag, self.max_size_window_features]\n                if ws is not None\n            ]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=estimator,\n            weight_func=weight_func,\n            series_weights=None,\n        )\n\n        if differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation &lt; 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation  # , window_size=self.window_size # TODO: TimeSeriesDifferentiator in preprocessing created only takes order, add window_size if needed\n            )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n            estimator=estimator, fit_kwargs=fit_kwargs\n        )\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n\n        Returns:\n            str: String representation of the forecaster with key information about its configuration and state.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; print(forecaster)  # doctest: +ELLIPSIS\n            =========================\n            ForecasterRecursive\n            =========================\n            Estimator: LinearRegression\n            Lags: [1, 2, 3]\n            Window features: []\n            Window size: 3\n            Series name: None\n            Exogenous included: False\n            Exogenous names: None\n            Transformer for y: None\n            Transformer for exog: None\n            Weight function included: False\n            Differentiation order: None\n            Training range: None\n            Training index type: None\n            Training index frequency: None\n            Estimator parameters: {...}\n            fit_kwargs: {...}\n            Creation date: ...\n            Last fit date: None\n            spotforecast version: ...\n            Python version: ...\n            Forecaster id: None\n\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Estimator: {type(self.estimator).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Estimator parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"spotforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n\n        Returns:\n            HTML string representation of the forecaster.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; forecaster._repr_html_()  # doctest: +ELLIPSIS\n            '&lt;div class=\"container-...\"&gt;...&lt;/div&gt;'\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n                &lt;ul&gt;\n                    {exog_names_in_}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Data Transformations&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Estimator Parameters&lt;/summary&gt;\n                &lt;ul&gt;\n                    {params}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n                &lt;ul&gt;\n                    {self.fit_kwargs}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"\n        Custom __setstate__ to ensure backward compatibility when unpickling.\n        Only sets __spotforecast_tags__ if not present, preserving custom tags.\n        \"\"\"\n        super().__setstate__(state)\n        if not hasattr(self, \"__spotforecast_tags__\"):\n            self.__spotforecast_tags__ = {\n                \"library\": \"spotforecast\",\n                \"forecaster_name\": \"ForecasterRecursive\",\n                \"forecaster_task\": \"regression\",\n                \"forecasting_scope\": \"single-series\",\n                \"forecasting_strategy\": \"recursive\",\n                \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n                \"requires_index_frequency\": True,\n                \"allowed_input_types_series\": [\"pandas.Series\"],\n                \"supports_exog\": True,\n                \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n                \"handles_missing_values_series\": False,\n                \"handles_missing_values_exog\": True,\n                \"supports_lags\": True,\n                \"supports_window_features\": True,\n                \"supports_transformer_series\": True,\n                \"supports_transformer_exog\": True,\n                \"supports_weight_func\": True,\n                \"supports_differentiation\": True,\n                \"prediction_types\": [\n                    \"point\",\n                    \"interval\",\n                    \"bootstrapping\",\n                    \"quantiles\",\n                    \"distribution\",\n                ],\n                \"supports_probabilistic\": True,\n                \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n                \"handles_binned_residuals\": True,\n            }\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None,\n    ) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create lagged predictors and aligned target values.\n\n        Args:\n            y: Target values used to build lag features. Expected shape is\n                (n_samples,) or (n_samples, 1).\n            X_as_pandas: If True, returns lagged features as a pandas DataFrame.\n            train_index: Index to use for the lagged feature DataFrame when\n                `X_as_pandas` is True.\n\n        Returns:\n            Tuple containing:\n                - X_data: Lagged predictors with shape (n_rows, n_lags) or None\n                  if no lags are configured.\n                - y_data: Target values aligned to the lagged predictors with\n                  shape (n_rows,).\n\n        Raises:\n            ValueError: If `X_as_pandas` is True but `train_index` is not provided.\n            ValueError: If the length of `y` is not sufficient to create the\n                specified lags.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(lags=3)\n            &gt;&gt;&gt; y = np.arange(10)\n            &gt;&gt;&gt; train_index = pd.RangeIndex(start=3, stop=10)\n            &gt;&gt;&gt; X_data, y_data = forecaster._create_lags(y=y, X_as_pandas=True, train_index=train_index)\n            &gt;&gt;&gt; isinstance(X_data, pd.DataFrame)\n            True\n            &gt;&gt;&gt; X_data.shape\n            (7, 3)\n            &gt;&gt;&gt; y_data.shape\n            (7,)\n        \"\"\"\n        if X_as_pandas and train_index is None:\n            raise ValueError(\n                \"If `X_as_pandas` is True, `train_index` must be provided.\"\n            )\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\"\n            )\n\n        X_data = None\n        if self.lags is not None:\n            # y = y.ravel() # Assuming y is already raveled\n            # Using stride_tricks for sliding window\n            y_strided = np.lib.stride_tricks.sliding_window_view(y, self.window_size)[\n                :-1\n            ]\n            X_data = y_strided[:, self.window_size - self.lags]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                    data=X_data, columns=self.lags_names, index=train_index\n                )\n\n        y_data = y[self.window_size :]\n\n        return X_data, y_data\n\n    def _create_window_features(\n        self,\n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -&gt; Tuple[List[Union[np.ndarray, pd.DataFrame]], List[str]]:\n        \"\"\"\n        Generate window features from the target series.\n\n        Args:\n            y: Target series used to compute window features. Must be a pandas\n                Series with an index aligned to `train_index` after trimming.\n            train_index: Index for the training rows to align the window features.\n            X_as_pandas: If True, keeps each window feature matrix as a pandas\n                DataFrame; otherwise converts to NumPy arrays.\n\n        Returns:\n            Tuple containing:\n                - X_train_window_features: List of window feature matrices, one\n                  per window feature transformer.\n                - X_train_window_features_names_out_: List of feature names for\n                  all generated window features.\n\n        Raises:\n            TypeError: If any window feature's `transform_batch` method does not\n                return a pandas DataFrame.\n            ValueError: If the output DataFrame from any window feature does not\n                have the same number of rows as `train_index` or if the index\n                does not match `train_index`.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; train_index = y.index[3:]  # Assuming window_size is 3\n            &gt;&gt;&gt; X_train_window_features, feature_names = forecaster._create_window_features(\n            ...     y=y,\n            ...     train_index=train_index,\n            ...     X_as_pandas=True\n            ... )\n            &gt;&gt;&gt; isinstance(X_train_window_features[0], pd.DataFrame)\n            True\n            &gt;&gt;&gt; X_train_window_features[0].shape[0] == len(train_index)\n            True\n            &gt;&gt;&gt; (X_train_window_features[0].index == train_index).all()\n            True\n\n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a pandas DataFrame.\"\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same number of rows as \"\n                    f\"the input time series - `window_size`: {len_train_index}.\"\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same index as \"\n                    f\"the input time series - `window_size`.\"\n                )\n\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        \"\"\"Create training predictors and target values.\n\n        Args:\n            y: Target series for training. Must be a pandas Series.\n            exog:\n                Optional exogenous variables for training. Can be a pandas Series or DataFrame.\n                Must have the same index as `y` and cover the same time range.\n\n        Returns:\n            Tuple containing:\n                - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided).\n                - y_train: Series of target values aligned with the predictors.\n                - X_train_features_names_out_: List of all predictor feature names.\n                - lags_names: List of lag feature names.\n                - window_features_names: List of window feature names.\n                - exog_names_in_: List of exogenous variable names (if exogenous variables are used).\n                - exog_dtypes_in_: Dictionary of input data types for exogenous variables.\n                - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).\n\n        Raises:\n            ValueError: If the length of `y` is not sufficient to create the specified lags and window features.\n            ValueError: If `exog` is provided but does not have the same index as `y` or does not cover the same time range.\n            ValueError: If `exog` is provided but contains data types that are not supported after transformation.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n            ...  exog_names_out, feature_names, exog_dtypes_in_,\n            ...  exog_dtypes_out_) = forecaster._create_train_X_y(y=y, exog=exog)\n            &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_train, pd.Series)\n            True\n            &gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\n            True\n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name=\"y\")\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n            df=y,\n            transformer=self.transformer_y,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        y_values, y_index = check_extract_values_and_index(data=y, data_label=\"`y`\")\n        if y_values.ndim == 2 and y_values.shape[1] == 1:\n            y_values = y_values.ravel()\n        train_index = y_index[self.window_size :]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                self.differentiator.fit(y_values)  # Differentiator requires fit first\n                y_values = self.differentiator.transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        X_as_pandas = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            _, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\", ignore_freq=True, return_values=False\n            )\n\n            len_y_original = len(y)\n            len_exog = len(exog)\n            len_train = len(train_index)\n\n            # Safety-critical validation: exog must be either full-length or pre-aligned\n            if len_exog == len_y_original:\n                # Standard case: exog covers full y range, trim by window_size\n                exog = exog.iloc[self.window_size :, :]\n            elif len_exog == len_train:\n                # Alternative case: exog already aligned to training index\n                pass\n            else:\n                raise ValueError(\n                    f\"Length mismatch for exogenous variables. Expected either:\\n\"\n                    f\"  - Full length matching `y`: {len_y_original} observations, OR\\n\"\n                    f\"  - Pre-aligned length: {len_train} observations (y length - window_size)\\n\"\n                    f\"Got: {len_exog} observations.\\n\"\n                    f\"Window size: {self.window_size}\"\n                )\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=fit_transformer,\n                inverse_transform=False,\n            )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n            X_as_pandas = any(\n                not pd.api.types.is_numeric_dtype(dtype)\n                or pd.api.types.is_bool_dtype(dtype)\n                for dtype in set(exog.dtypes)\n            )\n\n        X_train = []\n        X_train_features_names_out_ = []\n\n        # Create lags\n        # Note: y_values might have NaNs from differentiation.\n        # TODO: check if _create_lags handles this!\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            if isinstance(y_values, pd.Series):\n                y_vals_for_wf = y_values.iloc[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n            else:\n                y_vals_for_wf = y_values[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n\n            y_window_features = pd.Series(y_vals_for_wf, index=y_index_for_wf)\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features,\n                    X_as_pandas=X_as_pandas,\n                    train_index=train_index,\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                data=X_train, index=train_index, columns=X_train_features_names_out_\n            )\n\n        y_train = pd.Series(data=y_train, index=train_index, name=\"y\")\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        )\n\n    def create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        \"\"\"Public method to create training predictors and target values.\n\n        This method is a public wrapper around the internal method `_create_train_X_y`,\n        which generates the training predictors and target values based on the provided time series and exogenous variables.\n        It ensures that the necessary transformations and feature engineering steps are applied to prepare the data for training the forecaster.\n\n        Args:\n            y: Target series for training. Must be a pandas Series.\n            exog: Optional exogenous variables for training. Can be a pandas Series or DataFrame. Must have the same index as `y` and cover the same time range. Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided).\n                - y_train: Series of target values aligned with the predictors.\n                - X_train_features_names_out_: List of all predictor feature names.\n                - lags_names: List of lag feature names.\n                - window_features_names: List of window feature names.\n                - exog_names_in_: List of exogenous variable names (if exogenous variables are used).\n                - exog_dtypes_in_: Dictionary of input data types for exogenous variables.\n                - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n            ...  exog_names_out, feature_names, exog_dtypes_in_,\n            ...  exog_dtypes_out_) = forecaster.create_train_X_y(y=y, exog=exog)\n            &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_train, pd.Series)\n            True\n            &gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\n            True\n\n        \"\"\"\n        return self._create_train_X_y(y=y, exog=exog)\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Args:\n            y: Training time series.\n            initial_train_size: Initial size of the training set. It is the number of\n                observations used to train the forecaster before making the first\n                prediction.\n            exog: Exogenous variable/s included as predictor/s. Must have the same\n                number of observations as y and their indexes must be aligned.\n                Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: Predictor values used to train the model as pandas DataFrame.\n                - y_train: Values of the time series related to each row of X_train for\n                    each step in the form {step: y_step_[i]} as dict.\n                - X_test: Predictor values used to test the model as pandas DataFrame.\n                - y_test: Values of the time series related to each row of X_test for\n                    each step in the form {step: y_step_[i]} as dict.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; X_train, y_train, X_test, y_test = forecaster._train_test_split_one_step_ahead(y=y, initial_train_size=20, exog=exog)\n            &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_train, pd.Series)\n            True\n            &gt;&gt;&gt; isinstance(X_test, pd.DataFrame)\n            True\n            &gt;&gt;&gt; isinstance(y_test, pd.Series)\n            True\n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y=y.iloc[:initial_train_size],\n            exog=exog.iloc[:initial_train_size] if exog is not None else None,\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y=y.iloc[test_init:],\n            exog=exog.iloc[test_init:] if exog is not None else None,\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def get_params(self, deep: bool = True) -&gt; Dict[str, object]:\n        \"\"\"\n        Get parameters for this forecaster.\n\n        Args:\n            deep: If True, will return the parameters for this forecaster and\n                contained sub-objects that are estimators.\n\n        Returns:\n            params: Dictionary of parameter names mapped to their values.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; forecaster.get_params()  # doctest: +ELLIPSIS\n            {\n                'estimator': LinearRegression(), 'lags': 3, 'window_features': None,\n                'transformer_y': None, 'transformer_exog': None, 'weight_func': None,\n                'differentiation': None, 'fit_kwargs': {}, 'binner_kwargs': None, 'forecaster_id': '...'}\n        \"\"\"\n        params = {}\n        for key in [\n            \"estimator\",\n            \"lags\",\n            \"window_features\",\n            \"transformer_y\",\n            \"transformer_exog\",\n            \"weight_func\",\n            \"differentiation\",\n            \"fit_kwargs\",\n            \"binner_kwargs\",\n            \"forecaster_id\",\n        ]:\n            if hasattr(self, key):\n                params[key] = getattr(self, key)\n\n        if not deep:\n            return params\n\n        if hasattr(self, \"estimator\") and self.estimator is not None:\n            if hasattr(self.estimator, \"get_params\"):\n                for key, value in self.estimator.get_params(deep=True).items():\n                    params[f\"estimator__{key}\"] = value\n\n        return params\n\n    def set_params(\n        self, params: Dict[str, object] = None, **kwargs: object\n    ) -&gt; \"ForecasterRecursive\":\n        \"\"\"\n        Set the parameters of this forecaster.\n\n        Args:\n            params: Optional dictionary of parameter names mapped to their new values.\n                If provided, these parameters are set first.\n            **kwargs: Dictionary of parameter names mapped to their new values.\n                Parameters can be for the forecaster itself or for the contained estimator (using the `estimator__` prefix).\n\n        Returns:\n            self: The forecaster instance with updated parameters.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; forecaster.set_params(estimator__fit_intercept=False)\n            &gt;&gt;&gt; forecaster.estimator.get_params()[\"fit_intercept\"]\n            False\n        \"\"\"\n\n        # Merge params dict and kwargs\n        all_params = {}\n        if params is not None:\n            all_params.update(params)\n        all_params.update(kwargs)\n\n        if not all_params:\n            return self\n\n        valid_params = self.get_params(deep=True)\n        nested_params = {}\n\n        for key, value in all_params.items():\n            if key not in valid_params and \"__\" not in key:\n                # Relaxed check for now\n                pass\n\n            if \"__\" in key:\n                obj_name, param_name = key.split(\"__\", 1)\n                if obj_name not in nested_params:\n                    nested_params[obj_name] = {}\n                nested_params[obj_name][param_name] = value\n            else:\n                setattr(self, key, value)\n\n        for obj_name, obj_params in nested_params.items():\n            if hasattr(self, obj_name):\n                obj = getattr(self, obj_name)\n                if hasattr(obj, \"set_params\"):\n                    obj.set_params(**obj_params)\n                else:\n                    for param_name, value in obj_params.items():\n                        setattr(obj, param_name, value)\n\n        return self\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        suppress_warnings: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Fit the forecaster to the training data.\n\n        Args:\n            y:\n                  Target series for training. Must be a pandas Series.\n            exog:\n                  Optional exogenous variables for training. Can be a pandas Series or DataFrame.Must have the same index as `y` and cover the same time range. Defaults to None.\n            store_last_window:\n                  Whether to store the last window of the training series for use in prediction. Defaults to True.\n            store_in_sample_residuals:\n                  Whether to store in-sample residuals after fitting, which can be used for certain probabilistic prediction methods. Defaults to False.\n            random_state:\n                  Random seed for reproducibility when sampling residuals if `store_in_sample_residuals` is True. Defaults to 123.\n            suppress_warnings:\n                  Whether to suppress warnings during fitting, such as those related to insufficient data length for lags or window features. Defaults to False.\n\n        Returns:\n            None\n\n        Examples:\n                 &gt;&gt;&gt; import numpy as np\n                 &gt;&gt;&gt; import pandas as pd\n                 &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n                 &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n                 &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n                 &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n                 &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n                 &gt;&gt;&gt; forecaster = ForecasterRecursive(\n                 ...     estimator=LinearRegression(),\n                 ...     lags=3,\n                 ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n                 ... )\n                 &gt;&gt;&gt; forecaster.fit(y=y, exog=exog, store_in_sample_residuals=True)\n        \"\"\"\n\n        # Reset values\n        self.is_fitted = False\n        self.fit_date = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        SAMPLE_WEIGHT_NAME = \"sample_weight\"\n        if self.weight_func is not None:\n            sample_weight, _, _ = initialize_weights(\n                forecaster_name=type(self).__name__,\n                estimator=self.estimator,\n                weight_func=self.weight_func,\n                series_weights=None,\n            )\n            sample_weight = sample_weight(y.index[self.window_size :])\n            self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n        self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n            del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n        # Store attributes\n        self.last_window_ = y.iloc[-self.window_size :].copy()\n        self.index_type_ = type(y.index)\n        if isinstance(y.index, pd.DatetimeIndex):\n            self.index_freq_ = y.index.freqstr\n        else:\n            try:\n                self.index_freq_ = y.index.step\n            except AttributeError:\n                self.index_freq_ = None\n\n        self.training_range_ = y.index[[0, -1]]\n        self.series_name_in_ = y.name\n        self.exog_in_ = exog is not None\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog) if exog is not None else None\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        self.in_sample_residuals_ = None\n        self.binner_intervals_ = None\n        self.in_sample_residuals_by_bin_ = None\n\n        y_pred = self.estimator.predict(X_train)\n        self._binning_in_sample_residuals(\n            y_true=y_train.to_numpy(),\n            y_pred=y_pred,\n            store_in_sample_residuals=store_in_sample_residuals,\n            random_state=random_state,\n        )\n\n    def _create_predict_inputs(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        predict_probabilistic: bool = False,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        check_inputs: bool = True,\n    ) -&gt; Tuple[np.ndarray, Union[np.ndarray, None], pd.Index, pd.Index]:\n        \"\"\"\n        Create and validate inputs needed for prediction.\n\n        Args:\n            steps:\n                Number of future steps to predict. Can be an integer or a date (str/pd.Timestamp).\n            last_window:\n                Optional last window of observed values to use for prediction.\n                If None, uses the last window from training.\n                Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.\n            exog:\n                Optional exogenous variables for prediction.\n                Can be a pandas Series or DataFrame.\n                Must have the same structure as the exogenous variables used in training. Defaults to None.\n            check_inputs:\n                Whether to perform input validation checks. Defaults to True.\n\n        Returns:\n            Tuple containing:\n                - last_window_values:\n                    Numpy array of the last window values to use for prediction, transformed and ready for input into the prediction method.\n                - exog_values:\n                    Numpy array of exogenous variable values for prediction, transformed and ready for input into the prediction method,\n                    or None if no exogenous variables are used.\n                - prediction_index:\n                    Pandas Index for the predicted values, constructed based on the last window index and the number of steps to predict.\n                - exog_index:\n                    Pandas Index for the exogenous variable values, if exogenous variables are used; otherwise None.\n\n        Raises:\n            ValueError:\n                If input validation checks fail when `check_inputs` is True, such as if `last_window` does not have\n                the correct structure or if `exog` is not compatible with the training exogenous variables.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n            &gt;&gt;&gt; last_window = y.iloc[-3:]\n            &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n            &gt;&gt;&gt; last_window_values, exog_values, prediction_index, exog_index = forecaster._create_predict_inputs(\n            ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n            ... )\n            &gt;&gt;&gt; isinstance(last_window_values, np.ndarray)\n            True\n            &gt;&gt;&gt; isinstance(exog_values, np.ndarray)\n            True\n            &gt;&gt;&gt; isinstance(prediction_index, pd.Index)\n            True\n            &gt;&gt;&gt; isinstance(exog_index, pd.Index)\n            True\n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        # Transform steps to integer if it is a date\n        if not isinstance(steps, (int, np.integer)):\n            steps = date_to_index_position(\n                index=last_window.index, date_input=steps, method=\"prediction\"\n            )\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=self.exog_in_,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n                last_window_exog=None,\n                exog=exog,\n                exog_names_in_=self.exog_names_in_,\n                interval=None,\n                # alpha=None, # Removed alpha check for now\n            )\n\n            if predict_probabilistic:\n                check_residuals_input(\n                    forecaster_name=type(self).__name__,\n                    use_in_sample_residuals=use_in_sample_residuals,\n                    in_sample_residuals_=self.in_sample_residuals_,\n                    out_sample_residuals_=self.out_sample_residuals_,\n                    use_binned_residuals=use_binned_residuals,\n                    in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n                    out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n                )\n\n        last_window = input_to_frame(data=last_window, input_name=\"last_window\")\n        _, last_window_index = check_extract_values_and_index(\n            data=last_window,\n            data_label=\"`last_window`\",\n            ignore_freq=True,\n            return_values=False,\n        )\n\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n\n        last_window = transform_dataframe(\n            df=last_window,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, _ = check_extract_values_and_index(\n            data=last_window, data_label=\"`last_window`\"\n        )\n        last_window_values = last_window_values.ravel()\n\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        exog_values = None\n        exog_index = None\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=False,\n                inverse_transform=False,\n            )\n\n            exog_values, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\"\n            )\n\n            exog_values = (\n                exog_values if isinstance(exog, pd.Series) else exog.to_numpy()\n            )\n\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning,\n            )\n\n        return last_window_values, exog_values, prediction_index, exog_index\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Union[np.ndarray, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Create predictions recursively for the specified number of steps.\n\n        Args:\n            steps:\n                Number of future steps to predict.\n            last_window_values:\n                Numpy array of the last window values to use for prediction, transformed and ready for input into the prediction method.\n            exog_values:\n                Numpy array of exogenous variable values for prediction, transformed and ready for input into the prediction method.\n\n        Returns:\n            Numpy array of predicted values for the specified number of steps.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n            &gt;&gt;&gt; last_window = y.iloc[-3:]\n            &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n            &gt;&gt;&gt; last_window_values, exog_values, prediction_index, exog_index = forecaster._create_predict_inputs(\n            ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n            ... )\n            &gt;&gt;&gt; predictions = forecaster._recursive_predict(\n            ...     steps=5, last_window_values=last_window_values, exog_values=exog_values\n            ... )\n            &gt;&gt;&gt; isinstance(predictions, np.ndarray)\n            True\n        \"\"\"\n\n        predictions = np.full(shape=steps, fill_value=np.nan)\n\n        for step in range(steps):\n\n            X_gen = []\n\n            if self.lags is not None:\n                X_lags = last_window_values[-self.lags]\n                if X_lags.ndim == 1:\n                    X_lags = X_lags.reshape(1, -1)\n                X_gen.append(X_lags)\n\n            if self.window_features is not None:\n                X_window_features = []\n                for wf in self.window_features:\n                    wf_values = wf.transform(last_window_values)\n                    X_window_features.append(wf_values[-1:])\n\n                X_window_features = np.concatenate(X_window_features, axis=1)\n                X_gen.append(X_window_features)\n\n            if self.exog_in_:\n                X_exog = exog_values[step]\n                if X_exog.ndim &lt; 2:\n                    X_exog = X_exog.reshape(1, -1)\n                X_gen.append(X_exog)\n\n            X_gen = np.concatenate(X_gen, axis=1)\n\n            # Convert to DataFrame with feature names to avoid sklearn warning\n            if self.X_train_features_names_out_ is not None:\n                X_gen = pd.DataFrame(X_gen, columns=self.X_train_features_names_out_)\n\n            pred = self.estimator.predict(X_gen)\n            predictions[step] = pred[0]\n\n            last_window_values = np.append(last_window_values, pred)\n\n        return predictions\n\n    def _recursive_predict_bootstrapping(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        sampled_residuals: np.ndarray,\n        use_binned_residuals: bool,\n        n_boot: int,\n        exog_values: np.ndarray | None = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Vectorized bootstrap prediction - predict all n_boot iterations per step.\n        Instead of running n_boot sequential predictions, this method predicts\n        all bootstrap samples at once per step, significantly reducing overhead.\n\n        Args:\n            steps:\n                Number of steps to predict.\n            last_window_values:\n                Series values used to create the predictors needed in the first\n                iteration of the prediction (t + 1).\n            sampled_residuals:\n                Pre-sampled residuals for all bootstrap iterations.\n                - If `use_binned_residuals=True`: 3D array of shape (n_bins, steps, n_boot)\n                - If `use_binned_residuals=False`: 2D array of shape (steps, n_boot)\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values.\n                If `False`, residuals are selected randomly.\n            n_boot:\n                Number of bootstrap iterations.\n            exog_values:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n\n        Returns:\n            Numpy ndarray with the predicted values. Shape (steps, n_boot).\n\n        Raises:\n            ValueError:\n                If `sampled_residuals` does not match the expected shape/dimensions.\n            IndexError:\n                If `last_window_values` or `exog_values` are not of expected lengths.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n            &gt;&gt;&gt; _ = forecaster.fit(y=pd.Series(np.arange(10)))\n            &gt;&gt;&gt; last_window = np.array([8, 9])\n            &gt;&gt;&gt; residuals = np.random.normal(size=(3, 5)) # 3 steps, 5 boots\n            &gt;&gt;&gt; preds = forecaster._recursive_predict_bootstrapping(\n            ...     steps=3,\n            ...     last_window_values=last_window,\n            ...     sampled_residuals=residuals,\n            ...     use_binned_residuals=False,\n            ...     n_boot=5\n            ... )\n            &gt;&gt;&gt; preds.shape\n            (3, 5)\n        \"\"\"\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_)\n            if self.window_features is not None\n            else 0\n        )\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n        n_features = n_lags + n_window_features + n_exog\n\n        # Input matrix for prediction: shape (n_boot, n_features)\n        X = np.full((n_boot, n_features), fill_value=np.nan, dtype=float)\n\n        # Output predictions: shape (steps, n_boot)\n        predictions = np.full((steps, n_boot), fill_value=np.nan, dtype=float)\n\n        # Expand last_window to 2D: (window_size + steps, n_boot)\n        # Each column represents a separate bootstrap trajectory\n        last_window = np.tile(last_window_values[:, np.newaxis], (1, n_boot))\n        last_window = np.vstack([last_window, np.full((steps, n_boot), np.nan)])\n\n        estimator_name = type(self.estimator).__name__\n        is_linear = isinstance(self.estimator, LinearModel)\n        is_lightgbm = estimator_name == \"LGBMRegressor\"\n        is_xgboost = estimator_name == \"XGBRegressor\"\n\n        if is_linear:\n            coef = self.estimator.coef_\n            intercept = self.estimator.intercept_\n        elif is_lightgbm:\n            booster = self.estimator.booster_\n        elif is_xgboost:\n            booster = self.estimator.get_booster()\n\n        has_lags = self.lags is not None\n        has_window_features = self.window_features is not None\n        has_exog = exog_values is not None\n\n        for i in range(steps):\n\n            if has_lags:\n                for j, lag in enumerate(self.lags):\n                    X[:, j] = last_window[-(lag + steps - i), :]\n\n            if has_window_features:\n                window_data = last_window[: -(steps - i), :]\n                # transform accepts 2D: (window_length, n_boot) -&gt; (n_boot, n_stats)\n                # and concatenate along axis=1: (n_boot, total_window_features)\n                X[:, n_lags : n_lags + n_window_features] = np.concatenate(\n                    [wf.transform(window_data) for wf in self.window_features], axis=1\n                )\n\n            if has_exog:\n                X[:, n_lags + n_window_features :] = exog_values[i]\n\n            if is_linear:\n                pred = np.dot(X, coef) + intercept\n            elif is_lightgbm:\n                pred = booster.predict(X)\n            elif is_xgboost:\n                pred = booster.inplace_predict(X)\n            else:\n                pred = self.estimator.predict(X).ravel()\n\n            if use_binned_residuals:\n                # sampled_residuals is a 3D array: (n_bins, steps, n_boot)\n                boot_indices = np.arange(n_boot)\n                pred_bins = self.binner.transform(pred).astype(int)\n                pred += sampled_residuals[pred_bins, i, boot_indices]\n            else:\n                pred += sampled_residuals[i, :]\n\n            predictions[i, :] = pred\n            last_window[-(steps - i), :] = pred\n\n        return predictions\n\n    def predict(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; pd.Series:\n        \"\"\"\n        Predict future values recursively for the specified number of steps.\n\n        Args:\n            steps:\n                Number of future steps to predict.\n            last_window:\n                Optional last window of observed values to use for prediction. If None, uses the last window from training.\n                Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.\n            exog:\n                Optional exogenous variables for prediction. Can be a pandas Series or DataFrame.\n                Must have the same structure as the exogenous variables used in training. Defaults to None.\n            check_inputs:\n                Whether to perform input validation checks. Defaults to True.\n\n        Returns:\n            Pandas Series of predicted values for the specified number of steps,\n            indexed according to the prediction index constructed from the last window and the number of steps.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n            &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n            &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(\n            ...     estimator=LinearRegression(),\n            ...     lags=3,\n            ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n            ... )\n            &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n            &gt;&gt;&gt; last_window = y.iloc[-3:]\n            &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n            &gt;&gt;&gt; predictions = forecaster.predict(\n            ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n            ... )\n            &gt;&gt;&gt; isinstance(predictions, pd.Series)\n            True\n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, _ = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        predictions = self._recursive_predict(\n            steps=steps, last_window_values=last_window_values, exog_values=exog_values\n        )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_dataframe(\n            df=pd.Series(predictions, name=\"pred\").to_frame(),\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n        predictions = predictions.iloc[:, 0]\n        predictions.index = prediction_index\n\n        return predictions\n\n    def predict_bootstrapping(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n        n_boot: int = 250,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: int = 123,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process.\n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions.\n        See the References section for more information.\n\n        Args:\n            steps:\n                Number of steps to predict.\n                - If steps is int, number of steps to predict.\n                - If str or pandas Datetime, the prediction will be up to that date.\n            last_window:\n                Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1).\n                If `last_window = None`, the values stored in `self.last_window_` are\n                used to calculate the initial predictors, and the predictions start\n                right after training data. Defaults to None.\n            exog:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n            n_boot:\n                Number of bootstrapping iterations to perform when estimating prediction\n                intervals. Defaults to 250.\n            use_in_sample_residuals:\n                If `True`, residuals from the training data are used as proxy of\n                prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values\n                (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state:\n                Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n        Returns:\n            Pandas DataFrame with predictions generated by bootstrapping. Shape: (steps, n_boot).\n\n        Raises:\n            ValueError:\n                If `steps` is not an integer or a valid date.\n            ValueError:\n                If `exog` is missing or has invalid shape.\n            ValueError:\n                If `n_boot` is not a positive integer.\n            ValueError:\n                If `use_in_sample_residuals=True` and `in_sample_residuals_` are not available.\n            ValueError:\n                If `use_in_sample_residuals=False` and `out_sample_residuals_` are not available.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; rng = np.random.default_rng(123)\n            &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; _ = forecaster.fit(y=y)\n            &gt;&gt;&gt; boot_preds = forecaster.predict_bootstrapping(steps=3, n_boot=5)\n            &gt;&gt;&gt; boot_preds.shape\n            (3, 5)\n\n        References:\n            .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n                   https://otexts.com/fpp3/prediction-intervals.html\n        \"\"\"\n\n        (\n            last_window_values,\n            exog_values,\n            prediction_index,\n            exog_index,  # Added missing exog_index and ignored steps return which is not there\n        ) = self._create_predict_inputs(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            predict_probabilistic=True,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n            check_inputs=True,\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            # Create 3D array with sampled residuals: (n_bins, steps, n_boot)\n            n_bins = len(residuals_by_bin)\n            sampled_residuals = np.stack(\n                [\n                    residuals_by_bin[k][\n                        rng.integers(\n                            low=0, high=len(residuals_by_bin[k]), size=(steps, n_boot)\n                        )\n                    ]\n                    for k in range(n_bins)\n                ],\n                axis=0,\n            )\n        else:\n            sampled_residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n            ]\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"X does not have valid feature names\",\n                category=UserWarning,\n            )\n            boot_predictions = self._recursive_predict_bootstrapping(\n                steps=steps,\n                last_window_values=last_window_values,\n                exog_values=exog_values,\n                sampled_residuals=sampled_residuals,\n                use_binned_residuals=use_binned_residuals,\n                n_boot=n_boot,\n            )\n\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(\n                boot_predictions\n            )\n\n        if self.transformer_y:\n            boot_predictions = transform_numpy(\n                array=boot_predictions,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=True,\n            )\n\n        boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n        boot_predictions = pd.DataFrame(\n            data=boot_predictions, index=prediction_index, columns=boot_columns\n        )\n\n        return boot_predictions\n\n    def _predict_interval_conformal(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n        nominal_coverage: float = 0.95,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate prediction intervals using the conformal prediction\n        split method [1]_.\n\n        Args:\n            steps:\n                Number of steps to predict.\n                - If steps is int, number of steps to predict.\n                - If str or pandas Datetime, the prediction will be up to that date.\n            last_window:\n                Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1).\n                If `last_window = None`, the values stored in` self.last_window_` are\n                used to calculate the initial predictors, and the predictions start\n                right after training data. Defaults to None.\n            exog:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n            nominal_coverage:\n                Nominal coverage, also known as expected coverage, of the prediction\n                intervals. Must be between 0 and 1. Defaults to 0.95.\n            use_in_sample_residuals:\n                If `True`, residuals from the training data are used as proxy of\n                prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values\n                (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n\n        Returns:\n            Pandas DataFrame with values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Raises:\n            ValueError:\n                If `nominal_coverage` is not between 0 and 1.\n            ValueError:\n                If inputs are invalid (checked by `_create_predict_inputs`).\n\n        Examples:\n            &gt;&gt;&gt; # Internal method, typically used via predict_interval(method='conformal')\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; rng = np.random.default_rng(123)\n            &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; _ = forecaster.fit(y=y)\n            &gt;&gt;&gt; preds = forecaster._predict_interval_conformal(steps=3, nominal_coverage=0.9)\n            &gt;&gt;&gt; preds.columns.tolist()\n            ['pred', 'lower_bound', 'upper_bound']\n\n        References:\n            .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n                   https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, exog_index = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                predict_probabilistic=True,\n                use_in_sample_residuals=use_in_sample_residuals,\n                use_binned_residuals=use_binned_residuals,\n                check_inputs=True,\n            )\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"X does not have valid feature names\",\n                category=UserWarning,\n            )\n            predictions = self._recursive_predict(\n                steps=steps,\n                last_window_values=last_window_values,\n                exog_values=exog_values,\n            )\n\n        if use_binned_residuals:\n            # Fallback to global residuals if bin is empty\n            if len(residuals) &gt; 0:\n                global_cf = np.quantile(np.abs(residuals), nominal_coverage)\n            else:\n                global_cf = np.nan\n\n            correction_factor_by_bin = {}\n            for k, v in residuals_by_bin.items():\n                if len(v) &gt; 0:\n                    correction_factor_by_bin[k] = np.quantile(\n                        np.abs(v), nominal_coverage\n                    )\n                else:\n                    correction_factor_by_bin[k] = global_cf\n\n            replace_func = np.vectorize(\n                lambda x: correction_factor_by_bin.get(x, global_cf)\n            )\n\n            predictions_bin = self.binner.transform(predictions)\n            correction_factor = replace_func(predictions_bin)\n        else:\n            correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n        lower_bound = predictions - correction_factor\n        upper_bound = predictions + correction_factor\n        predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        if self.transformer_y:\n            predictions = transform_numpy(\n                array=predictions,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=True,\n            )\n\n        predictions = pd.DataFrame(\n            data=predictions,\n            index=prediction_index,\n            columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n        )\n\n        return predictions\n\n    def predict_interval(\n        self,\n        steps: int | str | pd.Timestamp,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n        method: str = \"bootstrapping\",\n        interval: float | list[float] | tuple[float] = [5, 95],\n        n_boot: int = 250,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: int = 123,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead and estimate prediction intervals using either\n        bootstrapping or conformal prediction methods. Refer to the References\n        section for additional details on these methods.\n\n        Args:\n            steps:\n                Number of steps to predict.\n                - If steps is int, number of steps to predict.\n                - If str or pandas Datetime, the prediction will be up to that date.\n            last_window:\n                Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1).\n                If `last_window = None`, the values stored in `self.last_window_` are\n                used to calculate the initial predictors, and the predictions start\n                right after training data. Defaults to None.\n            exog:\n                Exogenous variable/s included as predictor/s. Defaults to None.\n            method:\n                Technique used to estimate prediction intervals. Available options:\n                - 'bootstrapping': Bootstrapping is used to generate prediction\n                  intervals [1]_.\n                - 'conformal': Employs the conformal prediction split method for\n                  interval estimation [2]_.\n                Defaults to 'bootstrapping'.\n            interval:\n                Confidence level of the prediction interval. Interpretation depends\n                on the method used:\n                - If `float`, represents the nominal (expected) coverage (between 0\n                  and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n                  percentiles.\n                - If `list` or `tuple`, defines the exact percentiles to compute, which\n                  must be between 0 and 100 inclusive. For example, interval\n                  of 95% should be as `interval = [2.5, 97.5]`.\n                - When using `method='conformal'`, the interval must be a float or\n                  a list/tuple defining a symmetric interval.\n                Defaults to [5, 95].\n            n_boot:\n                Number of bootstrapping iterations to perform when estimating prediction\n                intervals. Defaults to 250.\n            use_in_sample_residuals:\n                If `True`, residuals from the training data are used as proxy of\n                prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals:\n                If `True`, residuals are selected based on the predicted values\n                (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state:\n                Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n        Returns:\n            Pandas DataFrame with values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Raises:\n            ValueError:\n                If `method` is not 'bootstrapping' or 'conformal'.\n            ValueError:\n                 If `interval` is invalid or not compatible with the chosen method.\n            ValueError:\n                If inputs (`steps`, `exog`, etc.) are invalid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; rng = np.random.default_rng(123)\n            &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n            &gt;&gt;&gt; _ = forecaster.fit(y=y)\n            &gt;&gt;&gt; # Bootstrapping method\n            &gt;&gt;&gt; intervals_boot = forecaster.predict_interval(\n            ...     steps=3, method='bootstrapping', interval=[5, 95]\n            ... )\n            &gt;&gt;&gt; intervals_boot.columns.tolist()\n            ['pred', 'lower_bound', 'upper_bound']\n\n            &gt;&gt;&gt; # Conformal method\n            &gt;&gt;&gt; intervals_conf = forecaster.predict_interval(\n            ...     steps=3, method='conformal', interval=0.95\n            ... )\n            &gt;&gt;&gt; intervals_conf.columns.tolist()\n            ['pred', 'lower_bound', 'upper_bound']\n\n        References:\n            .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n                   https://otexts.com/fpp3/prediction-intervals.html\n            .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n                   https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n        \"\"\"\n\n        if method == \"bootstrapping\":\n\n            if isinstance(interval, (list, tuple)):\n                check_interval(interval=interval, ensure_symmetric_intervals=False)\n                interval = np.array(interval) / 100\n            else:\n                check_interval(alpha=interval, alpha_literal=\"interval\")\n                interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n            boot_predictions = self.predict_bootstrapping(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                n_boot=n_boot,\n                random_state=random_state,\n                use_in_sample_residuals=use_in_sample_residuals,\n                use_binned_residuals=use_binned_residuals,\n            )\n\n            predictions = self.predict(\n                steps=steps, last_window=last_window, exog=exog, check_inputs=False\n            )\n\n            predictions_interval = boot_predictions.quantile(\n                q=interval, axis=1\n            ).transpose()\n            predictions_interval.columns = [\"lower_bound\", \"upper_bound\"]\n            predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        elif method == \"conformal\":\n\n            if isinstance(interval, (list, tuple)):\n                check_interval(interval=interval, ensure_symmetric_intervals=True)\n                nominal_coverage = (interval[1] - interval[0]) / 100\n            else:\n                check_interval(alpha=interval, alpha_literal=\"interval\")\n                nominal_coverage = interval\n\n            predictions = self._predict_interval_conformal(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                nominal_coverage=nominal_coverage,\n                use_in_sample_residuals=use_in_sample_residuals,\n                use_binned_residuals=use_binned_residuals,\n            )\n        else:\n            raise ValueError(\n                f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n            )\n\n        return predictions\n\n    def _binning_in_sample_residuals(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Bin residuals according to the predicted value each residual is\n        associated with.\n        \"\"\"\n        residuals = y_true - y_pred\n\n        if self.binner_kwargs is not None:\n            self.binner.fit(y_pred)\n            if hasattr(self.binner, \"intervals_\"):\n                self.binner_intervals_ = self.binner.intervals_\n            else:\n                self.binner_intervals_ = {\n                    i: (self.binner.bins_[i - 1], self.binner.bins_[i])\n                    for i in range(1, len(self.binner.bins_))\n                }\n\n        if store_in_sample_residuals:\n            rng = np.random.default_rng(seed=random_state)\n\n            if self.binner_kwargs is not None:\n                data = pd.DataFrame({\"prediction\": y_pred, \"residuals\": residuals})\n                data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n                self.in_sample_residuals_by_bin_ = (\n                    data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n                )\n\n                max_sample = 10_000 // self.binner.n_bins\n                for k, v in self.in_sample_residuals_by_bin_.items():\n                    if len(v) &gt; max_sample:\n                        self.in_sample_residuals_by_bin_[k] = rng.choice(\n                            v, size=max_sample, replace=False\n                        )\n\n            if len(residuals) &gt; 10_000:\n                residuals = rng.choice(residuals, size=10_000, replace=False)\n\n            self.in_sample_residuals_ = residuals\n\n    def set_in_sample_residuals(\n        self,\n        y: pd.Series,\n        exog: pd.Series | pd.DataFrame | None = None,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set in-sample residuals in case they were not calculated during the\n        training process.\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_in_sample_residuals()`.\"\n            )\n\n        check_y(y=y)\n        y_index_range = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )[1][[0, -1]]\n\n        if not y_index_range.equals(self.training_range_):\n            raise IndexError(\n                f\"The index range of `y` does not match the range \"\n                f\"used during training. Please ensure the index is aligned \"\n                f\"with the training data.\\n\"\n                f\"    Expected : {self.training_range_}\\n\"\n                f\"    Received : {y_index_range}\"\n            )\n\n        (\n            X_train,\n            y_train,\n            _,\n            _,\n            _,\n            X_train_features_names_out_,\n            *_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        if not X_train_features_names_out_ == self.X_train_features_names_out_:\n            raise ValueError(\n                f\"Feature mismatch detected after matrix creation. The features \"\n                f\"generated from the provided data do not match those used during \"\n                f\"the training process. To correctly set in-sample residuals, \"\n                f\"ensure that the same data and preprocessing steps are applied.\\n\"\n                f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n                f\"    Current output  : {X_train_features_names_out_}\"\n            )\n\n        self._binning_in_sample_residuals(\n            y_true=y_train.to_numpy(),\n            y_pred=self.estimator.predict(X_train).ravel(),\n            store_in_sample_residuals=True,\n            random_state=random_state,\n        )\n\n    def set_out_sample_residuals(\n        self,\n        y_true: np.ndarray | pd.Series,\n        y_pred: np.ndarray | pd.Series,\n        append: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`.\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n\n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        if self.transformer_y:\n            y_true = transform_numpy(\n                array=y_true,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=False,\n            )\n            y_pred = transform_numpy(\n                array=y_pred,\n                transformer=self.transformer_y,\n                fit=False,\n                inverse_transform=False,\n            )\n\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation :]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation :]\n\n        data = pd.DataFrame(\n            {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n        ).dropna()\n        y_pred = data[\"prediction\"].to_numpy()\n        residuals = data[\"residuals\"].to_numpy()\n\n        if self.binner is not None:\n            data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n            residuals_by_bin = (\n                data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n            )\n        else:\n            residuals_by_bin = {}\n\n        out_sample_residuals = (\n            np.array([])\n            if self.out_sample_residuals_ is None\n            else self.out_sample_residuals_\n        )\n        out_sample_residuals_by_bin = (\n            {}\n            if self.out_sample_residuals_by_bin_ is None\n            else self.out_sample_residuals_by_bin_\n        )\n        if append:\n            out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n            for k, v in residuals_by_bin.items():\n                if k in out_sample_residuals_by_bin:\n                    out_sample_residuals_by_bin[k] = np.concatenate(\n                        (out_sample_residuals_by_bin[k], v)\n                    )\n                else:\n                    out_sample_residuals_by_bin[k] = v\n        else:\n            out_sample_residuals = residuals\n            out_sample_residuals_by_bin = residuals_by_bin\n\n        if self.binner is not None:\n            max_samples = 10_000 // self.binner.n_bins\n            rng = np.random.default_rng(seed=random_state)\n\n            for k, v in out_sample_residuals_by_bin.items():\n                if len(v) &gt; max_samples:\n                    out_sample_residuals_by_bin[k] = rng.choice(\n                        v, size=max_samples, replace=False\n                    )\n\n            bin_keys = (\n                [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n            )\n            empty_bins = [\n                k\n                for k in bin_keys\n                if k not in out_sample_residuals_by_bin\n                or len(out_sample_residuals_by_bin[k]) == 0\n            ]\n\n            if empty_bins:\n                warnings.warn(\n                    f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                    f\"No predicted values fall in the interval \"\n                    f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                    f\"Empty bins will be filled with a random sample of residuals.\",\n                    ResidualsUsageWarning,\n                )\n                empty_bin_size = min(max_samples, len(out_sample_residuals))\n                for k in empty_bins:\n                    out_sample_residuals_by_bin[k] = rng.choice(\n                        a=out_sample_residuals, size=empty_bin_size, replace=False\n                    )\n\n        self.out_sample_residuals_ = out_sample_residuals\n        self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a ForecasterRecursive object is printed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the forecaster with key information about its configuration and state.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; print(forecaster)\n=========================\nForecasterRecursive\n=========================\nEstimator: LinearRegression\nLags: [1, 2, 3]\nWindow features: []\nWindow size: 3\nSeries name: None\nExogenous included: False\nExogenous names: None\nTransformer for y: None\nTransformer for exog: None\nWeight function included: False\nDifferentiation order: None\nTraining range: None\nTraining index type: None\nTraining index frequency: None\nEstimator parameters: {...}\nfit_kwargs: {...}\nCreation date: ...\nLast fit date: None\nspotforecast version: ...\nPython version: ...\nForecaster id: None\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a ForecasterRecursive object is printed.\n\n    Returns:\n        str: String representation of the forecaster with key information about its configuration and state.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; print(forecaster)  # doctest: +ELLIPSIS\n        =========================\n        ForecasterRecursive\n        =========================\n        Estimator: LinearRegression\n        Lags: [1, 2, 3]\n        Window features: []\n        Window size: 3\n        Series name: None\n        Exogenous included: False\n        Exogenous names: None\n        Transformer for y: None\n        Transformer for exog: None\n        Weight function included: False\n        Differentiation order: None\n        Training range: None\n        Training index type: None\n        Training index frequency: None\n        Estimator parameters: {...}\n        fit_kwargs: {...}\n        Creation date: ...\n        Last fit date: None\n        spotforecast version: ...\n        Python version: ...\n        Forecaster id: None\n\n    \"\"\"\n\n    params = (\n        self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n    )\n    exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Estimator: {type(self.estimator).__name__} \\n\"\n        f\"Lags: {self.lags} \\n\"\n        f\"Window features: {self.window_features_names} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Exogenous included: {self.exog_in_} \\n\"\n        f\"Exogenous names: {exog_names_in_} \\n\"\n        f\"Transformer for y: {self.transformer_y} \\n\"\n        f\"Transformer for exog: {self.transformer_exog} \\n\"\n        f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n        f\"Differentiation order: {self.differentiation} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Estimator parameters: {params} \\n\"\n        f\"fit_kwargs: {self.fit_kwargs} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"spotforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling. Only sets spotforecast_tags if not present, preserving custom tags.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"\n    Custom __setstate__ to ensure backward compatibility when unpickling.\n    Only sets __spotforecast_tags__ if not present, preserving custom tags.\n    \"\"\"\n    super().__setstate__(state)\n    if not hasattr(self, \"__spotforecast_tags__\"):\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",\n            \"forecasting_strategy\": \"recursive\",\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Public method to create training predictors and target values.</p> <p>This method is a public wrapper around the internal method <code>_create_train_X_y</code>, which generates the training predictors and target values based on the provided time series and exogenous variables. It ensures that the necessary transformations and feature engineering steps are applied to prepare the data for training the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Target series for training. Must be a pandas Series.</p> required <code>exog</code> <code>Union[Series, DataFrame, None]</code> <p>Optional exogenous variables for training. Can be a pandas Series or DataFrame. Must have the same index as <code>y</code> and cover the same time range. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Series, List[str], List[str], List[str], List[str], Dict[str, type], Dict[str, type]]</code> <p>Tuple containing: - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided). - y_train: Series of target values aligned with the predictors. - X_train_features_names_out_: List of all predictor feature names. - lags_names: List of lag feature names. - window_features_names: List of window feature names. - exog_names_in_: List of exogenous variable names (if exogenous variables are used). - exog_dtypes_in_: Dictionary of input data types for exogenous variables. - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=3,\n...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n... )\n&gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n...  exog_names_out, feature_names, exog_dtypes_in_,\n...  exog_dtypes_out_) = forecaster.create_train_X_y(y=y, exog=exog)\n&gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\nTrue\n&gt;&gt;&gt; isinstance(y_train, pd.Series)\nTrue\n&gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def create_train_X_y(\n    self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n) -&gt; Tuple[\n    pd.DataFrame,\n    pd.Series,\n    List[str],\n    List[str],\n    List[str],\n    List[str],\n    Dict[str, type],\n    Dict[str, type],\n]:\n    \"\"\"Public method to create training predictors and target values.\n\n    This method is a public wrapper around the internal method `_create_train_X_y`,\n    which generates the training predictors and target values based on the provided time series and exogenous variables.\n    It ensures that the necessary transformations and feature engineering steps are applied to prepare the data for training the forecaster.\n\n    Args:\n        y: Target series for training. Must be a pandas Series.\n        exog: Optional exogenous variables for training. Can be a pandas Series or DataFrame. Must have the same index as `y` and cover the same time range. Defaults to None.\n\n    Returns:\n        Tuple containing:\n            - X_train: DataFrame of training predictors including lags, window features, and exogenous variables (if provided).\n            - y_train: Series of target values aligned with the predictors.\n            - X_train_features_names_out_: List of all predictor feature names.\n            - lags_names: List of lag feature names.\n            - window_features_names: List of window feature names.\n            - exog_names_in_: List of exogenous variable names (if exogenous variables are used).\n            - exog_dtypes_in_: Dictionary of input data types for exogenous variables.\n            - exog_dtypes_out_: Dictionary of output data types for exogenous variables after transformation (if exogenous variables are used).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=3,\n        ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n        ... )\n        &gt;&gt;&gt; (X_train, y_train, exog_names_in_, window_features_names,\n        ...  exog_names_out, feature_names, exog_dtypes_in_,\n        ...  exog_dtypes_out_) = forecaster.create_train_X_y(y=y, exog=exog)\n        &gt;&gt;&gt; isinstance(X_train, pd.DataFrame)\n        True\n        &gt;&gt;&gt; isinstance(y_train, pd.Series)\n        True\n        &gt;&gt;&gt; feature_names == forecaster.lags_names + window_features_names + exog_names_out\n        True\n\n    \"\"\"\n    return self._create_train_X_y(y=y, exog=exog)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.fit","title":"<code>fit(y, exog=None, store_last_window=True, store_in_sample_residuals=False, random_state=123, suppress_warnings=False)</code>","text":"<p>Fit the forecaster to the training data.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Target series for training. Must be a pandas Series.</p> required <code>exog</code> <code>Union[Series, DataFrame, None]</code> <p>Optional exogenous variables for training. Can be a pandas Series or DataFrame.Must have the same index as <code>y</code> and cover the same time range. Defaults to None.</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether to store the last window of the training series for use in prediction. Defaults to True.</p> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>Whether to store in-sample residuals after fitting, which can be used for certain probabilistic prediction methods. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility when sampling residuals if <code>store_in_sample_residuals</code> is True. Defaults to 123.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>Whether to suppress warnings during fitting, such as those related to insufficient data length for lags or window features. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=3,\n...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n... )\n&gt;&gt;&gt; forecaster.fit(y=y, exog=exog, store_in_sample_residuals=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Union[pd.Series, pd.DataFrame, None] = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    suppress_warnings: bool = False,\n) -&gt; None:\n    \"\"\"\n    Fit the forecaster to the training data.\n\n    Args:\n        y:\n              Target series for training. Must be a pandas Series.\n        exog:\n              Optional exogenous variables for training. Can be a pandas Series or DataFrame.Must have the same index as `y` and cover the same time range. Defaults to None.\n        store_last_window:\n              Whether to store the last window of the training series for use in prediction. Defaults to True.\n        store_in_sample_residuals:\n              Whether to store in-sample residuals after fitting, which can be used for certain probabilistic prediction methods. Defaults to False.\n        random_state:\n              Random seed for reproducibility when sampling residuals if `store_in_sample_residuals` is True. Defaults to 123.\n        suppress_warnings:\n              Whether to suppress warnings during fitting, such as those related to insufficient data length for lags or window features. Defaults to False.\n\n    Returns:\n        None\n\n    Examples:\n             &gt;&gt;&gt; import numpy as np\n             &gt;&gt;&gt; import pandas as pd\n             &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n             &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n             &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n             &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n             &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n             &gt;&gt;&gt; forecaster = ForecasterRecursive(\n             ...     estimator=LinearRegression(),\n             ...     lags=3,\n             ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n             ... )\n             &gt;&gt;&gt; forecaster.fit(y=y, exog=exog, store_in_sample_residuals=True)\n    \"\"\"\n\n    # Reset values\n    self.is_fitted = False\n    self.fit_date = None\n\n    (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_,\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    SAMPLE_WEIGHT_NAME = \"sample_weight\"\n    if self.weight_func is not None:\n        sample_weight, _, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=self.estimator,\n            weight_func=self.weight_func,\n            series_weights=None,\n        )\n        sample_weight = sample_weight(y.index[self.window_size :])\n        self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n    self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n        del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n    # Store attributes\n    self.last_window_ = y.iloc[-self.window_size :].copy()\n    self.index_type_ = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq_ = y.index.freqstr\n    else:\n        try:\n            self.index_freq_ = y.index.step\n        except AttributeError:\n            self.index_freq_ = None\n\n    self.training_range_ = y.index[[0, -1]]\n    self.series_name_in_ = y.name\n    self.exog_in_ = exog is not None\n    self.exog_names_in_ = exog_names_in_\n    self.exog_type_in_ = type(exog) if exog is not None else None\n    self.exog_dtypes_in_ = exog_dtypes_in_\n    self.exog_dtypes_out_ = exog_dtypes_out_\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n    self.X_train_exog_names_out_ = X_train_exog_names_out_\n    self.X_train_features_names_out_ = X_train_features_names_out_\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    self.in_sample_residuals_ = None\n    self.binner_intervals_ = None\n    self.in_sample_residuals_by_bin_ = None\n\n    y_pred = self.estimator.predict(X_train)\n    self._binning_in_sample_residuals(\n        y_true=y_train.to_numpy(),\n        y_pred=y_pred,\n        store_in_sample_residuals=store_in_sample_residuals,\n        random_state=random_state,\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters for this forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, will return the parameters for this forecaster and contained sub-objects that are estimators.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>params</code> <code>Dict[str, object]</code> <p>Dictionary of parameter names mapped to their values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; forecaster.get_params()\n{\n    'estimator': LinearRegression(), 'lags': 3, 'window_features': None,\n    'transformer_y': None, 'transformer_exog': None, 'weight_func': None,\n    'differentiation': None, 'fit_kwargs': {}, 'binner_kwargs': None, 'forecaster_id': '...'}\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; Dict[str, object]:\n    \"\"\"\n    Get parameters for this forecaster.\n\n    Args:\n        deep: If True, will return the parameters for this forecaster and\n            contained sub-objects that are estimators.\n\n    Returns:\n        params: Dictionary of parameter names mapped to their values.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; forecaster.get_params()  # doctest: +ELLIPSIS\n        {\n            'estimator': LinearRegression(), 'lags': 3, 'window_features': None,\n            'transformer_y': None, 'transformer_exog': None, 'weight_func': None,\n            'differentiation': None, 'fit_kwargs': {}, 'binner_kwargs': None, 'forecaster_id': '...'}\n    \"\"\"\n    params = {}\n    for key in [\n        \"estimator\",\n        \"lags\",\n        \"window_features\",\n        \"transformer_y\",\n        \"transformer_exog\",\n        \"weight_func\",\n        \"differentiation\",\n        \"fit_kwargs\",\n        \"binner_kwargs\",\n        \"forecaster_id\",\n    ]:\n        if hasattr(self, key):\n            params[key] = getattr(self, key)\n\n    if not deep:\n        return params\n\n    if hasattr(self, \"estimator\") and self.estimator is not None:\n        if hasattr(self.estimator, \"get_params\"):\n            for key, value in self.estimator.get_params(deep=True).items():\n                params[f\"estimator__{key}\"] = value\n\n    return params\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.predict","title":"<code>predict(steps, last_window=None, exog=None, check_inputs=True)</code>","text":"<p>Predict future values recursively for the specified number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int | str | Timestamp</code> <p>Number of future steps to predict.</p> required <code>last_window</code> <code>Union[Series, DataFrame, None]</code> <p>Optional last window of observed values to use for prediction. If None, uses the last window from training. Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.</p> <code>None</code> <code>exog</code> <code>Union[Series, DataFrame, None]</code> <p>Optional exogenous variables for prediction. Can be a pandas Series or DataFrame. Must have the same structure as the exogenous variables used in training. Defaults to None.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>Whether to perform input validation checks. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Series</code> <p>Pandas Series of predicted values for the specified number of steps,</p> <code>Series</code> <p>indexed according to the prediction index constructed from the last window and the number of steps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=3,\n...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n... )\n&gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n&gt;&gt;&gt; last_window = y.iloc[-3:]\n&gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n&gt;&gt;&gt; predictions = forecaster.predict(\n...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n... )\n&gt;&gt;&gt; isinstance(predictions, pd.Series)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def predict(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: Union[pd.Series, pd.DataFrame, None] = None,\n    exog: Union[pd.Series, pd.DataFrame, None] = None,\n    check_inputs: bool = True,\n) -&gt; pd.Series:\n    \"\"\"\n    Predict future values recursively for the specified number of steps.\n\n    Args:\n        steps:\n            Number of future steps to predict.\n        last_window:\n            Optional last window of observed values to use for prediction. If None, uses the last window from training.\n            Must be a pandas Series or DataFrame with the same structure as the training target series. Defaults to None.\n        exog:\n            Optional exogenous variables for prediction. Can be a pandas Series or DataFrame.\n            Must have the same structure as the exogenous variables used in training. Defaults to None.\n        check_inputs:\n            Whether to perform input validation checks. Defaults to True.\n\n    Returns:\n        Pandas Series of predicted values for the specified number of steps,\n        indexed according to the prediction index constructed from the last window and the number of steps.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = pd.Series(np.arange(30), name='y')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(30)}, index=y.index)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=3,\n        ...     window_features=[RollingFeatures(stats='mean', window_sizes=3)]\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y=y, exog=exog)\n        &gt;&gt;&gt; last_window = y.iloc[-3:]\n        &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)}, index=pd.RangeIndex(start=30, stop=35))\n        &gt;&gt;&gt; predictions = forecaster.predict(\n        ...     steps=5, last_window=last_window, exog=exog_future, check_inputs=True\n        ... )\n        &gt;&gt;&gt; isinstance(predictions, pd.Series)\n        True\n    \"\"\"\n\n    last_window_values, exog_values, prediction_index, _ = (\n        self._create_predict_inputs(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            check_inputs=check_inputs,\n        )\n    )\n\n    predictions = self._recursive_predict(\n        steps=steps, last_window_values=last_window_values, exog_values=exog_values\n    )\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = transform_dataframe(\n        df=pd.Series(predictions, name=\"pred\").to_frame(),\n        transformer=self.transformer_y,\n        fit=False,\n        inverse_transform=True,\n    )\n\n    predictions = predictions.iloc[:, 0]\n    predictions.index = prediction_index\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions. See the References section for more information.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int | str | Timestamp</code> <p>Number of steps to predict. - If steps is int, number of steps to predict. - If str or pandas Datetime, the prediction will be up to that date.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data. Defaults to None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable/s included as predictor/s. Defaults to None.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with predictions generated by bootstrapping. Shape: (steps, n_boot).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>steps</code> is not an integer or a valid date.</p> <code>ValueError</code> <p>If <code>exog</code> is missing or has invalid shape.</p> <code>ValueError</code> <p>If <code>n_boot</code> is not a positive integer.</p> <code>ValueError</code> <p>If <code>use_in_sample_residuals=True</code> and <code>in_sample_residuals_</code> are not available.</p> <code>ValueError</code> <p>If <code>use_in_sample_residuals=False</code> and <code>out_sample_residuals_</code> are not available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; rng = np.random.default_rng(123)\n&gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; _ = forecaster.fit(y=y)\n&gt;&gt;&gt; boot_preds = forecaster.predict_bootstrapping(steps=3, n_boot=5)\n&gt;&gt;&gt; boot_preds.shape\n(3, 5)\n</code></pre> References <p>.. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process.\n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions.\n    See the References section for more information.\n\n    Args:\n        steps:\n            Number of steps to predict.\n            - If steps is int, number of steps to predict.\n            - If str or pandas Datetime, the prediction will be up to that date.\n        last_window:\n            Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data. Defaults to None.\n        exog:\n            Exogenous variable/s included as predictor/s. Defaults to None.\n        n_boot:\n            Number of bootstrapping iterations to perform when estimating prediction\n            intervals. Defaults to 250.\n        use_in_sample_residuals:\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals:\n            If `True`, residuals are selected based on the predicted values\n            (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state:\n            Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n    Returns:\n        Pandas DataFrame with predictions generated by bootstrapping. Shape: (steps, n_boot).\n\n    Raises:\n        ValueError:\n            If `steps` is not an integer or a valid date.\n        ValueError:\n            If `exog` is missing or has invalid shape.\n        ValueError:\n            If `n_boot` is not a positive integer.\n        ValueError:\n            If `use_in_sample_residuals=True` and `in_sample_residuals_` are not available.\n        ValueError:\n            If `use_in_sample_residuals=False` and `out_sample_residuals_` are not available.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; rng = np.random.default_rng(123)\n        &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; _ = forecaster.fit(y=y)\n        &gt;&gt;&gt; boot_preds = forecaster.predict_bootstrapping(steps=3, n_boot=5)\n        &gt;&gt;&gt; boot_preds.shape\n        (3, 5)\n\n    References:\n        .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n               https://otexts.com/fpp3/prediction-intervals.html\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index,\n        exog_index,  # Added missing exog_index and ignored steps return which is not there\n    ) = self._create_predict_inputs(\n        steps=steps,\n        last_window=last_window,\n        exog=exog,\n        predict_probabilistic=True,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        check_inputs=True,\n    )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    rng = np.random.default_rng(seed=random_state)\n    if use_binned_residuals:\n        # Create 3D array with sampled residuals: (n_bins, steps, n_boot)\n        n_bins = len(residuals_by_bin)\n        sampled_residuals = np.stack(\n            [\n                residuals_by_bin[k][\n                    rng.integers(\n                        low=0, high=len(residuals_by_bin[k]), size=(steps, n_boot)\n                    )\n                ]\n                for k in range(n_bins)\n            ],\n            axis=0,\n        )\n    else:\n        sampled_residuals = residuals[\n            rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n        ]\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n        )\n        boot_predictions = self._recursive_predict_bootstrapping(\n            steps=steps,\n            last_window_values=last_window_values,\n            exog_values=exog_values,\n            sampled_residuals=sampled_residuals,\n            use_binned_residuals=use_binned_residuals,\n            n_boot=n_boot,\n        )\n\n    if self.differentiation is not None:\n        boot_predictions = self.differentiator.inverse_transform_next_window(\n            boot_predictions\n        )\n\n    if self.transformer_y:\n        boot_predictions = transform_numpy(\n            array=boot_predictions,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n    boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n    boot_predictions = pd.DataFrame(\n        data=boot_predictions, index=prediction_index, columns=boot_columns\n    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, method='bootstrapping', interval=[5, 95], n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123)</code>","text":"<p>Predict n steps ahead and estimate prediction intervals using either bootstrapping or conformal prediction methods. Refer to the References section for additional details on these methods.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int | str | Timestamp</code> <p>Number of steps to predict. - If steps is int, number of steps to predict. - If str or pandas Datetime, the prediction will be up to that date.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data. Defaults to None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable/s included as predictor/s. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options: - 'bootstrapping': Bootstrapping is used to generate prediction   intervals [1]. - 'conformal': Employs the conformal prediction split method for   interval estimation [2]. Defaults to 'bootstrapping'.</p> <code>'bootstrapping'</code> <code>interval</code> <code>float | list[float] | tuple[float]</code> <p>Confidence level of the prediction interval. Interpretation depends on the method used: - If <code>float</code>, represents the nominal (expected) coverage (between 0   and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>   percentiles. - If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which   must be between 0 and 100 inclusive. For example, interval   of 95% should be as <code>interval = [2.5, 97.5]</code>. - When using <code>method='conformal'</code>, the interval must be a float or   a list/tuple defining a symmetric interval. Defaults to [5, 95].</p> <code>[5, 95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with values predicted by the forecaster and their estimated interval.</p> <code>DataFrame</code> <ul> <li>pred: predictions.</li> </ul> <code>DataFrame</code> <ul> <li>lower_bound: lower bound of the interval.</li> </ul> <code>DataFrame</code> <ul> <li>upper_bound: upper bound of the interval.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not 'bootstrapping' or 'conformal'.</p> <code>ValueError</code> <p>If <code>interval</code> is invalid or not compatible with the chosen method.</p> <code>ValueError</code> <p>If inputs (<code>steps</code>, <code>exog</code>, etc.) are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; rng = np.random.default_rng(123)\n&gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; _ = forecaster.fit(y=y)\n&gt;&gt;&gt; # Bootstrapping method\n&gt;&gt;&gt; intervals_boot = forecaster.predict_interval(\n...     steps=3, method='bootstrapping', interval=[5, 95]\n... )\n&gt;&gt;&gt; intervals_boot.columns.tolist()\n['pred', 'lower_bound', 'upper_bound']\n</code></pre> <pre><code>&gt;&gt;&gt; # Conformal method\n&gt;&gt;&gt; intervals_conf = forecaster.predict_interval(\n...     steps=3, method='conformal', interval=0.95\n... )\n&gt;&gt;&gt; intervals_conf.columns.tolist()\n['pred', 'lower_bound', 'upper_bound']\n</code></pre> References <p>.. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    method: str = \"bootstrapping\",\n    interval: float | list[float] | tuple[float] = [5, 95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using either\n    bootstrapping or conformal prediction methods. Refer to the References\n    section for additional details on these methods.\n\n    Args:\n        steps:\n            Number of steps to predict.\n            - If steps is int, number of steps to predict.\n            - If str or pandas Datetime, the prediction will be up to that date.\n        last_window:\n            Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data. Defaults to None.\n        exog:\n            Exogenous variable/s included as predictor/s. Defaults to None.\n        method:\n            Technique used to estimate prediction intervals. Available options:\n            - 'bootstrapping': Bootstrapping is used to generate prediction\n              intervals [1]_.\n            - 'conformal': Employs the conformal prediction split method for\n              interval estimation [2]_.\n            Defaults to 'bootstrapping'.\n        interval:\n            Confidence level of the prediction interval. Interpretation depends\n            on the method used:\n            - If `float`, represents the nominal (expected) coverage (between 0\n              and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n              percentiles.\n            - If `list` or `tuple`, defines the exact percentiles to compute, which\n              must be between 0 and 100 inclusive. For example, interval\n              of 95% should be as `interval = [2.5, 97.5]`.\n            - When using `method='conformal'`, the interval must be a float or\n              a list/tuple defining a symmetric interval.\n            Defaults to [5, 95].\n        n_boot:\n            Number of bootstrapping iterations to perform when estimating prediction\n            intervals. Defaults to 250.\n        use_in_sample_residuals:\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals:\n            If `True`, residuals are selected based on the predicted values\n            (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state:\n            Seed for the random number generator to ensure reproducibility. Defaults to 123.\n\n    Returns:\n        Pandas DataFrame with values predicted by the forecaster and their estimated interval.\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    Raises:\n        ValueError:\n            If `method` is not 'bootstrapping' or 'conformal'.\n        ValueError:\n             If `interval` is invalid or not compatible with the chosen method.\n        ValueError:\n            If inputs (`steps`, `exog`, etc.) are invalid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; rng = np.random.default_rng(123)\n        &gt;&gt;&gt; y = pd.Series(rng.normal(size=100), name='y')\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; _ = forecaster.fit(y=y)\n        &gt;&gt;&gt; # Bootstrapping method\n        &gt;&gt;&gt; intervals_boot = forecaster.predict_interval(\n        ...     steps=3, method='bootstrapping', interval=[5, 95]\n        ... )\n        &gt;&gt;&gt; intervals_boot.columns.tolist()\n        ['pred', 'lower_bound', 'upper_bound']\n\n        &gt;&gt;&gt; # Conformal method\n        &gt;&gt;&gt; intervals_conf = forecaster.predict_interval(\n        ...     steps=3, method='conformal', interval=0.95\n        ... )\n        &gt;&gt;&gt; intervals_conf.columns.tolist()\n        ['pred', 'lower_bound', 'upper_bound']\n\n    References:\n        .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n               https://otexts.com/fpp3/prediction-intervals.html\n        .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n               https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n    \"\"\"\n\n    if method == \"bootstrapping\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=False)\n            interval = np.array(interval) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n        boot_predictions = self.predict_bootstrapping(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            n_boot=n_boot,\n            random_state=random_state,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n        )\n\n        predictions = self.predict(\n            steps=steps, last_window=last_window, exog=exog, check_inputs=False\n        )\n\n        predictions_interval = boot_predictions.quantile(\n            q=interval, axis=1\n        ).transpose()\n        predictions_interval.columns = [\"lower_bound\", \"upper_bound\"]\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    elif method == \"conformal\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n            steps=steps,\n            last_window=last_window,\n            exog=exog,\n            nominal_coverage=nominal_coverage,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n        )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n        )\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.set_in_sample_residuals","title":"<code>set_in_sample_residuals(y, exog=None, random_state=123)</code>","text":"<p>Set in-sample residuals in case they were not calculated during the training process.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )[1][[0, -1]]\n\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    (\n        X_train,\n        y_train,\n        _,\n        _,\n        _,\n        X_train_features_names_out_,\n        *_,\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y_true=y_train.to_numpy(),\n        y_pred=self.estimator.predict(X_train).ravel(),\n        store_in_sample_residuals=True,\n        random_state=random_state,\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.set_out_sample_residuals","title":"<code>set_out_sample_residuals(y_true, y_pred, append=False, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    if self.transformer_y:\n        y_true = transform_numpy(\n            array=y_true,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        y_pred = transform_numpy(\n            array=y_pred,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n\n    if self.differentiation is not None:\n        differentiator = copy(self.differentiator)\n        differentiator.set_params(window_size=None)\n        y_true = differentiator.fit_transform(y_true)[self.differentiation :]\n        y_pred = differentiator.fit_transform(y_pred)[self.differentiation :]\n\n    data = pd.DataFrame(\n        {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n    ).dropna()\n    y_pred = data[\"prediction\"].to_numpy()\n    residuals = data[\"residuals\"].to_numpy()\n\n    if self.binner is not None:\n        data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = (\n            data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n        )\n    else:\n        residuals_by_bin = {}\n\n    out_sample_residuals = (\n        np.array([])\n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {}\n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    if self.binner is not None:\n        max_samples = 10_000 // self.binner.n_bins\n        rng = np.random.default_rng(seed=random_state)\n\n        for k, v in out_sample_residuals_by_bin.items():\n            if len(v) &gt; max_samples:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    v, size=max_samples, replace=False\n                )\n\n        bin_keys = (\n            [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n        )\n        empty_bins = [\n            k\n            for k in bin_keys\n            if k not in out_sample_residuals_by_bin\n            or len(out_sample_residuals_by_bin[k]) == 0\n        ]\n\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\",\n                ResidualsUsageWarning,\n            )\n            empty_bin_size = min(max_samples, len(out_sample_residuals))\n            for k in empty_bins:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    a=out_sample_residuals, size=empty_bin_size, replace=False\n                )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.set_params","title":"<code>set_params(params=None, **kwargs)</code>","text":"<p>Set the parameters of this forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, object]</code> <p>Optional dictionary of parameter names mapped to their new values. If provided, these parameters are set first.</p> <code>None</code> <code>**kwargs</code> <code>object</code> <p>Dictionary of parameter names mapped to their new values. Parameters can be for the forecaster itself or for the contained estimator (using the <code>estimator__</code> prefix).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'ForecasterRecursive'</code> <p>The forecaster instance with updated parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n&gt;&gt;&gt; forecaster.set_params(estimator__fit_intercept=False)\n&gt;&gt;&gt; forecaster.estimator.get_params()[\"fit_intercept\"]\nFalse\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def set_params(\n    self, params: Dict[str, object] = None, **kwargs: object\n) -&gt; \"ForecasterRecursive\":\n    \"\"\"\n    Set the parameters of this forecaster.\n\n    Args:\n        params: Optional dictionary of parameter names mapped to their new values.\n            If provided, these parameters are set first.\n        **kwargs: Dictionary of parameter names mapped to their new values.\n            Parameters can be for the forecaster itself or for the contained estimator (using the `estimator__` prefix).\n\n    Returns:\n        self: The forecaster instance with updated parameters.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=3)\n        &gt;&gt;&gt; forecaster.set_params(estimator__fit_intercept=False)\n        &gt;&gt;&gt; forecaster.estimator.get_params()[\"fit_intercept\"]\n        False\n    \"\"\"\n\n    # Merge params dict and kwargs\n    all_params = {}\n    if params is not None:\n        all_params.update(params)\n    all_params.update(kwargs)\n\n    if not all_params:\n        return self\n\n    valid_params = self.get_params(deep=True)\n    nested_params = {}\n\n    for key, value in all_params.items():\n        if key not in valid_params and \"__\" not in key:\n            # Relaxed check for now\n            pass\n\n        if \"__\" in key:\n            obj_name, param_name = key.split(\"__\", 1)\n            if obj_name not in nested_params:\n                nested_params[obj_name] = {}\n            nested_params[obj_name][param_name] = value\n        else:\n            setattr(self, key, value)\n\n    for obj_name, obj_params in nested_params.items():\n        if hasattr(self, obj_name):\n            obj = getattr(self, obj_name)\n            if hasattr(obj, \"set_params\"):\n                obj.set_params(**obj_params)\n            else:\n                for param_name, value in obj_params.items():\n                    setattr(obj, param_name, value)\n\n    return self\n</code></pre>"},{"location":"api/forecaster/#forecasting-utilities","title":"Forecasting Utilities","text":""},{"location":"api/forecaster/#utils","title":"utils","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils","title":"<code>spotforecast2_safe.forecaster.utils</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_extract_values_and_index","title":"<code>check_extract_values_and_index(data, data_label='`y`', ignore_freq=False, return_values=True)</code>","text":"<p>Extract values and index from a pandas Series or DataFrame, ensuring they are valid.</p> <p>Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts its values and index for use in forecasting operations. Optionally checks for index frequency consistency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data (pandas Series or DataFrame) to extract values and index from.</p> required <code>data_label</code> <code>str</code> <p>Label used in exception messages for better error reporting. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <code>ignore_freq</code> <code>bool</code> <p>If True, the frequency of the index is not checked. Defaults to False.</p> <code>False</code> <code>return_values</code> <code>bool</code> <p>If True, the values of the data are returned. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[ndarray], Index]</code> <p>A tuple containing: - values (numpy.ndarray or None): Values of the data as numpy array,   or None if return_values is False. - index (pandas.Index): Index of the data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame.</p> <code>TypeError</code> <p>If data index is not a DatetimeIndex or RangeIndex.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If DatetimeIndex has no frequency (inferred automatically).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n&gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n&gt;&gt;&gt; print(values.shape)\n(10,)\n&gt;&gt;&gt; print(type(index))\n&lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n</code></pre> <p>Extract index only:</p> <pre><code>&gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n&gt;&gt;&gt; print(index[0])\n2020-01-01 00:00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_extract_values_and_index(\n    data: Union[pd.Series, pd.DataFrame],\n    data_label: str = \"`y`\",\n    ignore_freq: bool = False,\n    return_values: bool = True,\n) -&gt; Tuple[Optional[np.ndarray], pd.Index]:\n    \"\"\"Extract values and index from a pandas Series or DataFrame, ensuring they are valid.\n\n    Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts\n    its values and index for use in forecasting operations. Optionally checks for\n    index frequency consistency.\n\n    Args:\n        data: Input data (pandas Series or DataFrame) to extract values and index from.\n        data_label: Label used in exception messages for better error reporting.\n            Defaults to \"`y`\".\n        ignore_freq: If True, the frequency of the index is not checked.\n            Defaults to False.\n        return_values: If True, the values of the data are returned.\n            Defaults to True.\n\n    Returns:\n        tuple: A tuple containing:\n            - values (numpy.ndarray or None): Values of the data as numpy array,\n              or None if return_values is False.\n            - index (pandas.Index): Index of the data.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame.\n        TypeError: If data index is not a DatetimeIndex or RangeIndex.\n\n    Warnings:\n        UserWarning: If DatetimeIndex has no frequency (inferred automatically).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n        &gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n        &gt;&gt;&gt; print(values.shape)\n        (10,)\n        &gt;&gt;&gt; print(type(index))\n        &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n\n        Extract index only:\n        &gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n        &gt;&gt;&gt; print(index[0])\n        2020-01-01 00:00:00\n    \"\"\"\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"{data_label} must be a pandas Series or DataFrame.\")\n\n    if not isinstance(data.index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(f\"{data_label} must have a pandas DatetimeIndex or RangeIndex.\")\n\n    if isinstance(data.index, pd.DatetimeIndex) and not ignore_freq:\n        if data.index.freq is None:\n            warnings.warn(\n                f\"{data_label} has a DatetimeIndex but no frequency. \"\n                \"The frequency has been inferred from the index.\",\n                UserWarning,\n            )\n\n    values = data.to_numpy() if return_values else None\n\n    return values, data.index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_preprocess_series","title":"<code>check_preprocess_series(series)</code>","text":"<p>Check and preprocess <code>series</code> argument in <code>ForecasterRecursiveMultiSeries</code> class.</p> <pre><code>- If `series` is a wide-format pandas DataFrame, each column represents a\ndifferent time series, and the index must be either a `DatetimeIndex` or\na `RangeIndex` with frequency or step size, as appropriate\n- If `series` is a long-format pandas DataFrame with a MultiIndex, the\nfirst level of the index must contain the series IDs, and the second\nlevel must be a `DatetimeIndex` with the same frequency across all series.\n- If series is a dictionary, each key must be a series ID, and each value\nmust be a named pandas Series. All series must have the same index, which\nmust be either a `DatetimeIndex` or a `RangeIndex`, and they must share the\nsame frequency or step size, as appropriate.\n</code></pre> <p>When <code>series</code> is a pandas DataFrame, it is converted to a dictionary of pandas Series, where the keys are the series IDs and the values are the Series with the same index as the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>DataFrame | dict[str, Series | DataFrame]</code> <p>pandas DataFrame or dictionary of pandas Series/DataFrames</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Series], dict[str, Index]]</code> <p>tuple[dict[str, pd.Series], dict[str, pd.Index]]: - series_dict: Dictionary where keys are series IDs and values are pandas Series. - series_indexes: Dictionary where keys are series IDs and values are the index of each series.</p> <p>Raises:     TypeError:         If <code>series</code> is not a pandas DataFrame or a dictionary of pandas Series/DataFrames.     TypeError:         If the index of <code>series</code> is not a DatetimeIndex or RangeIndex with frequency/step size.     ValueError:         If the series in <code>series</code> have different frequencies or step sizes.     ValueError:         If all values of any series are NaN.     UserWarning:         If <code>series</code> is a wide-format DataFrame, only the first column will be used as series values.     UserWarning:         If <code>series</code> is a DataFrame (either wide or long format), additional internal transformations are required, which can increase computational time.         It is recommended to use a dictionary of pandas Series instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import check_preprocess_series\n&gt;&gt;&gt; # Example with wide-format DataFrame\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=5, freq='D')\n&gt;&gt;&gt; df_wide = pd.DataFrame({\n...     'series_1': [1, 2, 3, 4, 5],\n...     'series_2': [5, 4, 3, 2, 1],\n... }, index=dates)\n&gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_wide)\nUserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'series_1', will be used as series values. All other columns will be ignored.\nUserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\nIt is recommended to use a dictionary of pandas Series instead.\n&gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05'],\n              dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt; # Example with long-format DataFrame\n&gt;&gt;&gt; df_long = pd.DataFrame({\n...     'series_id': ['series_1'] * 5 + ['series_2'] * 5,\n...     'value': [1, 2, 3, 4, 5, 5, 4, 3, 2, 1],\n... }, index=pd.MultiIndex.from_product([['series_1', 'series_2'], dates], names=['series_id', 'date']))\n&gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_long)\nUserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'value', will be used as series values. All other columns will be ignored.\nUserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\nIt is recommended to use a dictionary of pandas Series instead.\n&gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                  '2020-01-05'],\n                 dtype='datetime64[ns]', freq='D')\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with dictionary of Series\n&gt;&gt;&gt; series_dict_input = {\n...     'series_1': pd.Series([1, 2, 3, 4, 5], index=dates),\n...     'series_2': pd.Series([5, 4, 3, 2, 1], index=dates),\n... }\n&gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n&gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05'],\n              dtype='datetime64[ns]', freq='D')\n    &gt;&gt;&gt; # Example with dictionary of DataFrames\n    &gt;&gt;&gt; df_series_1 = pd.DataFrame({'value': [1, 2, 3, 4, 5]}, index=dates)\n    &gt;&gt;&gt; df_series_2 = pd.DataFrame({'value': [5, 4, 3, 2, 1]}, index=dates)\n    &gt;&gt;&gt; series_dict_input = {\n    ...     'series_1': df_series_1,\n    ...     'series_2': df_series_2,\n    ... }\n    &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n    &gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05'],\n              dtype='datetime64[ns]', freq='D')\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_preprocess_series(\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n) -&gt; tuple[dict[str, pd.Series], dict[str, pd.Index]]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n        - If `series` is a wide-format pandas DataFrame, each column represents a\n        different time series, and the index must be either a `DatetimeIndex` or\n        a `RangeIndex` with frequency or step size, as appropriate\n        - If `series` is a long-format pandas DataFrame with a MultiIndex, the\n        first level of the index must contain the series IDs, and the second\n        level must be a `DatetimeIndex` with the same frequency across all series.\n        - If series is a dictionary, each key must be a series ID, and each value\n        must be a named pandas Series. All series must have the same index, which\n        must be either a `DatetimeIndex` or a `RangeIndex`, and they must share the\n        same frequency or step size, as appropriate.\n\n    When `series` is a pandas DataFrame, it is converted to a dictionary of pandas\n    Series, where the keys are the series IDs and the values are the Series with\n    the same index as the original DataFrame.\n\n    Args:\n        series: pandas DataFrame or dictionary of pandas Series/DataFrames\n\n    Returns:\n        tuple[dict[str, pd.Series], dict[str, pd.Index]]:\n            - series_dict: Dictionary where keys are series IDs and values are pandas Series.\n            - series_indexes: Dictionary where keys are series IDs and values are the index of each series.\n    Raises:\n        TypeError:\n            If `series` is not a pandas DataFrame or a dictionary of pandas Series/DataFrames.\n        TypeError:\n            If the index of `series` is not a DatetimeIndex or RangeIndex with frequency/step size.\n        ValueError:\n            If the series in `series` have different frequencies or step sizes.\n        ValueError:\n            If all values of any series are NaN.\n        UserWarning:\n            If `series` is a wide-format DataFrame, only the first column will be used as series values.\n        UserWarning:\n            If `series` is a DataFrame (either wide or long format), additional internal transformations are required, which can increase computational time.\n            It is recommended to use a dictionary of pandas Series instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import check_preprocess_series\n        &gt;&gt;&gt; # Example with wide-format DataFrame\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=5, freq='D')\n        &gt;&gt;&gt; df_wide = pd.DataFrame({\n        ...     'series_1': [1, 2, 3, 4, 5],\n        ...     'series_2': [5, 4, 3, 2, 1],\n        ... }, index=dates)\n        &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_wide)\n        UserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'series_1', will be used as series values. All other columns will be ignored.\n        UserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\n        It is recommended to use a dictionary of pandas Series instead.\n        &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                       '2020-01-05'],\n                      dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt; # Example with long-format DataFrame\n        &gt;&gt;&gt; df_long = pd.DataFrame({\n        ...     'series_id': ['series_1'] * 5 + ['series_2'] * 5,\n        ...     'value': [1, 2, 3, 4, 5, 5, 4, 3, 2, 1],\n        ... }, index=pd.MultiIndex.from_product([['series_1', 'series_2'], dates], names=['series_id', 'date']))\n        &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_long)\n        UserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'value', will be used as series values. All other columns will be ignored.\n        UserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\n        It is recommended to use a dictionary of pandas Series instead.\n        &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                          '2020-01-05'],\n                         dtype='datetime64[ns]', freq='D')\n\n        &gt;&gt;&gt; # Example with dictionary of Series\n        &gt;&gt;&gt; series_dict_input = {\n        ...     'series_1': pd.Series([1, 2, 3, 4, 5], index=dates),\n        ...     'series_2': pd.Series([5, 4, 3, 2, 1], index=dates),\n        ... }\n        &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n        &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                       '2020-01-05'],\n                      dtype='datetime64[ns]', freq='D')\n            &gt;&gt;&gt; # Example with dictionary of DataFrames\n            &gt;&gt;&gt; df_series_1 = pd.DataFrame({'value': [1, 2, 3, 4, 5]}, index=dates)\n            &gt;&gt;&gt; df_series_2 = pd.DataFrame({'value': [5, 4, 3, 2, 1]}, index=dates)\n            &gt;&gt;&gt; series_dict_input = {\n            ...     'series_1': df_series_1,\n            ...     'series_2': df_series_2,\n            ... }\n            &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n            &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                       '2020-01-05'],\n                      dtype='datetime64[ns]', freq='D')\n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n            f\"Got {type(series)}.\"\n        )\n\n    if isinstance(series, pd.DataFrame):\n\n        if not isinstance(series.index, pd.MultiIndex):\n            _, _ = check_extract_values_and_index(\n                data=series, data_label=\"`series`\", return_values=False\n            )\n            series = series.copy()\n            series.index.name = None\n            series_dict = series.to_dict(orient=\"series\")\n        else:\n            if not isinstance(series.index.levels[1], pd.DatetimeIndex):\n                raise TypeError(\n                    f\"The second level of the MultiIndex in `series` must be a \"\n                    f\"pandas DatetimeIndex with the same frequency for each series. \"\n                    f\"Found {type(series.index.levels[1])}.\"\n                )\n\n            first_col = series.columns[0]\n            if len(series.columns) != 1:\n                warnings.warn(\n                    f\"`series` DataFrame has multiple columns. Only the values of \"\n                    f\"first column, '{first_col}', will be used as series values. \"\n                    f\"All other columns will be ignored.\",\n                    IgnoredArgumentWarning,\n                )\n\n            series = series.copy()\n            series.index = series.index.set_names([series.index.names[0], None])\n            series_dict = {\n                series_id: series.loc[series_id][first_col].rename(series_id)\n                for series_id in series.index.levels[0]\n            }\n\n        warnings.warn(\n            \"Passing a DataFrame (either wide or long format) as `series` requires \"\n            \"additional internal transformations, which can increase computational \"\n            \"time. It is recommended to use a dictionary of pandas Series instead. \",\n            InputTypeWarning,\n        )\n\n    else:\n\n        not_valid_series = [\n            k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))\n        ]\n        if not_valid_series:\n            raise TypeError(\n                f\"If `series` is a dictionary, all series must be a named \"\n                f\"pandas Series or a pandas DataFrame with a single column. \"\n                f\"Review series: {not_valid_series}\"\n            )\n\n        series_dict = {k: v.copy() for k, v in series.items()}\n\n    not_valid_index = []\n    indexes_freq = set()\n    series_indexes = {}\n    for k, v in series_dict.items():\n        if isinstance(v, pd.DataFrame):\n            if v.shape[1] != 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: '{k}'\"\n                )\n            series_dict[k] = v.iloc[:, 0]\n\n        series_dict[k].name = k\n        idx = v.index\n        if isinstance(idx, pd.DatetimeIndex):\n            indexes_freq.add(idx.freq)\n        elif isinstance(idx, pd.RangeIndex):\n            indexes_freq.add(idx.step)\n        else:\n            not_valid_index.append(k)\n\n        if v.isna().to_numpy().all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n\n        series_indexes[k] = idx\n\n    if not_valid_index:\n        raise TypeError(\n            f\"If `series` is a dictionary, all series must have a Pandas \"\n            f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            f\"Review series: {not_valid_index}\"\n        )\n    if None in indexes_freq:\n        raise ValueError(\n            \"If `series` is a dictionary, all series must have a Pandas \"\n            \"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            \"If it a MultiIndex DataFrame, the second level must be a DatetimeIndex \"\n            \"with the same frequency for each series. Found series with no \"\n            \"frequency or step.\"\n        )\n    if not len(indexes_freq) == 1:\n        raise ValueError(\n            f\"If `series` is a dictionary, all series must have a Pandas \"\n            f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            f\"If it a MultiIndex DataFrame, the second level must be a DatetimeIndex \"\n            f\"with the same frequency for each series. \"\n            f\"Found frequencies: {sorted(indexes_freq)}\"\n        )\n\n    return series_dict, series_indexes\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_residuals_input","title":"<code>check_residuals_input(forecaster_name, use_in_sample_residuals, in_sample_residuals_, out_sample_residuals_, use_binned_residuals, in_sample_residuals_by_bin_, out_sample_residuals_by_bin_, levels=None, encoding=None)</code>","text":"<p>Check residuals input arguments in Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>use_in_sample_residuals</code> <code>bool</code> <p>bool Indicates if in sample or out sample residuals are used.</p> required <code>in_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting training data.</p> required <code>out_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting non training data.</p> required <code>use_binned_residuals</code> <code>bool</code> <p>bool Indicates if residuals are binned.</p> required <code>in_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict In sample residuals binned according to the predicted value each residual is associated with.</p> required <code>out_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict Out of sample residuals binned according to the predicted value each residual is associated with.</p> required <code>levels</code> <code>list[str] | None</code> <p>list, default None Names of the series (levels) to be predicted (Forecasters multiseries).</p> <code>None</code> <code>encoding</code> <code>str | None</code> <p>str, default None Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <p>from spotforecast2_safe.forecaster.utils import check_residuals_input import numpy as np forecaster_name = \"ForecasterRecursiveMultiSeries\" use_in_sample_residuals = True in_sample_residuals_ = np.array([0.1, -0.2 out_sample_residuals_ = np.array([0.3, -0.1]) use_binned_residuals = False check_residuals_input(     forecaster_name,     use_in_sample_residuals,     in_sample_residuals_,     out_sample_residuals_,     use_binned_residuals,     in_sample_residuals_by_bin_=None,     out_sample_residuals_by_bin_=None,     levels=['series_1', 'series_2'],     encoding='onehot' )</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    out_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    levels: list[str] | None = None,\n    encoding: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        use_in_sample_residuals: bool\n            Indicates if in sample or out sample residuals are used.\n        in_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting training data.\n        out_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting non training data.\n        use_binned_residuals: bool\n            Indicates if residuals are binned.\n        in_sample_residuals_by_bin_: dict\n            In sample residuals binned according to the predicted value each residual\n            is associated with.\n        out_sample_residuals_by_bin_: dict\n            Out of sample residuals binned according to the predicted value each residual\n            is associated with.\n        levels: list, default None\n            Names of the series (levels) to be predicted (Forecasters multiseries).\n        encoding: str, default None\n            Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns:\n        None\n\n    Examples:\n        from spotforecast2_safe.forecaster.utils import check_residuals_input\n        import numpy as np\n        forecaster_name = \"ForecasterRecursiveMultiSeries\"\n        use_in_sample_residuals = True\n        in_sample_residuals_ = np.array([0.1, -0.2\n        out_sample_residuals_ = np.array([0.3, -0.1])\n        use_binned_residuals = False\n        check_residuals_input(\n            forecaster_name,\n            use_in_sample_residuals,\n            in_sample_residuals_,\n            out_sample_residuals_,\n            use_binned_residuals,\n            in_sample_residuals_by_bin_=None,\n            out_sample_residuals_by_bin_=None,\n            levels=['series_1', 'series_2'],\n            encoding='onehot'\n        )\n    \"\"\"\n\n    forecasters_multiseries = (\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    )\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        # Check if residuals are empty or None\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position","title":"<code>date_to_index_position(index, date_input, method='prediction', date_literal='steps', kwargs_pd_to_datetime={})</code>","text":"<p>Transform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>pandas Index Original datetime index (must be a pandas DatetimeIndex if <code>date_input</code> is not an int).</p> required <code>date_input</code> <code>int | str | Timestamp</code> <p>int, str, pandas Timestamp Datetime to transform to integer.</p> <ul> <li>If int, returns the same integer.</li> <li>If str or pandas Timestamp, it is converted and expanded into the index.</li> </ul> required <code>method</code> <code>str</code> <p>str, default 'prediction' Can be 'prediction' or 'validation'.</p> <ul> <li>If 'prediction', the date must be later than the last date in the index.</li> <li>If 'validation', the date must be within the index range.</li> </ul> <code>'prediction'</code> <code>date_literal</code> <code>str</code> <p>str, default 'steps' Variable name used in error messages.</p> <code>'steps'</code> <code>kwargs_pd_to_datetime</code> <code>dict</code> <p>dict, default {} Additional keyword arguments to pass to <code>pd.to_datetime()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p><code>date_input</code> transformed to integer position in the <code>index</code>.</p> <code>int</code> <ul> <li>If <code>date_input</code> is an integer, it returns the same integer.</li> </ul> <code>int</code> <ul> <li>If method is 'prediction', number of steps to predict from the last</li> </ul> <code>int</code> <p>date in the index.</p> <code>int</code> <ul> <li>If method is 'validation', position plus one of the date in the index,</li> </ul> <code>int</code> <p>this is done to include the target date in the training set when using</p> <code>int</code> <p>pandas iloc with slices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not 'prediction' or 'validation'.</p> <code>TypeError</code> <p>If <code>date_input</code> is not an int, str, or pandas Timestamp.</p> <code>TypeError</code> <p>If <code>index</code> is not a pandas DatetimeIndex when <code>date_input</code> is not an int.</p> <code>ValueError</code> <p>If <code>date_input</code> is a date and does not meet the requirements based on the <code>method</code> argument.</p> <p>Examples:</p> <p>from spotforecast2_safe.forecaster.utils import date_to_index_position import pandas as pd index = pd.date_range(start='2020-01-01', periods=10, freq='D')</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position--using-an-integer-input","title":"Using an integer input","text":"<p>position = date_to_index_position(index, 5) print(position)</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position--output-5","title":"Output: 5","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position--using-a-date-input-for-prediction","title":"Using a date input for prediction","text":"<p>position = date_to_index_position(index, '2020-01-15', method='prediction') print(position)</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position--output-5-number-of-steps-from-the-last-date-in-the-index-to-the-target-date","title":"Output: 5 (number of steps from the last date in the index to the target date)","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position--using-a-date-input-for-validation","title":"Using a date input for validation","text":"<p>position = date_to_index_position(index, '2020-01-05', method='validation') print(position)</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position--output-5-position-plus-one-of-the-target-date-in-the-index","title":"Output: 5 (position plus one of the target date in the index)","text":"Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def date_to_index_position(\n    index: pd.Index,\n    date_input: int | str | pd.Timestamp,\n    method: str = \"prediction\",\n    date_literal: str = \"steps\",\n    kwargs_pd_to_datetime: dict = {},\n) -&gt; int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n\n    Args:\n        index: pandas Index\n            Original datetime index (must be a pandas DatetimeIndex if `date_input`\n            is not an int).\n        date_input: int, str, pandas Timestamp\n            Datetime to transform to integer.\n\n            - If int, returns the same integer.\n            - If str or pandas Timestamp, it is converted and expanded into the index.\n        method: str, default 'prediction'\n            Can be 'prediction' or 'validation'.\n\n            - If 'prediction', the date must be later than the last date in the index.\n            - If 'validation', the date must be within the index range.\n        date_literal: str, default 'steps'\n            Variable name used in error messages.\n        kwargs_pd_to_datetime: dict, default {}\n            Additional keyword arguments to pass to `pd.to_datetime()`.\n\n    Returns:\n        int:\n            `date_input` transformed to integer position in the `index`.\n\n        + If `date_input` is an integer, it returns the same integer.\n        + If method is 'prediction', number of steps to predict from the last\n        date in the index.\n        + If method is 'validation', position plus one of the date in the index,\n        this is done to include the target date in the training set when using\n        pandas iloc with slices.\n\n    Raises:\n        ValueError: If `method` is not 'prediction' or 'validation'.\n        TypeError: If `date_input` is not an int, str, or pandas Timestamp.\n        TypeError: If `index` is not a pandas DatetimeIndex when `date_input` is not an int.\n        ValueError: If `date_input` is a date and does not meet the requirements based on the `method` argument.\n\n    Examples:\n        from spotforecast2_safe.forecaster.utils import date_to_index_position\n        import pandas as pd\n        index = pd.date_range(start='2020-01-01', periods=10, freq='D')\n        # Using an integer input\n        position = date_to_index_position(index, 5)\n        print(position)\n        # Output: 5\n        # Using a date input for prediction\n        position = date_to_index_position(index, '2020-01-15', method='prediction')\n        print(position)\n        # Output: 5 (number of steps from the last date in the index to the target date)\n        # Using a date input for validation\n        position = date_to_index_position(index, '2020-01-05', method='validation')\n        print(position)\n        # Output: 5 (position plus one of the target date in the index)\n    \"\"\"\n\n    if method not in [\"prediction\", \"validation\"]:\n        raise ValueError(\"`method` must be 'prediction' or 'validation'.\")\n\n    # Initialize output; will be set in all valid code paths below\n    output: int = 0\n\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n\n        if method == \"prediction\":\n            if target_date &lt;= last_date:\n                raise ValueError(\n                    \"If `steps` is a date, it must be greater than the last date \"\n                    \"in the index.\"\n                )\n            span_index = pd.date_range(\n                start=last_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index) - 1\n        elif method == \"validation\":\n            first_date = pd.to_datetime(index[0])\n            if target_date &lt; first_date or target_date &gt; last_date:\n                raise ValueError(\n                    \"If `initial_train_size` is a date, it must be greater than \"\n                    \"the first date in the index and less than the last date.\"\n                )\n            span_index = pd.date_range(\n                start=first_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index)\n\n    elif isinstance(date_input, (int, np.integer)):\n        output = date_input\n\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Series | DataFrame</code> <p>pandas Series, pandas DataFrame Exogenous variables.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, list[str]]</code> <p>tuple[pd.DataFrame, list[str]]: exog_direct: pandas DataFrame     Exogenous variables transformed. exog_direct_names: list     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct(\n    exog: pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: pandas Series, pandas DataFrame\n            Exogenous variables.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[pd.DataFrame, list[str]]:\n            exog_direct: pandas DataFrame\n                Exogenous variables transformed.\n            exog_direct_names: list\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i),]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    exog_direct = pd.concat(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct) :]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>ndarray | Series | DataFrame</code> <p>numpy ndarray, pandas Series, pandas DataFrame Exogenous variables, shape(samples,). If exog is a pandas format, the direct exog names are created.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, list[str] | None]</code> <p>tuple[np.ndarray, list[str] | None]: exog_direct: numpy ndarray     Exogenous variables transformed. exog_direct_names: list, None     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> <p>Examples:</p> <p>from spotforecast2_safe.forecaster.utils import exog_to_direct_numpy import numpy as np exog = np.array([10, 20, 30, 40, 50]) steps = 3 exog_direct, exog_direct_names = exog_to_direct_numpy(exog, steps) print(exog_direct)     [[10 20 30]     [20 30 40]     [30 40 50]] print(exog_direct_names) None</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray | pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[np.ndarray, list[str] | None]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: numpy ndarray, pandas Series, pandas DataFrame\n            Exogenous variables, shape(samples,). If exog is a pandas format, the\n            direct exog names are created.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[np.ndarray, list[str] | None]:\n            exog_direct: numpy ndarray\n                Exogenous variables transformed.\n            exog_direct_names: list, None\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n\n    Examples:\n        from spotforecast2_safe.forecaster.utils import exog_to_direct_numpy\n        import numpy as np\n        exog = np.array([10, 20, 30, 40, 50])\n        steps = 3\n        exog_direct, exog_direct_names = exog_to_direct_numpy(exog, steps)\n        print(exog_direct)\n            [[10 20 30]\n            [20 30 40]\n            [30 40 50]]\n        print(exog_direct_names)\n        None\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = [exog[i : n_rows - (steps - 1 - i)] for i in range(steps)]\n    exog_direct = np.concatenate(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.get_style_repr_html","title":"<code>get_style_repr_html(is_fitted=False)</code>","text":"<p>Generate CSS style for HTML representation of the Forecaster.</p> <p>Creates a unique CSS style block with a container ID for rendering forecaster objects in Jupyter notebooks or HTML documents. The styling provides a clean, monospace display with a light gray background.</p> <p>Parameters:</p> Name Type Description Default <code>is_fitted</code> <code>bool</code> <p>Parameter to indicate if the Forecaster has been fitted. Currently not used in styling but reserved for future extensions.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, str]</code> <p>A tuple containing: - style (str): CSS style block as a string with unique container class. - unique_id (str): Unique 8-character ID for the container element.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n&gt;&gt;&gt; print(f\"Container ID: {uid}\")\nContainer ID: a1b2c3d4\n&gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\nStyle contains CSS: True\n</code></pre> <p>Using in HTML rendering:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n&gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n&gt;&gt;&gt; print(\"background-color\" in html)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def get_style_repr_html(is_fitted: bool = False) -&gt; Tuple[str, str]:\n    \"\"\"Generate CSS style for HTML representation of the Forecaster.\n\n    Creates a unique CSS style block with a container ID for rendering\n    forecaster objects in Jupyter notebooks or HTML documents. The styling\n    provides a clean, monospace display with a light gray background.\n\n    Args:\n        is_fitted: Parameter to indicate if the Forecaster has been fitted.\n            Currently not used in styling but reserved for future extensions.\n\n    Returns:\n        tuple: A tuple containing:\n            - style (str): CSS style block as a string with unique container class.\n            - unique_id (str): Unique 8-character ID for the container element.\n\n    Examples:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n        &gt;&gt;&gt; print(f\"Container ID: {uid}\")\n        Container ID: a1b2c3d4\n        &gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\n        Style contains CSS: True\n\n        Using in HTML rendering:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n        &gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n        &gt;&gt;&gt; print(\"background-color\" in html)\n        True\n    \"\"\"\n\n    unique_id = str(uuid.uuid4())[:8]\n    style = f\"\"\"\n    &lt;style&gt;\n        .container-{unique_id} {{\n            font-family: monospace;\n            background-color: #f0f0f0;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n    &lt;/style&gt;\n    \"\"\"\n    return style, unique_id\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_estimator","title":"<code>initialize_estimator(estimator=None, regressor=None)</code>","text":"<p>Helper to handle the deprecation of 'regressor' in favor of 'estimator'. Returns the valid estimator object.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <code>regressor</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None Deprecated. An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>estimator or pipeline compatible with the scikit-learn API The valid estimator object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>estimator</code> and <code>regressor</code> are provided. Use only <code>estimator</code>.</p> <code>Warning</code> <p>If <code>regressor</code> is provided, a FutureWarning is raised indicating that it is deprecated and will be removed in a future version.</p> <p>Examples:</p> <p>from spotforecast2_safe.forecaster.utils import initialize_estimator from sklearn.linear_model import LinearRegression</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_estimator--using-the-estimator-argument","title":"Using the <code>estimator</code> argument","text":"<p>estimator = LinearRegression() result = initialize_estimator(estimator=estimator) print(result) LinearRegression()</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_estimator--using-the-deprecated-regressor-argument","title":"Using the deprecated <code>regressor</code> argument","text":"<p>regressor = LinearRegression() result = initialize_estimator(regressor=regressor) print(result) LinearRegression()</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_estimator(\n    estimator: object | None = None, regressor: object | None = None\n) -&gt; None:\n    \"\"\"\n    Helper to handle the deprecation of 'regressor' in favor of 'estimator'.\n    Returns the valid estimator object.\n\n    Args:\n        estimator: estimator or pipeline compatible with the scikit-learn API, default None\n            An instance of a estimator or pipeline compatible with the scikit-learn API.\n        regressor: estimator or pipeline compatible with the scikit-learn API, default None\n            Deprecated. An instance of a estimator or pipeline compatible with the\n            scikit-learn API.\n\n    Returns:\n        estimator or pipeline compatible with the scikit-learn API\n            The valid estimator object.\n\n    Raises:\n        ValueError: If both `estimator` and `regressor` are provided. Use only `estimator`.\n        Warning: If `regressor` is provided, a FutureWarning is raised indicating that it is deprecated and will be removed in a future version.\n\n    Examples:\n        from spotforecast2_safe.forecaster.utils import initialize_estimator\n        from sklearn.linear_model import LinearRegression\n        # Using the `estimator` argument\n        estimator = LinearRegression()\n        result = initialize_estimator(estimator=estimator)\n        print(result)\n        LinearRegression()\n        # Using the deprecated `regressor` argument\n        regressor = LinearRegression()\n        result = initialize_estimator(regressor=regressor)\n        print(result)\n        LinearRegression()\n\n    \"\"\"\n\n    if regressor is not None:\n        warnings.warn(\n            \"The `regressor` argument is deprecated and will be removed in a future \"\n            \"version. Please use `estimator` instead.\",\n            FutureWarning,\n            stacklevel=3,  # Important: to point to the user's code\n        )\n        if estimator is not None:\n            raise ValueError(\n                \"Both `estimator` and `regressor` were provided. Use only `estimator`.\"\n            )\n        return regressor\n\n    return estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_transformer_series","title":"<code>initialize_transformer_series(forecaster_name, series_names_in_, encoding=None, transformer_series=None)</code>","text":"<p>Initialize transformer_series_ attribute for multivariate/multiseries forecasters.</p> <p>Creates a dictionary of transformers for each time series in multivariate or multiseries forecasting. Handles three cases: no transformation (None), same transformer for all series (single object), or different transformers per series (dictionary). Clones transformer objects to avoid overwriting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster using this function. Special handling is applied for 'ForecasterRecursiveMultiSeries'.</p> required <code>series_names_in_</code> <code>list[str]</code> <p>Names of the time series (levels) used during training. These will be the keys in the returned transformer dictionary.</p> required <code>encoding</code> <code>str | None</code> <p>Encoding used to identify different series. Only used for ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level' entry. Defaults to None.</p> <code>None</code> <code>transformer_series</code> <code>object | dict[str, object | None] | None</code> <p>Transformer(s) to apply to series. Can be: - None: No transformation applied - Single transformer object: Same transformer cloned for all series - Dict mapping series names to transformers: Different transformer per series Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, object | None]</code> <p>Dictionary with series names as keys and transformer objects (or None) as values. Transformers are cloned to prevent overwriting.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If transformer_series is a dict and some series_names_in_ are not present in the dict keys (those series get no transformation).</p> <p>Examples:</p> <p>No transformation:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import initialize_transformer_series\n&gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=series,\n...     transformer_series=None\n... )\n&gt;&gt;&gt; print(result)\n{'series1': None, 'series2': None, 'series3': None}\n</code></pre> <p>Same transformer for all series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=scaler\n... )\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\nTrue\n&gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\nFalse\n</code></pre> <p>Different transformer per series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; transformers = {\n...     'series1': StandardScaler(),\n...     'series2': MinMaxScaler()\n... }\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=transformers\n... )\n&gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\nTrue\n&gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list[str],\n    encoding: str | None = None,\n    transformer_series: object | dict[str, object | None] | None = None,\n) -&gt; dict[str, object | None]:\n    \"\"\"Initialize transformer_series_ attribute for multivariate/multiseries forecasters.\n\n    Creates a dictionary of transformers for each time series in multivariate or\n    multiseries forecasting. Handles three cases: no transformation (None), same\n    transformer for all series (single object), or different transformers per series\n    (dictionary). Clones transformer objects to avoid overwriting.\n\n    Args:\n        forecaster_name: Name of the forecaster using this function. Special handling\n            is applied for 'ForecasterRecursiveMultiSeries'.\n        series_names_in_: Names of the time series (levels) used during training.\n            These will be the keys in the returned transformer dictionary.\n        encoding: Encoding used to identify different series. Only used for\n            ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level'\n            entry. Defaults to None.\n        transformer_series: Transformer(s) to apply to series. Can be:\n            - None: No transformation applied\n            - Single transformer object: Same transformer cloned for all series\n            - Dict mapping series names to transformers: Different transformer per series\n            Defaults to None.\n\n    Returns:\n        dict: Dictionary with series names as keys and transformer objects (or None)\n            as values. Transformers are cloned to prevent overwriting.\n\n    Warnings:\n        IgnoredArgumentWarning: If transformer_series is a dict and some series_names_in_\n            are not present in the dict keys (those series get no transformation).\n\n    Examples:\n        No transformation:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import initialize_transformer_series\n        &gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=series,\n        ...     transformer_series=None\n        ... )\n        &gt;&gt;&gt; print(result)\n        {'series1': None, 'series2': None, 'series3': None}\n\n        Same transformer for all series:\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=scaler\n        ... )\n        &gt;&gt;&gt; len(result)\n        2\n        &gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\n        True\n        &gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\n        False\n\n        Different transformer per series:\n        &gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n        &gt;&gt;&gt; transformers = {\n        ...     'series1': StandardScaler(),\n        ...     'series2': MinMaxScaler()\n        ... }\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=transformers\n        ... )\n        &gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\n        True\n        &gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\n        True\n    \"\"\"\n    from copy import deepcopy\n    from sklearn.base import clone\n    from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n\n    if forecaster_name == \"ForecasterRecursiveMultiSeries\":\n        if encoding is None:\n            series_names_in_ = [\"_unknown_level\"]\n        else:\n            series_names_in_ = series_names_in_ + [\"_unknown_level\"]\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {\n            serie: clone(transformer_series) for serie in series_names_in_\n        }\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            {\n                k: deepcopy(v)\n                for k, v in transformer_series.items()\n                if k in transformer_series_\n            }\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {\"_unknown_level\"}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\",\n                IgnoredArgumentWarning,\n            )\n\n    return transformer_series_\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_window_features","title":"<code>initialize_window_features(window_features)</code>","text":"<p>Check window_features argument input and generate the corresponding list.</p> <p>This function validates window feature objects and extracts their metadata, ensuring they have the required attributes (window_sizes, features_names) and methods (transform_batch, transform) for proper forecasting operations.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>Any</code> <p>Classes used to create window features. Can be a single object or a list of objects. Each object must have <code>window_sizes</code>, <code>features_names</code> attributes and <code>transform_batch</code>, <code>transform</code> methods.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]</code> <p>A tuple containing: - window_features (list or None): List of classes used to create window features. - window_features_names (list or None): List with all the features names of the window features. - max_size_window_features (int or None): Maximum value of the <code>window_sizes</code> attribute of all classes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>window_features</code> is an empty list.</p> <code>ValueError</code> <p>If a window feature is missing required attributes or methods.</p> <code>TypeError</code> <p>If <code>window_sizes</code> or <code>features_names</code> have incorrect types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 14\n&gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\nNumber of features: 4\n</code></pre> <p>Multiple window features:</p> <pre><code>&gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n&gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 7\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_window_features(\n    window_features: Any,\n) -&gt; Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]:\n    \"\"\"Check window_features argument input and generate the corresponding list.\n\n    This function validates window feature objects and extracts their metadata,\n    ensuring they have the required attributes (window_sizes, features_names) and\n    methods (transform_batch, transform) for proper forecasting operations.\n\n    Args:\n        window_features: Classes used to create window features. Can be a single\n            object or a list of objects. Each object must have `window_sizes`,\n            `features_names` attributes and `transform_batch`, `transform` methods.\n\n    Returns:\n        tuple: A tuple containing:\n            - window_features (list or None): List of classes used to create window features.\n            - window_features_names (list or None): List with all the features names of the window features.\n            - max_size_window_features (int or None): Maximum value of the `window_sizes` attribute of all classes.\n\n    Raises:\n        ValueError: If `window_features` is an empty list.\n        ValueError: If a window feature is missing required attributes or methods.\n        TypeError: If `window_sizes` or `features_names` have incorrect types.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 14\n        &gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\n        Number of features: 4\n\n        Multiple window features:\n        &gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n        &gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 7\n    \"\"\"\n\n    needed_atts = [\"window_sizes\", \"features_names\"]\n    needed_methods = [\"transform_batch\", \"transform\"]\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) &lt; 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features.\"\n        )\n\n        max_window_sizes = []\n        window_features_names = []\n        needed_atts_set = set(needed_atts)\n        needed_methods_set = set(needed_methods)\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set(dir(wf))\n            if not needed_atts_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not needed_methods_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n\n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n\n            if isinstance(window_sizes, int):\n                if window_sizes &lt; 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws &gt;= 1 for ws in window_sizes\n                ):\n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.predict_multivariate","title":"<code>predict_multivariate(forecasters, steps_ahead, exog=None, show_progress=False)</code>","text":"<p>Generate multi-output predictions using multiple baseline forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>dict</code> <p>Dictionary of fitted forecaster instances (one per target). Keys are target names, values are the fitted forecasters (e.g., ForecasterRecursive, ForecasterEquivalentDate).</p> required <code>steps_ahead</code> <code>int</code> <p>Number of steps to forecast.</p> required <code>exog</code> <code>DataFrame</code> <p>Exogenous variables for prediction. If provided, will be passed to each forecaster's predict method.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar while predicting per target forecaster. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with predictions for all targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import predict_multivariate\n&gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n&gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f1.fit(y=y1)\n&gt;&gt;&gt; f2.fit(y=y2)\n&gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n&gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n&gt;&gt;&gt; predictions\n   target1  target2\n5      6.0     12.0\n6      7.0     14.0\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def predict_multivariate(\n    forecasters: dict[str, Any],\n    steps_ahead: int,\n    exog: pd.DataFrame | None = None,\n    show_progress: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multi-output predictions using multiple baseline forecasters.\n\n    Args:\n        forecasters (dict): Dictionary of fitted forecaster instances (one per target).\n            Keys are target names, values are the fitted forecasters (e.g.,\n            ForecasterRecursive, ForecasterEquivalentDate).\n        steps_ahead (int): Number of steps to forecast.\n        exog (pd.DataFrame, optional): Exogenous variables for prediction.\n            If provided, will be passed to each forecaster's predict method.\n        show_progress (bool, optional): Show progress bar while predicting\n            per target forecaster. Default: False.\n\n    Returns:\n        pd.DataFrame: DataFrame with predictions for all targets.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import predict_multivariate\n        &gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n        &gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f1.fit(y=y1)\n        &gt;&gt;&gt; f2.fit(y=y2)\n        &gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n        &gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n        &gt;&gt;&gt; predictions\n           target1  target2\n        5      6.0     12.0\n        6      7.0     14.0\n    \"\"\"\n\n    if not forecasters:\n        return pd.DataFrame()\n\n    predictions = {}\n\n    target_iter = forecasters.items()\n    if show_progress and tqdm is not None:\n        target_iter = tqdm(\n            forecasters.items(),\n            desc=\"Predicting targets\",\n            unit=\"model\",\n        )\n\n    for target, forecaster in target_iter:\n        # Generate predictions for this target\n        if exog is not None:\n            pred = forecaster.predict(steps=steps_ahead, exog=exog)\n        else:\n            pred = forecaster.predict(steps=steps_ahead)\n        predictions[target] = pred\n\n    # Combine into a single DataFrame\n    return pd.concat(predictions, axis=1)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.prepare_steps_direct","title":"<code>prepare_steps_direct(max_step, steps=None)</code>","text":"<p>Prepare list of steps to be predicted in Direct Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>max_step</code> <code>int | list[int] | ndarray</code> <p>int, list, numpy ndarray Maximum number of future steps the forecaster will predict when using predict methods.</p> required <code>steps</code> <code>int | list[int] | None</code> <p>int, list, None, default None Predict n steps. The value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list   are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at   initialization.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: Steps to be predicted.</p> <p>Examples:</p> <p>from spotforecast2_safe.forecaster.utils import prepare_steps_direct max_step = 5 steps = 3 steps_direct = prepare_steps_direct(max_step, steps) print(steps_direct) [1, 2, 3]</p> <p>max_step = 5 steps = [1, 3, 5] steps_direct = prepare_steps_direct(max_step, steps) print(steps_direct) [1, 3, 5]</p> <p>max_step = 5 steps = None steps_direct = prepare_steps_direct(max_step, steps) print(steps_direct) [1, 2, 3, 4, 5]</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def prepare_steps_direct(\n    max_step: int | list[int] | np.ndarray, steps: int | list[int] | None = None\n) -&gt; list[int]:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Args:\n        max_step: int, list, numpy ndarray\n            Maximum number of future steps the forecaster will predict\n            when using predict methods.\n        steps: int, list, None, default None\n            Predict n steps. The value of `steps` must be less than or equal to the\n            value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list\n              are predicted.\n            - If `None`: As many steps are predicted as were defined at\n              initialization.\n\n    Returns:\n        list[int]:\n            Steps to be predicted.\n\n    Examples:\n        from spotforecast2_safe.forecaster.utils import prepare_steps_direct\n        max_step = 5\n        steps = 3\n        steps_direct = prepare_steps_direct(max_step, steps)\n        print(steps_direct)\n        [1, 2, 3]\n\n        max_step = 5\n        steps = [1, 3, 5]\n        steps_direct = prepare_steps_direct(max_step, steps)\n        print(steps_direct)\n        [1, 3, 5]\n\n        max_step = 5\n        steps = None\n        steps_direct = prepare_steps_direct(max_step, steps)\n        print(steps_direct)\n        [1, 2, 3, 4, 5]\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps_direct = list(range(1, steps + 1))\n    elif steps is None:\n        if isinstance(max_step, int):\n            steps_direct = list(range(1, max_step + 1))\n        else:\n            steps_direct = [int(s) for s in max_step]\n    elif isinstance(steps, list):\n        steps_direct = []\n        for step in steps:\n            if not isinstance(step, (int, np.integer)):\n                raise TypeError(\n                    f\"`steps` argument must be an int, a list of ints or `None`. \"\n                    f\"Got {type(steps)}.\"\n                )\n            steps_direct.append(int(step))\n\n    return steps_direct\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.select_n_jobs_fit_forecaster","title":"<code>select_n_jobs_fit_forecaster(forecaster_name, estimator)</code>","text":"<p>Select the number of jobs to run in parallel during the fit process.</p> <p>This function determines the optimal number of parallel processes for fitting the forecaster based on the available system resources. In safety-critical environments, this helps manage computational load and ensures system predictability.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster being fitted. Currently unused but reserved for granular resource allocation based on model complexity.</p> required <code>estimator</code> <code>object</code> <p>The estimator object being used by the forecaster. Currently unused but reserved for checking if the estimator itself supports internal parallelism.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of jobs (CPUs) to use for parallel processing. Defaults to</p> <code>int</code> <p>the system CPU count, with a fallback to 1 if the count cannot be</p> <code>int</code> <p>determined.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(forecaster_name: str, estimator: object) -&gt; int:\n    \"\"\"Select the number of jobs to run in parallel during the fit process.\n\n    This function determines the optimal number of parallel processes for fitting\n    the forecaster based on the available system resources. In safety-critical\n    environments, this helps manage computational load and ensures system\n    predictability.\n\n    Args:\n        forecaster_name: Name of the forecaster being fitted. Currently unused but\n            reserved for granular resource allocation based on model complexity.\n        estimator: The estimator object being used by the forecaster. Currently\n            unused but reserved for checking if the estimator itself supports\n            internal parallelism.\n\n    Returns:\n        The number of jobs (CPUs) to use for parallel processing. Defaults to\n        the system CPU count, with a fallback to 1 if the count cannot be\n        determined.\n    \"\"\"\n    import os\n\n    return os.cpu_count() or 1\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_numpy","title":"<code>transform_numpy(array, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of a numpy ndarray with a scikit-learn alike transformer, preprocessor or ColumnTransformer. The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>numpy ndarray Array to be transformed.</p> required <code>transformer</code> <code>object | None</code> <p>scikit-learn alike transformer, preprocessor, or ColumnTransformer. Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <p>fit: bool, default False     Train the transformer before applying it. inverse_transform: bool, default False     Transform back the data to the original representation. This is not available     when using transformers of class scikit-learn ColumnTransformers.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy ndarray: Transformed array.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>array</code> is not a numpy ndarray.</p> <code>TypeError</code> <p>If <code>transformer</code> is not a scikit-learn alike transformer, preprocessor, or ColumnTransformer.</p> <code>ValueError</code> <p>If <code>inverse_transform</code> is True and <code>transformer</code> is a ColumnTransformer.</p> <p>Examples:</p> <p>ffrom spotforecast2_safe.forecaster.utils import transform_numpy from sklearn.preprocessing import StandardScaler import numpy as np array = np.array([[1, 2], [3, 4], [5, 6]]) transformer = StandardScaler() array_transformed = transform_numpy(array, transformer, fit=True) print(array_transformed) [[-1.22474487 -1.22474487]  [ 0.          0.        ]  [ 1.22474487  1.22474487]]  array_inversed = transform_numpy(array_transformed, transformer, inverse_transform=True)  print(array_inversed)  [[1. 2.]   [3. 4.]   [5. 6.]]</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def transform_numpy(\n    array: np.ndarray,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer. The transformer used must\n    have the following methods: fit, transform, fit_transform and\n    inverse_transform. ColumnTransformers are not allowed since they do not\n    have inverse_transform method.\n\n    Args:\n        array: numpy ndarray\n            Array to be transformed.\n        transformer: scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n            fit_transform and inverse_transform.\n    fit: bool, default False\n        Train the transformer before applying it.\n    inverse_transform: bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns:\n        numpy ndarray: Transformed array.\n\n    Raises:\n        TypeError: If `array` is not a numpy ndarray.\n        TypeError: If `transformer` is not a scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        ValueError: If `inverse_transform` is True and `transformer` is a ColumnTransformer.\n\n    Examples:\n        ffrom spotforecast2_safe.forecaster.utils import transform_numpy\n        from sklearn.preprocessing import StandardScaler\n        import numpy as np\n        array = np.array([[1, 2], [3, 4], [5, 6]])\n        transformer = StandardScaler()\n        array_transformed = transform_numpy(array, transformer, fit=True)\n        print(array_transformed)\n        [[-1.22474487 -1.22474487]\n         [ 0.          0.        ]\n         [ 1.22474487  1.22474487]]\n         array_inversed = transform_numpy(array_transformed, transformer, inverse_transform=True)\n         print(array_inversed)\n         [[1. 2.]\n          [3. 4.]\n          [5. 6.]]\n    \"\"\"\n\n    if transformer is None:\n        return array\n\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"`array` argument must be a numpy ndarray. Got {type(array)}\")\n\n    original_ndim = array.ndim\n    original_shape = array.shape\n    reshaped_for_inverse = False\n\n    if original_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n        )\n        if not inverse_transform:\n            if fit:\n                array_transformed = transformer.fit_transform(array)\n            else:\n                array_transformed = transformer.transform(array)\n        else:\n            # Vectorized inverse transformation for 2D arrays with multiple columns.\n            # Reshape to single column, transform, and reshape back.\n            # This is faster than applying the transformer column by column.\n            if array.shape[1] &gt; 1:\n                array = array.reshape(-1, 1)\n                reshaped_for_inverse = True\n            array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if isinstance(array_transformed, (pd.Series, pd.DataFrame)):\n        array_transformed = array_transformed.to_numpy()\n\n    # Reshape back to original shape only if we reshaped for inverse_transform\n    if reshaped_for_inverse:\n        array_transformed = array_transformed.reshape(original_shape)\n\n    if original_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing Module","text":"<p>Tools for data preprocessing, cleaning, and transformation.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing","title":"<code>spotforecast2_safe.preprocessing</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ExogBuilder","title":"<code>ExogBuilder</code>","text":"<p>Builds a set of exogenous features for a given date range.</p> <p>This builder combines temporal features (day of year, day of week, hour, etc.) with cyclical features encoded via RepeatingBasisFunctions and optional holiday indicators.</p> <p>Attributes:</p> Name Type Description <code>periods</code> <code>List[Period]</code> <p>List of periodic features to encode.</p> <code>country_code</code> <code>Optional[str]</code> <p>Country code for holiday lookups.</p> <code>holidays_list</code> <code>Optional[HolidayBase]</code> <p>List of holidays for the specified country.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.data.data import Period\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.exog_builder import ExogBuilder\n&gt;&gt;&gt; periods = [Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))]\n&gt;&gt;&gt; builder = ExogBuilder(periods=periods, country_code=\"DE\")\n&gt;&gt;&gt; start = pd.Timestamp(\"2025-01-01\", tz=\"UTC\")\n&gt;&gt;&gt; end = pd.Timestamp(\"2025-01-02\", tz=\"UTC\")\n&gt;&gt;&gt; exog = builder.build(start, end)\n&gt;&gt;&gt; exog.shape[1] &gt; 0\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/exog_builder.py</code> <pre><code>class ExogBuilder:\n    \"\"\"\n    Builds a set of exogenous features for a given date range.\n\n    This builder combines temporal features (day of year, day of week, hour, etc.)\n    with cyclical features encoded via RepeatingBasisFunctions and optional\n    holiday indicators.\n\n    Attributes:\n        periods (List[Period]): List of periodic features to encode.\n        country_code (Optional[str]): Country code for holiday lookups.\n        holidays_list (Optional[holidays.HolidayBase]): List of holidays for the specified country.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.data.data import Period\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.exog_builder import ExogBuilder\n        &gt;&gt;&gt; periods = [Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))]\n        &gt;&gt;&gt; builder = ExogBuilder(periods=periods, country_code=\"DE\")\n        &gt;&gt;&gt; start = pd.Timestamp(\"2025-01-01\", tz=\"UTC\")\n        &gt;&gt;&gt; end = pd.Timestamp(\"2025-01-02\", tz=\"UTC\")\n        &gt;&gt;&gt; exog = builder.build(start, end)\n        &gt;&gt;&gt; exog.shape[1] &gt; 0\n        True\n    \"\"\"\n\n    def __init__(\n        self, periods: Optional[List[Period]] = None, country_code: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize the ExogBuilder.\n\n        Args:\n            periods: List of Period objects defining cyclical features.\n            country_code: country code (ISO) for holiday detection.\n        \"\"\"\n        self.periods = periods or []\n        self.country_code = country_code\n        self.holidays_list = (\n            holidays.country_holidays(country_code) if country_code else None\n        )\n\n    def _get_time_columns(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Extract basic time-based columns from the DataFrame index.\n\n        Args:\n            X: DataFrame with DatetimeIndex.\n\n        Returns:\n            pd.DataFrame: Copy of X with extra time columns.\n        \"\"\"\n        X = X.copy()\n        X[\"dayofyear\"] = X.index.dayofyear\n        X[\"dayofweek\"] = X.index.dayofweek\n        X[\"quarter\"] = X.index.quarter\n        X[\"month\"] = X.index.month\n        X[\"hour\"] = X.index.hour\n        return X\n\n    def build(self, start_date: pd.Timestamp, end_date: pd.Timestamp) -&gt; pd.DataFrame:\n        \"\"\"\n        Build the exogenous feature DataFrame for a date range.\n\n        The generated DataFrame has an hourly frequency.\n\n        Args:\n            start_date: Start of the date range (inclusive).\n            end_date: End of the date range (inclusive).\n\n        Returns:\n            pd.DataFrame: DataFrame containing exogenous features.\n\n        Raises:\n            ValueError: If the date range is invalid.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from spotforecast2_safe.data.data import Period\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing.exog_builder import ExogBuilder\n            &gt;&gt;&gt; periods = [Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))]\n            &gt;&gt;&gt; builder = ExogBuilder(periods=periods, country_code=\"DE\")\n            &gt;&gt;&gt; start = pd.Timestamp(\"2025-01-01\", tz=\"UTC\")\n            &gt;&gt;&gt; end = pd.Timestamp(\"2025-01-02\", tz=\"UTC\")\n            &gt;&gt;&gt; exog = builder.build(start, end)\n            &gt;&gt;&gt; exog.shape[1] &gt; 0\n            True\n        \"\"\"\n        date_range = pd.date_range(start=start_date, end=end_date, freq=\"h\")\n        X = pd.DataFrame(index=date_range)\n        X = self._get_time_columns(X)\n\n        seasons_encoded = []\n        for period in self.periods:\n            rbf = RepeatingBasisFunction(\n                n_periods=period.n_periods,\n                column=period.column,\n                input_range=period.input_range,\n            )\n            season_encoded = rbf.transform(X)\n            cols = [f\"{period.name}_{i}\" for i in range(season_encoded.shape[1])]\n            seasons_encoded.append(\n                pd.DataFrame(season_encoded, index=X.index, columns=cols)\n            )\n\n        X_ = pd.concat(seasons_encoded, axis=1) if seasons_encoded else X\n\n        if self.holidays_list is not None:\n            # List comprehension is robust for holiday detection across different\n            # pandas/holidays versions and handling of DatetimeIndex\n            X_[\"holidays\"] = [int(d in self.holidays_list) for d in X_.index]\n\n        X_[\"is_weekend\"] = X_.index.dayofweek.isin([5, 6]).astype(int)\n        return X_\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ExogBuilder.__init__","title":"<code>__init__(periods=None, country_code=None)</code>","text":"<p>Initialize the ExogBuilder.</p> <p>Parameters:</p> Name Type Description Default <code>periods</code> <code>Optional[List[Period]]</code> <p>List of Period objects defining cyclical features.</p> <code>None</code> <code>country_code</code> <code>Optional[str]</code> <p>country code (ISO) for holiday detection.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/exog_builder.py</code> <pre><code>def __init__(\n    self, periods: Optional[List[Period]] = None, country_code: Optional[str] = None\n):\n    \"\"\"\n    Initialize the ExogBuilder.\n\n    Args:\n        periods: List of Period objects defining cyclical features.\n        country_code: country code (ISO) for holiday detection.\n    \"\"\"\n    self.periods = periods or []\n    self.country_code = country_code\n    self.holidays_list = (\n        holidays.country_holidays(country_code) if country_code else None\n    )\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ExogBuilder.build","title":"<code>build(start_date, end_date)</code>","text":"<p>Build the exogenous feature DataFrame for a date range.</p> <p>The generated DataFrame has an hourly frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp</code> <p>Start of the date range (inclusive).</p> required <code>end_date</code> <code>Timestamp</code> <p>End of the date range (inclusive).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing exogenous features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the date range is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.data.data import Period\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.exog_builder import ExogBuilder\n&gt;&gt;&gt; periods = [Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))]\n&gt;&gt;&gt; builder = ExogBuilder(periods=periods, country_code=\"DE\")\n&gt;&gt;&gt; start = pd.Timestamp(\"2025-01-01\", tz=\"UTC\")\n&gt;&gt;&gt; end = pd.Timestamp(\"2025-01-02\", tz=\"UTC\")\n&gt;&gt;&gt; exog = builder.build(start, end)\n&gt;&gt;&gt; exog.shape[1] &gt; 0\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/exog_builder.py</code> <pre><code>def build(self, start_date: pd.Timestamp, end_date: pd.Timestamp) -&gt; pd.DataFrame:\n    \"\"\"\n    Build the exogenous feature DataFrame for a date range.\n\n    The generated DataFrame has an hourly frequency.\n\n    Args:\n        start_date: Start of the date range (inclusive).\n        end_date: End of the date range (inclusive).\n\n    Returns:\n        pd.DataFrame: DataFrame containing exogenous features.\n\n    Raises:\n        ValueError: If the date range is invalid.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.data.data import Period\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.exog_builder import ExogBuilder\n        &gt;&gt;&gt; periods = [Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))]\n        &gt;&gt;&gt; builder = ExogBuilder(periods=periods, country_code=\"DE\")\n        &gt;&gt;&gt; start = pd.Timestamp(\"2025-01-01\", tz=\"UTC\")\n        &gt;&gt;&gt; end = pd.Timestamp(\"2025-01-02\", tz=\"UTC\")\n        &gt;&gt;&gt; exog = builder.build(start, end)\n        &gt;&gt;&gt; exog.shape[1] &gt; 0\n        True\n    \"\"\"\n    date_range = pd.date_range(start=start_date, end=end_date, freq=\"h\")\n    X = pd.DataFrame(index=date_range)\n    X = self._get_time_columns(X)\n\n    seasons_encoded = []\n    for period in self.periods:\n        rbf = RepeatingBasisFunction(\n            n_periods=period.n_periods,\n            column=period.column,\n            input_range=period.input_range,\n        )\n        season_encoded = rbf.transform(X)\n        cols = [f\"{period.name}_{i}\" for i in range(season_encoded.shape[1])]\n        seasons_encoded.append(\n            pd.DataFrame(season_encoded, index=X.index, columns=cols)\n        )\n\n    X_ = pd.concat(seasons_encoded, axis=1) if seasons_encoded else X\n\n    if self.holidays_list is not None:\n        # List comprehension is robust for holiday detection across different\n        # pandas/holidays versions and handling of DatetimeIndex\n        X_[\"holidays\"] = [int(d in self.holidays_list) for d in X_.index]\n\n    X_[\"is_weekend\"] = X_.index.dayofweek.isin([5, 6]).astype(int)\n    return X_\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveLGBM","title":"<code>ForecasterRecursiveLGBM</code>","text":"<p>               Bases: <code>ForecasterRecursiveModel</code></p> <p>ForecasterRecursive specialization using LightGBM.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.preprocessing.forecaster_recursive_model import ForecasterRecursiveLGBM\n&gt;&gt;&gt; model = ForecasterRecursiveLGBM(iteration=0)\n&gt;&gt;&gt; model.name\n'lgbm'\n&gt;&gt;&gt; model.forecaster is not None\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>class ForecasterRecursiveLGBM(ForecasterRecursiveModel):\n    \"\"\"\n    ForecasterRecursive specialization using LightGBM.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.forecaster_recursive_model import ForecasterRecursiveLGBM\n        &gt;&gt;&gt; model = ForecasterRecursiveLGBM(iteration=0)\n        &gt;&gt;&gt; model.name\n        'lgbm'\n        &gt;&gt;&gt; model.forecaster is not None\n        True\n    \"\"\"\n\n    def __init__(self, iteration: int, lags: int = 12, **kwargs: Any):\n        \"\"\"\n        Initialize the LGBM Recursive Forecaster.\n\n        Args:\n            iteration: Current iteration index.\n            lags: Number of autoregressive lags.\n            **kwargs: Passed to ForecasterRecursiveModel.\n        \"\"\"\n        super().__init__(iteration, name=\"lgbm\", **kwargs)\n        self.forecaster = ForecasterRecursive(\n            estimator=LGBMRegressor(\n                n_jobs=-1, verbose=-1, random_state=self.random_state\n            ),\n            lags=lags,\n        )\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveLGBM.__init__","title":"<code>__init__(iteration, lags=12, **kwargs)</code>","text":"<p>Initialize the LGBM Recursive Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Current iteration index.</p> required <code>lags</code> <code>int</code> <p>Number of autoregressive lags.</p> <code>12</code> <code>**kwargs</code> <code>Any</code> <p>Passed to ForecasterRecursiveModel.</p> <code>{}</code> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>def __init__(self, iteration: int, lags: int = 12, **kwargs: Any):\n    \"\"\"\n    Initialize the LGBM Recursive Forecaster.\n\n    Args:\n        iteration: Current iteration index.\n        lags: Number of autoregressive lags.\n        **kwargs: Passed to ForecasterRecursiveModel.\n    \"\"\"\n    super().__init__(iteration, name=\"lgbm\", **kwargs)\n    self.forecaster = ForecasterRecursive(\n        estimator=LGBMRegressor(\n            n_jobs=-1, verbose=-1, random_state=self.random_state\n        ),\n        lags=lags,\n    )\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveModel","title":"<code>ForecasterRecursiveModel</code>","text":"<p>Base wrapper around ForecasterRecursive to match application logic.</p> <p>This class manages the lifecycle of a recursive forecaster, including feature building, tuning (simulated), and packaging predictions for UI.</p> <p>Attributes:</p> Name Type Description <code>iteration</code> <code>int</code> <p>The current training iteration.</p> <code>end_dev</code> <code>Timestamp</code> <p>The end date of the development/training period.</p> <code>train_size</code> <code>Optional[Timedelta]</code> <p>Lookback window for training data.</p> <code>preprocessor</code> <code>ExogBuilder</code> <p>Builder for exogenous features.</p> <code>name</code> <code>str</code> <p>Label for the model type.</p> <code>forecaster</code> <code>Optional[ForecasterRecursive]</code> <p>The underlying forecaster instance.</p> <code>is_tuned</code> <code>bool</code> <p>Flag indicating if hyperparameter tuning has been performed.</p> <code>predict_size</code> <code>int</code> <p>Prediction horizon in hours.</p> <code>refit_size</code> <code>int</code> <p>Refit interval in days.</p> <code>random_state</code> <code>int</code> <p>Seed for reproducibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.forecaster_recursive_model import ForecasterRecursiveModel\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt;\n&gt;&gt;&gt; model = ForecasterRecursiveModel(iteration=0)\n&gt;&gt;&gt; model.forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=1)\n&gt;&gt;&gt; model.name = \"linear\"\n&gt;&gt;&gt; model.tune()\n&gt;&gt;&gt; model.is_tuned\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>class ForecasterRecursiveModel:\n    \"\"\"\n    Base wrapper around ForecasterRecursive to match application logic.\n\n    This class manages the lifecycle of a recursive forecaster, including\n    feature building, tuning (simulated), and packaging predictions for UI.\n\n    Attributes:\n        iteration (int): The current training iteration.\n        end_dev (pd.Timestamp): The end date of the development/training period.\n        train_size (Optional[pd.Timedelta]): Lookback window for training data.\n        preprocessor (ExogBuilder): Builder for exogenous features.\n        name (str): Label for the model type.\n        forecaster (Optional[ForecasterRecursive]): The underlying forecaster instance.\n        is_tuned (bool): Flag indicating if hyperparameter tuning has been performed.\n        predict_size (int): Prediction horizon in hours.\n        refit_size (int): Refit interval in days.\n        random_state (int): Seed for reproducibility.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.forecaster_recursive_model import ForecasterRecursiveModel\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; model = ForecasterRecursiveModel(iteration=0)\n        &gt;&gt;&gt; model.forecaster = ForecasterRecursive(estimator=LinearRegression(), lags=1)\n        &gt;&gt;&gt; model.name = \"linear\"\n        &gt;&gt;&gt; model.tune()\n        &gt;&gt;&gt; model.is_tuned\n        True\n    \"\"\"\n\n    def __init__(\n        self,\n        iteration: int,\n        end_dev: Optional[Union[str, pd.Timestamp]] = None,\n        train_size: Optional[pd.Timedelta] = None,\n        periods: Optional[List[Period]] = None,\n        country_code: str = \"DE\",\n        random_state: int = 314159,\n        predict_size: int = 24,\n        refit_size: int = 7,\n        name: str = \"base\",\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Initialize the Recursive Forecaster Model.\n\n        Args:\n            iteration: Current iteration index.\n            end_dev: Cutoff date for training. Defaults to a reasonable date if None.\n            train_size: Time window for training data lookback.\n            periods: List of Period objects for cyclical encoding.\n            country_code: ISO country code for holidays.\n            random_state: Random seed.\n            predict_size: Forecast horizon in hours.\n            refit_size: Retraining frequency in days.\n            name: Model name identifier. Defaults to \"base\".\n            **kwargs: Additional parameters for forward compatibility.\n        \"\"\"\n        self.iteration = iteration\n        # Default end date if not provided (safety fallback)\n        default_end = \"2025-12-31 00:00+00:00\"\n        self.end_dev = pd.to_datetime(end_dev if end_dev else default_end, utc=True)\n        self.train_size = train_size\n        self.random_state = random_state\n        self.predict_size = predict_size\n        self.refit_size = refit_size\n\n        self.preprocessor = ExogBuilder(periods=periods, country_code=country_code)\n        self.name = name\n        self.forecaster: Optional[ForecasterRecursive] = None\n        self.is_tuned = False\n\n    def tune(self) -&gt; None:\n        \"\"\"\n        Simulate hyperparameter tuning.\n\n        In a production environment, this would implement Bayesian search or\n        similar optimization. For safety-critical stability, we currently\n        default to robust parameters.\n        #TODO: Implement hyperparameter tuning in spotforecast2\n        \"\"\"\n        logger.info(\"Tuning %s model (simulated)...\", self.name)\n        self.is_tuned = True\n\n    def fit(self, y: pd.Series, exog: Optional[pd.DataFrame] = None) -&gt; None:\n        \"\"\"\n        Fit the underlying forecaster.\n\n        Args:\n            y: Target time series.\n            exog: Optional exogenous features.\n\n        Raises:\n            ValueError: If the forecaster has not been initialized.\n        \"\"\"\n        if self.forecaster is None:\n            raise ValueError(\"Forecaster not initialized\")\n\n        self.forecaster.fit(y=y, exog=exog)\n\n    def package_prediction(self, predict_size: Optional[int] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate predictions and package them with metrics for the UI.\n\n        This method handles data loading (from interim), alignment,\n        scoring, and benchmark comparison.\n\n        Args:\n            predict_size: Optional override for the prediction horizon.\n\n        Returns:\n            Dict[str, Any]: A result package containing actual values,\n                predictions, and calculated metrics (MAE, MAPE).\n        \"\"\"\n        from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n\n        from spotforecast2_safe.data.fetch_data import get_data_home\n\n        if self.forecaster is None:\n            logger.error(\"Forecaster not initialized\")\n            return {}\n\n        try:\n            # Load data from interim directory\n            data_home = get_data_home()\n            data_path = data_home / \"interim\" / \"energy_load.csv\"\n\n            if not data_path.exists():\n                logger.error(\"Data file not found: %s\", data_path)\n                return {}\n\n            # Read and prepare data\n            df = pd.read_csv(data_path, parse_dates=[\"Time (UTC)\"])\n            df = df.set_index(\"Time (UTC)\")\n            df.index = pd.to_datetime(df.index, utc=True)\n            df.index.name = \"datetime\"\n            df = df.asfreq(\"h\")\n\n            if \"Actual Load\" not in df.columns:\n                logger.error(\"'Actual Load' column missing in %s\", data_path)\n                return {}\n\n            y = df[\"Actual Load\"]\n            # Basic imputation\n            if y.isna().any():\n                y = y.ffill().bfill()\n\n            # Benchmark\n            future_forecast = df.get(\"Forecasted Load\", None)\n            if future_forecast is not None and future_forecast.isna().any():\n                future_forecast = future_forecast.ffill().bfill()\n\n            # Train/test split using end_dev\n            y_train = y.loc[: self.end_dev]\n            y_test = y.loc[self.end_dev :]\n\n            if predict_size is None:\n                predict_size = self.predict_size\n\n            # Limit test to prediction window\n            predict_hours = predict_size * self.refit_size\n            if len(y_test) &gt; predict_hours:\n                y_test = y_test.iloc[:predict_hours]\n\n            # Fit on training data\n            self.forecaster.fit(y=y_train)\n\n            # In-sample (train) predictions\n            train_pred = self.forecaster.predict(steps=len(y_train))\n            train_pred.index = y_train.index[-len(train_pred) :]\n            y_train_aligned = y_train.loc[train_pred.index]\n\n            # Out-of-sample (future) predictions\n            future_pred = self.forecaster.predict(steps=len(y_test))\n            future_pred.index = y_test.index[: len(future_pred)]\n            y_test_aligned = y_test.loc[future_pred.index]\n\n            # Metrics\n            metrics_train = {\n                \"mae\": mean_absolute_error(y_train_aligned, train_pred),\n                \"mape\": mean_absolute_percentage_error(y_train_aligned, train_pred),\n            }\n            metrics_future = {\n                \"mae\": mean_absolute_error(y_test_aligned, future_pred),\n                \"mape\": mean_absolute_percentage_error(y_test_aligned, future_pred),\n            }\n\n            # 24h window metrics\n            f_24h_pred = future_pred.iloc[: min(24, len(future_pred))]\n            f_24h_actual = y_test_aligned.iloc[: min(24, len(y_test_aligned))]\n            metrics_future_24h = {\n                \"mae\": mean_absolute_error(f_24h_actual, f_24h_pred),\n                \"mape\": mean_absolute_percentage_error(f_24h_actual, f_24h_pred),\n            }\n\n            result = {\n                \"train_actual\": y_train_aligned,\n                \"future_actual\": y_test_aligned,\n                \"train_pred\": train_pred,\n                \"future_pred\": future_pred,\n                \"metrics_train\": metrics_train,\n                \"metrics_future\": metrics_future,\n                \"metrics_future_one_day\": metrics_future_24h,\n            }\n\n            # Add benchmark if available\n            if future_forecast is not None:\n                forecast_test = future_forecast.loc[y_test_aligned.index]\n                result[\"future_forecast\"] = forecast_test\n                result[\"metrics_forecast\"] = {\n                    \"mae\": mean_absolute_error(y_test_aligned, forecast_test),\n                    \"mape\": mean_absolute_percentage_error(\n                        y_test_aligned, forecast_test\n                    ),\n                }\n\n            return result\n\n        except Exception as e:\n            logger.error(\"Error generating prediction package: %s\", e, exc_info=True)\n            return {}\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveModel.__init__","title":"<code>__init__(iteration, end_dev=None, train_size=None, periods=None, country_code='DE', random_state=314159, predict_size=24, refit_size=7, name='base', **kwargs)</code>","text":"<p>Initialize the Recursive Forecaster Model.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Current iteration index.</p> required <code>end_dev</code> <code>Optional[Union[str, Timestamp]]</code> <p>Cutoff date for training. Defaults to a reasonable date if None.</p> <code>None</code> <code>train_size</code> <code>Optional[Timedelta]</code> <p>Time window for training data lookback.</p> <code>None</code> <code>periods</code> <code>Optional[List[Period]]</code> <p>List of Period objects for cyclical encoding.</p> <code>None</code> <code>country_code</code> <code>str</code> <p>ISO country code for holidays.</p> <code>'DE'</code> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>314159</code> <code>predict_size</code> <code>int</code> <p>Forecast horizon in hours.</p> <code>24</code> <code>refit_size</code> <code>int</code> <p>Retraining frequency in days.</p> <code>7</code> <code>name</code> <code>str</code> <p>Model name identifier. Defaults to \"base\".</p> <code>'base'</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for forward compatibility.</p> <code>{}</code> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>def __init__(\n    self,\n    iteration: int,\n    end_dev: Optional[Union[str, pd.Timestamp]] = None,\n    train_size: Optional[pd.Timedelta] = None,\n    periods: Optional[List[Period]] = None,\n    country_code: str = \"DE\",\n    random_state: int = 314159,\n    predict_size: int = 24,\n    refit_size: int = 7,\n    name: str = \"base\",\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize the Recursive Forecaster Model.\n\n    Args:\n        iteration: Current iteration index.\n        end_dev: Cutoff date for training. Defaults to a reasonable date if None.\n        train_size: Time window for training data lookback.\n        periods: List of Period objects for cyclical encoding.\n        country_code: ISO country code for holidays.\n        random_state: Random seed.\n        predict_size: Forecast horizon in hours.\n        refit_size: Retraining frequency in days.\n        name: Model name identifier. Defaults to \"base\".\n        **kwargs: Additional parameters for forward compatibility.\n    \"\"\"\n    self.iteration = iteration\n    # Default end date if not provided (safety fallback)\n    default_end = \"2025-12-31 00:00+00:00\"\n    self.end_dev = pd.to_datetime(end_dev if end_dev else default_end, utc=True)\n    self.train_size = train_size\n    self.random_state = random_state\n    self.predict_size = predict_size\n    self.refit_size = refit_size\n\n    self.preprocessor = ExogBuilder(periods=periods, country_code=country_code)\n    self.name = name\n    self.forecaster: Optional[ForecasterRecursive] = None\n    self.is_tuned = False\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveModel.fit","title":"<code>fit(y, exog=None)</code>","text":"<p>Fit the underlying forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Target time series.</p> required <code>exog</code> <code>Optional[DataFrame]</code> <p>Optional exogenous features.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the forecaster has not been initialized.</p> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>def fit(self, y: pd.Series, exog: Optional[pd.DataFrame] = None) -&gt; None:\n    \"\"\"\n    Fit the underlying forecaster.\n\n    Args:\n        y: Target time series.\n        exog: Optional exogenous features.\n\n    Raises:\n        ValueError: If the forecaster has not been initialized.\n    \"\"\"\n    if self.forecaster is None:\n        raise ValueError(\"Forecaster not initialized\")\n\n    self.forecaster.fit(y=y, exog=exog)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveModel.package_prediction","title":"<code>package_prediction(predict_size=None)</code>","text":"<p>Generate predictions and package them with metrics for the UI.</p> <p>This method handles data loading (from interim), alignment, scoring, and benchmark comparison.</p> <p>Parameters:</p> Name Type Description Default <code>predict_size</code> <code>Optional[int]</code> <p>Optional override for the prediction horizon.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A result package containing actual values, predictions, and calculated metrics (MAE, MAPE).</p> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>def package_prediction(self, predict_size: Optional[int] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate predictions and package them with metrics for the UI.\n\n    This method handles data loading (from interim), alignment,\n    scoring, and benchmark comparison.\n\n    Args:\n        predict_size: Optional override for the prediction horizon.\n\n    Returns:\n        Dict[str, Any]: A result package containing actual values,\n            predictions, and calculated metrics (MAE, MAPE).\n    \"\"\"\n    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n\n    from spotforecast2_safe.data.fetch_data import get_data_home\n\n    if self.forecaster is None:\n        logger.error(\"Forecaster not initialized\")\n        return {}\n\n    try:\n        # Load data from interim directory\n        data_home = get_data_home()\n        data_path = data_home / \"interim\" / \"energy_load.csv\"\n\n        if not data_path.exists():\n            logger.error(\"Data file not found: %s\", data_path)\n            return {}\n\n        # Read and prepare data\n        df = pd.read_csv(data_path, parse_dates=[\"Time (UTC)\"])\n        df = df.set_index(\"Time (UTC)\")\n        df.index = pd.to_datetime(df.index, utc=True)\n        df.index.name = \"datetime\"\n        df = df.asfreq(\"h\")\n\n        if \"Actual Load\" not in df.columns:\n            logger.error(\"'Actual Load' column missing in %s\", data_path)\n            return {}\n\n        y = df[\"Actual Load\"]\n        # Basic imputation\n        if y.isna().any():\n            y = y.ffill().bfill()\n\n        # Benchmark\n        future_forecast = df.get(\"Forecasted Load\", None)\n        if future_forecast is not None and future_forecast.isna().any():\n            future_forecast = future_forecast.ffill().bfill()\n\n        # Train/test split using end_dev\n        y_train = y.loc[: self.end_dev]\n        y_test = y.loc[self.end_dev :]\n\n        if predict_size is None:\n            predict_size = self.predict_size\n\n        # Limit test to prediction window\n        predict_hours = predict_size * self.refit_size\n        if len(y_test) &gt; predict_hours:\n            y_test = y_test.iloc[:predict_hours]\n\n        # Fit on training data\n        self.forecaster.fit(y=y_train)\n\n        # In-sample (train) predictions\n        train_pred = self.forecaster.predict(steps=len(y_train))\n        train_pred.index = y_train.index[-len(train_pred) :]\n        y_train_aligned = y_train.loc[train_pred.index]\n\n        # Out-of-sample (future) predictions\n        future_pred = self.forecaster.predict(steps=len(y_test))\n        future_pred.index = y_test.index[: len(future_pred)]\n        y_test_aligned = y_test.loc[future_pred.index]\n\n        # Metrics\n        metrics_train = {\n            \"mae\": mean_absolute_error(y_train_aligned, train_pred),\n            \"mape\": mean_absolute_percentage_error(y_train_aligned, train_pred),\n        }\n        metrics_future = {\n            \"mae\": mean_absolute_error(y_test_aligned, future_pred),\n            \"mape\": mean_absolute_percentage_error(y_test_aligned, future_pred),\n        }\n\n        # 24h window metrics\n        f_24h_pred = future_pred.iloc[: min(24, len(future_pred))]\n        f_24h_actual = y_test_aligned.iloc[: min(24, len(y_test_aligned))]\n        metrics_future_24h = {\n            \"mae\": mean_absolute_error(f_24h_actual, f_24h_pred),\n            \"mape\": mean_absolute_percentage_error(f_24h_actual, f_24h_pred),\n        }\n\n        result = {\n            \"train_actual\": y_train_aligned,\n            \"future_actual\": y_test_aligned,\n            \"train_pred\": train_pred,\n            \"future_pred\": future_pred,\n            \"metrics_train\": metrics_train,\n            \"metrics_future\": metrics_future,\n            \"metrics_future_one_day\": metrics_future_24h,\n        }\n\n        # Add benchmark if available\n        if future_forecast is not None:\n            forecast_test = future_forecast.loc[y_test_aligned.index]\n            result[\"future_forecast\"] = forecast_test\n            result[\"metrics_forecast\"] = {\n                \"mae\": mean_absolute_error(y_test_aligned, forecast_test),\n                \"mape\": mean_absolute_percentage_error(\n                    y_test_aligned, forecast_test\n                ),\n            }\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error generating prediction package: %s\", e, exc_info=True)\n        return {}\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveModel.tune","title":"<code>tune()</code>","text":"<p>Simulate hyperparameter tuning.</p> <p>In a production environment, this would implement Bayesian search or similar optimization. For safety-critical stability, we currently default to robust parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveModel.tune--todo-implement-hyperparameter-tuning-in-spotforecast2","title":"TODO: Implement hyperparameter tuning in spotforecast2","text":"Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>def tune(self) -&gt; None:\n    \"\"\"\n    Simulate hyperparameter tuning.\n\n    In a production environment, this would implement Bayesian search or\n    similar optimization. For safety-critical stability, we currently\n    default to robust parameters.\n    #TODO: Implement hyperparameter tuning in spotforecast2\n    \"\"\"\n    logger.info(\"Tuning %s model (simulated)...\", self.name)\n    self.is_tuned = True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveXGB","title":"<code>ForecasterRecursiveXGB</code>","text":"<p>               Bases: <code>ForecasterRecursiveModel</code></p> <p>ForecasterRecursive specialization using XGBoost.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.preprocessing.forecaster_recursive_model import ForecasterRecursiveXGB\n&gt;&gt;&gt; # Only works if xgboost is installed\n&gt;&gt;&gt; try:\n...     model = ForecasterRecursiveXGB(iteration=0)\n...     print(model.name)\n... except Exception:\n...     print(\"xgb\")\nxgb\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>class ForecasterRecursiveXGB(ForecasterRecursiveModel):\n    \"\"\"\n    ForecasterRecursive specialization using XGBoost.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.forecaster_recursive_model import ForecasterRecursiveXGB\n        &gt;&gt;&gt; # Only works if xgboost is installed\n        &gt;&gt;&gt; try:\n        ...     model = ForecasterRecursiveXGB(iteration=0)\n        ...     print(model.name)\n        ... except Exception:\n        ...     print(\"xgb\")\n        xgb\n    \"\"\"\n\n    def __init__(self, iteration: int, lags: int = 12, **kwargs: Any):\n        \"\"\"\n        Initialize the XGBoost Recursive Forecaster.\n\n        Args:\n            iteration: Current iteration index.\n            lags: Number of autoregressive lags.\n            **kwargs: Passed to ForecasterRecursiveModel.\n        \"\"\"\n        super().__init__(iteration, name=\"xgb\", **kwargs)\n        if XGBRegressor is not None:\n            self.forecaster = ForecasterRecursive(\n                estimator=XGBRegressor(n_jobs=-1, random_state=self.random_state),\n                lags=lags,\n            )\n        else:\n            logger.warning(\n                \"XGBoost not installed. This model will fail during fit/predict.\"\n            )\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.ForecasterRecursiveXGB.__init__","title":"<code>__init__(iteration, lags=12, **kwargs)</code>","text":"<p>Initialize the XGBoost Recursive Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Current iteration index.</p> required <code>lags</code> <code>int</code> <p>Number of autoregressive lags.</p> <code>12</code> <code>**kwargs</code> <code>Any</code> <p>Passed to ForecasterRecursiveModel.</p> <code>{}</code> Source code in <code>src/spotforecast2_safe/preprocessing/forecaster_recursive_model.py</code> <pre><code>def __init__(self, iteration: int, lags: int = 12, **kwargs: Any):\n    \"\"\"\n    Initialize the XGBoost Recursive Forecaster.\n\n    Args:\n        iteration: Current iteration index.\n        lags: Number of autoregressive lags.\n        **kwargs: Passed to ForecasterRecursiveModel.\n    \"\"\"\n    super().__init__(iteration, name=\"xgb\", **kwargs)\n    if XGBRegressor is not None:\n        self.forecaster = ForecasterRecursive(\n            estimator=XGBRegressor(n_jobs=-1, random_state=self.random_state),\n            lags=lags,\n        )\n    else:\n        logger.warning(\n            \"XGBoost not installed. This model will fail during fit/predict.\"\n        )\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.LinearlyInterpolateTS","title":"<code>LinearlyInterpolateTS</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer that applies linear interpolation to time series data.</p> <p>This transformer fills missing values using linear interpolation and forward-fills any remaining gaps (e.g., at the end of the series).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.linearly_interpolate_ts import LinearlyInterpolateTS\n&gt;&gt;&gt; s = pd.Series([1.0, np.nan, 3.0, np.nan])\n&gt;&gt;&gt; interpolator = LinearlyInterpolateTS()\n&gt;&gt;&gt; s_filled = interpolator.fit_transform(s)\n&gt;&gt;&gt; s_filled.tolist()\n[1.0, 2.0, 3.0, 3.0]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/linearly_interpolate_ts.py</code> <pre><code>@dataclass\nclass LinearlyInterpolateTS(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer that applies linear interpolation to time series data.\n\n    This transformer fills missing values using linear interpolation and\n    forward-fills any remaining gaps (e.g., at the end of the series).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.linearly_interpolate_ts import LinearlyInterpolateTS\n        &gt;&gt;&gt; s = pd.Series([1.0, np.nan, 3.0, np.nan])\n        &gt;&gt;&gt; interpolator = LinearlyInterpolateTS()\n        &gt;&gt;&gt; s_filled = interpolator.fit_transform(s)\n        &gt;&gt;&gt; s_filled.tolist()\n        [1.0, 2.0, 3.0, 3.0]\n    \"\"\"\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"LinearlyInterpolateTS\":\n        \"\"\"\n        Fitted transformer (no-op).\n\n        Args:\n            X: Input data.\n            y: Ignored.\n\n        Returns:\n            self: The fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(\n        self, X: Union[pd.Series, pd.DataFrame]\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"\n        Transform the input data by applying linear interpolation.\n\n        Args:\n            X: Input Series or DataFrame to interpolate.\n\n        Returns:\n            Union[pd.Series, pd.DataFrame]: Interpolated data.\n        \"\"\"\n        return self.apply(X)\n\n    def apply(\n        self, y: Union[pd.Series, pd.DataFrame]\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"\n        Apply linear interpolation and forward-fill.\n\n        Args:\n            y: Input Series or DataFrame.\n\n        Returns:\n            Union[pd.Series, pd.DataFrame]: Interpolated and ffilled data.\n        \"\"\"\n        y_filled = y.interpolate(method=\"linear\")\n        y_filled = y_filled.astype(\"float\").ffill()\n        return y_filled\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.LinearlyInterpolateTS.apply","title":"<code>apply(y)</code>","text":"<p>Apply linear interpolation and forward-fill.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Union[Series, DataFrame]</code> <p>Input Series or DataFrame.</p> required <p>Returns:</p> Type Description <code>Union[Series, DataFrame]</code> <p>Union[pd.Series, pd.DataFrame]: Interpolated and ffilled data.</p> Source code in <code>src/spotforecast2_safe/preprocessing/linearly_interpolate_ts.py</code> <pre><code>def apply(\n    self, y: Union[pd.Series, pd.DataFrame]\n) -&gt; Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Apply linear interpolation and forward-fill.\n\n    Args:\n        y: Input Series or DataFrame.\n\n    Returns:\n        Union[pd.Series, pd.DataFrame]: Interpolated and ffilled data.\n    \"\"\"\n    y_filled = y.interpolate(method=\"linear\")\n    y_filled = y_filled.astype(\"float\").ffill()\n    return y_filled\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.LinearlyInterpolateTS.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fitted transformer (no-op).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Input data.</p> required <code>y</code> <code>Any</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>LinearlyInterpolateTS</code> <p>The fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/linearly_interpolate_ts.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"LinearlyInterpolateTS\":\n    \"\"\"\n    Fitted transformer (no-op).\n\n    Args:\n        X: Input data.\n        y: Ignored.\n\n    Returns:\n        self: The fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.LinearlyInterpolateTS.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input data by applying linear interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Series, DataFrame]</code> <p>Input Series or DataFrame to interpolate.</p> required <p>Returns:</p> Type Description <code>Union[Series, DataFrame]</code> <p>Union[pd.Series, pd.DataFrame]: Interpolated data.</p> Source code in <code>src/spotforecast2_safe/preprocessing/linearly_interpolate_ts.py</code> <pre><code>def transform(\n    self, X: Union[pd.Series, pd.DataFrame]\n) -&gt; Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Transform the input data by applying linear interpolation.\n\n    Args:\n        X: Input Series or DataFrame to interpolate.\n\n    Returns:\n        Union[pd.Series, pd.DataFrame]: Interpolated data.\n    \"\"\"\n    return self.apply(X)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner","title":"<code>QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RepeatingBasisFunction","title":"<code>RepeatingBasisFunction</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer that encodes cyclical features using repeating radial basis functions.</p> <p>This transformer places Gaussian basis functions across the specified input range and wraps them around to handle periodicity (e.g., day of year, hour of day). It is a simplified implementation to avoid external dependencies like scikit-lego.</p> <p>Attributes:</p> Name Type Description <code>n_periods</code> <code>int</code> <p>Number of basis functions to place.</p> <code>column</code> <code>str</code> <p>Name of the column in the input DataFrame/Series to transform.</p> <code>input_range</code> <code>Tuple[int, int]</code> <p>The range of the input values (min, max).</p> <code>remainder</code> <code>str</code> <p>Policy for remaining columns (currently only 'drop' is supported).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.repeating_basis_function import RepeatingBasisFunction\n&gt;&gt;&gt; X = pd.DataFrame({\"hour\": [0, 6, 12, 18, 23]})\n&gt;&gt;&gt; rbf = RepeatingBasisFunction(n_periods=4, column=\"hour\", input_range=(0, 23))\n&gt;&gt;&gt; features = rbf.fit_transform(X)\n&gt;&gt;&gt; features.shape\n(5, 4)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/repeating_basis_function.py</code> <pre><code>class RepeatingBasisFunction(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer that encodes cyclical features using repeating radial basis functions.\n\n    This transformer places Gaussian basis functions across the specified input range\n    and wraps them around to handle periodicity (e.g., day of year, hour of day).\n    It is a simplified implementation to avoid external dependencies like scikit-lego.\n\n    Attributes:\n        n_periods (int): Number of basis functions to place.\n        column (str): Name of the column in the input DataFrame/Series to transform.\n        input_range (Tuple[int, int]): The range of the input values (min, max).\n        remainder (str): Policy for remaining columns (currently only 'drop' is supported).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.repeating_basis_function import RepeatingBasisFunction\n        &gt;&gt;&gt; X = pd.DataFrame({\"hour\": [0, 6, 12, 18, 23]})\n        &gt;&gt;&gt; rbf = RepeatingBasisFunction(n_periods=4, column=\"hour\", input_range=(0, 23))\n        &gt;&gt;&gt; features = rbf.fit_transform(X)\n        &gt;&gt;&gt; features.shape\n        (5, 4)\n    \"\"\"\n\n    def __init__(\n        self,\n        n_periods: int,\n        column: str,\n        input_range: Tuple[int, int],\n        remainder: str = \"drop\",\n    ):\n        \"\"\"\n        Initialize the RepeatingBasisFunction transformer.\n\n        Args:\n            n_periods: Number of basis functions.\n            column: Name of the column to transform.\n            input_range: Min and max values of the periodic feature.\n            remainder: How to handle other columns. Defaults to \"drop\".\n        \"\"\"\n        self.n_periods = n_periods\n        self.column = column\n        self.input_range = input_range\n        self.remainder = remainder\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RepeatingBasisFunction\":\n        \"\"\"\n        Fitted transformer (no-op).\n\n        Args:\n            X: Input data.\n            y: Ignored.\n\n        Returns:\n            self: The fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: Union[pd.Series, pd.DataFrame]) -&gt; np.ndarray:\n        \"\"\"\n        Transform the input data into RBF features.\n\n        Args:\n            X: Input DataFrame or Series containing the column to transform.\n\n        Returns:\n            np.ndarray: Array of transformed features with shape (n_samples, n_periods).\n\n        Raises:\n            ValueError: If the specified column is not found in the input.\n        \"\"\"\n        # Allow passing just the column series if X is not a DataFrame\n        if isinstance(X, pd.Series):\n            vals = X.values\n        elif isinstance(X, pd.DataFrame) and self.column in X.columns:\n            vals = X[self.column].values\n        else:\n            raise ValueError(f\"Column {self.column} not found in input\")\n\n        # Normalize to [0, 1] relative to input range\n        vals_norm = (vals - self.input_range[0]) / (\n            self.input_range[1] - self.input_range[0]\n        )\n\n        features = []\n        for i in range(self.n_periods):\n            mu = i / self.n_periods\n            # Gaussian with wraparound handling for cyclic\n            diff = np.abs(vals_norm - mu)\n            diff = np.minimum(diff, 1 - diff)  # cyclic distance\n            # sigma estimated as 1 / n_periods for reasonable overlap\n            sigma = 1 / self.n_periods\n            val = np.exp(-(diff**2) / (2 * sigma**2))\n            features.append(val)\n\n        return np.stack(features, axis=1)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RepeatingBasisFunction.__init__","title":"<code>__init__(n_periods, column, input_range, remainder='drop')</code>","text":"<p>Initialize the RepeatingBasisFunction transformer.</p> <p>Parameters:</p> Name Type Description Default <code>n_periods</code> <code>int</code> <p>Number of basis functions.</p> required <code>column</code> <code>str</code> <p>Name of the column to transform.</p> required <code>input_range</code> <code>Tuple[int, int]</code> <p>Min and max values of the periodic feature.</p> required <code>remainder</code> <code>str</code> <p>How to handle other columns. Defaults to \"drop\".</p> <code>'drop'</code> Source code in <code>src/spotforecast2_safe/preprocessing/repeating_basis_function.py</code> <pre><code>def __init__(\n    self,\n    n_periods: int,\n    column: str,\n    input_range: Tuple[int, int],\n    remainder: str = \"drop\",\n):\n    \"\"\"\n    Initialize the RepeatingBasisFunction transformer.\n\n    Args:\n        n_periods: Number of basis functions.\n        column: Name of the column to transform.\n        input_range: Min and max values of the periodic feature.\n        remainder: How to handle other columns. Defaults to \"drop\".\n    \"\"\"\n    self.n_periods = n_periods\n    self.column = column\n    self.input_range = input_range\n    self.remainder = remainder\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RepeatingBasisFunction.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fitted transformer (no-op).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Input data.</p> required <code>y</code> <code>Any</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RepeatingBasisFunction</code> <p>The fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/repeating_basis_function.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RepeatingBasisFunction\":\n    \"\"\"\n    Fitted transformer (no-op).\n\n    Args:\n        X: Input data.\n        y: Ignored.\n\n    Returns:\n        self: The fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RepeatingBasisFunction.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input data into RBF features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Series, DataFrame]</code> <p>Input DataFrame or Series containing the column to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of transformed features with shape (n_samples, n_periods).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified column is not found in the input.</p> Source code in <code>src/spotforecast2_safe/preprocessing/repeating_basis_function.py</code> <pre><code>def transform(self, X: Union[pd.Series, pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"\n    Transform the input data into RBF features.\n\n    Args:\n        X: Input DataFrame or Series containing the column to transform.\n\n    Returns:\n        np.ndarray: Array of transformed features with shape (n_samples, n_periods).\n\n    Raises:\n        ValueError: If the specified column is not found in the input.\n    \"\"\"\n    # Allow passing just the column series if X is not a DataFrame\n    if isinstance(X, pd.Series):\n        vals = X.values\n    elif isinstance(X, pd.DataFrame) and self.column in X.columns:\n        vals = X[self.column].values\n    else:\n        raise ValueError(f\"Column {self.column} not found in input\")\n\n    # Normalize to [0, 1] relative to input range\n    vals_norm = (vals - self.input_range[0]) / (\n        self.input_range[1] - self.input_range[0]\n    )\n\n    features = []\n    for i in range(self.n_periods):\n        mu = i / self.n_periods\n        # Gaussian with wraparound handling for cyclic\n        diff = np.abs(vals_norm - mu)\n        diff = np.minimum(diff, 1 - diff)  # cyclic distance\n        # sigma estimated as 1 / n_periods for reasonable overlap\n        sigma = 1 / self.n_periods\n        val = np.exp(-(diff**2) / (2 * sigma**2))\n        features.append(val)\n\n    return np.stack(features, axis=1)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures","title":"<code>RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n            \"ratio_min_max\": _np_min_max_ratio_jit,\n            \"coef_variation\": _np_cv_jit,\n            \"ewm\": _ewm_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        Args:\n            X: Time series data as 1D or 2D numpy array.\n\n        Returns:\n            np.ndarray: Array of rolling statistics.\n                - If X is 1D: shape (len(X), n_features)\n                - If X is 2D: shape (X.shape[1], n_features) - used for vectorized bootstrap.\n        \"\"\"\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X_2d = X[:, np.newaxis]\n        else:\n            X_2d = X\n\n        vectorizable_stats = {\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}\n        has_vectorizable = bool(set(self.stats) &amp; vectorizable_stats)\n\n        # Output shape: (n_columns, n_features)\n        # n_features = n_stats * n_window_sizes\n        rolling_features = np.full(\n            (X_2d.shape[1], len(self.features_names)), np.nan, dtype=float\n        )\n\n        if has_vectorizable:\n            self._transform_vectorized(X_2d, rolling_features)\n\n        # Non-vectorizable or fallback for single columns\n        for i in range(X_2d.shape[1]):\n            col = X_2d[:, i]\n            for j, ws in enumerate(self.window_sizes):\n                for k, stat_name in enumerate(self.stats):\n                    idx_feature = j * len(self.stats) + k\n                    if stat_name in vectorizable_stats and array_ndim == 1:\n                        # For 1D transform (batch), we use pandas for the full series\n                        # For 2D transform (bootstrap), it's already handled in _transform_vectorized\n                        continue\n\n                    if stat_name not in vectorizable_stats:\n                        # Custom/Non-vectorized stats only need the last window for bootstrap\n                        # but transform() is also used in transform_batch() for the whole series.\n                        # If it's a batch transform (1D input or many rows), we use the slow path.\n                        # If it's for bootstrapping (2D input, small window), we use the last window.\n\n                        # Bootstrap case or single-step case:\n                        # X_2d is typically (window_size, n_boot) or (window_size, 1)\n                        # We only need the result for the last window\n                        window = col[-ws:]\n                        window = window[~np.isnan(window)]\n                        if len(window) &gt; 0:\n                            rolling_features[i, idx_feature] = self.stats_funcs[k](\n                                window\n                            )\n\n        if array_ndim == 1:\n            # For 1D input, we want (n_samples, n_features)\n            # This logic is slightly different from skforecast because skforecast's\n            # transform() seems optimized for the bootstrap case (returning 1 row per sample)\n            # whereas our original transform() was a batch transform.\n            # Let's align with skforecast's RollingFeatures.transform which returns\n            # (n_samples, n_features) if n_samples == 1? No, skforecast returns (n_boot, n_stats).\n\n            # Re-evaluating: spotforecast2-safe's ForecasterRecursive._create_window_features\n            # calls wf.transform_batch(y) which calls wf.transform(y.to_numpy()).\n            # ForecasterRecursive._recursive_predict calls wf.transform(window_data).\n\n            # In _recursive_predict, window_data is (window_size, n_boot).\n            # The result should be (n_boot, n_features).\n\n            return rolling_features\n        else:\n            return rolling_features\n\n    def _transform_vectorized(self, X: np.ndarray, rolling_features: np.ndarray):\n        \"\"\"\n        Vectorized transform for bootstrap predictions.\n        X: (window_length, n_samples)\n        rolling_features: (n_samples, n_features) - modified in place\n        \"\"\"\n        vectorizable_stats = {\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}\n\n        for j, ws in enumerate(self.window_sizes):\n            for k, stat_name in enumerate(self.stats):\n                if stat_name not in vectorizable_stats:\n                    continue\n\n                idx_feature = j * len(self.stats) + k\n                window = X[-ws:, :]\n\n                with warnings.catch_warnings():\n                    warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n                    warnings.filterwarnings(\n                        \"ignore\", message=\"Degrees of freedom &lt;= 0 for slice\"\n                    )\n                    warnings.filterwarnings(\n                        \"ignore\", message=\"All-NaN slice encountered\"\n                    )\n\n                    if stat_name == \"mean\":\n                        rolling_features[:, idx_feature] = np.nanmean(window, axis=0)\n                    elif stat_name == \"std\":\n                        result = np.nanstd(window, axis=0, ddof=1)\n                        n_valid = np.sum(~np.isnan(window), axis=0)\n                        result[n_valid == 1] = 0.0\n                        rolling_features[:, idx_feature] = result\n                    elif stat_name == \"min\":\n                        rolling_features[:, idx_feature] = np.nanmin(window, axis=0)\n                    elif stat_name == \"max\":\n                        rolling_features[:, idx_feature] = np.nanmax(window, axis=0)\n                    elif stat_name == \"sum\":\n                        result = np.nansum(window, axis=0, dtype=float)\n                        all_nan_mask = np.all(np.isnan(window), axis=0)\n                        result[all_nan_mask] = np.nan\n                        rolling_features[:, idx_feature] = result\n                    elif stat_name == \"median\":\n                        rolling_features[:, idx_feature] = np.nanmedian(window, axis=0)\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n        \"\"\"\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n        values = X.to_numpy()\n\n        for j, ws in enumerate(self.window_sizes):\n            for k, stat_name in enumerate(self.stats):\n                idx_feature = j * len(self.stats) + k\n                func = self.stats_funcs[k]\n\n                # Use pandas rolling for batch transformation\n                series = pd.Series(values)\n                # Note: skforecast uses closed='left' for RollingFeatures by default to avoid leakage.\n                # However, RollingFeatures in spotforecast2-safe's original implementation\n                # seemed to be a simple rolling. Let's check skforecast's default again.\n                # skforecast: self.unique_rolling_windows[key]['params'] = {'window': params[0], 'min_periods': params[1], 'center': False, 'closed': 'left'}\n                # Wait, if closed='left', then the current value is NOT included.\n                # Let's align with skforecast's behavior if it's ported.\n\n                # Original spotforecast2-safe implementation used standard rolling (closed='right')\n                # but if we want to be exactly like skforecast, we should use closed='left'.\n                # Actually, ForecasterRecursive handles the lags manually, so if window features are\n                # calculated on the same 'y' as lags, they should probably be shifted too.\n\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n\n        return pd.DataFrame(output, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D or 2D numpy array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of rolling statistics. - If X is 1D: shape (len(X), n_features) - If X is 2D: shape (X.shape[1], n_features) - used for vectorized bootstrap.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    Args:\n        X: Time series data as 1D or 2D numpy array.\n\n    Returns:\n        np.ndarray: Array of rolling statistics.\n            - If X is 1D: shape (len(X), n_features)\n            - If X is 2D: shape (X.shape[1], n_features) - used for vectorized bootstrap.\n    \"\"\"\n    array_ndim = X.ndim\n    if array_ndim == 1:\n        X_2d = X[:, np.newaxis]\n    else:\n        X_2d = X\n\n    vectorizable_stats = {\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}\n    has_vectorizable = bool(set(self.stats) &amp; vectorizable_stats)\n\n    # Output shape: (n_columns, n_features)\n    # n_features = n_stats * n_window_sizes\n    rolling_features = np.full(\n        (X_2d.shape[1], len(self.features_names)), np.nan, dtype=float\n    )\n\n    if has_vectorizable:\n        self._transform_vectorized(X_2d, rolling_features)\n\n    # Non-vectorizable or fallback for single columns\n    for i in range(X_2d.shape[1]):\n        col = X_2d[:, i]\n        for j, ws in enumerate(self.window_sizes):\n            for k, stat_name in enumerate(self.stats):\n                idx_feature = j * len(self.stats) + k\n                if stat_name in vectorizable_stats and array_ndim == 1:\n                    # For 1D transform (batch), we use pandas for the full series\n                    # For 2D transform (bootstrap), it's already handled in _transform_vectorized\n                    continue\n\n                if stat_name not in vectorizable_stats:\n                    # Custom/Non-vectorized stats only need the last window for bootstrap\n                    # but transform() is also used in transform_batch() for the whole series.\n                    # If it's a batch transform (1D input or many rows), we use the slow path.\n                    # If it's for bootstrapping (2D input, small window), we use the last window.\n\n                    # Bootstrap case or single-step case:\n                    # X_2d is typically (window_size, n_boot) or (window_size, 1)\n                    # We only need the result for the last window\n                    window = col[-ws:]\n                    window = window[~np.isnan(window)]\n                    if len(window) &gt; 0:\n                        rolling_features[i, idx_feature] = self.stats_funcs[k](\n                            window\n                        )\n\n    if array_ndim == 1:\n        # For 1D input, we want (n_samples, n_features)\n        # This logic is slightly different from skforecast because skforecast's\n        # transform() seems optimized for the bootstrap case (returning 1 row per sample)\n        # whereas our original transform() was a batch transform.\n        # Let's align with skforecast's RollingFeatures.transform which returns\n        # (n_samples, n_features) if n_samples == 1? No, skforecast returns (n_boot, n_stats).\n\n        # Re-evaluating: spotforecast2-safe's ForecasterRecursive._create_window_features\n        # calls wf.transform_batch(y) which calls wf.transform(y.to_numpy()).\n        # ForecasterRecursive._recursive_predict calls wf.transform(window_data).\n\n        # In _recursive_predict, window_data is (window_size, n_boot).\n        # The result should be (n_boot, n_features).\n\n        return rolling_features\n    else:\n        return rolling_features\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n    \"\"\"\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n    values = X.to_numpy()\n\n    for j, ws in enumerate(self.window_sizes):\n        for k, stat_name in enumerate(self.stats):\n            idx_feature = j * len(self.stats) + k\n            func = self.stats_funcs[k]\n\n            # Use pandas rolling for batch transformation\n            series = pd.Series(values)\n            # Note: skforecast uses closed='left' for RollingFeatures by default to avoid leakage.\n            # However, RollingFeatures in spotforecast2-safe's original implementation\n            # seemed to be a simple rolling. Let's check skforecast's default again.\n            # skforecast: self.unique_rolling_windows[key]['params'] = {'window': params[0], 'min_periods': params[1], 'center': False, 'closed': 'left'}\n            # Wait, if closed='left', then the current value is NOT included.\n            # Let's align with skforecast's behavior if it's ported.\n\n            # Original spotforecast2-safe implementation used standard rolling (closed='right')\n            # but if we want to be exactly like skforecast, we should use closed='left'.\n            # Actually, ForecasterRecursive handles the lags manually, so if window features are\n            # calculated on the same 'y' as lags, they should probably be shifted too.\n\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n\n    return pd.DataFrame(output, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction","title":"<code>WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.agg_and_resample_data","title":"<code>agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.basic_ts_checks","title":"<code>basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_holidays","title":"<code>curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_weather","title":"<code>curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.custom_weights","title":"<code>custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_missing_weights","title":"<code>get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_start_end","title":"<code>get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split_abs_train_val_test","title":"<code>split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split_rel_train_val_test","title":"<code>split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#data-curation","title":"Data Curation","text":""},{"location":"api/preprocessing/#curate_data","title":"curate_data","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data","title":"<code>spotforecast2_safe.preprocessing.curate_data</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.agg_and_resample_data","title":"<code>agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.basic_ts_checks","title":"<code>basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.curate_holidays","title":"<code>curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.curate_weather","title":"<code>curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.get_start_end","title":"<code>get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#imputation","title":"Imputation","text":""},{"location":"api/preprocessing/#imputation_1","title":"imputation","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation","title":"<code>spotforecast2_safe.preprocessing.imputation</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction","title":"<code>WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.custom_weights","title":"<code>custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.get_missing_weights","title":"<code>get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#outlier-detection-and-handling","title":"Outlier Detection and Handling","text":""},{"location":"api/preprocessing/#outlier","title":"outlier","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier","title":"<code>spotforecast2_safe.preprocessing.outlier</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#time-series-splitting","title":"Time Series Splitting","text":""},{"location":"api/preprocessing/#split","title":"split","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split","title":"<code>spotforecast2_safe.preprocessing.split</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split.split_abs_train_val_test","title":"<code>split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split.split_rel_train_val_test","title":"<code>split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#rolling-features","title":"Rolling Features","text":""},{"location":"api/preprocessing/#_rolling","title":"_rolling","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling","title":"<code>spotforecast2_safe.preprocessing._rolling</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures","title":"<code>RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n            \"ratio_min_max\": _np_min_max_ratio_jit,\n            \"coef_variation\": _np_cv_jit,\n            \"ewm\": _ewm_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        Args:\n            X: Time series data as 1D or 2D numpy array.\n\n        Returns:\n            np.ndarray: Array of rolling statistics.\n                - If X is 1D: shape (len(X), n_features)\n                - If X is 2D: shape (X.shape[1], n_features) - used for vectorized bootstrap.\n        \"\"\"\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X_2d = X[:, np.newaxis]\n        else:\n            X_2d = X\n\n        vectorizable_stats = {\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}\n        has_vectorizable = bool(set(self.stats) &amp; vectorizable_stats)\n\n        # Output shape: (n_columns, n_features)\n        # n_features = n_stats * n_window_sizes\n        rolling_features = np.full(\n            (X_2d.shape[1], len(self.features_names)), np.nan, dtype=float\n        )\n\n        if has_vectorizable:\n            self._transform_vectorized(X_2d, rolling_features)\n\n        # Non-vectorizable or fallback for single columns\n        for i in range(X_2d.shape[1]):\n            col = X_2d[:, i]\n            for j, ws in enumerate(self.window_sizes):\n                for k, stat_name in enumerate(self.stats):\n                    idx_feature = j * len(self.stats) + k\n                    if stat_name in vectorizable_stats and array_ndim == 1:\n                        # For 1D transform (batch), we use pandas for the full series\n                        # For 2D transform (bootstrap), it's already handled in _transform_vectorized\n                        continue\n\n                    if stat_name not in vectorizable_stats:\n                        # Custom/Non-vectorized stats only need the last window for bootstrap\n                        # but transform() is also used in transform_batch() for the whole series.\n                        # If it's a batch transform (1D input or many rows), we use the slow path.\n                        # If it's for bootstrapping (2D input, small window), we use the last window.\n\n                        # Bootstrap case or single-step case:\n                        # X_2d is typically (window_size, n_boot) or (window_size, 1)\n                        # We only need the result for the last window\n                        window = col[-ws:]\n                        window = window[~np.isnan(window)]\n                        if len(window) &gt; 0:\n                            rolling_features[i, idx_feature] = self.stats_funcs[k](\n                                window\n                            )\n\n        if array_ndim == 1:\n            # For 1D input, we want (n_samples, n_features)\n            # This logic is slightly different from skforecast because skforecast's\n            # transform() seems optimized for the bootstrap case (returning 1 row per sample)\n            # whereas our original transform() was a batch transform.\n            # Let's align with skforecast's RollingFeatures.transform which returns\n            # (n_samples, n_features) if n_samples == 1? No, skforecast returns (n_boot, n_stats).\n\n            # Re-evaluating: spotforecast2-safe's ForecasterRecursive._create_window_features\n            # calls wf.transform_batch(y) which calls wf.transform(y.to_numpy()).\n            # ForecasterRecursive._recursive_predict calls wf.transform(window_data).\n\n            # In _recursive_predict, window_data is (window_size, n_boot).\n            # The result should be (n_boot, n_features).\n\n            return rolling_features\n        else:\n            return rolling_features\n\n    def _transform_vectorized(self, X: np.ndarray, rolling_features: np.ndarray):\n        \"\"\"\n        Vectorized transform for bootstrap predictions.\n        X: (window_length, n_samples)\n        rolling_features: (n_samples, n_features) - modified in place\n        \"\"\"\n        vectorizable_stats = {\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}\n\n        for j, ws in enumerate(self.window_sizes):\n            for k, stat_name in enumerate(self.stats):\n                if stat_name not in vectorizable_stats:\n                    continue\n\n                idx_feature = j * len(self.stats) + k\n                window = X[-ws:, :]\n\n                with warnings.catch_warnings():\n                    warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n                    warnings.filterwarnings(\n                        \"ignore\", message=\"Degrees of freedom &lt;= 0 for slice\"\n                    )\n                    warnings.filterwarnings(\n                        \"ignore\", message=\"All-NaN slice encountered\"\n                    )\n\n                    if stat_name == \"mean\":\n                        rolling_features[:, idx_feature] = np.nanmean(window, axis=0)\n                    elif stat_name == \"std\":\n                        result = np.nanstd(window, axis=0, ddof=1)\n                        n_valid = np.sum(~np.isnan(window), axis=0)\n                        result[n_valid == 1] = 0.0\n                        rolling_features[:, idx_feature] = result\n                    elif stat_name == \"min\":\n                        rolling_features[:, idx_feature] = np.nanmin(window, axis=0)\n                    elif stat_name == \"max\":\n                        rolling_features[:, idx_feature] = np.nanmax(window, axis=0)\n                    elif stat_name == \"sum\":\n                        result = np.nansum(window, axis=0, dtype=float)\n                        all_nan_mask = np.all(np.isnan(window), axis=0)\n                        result[all_nan_mask] = np.nan\n                        rolling_features[:, idx_feature] = result\n                    elif stat_name == \"median\":\n                        rolling_features[:, idx_feature] = np.nanmedian(window, axis=0)\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n        \"\"\"\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n        values = X.to_numpy()\n\n        for j, ws in enumerate(self.window_sizes):\n            for k, stat_name in enumerate(self.stats):\n                idx_feature = j * len(self.stats) + k\n                func = self.stats_funcs[k]\n\n                # Use pandas rolling for batch transformation\n                series = pd.Series(values)\n                # Note: skforecast uses closed='left' for RollingFeatures by default to avoid leakage.\n                # However, RollingFeatures in spotforecast2-safe's original implementation\n                # seemed to be a simple rolling. Let's check skforecast's default again.\n                # skforecast: self.unique_rolling_windows[key]['params'] = {'window': params[0], 'min_periods': params[1], 'center': False, 'closed': 'left'}\n                # Wait, if closed='left', then the current value is NOT included.\n                # Let's align with skforecast's behavior if it's ported.\n\n                # Original spotforecast2-safe implementation used standard rolling (closed='right')\n                # but if we want to be exactly like skforecast, we should use closed='left'.\n                # Actually, ForecasterRecursive handles the lags manually, so if window features are\n                # calculated on the same 'y' as lags, they should probably be shifted too.\n\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n\n        return pd.DataFrame(output, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D or 2D numpy array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of rolling statistics. - If X is 1D: shape (len(X), n_features) - If X is 2D: shape (X.shape[1], n_features) - used for vectorized bootstrap.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    Args:\n        X: Time series data as 1D or 2D numpy array.\n\n    Returns:\n        np.ndarray: Array of rolling statistics.\n            - If X is 1D: shape (len(X), n_features)\n            - If X is 2D: shape (X.shape[1], n_features) - used for vectorized bootstrap.\n    \"\"\"\n    array_ndim = X.ndim\n    if array_ndim == 1:\n        X_2d = X[:, np.newaxis]\n    else:\n        X_2d = X\n\n    vectorizable_stats = {\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}\n    has_vectorizable = bool(set(self.stats) &amp; vectorizable_stats)\n\n    # Output shape: (n_columns, n_features)\n    # n_features = n_stats * n_window_sizes\n    rolling_features = np.full(\n        (X_2d.shape[1], len(self.features_names)), np.nan, dtype=float\n    )\n\n    if has_vectorizable:\n        self._transform_vectorized(X_2d, rolling_features)\n\n    # Non-vectorizable or fallback for single columns\n    for i in range(X_2d.shape[1]):\n        col = X_2d[:, i]\n        for j, ws in enumerate(self.window_sizes):\n            for k, stat_name in enumerate(self.stats):\n                idx_feature = j * len(self.stats) + k\n                if stat_name in vectorizable_stats and array_ndim == 1:\n                    # For 1D transform (batch), we use pandas for the full series\n                    # For 2D transform (bootstrap), it's already handled in _transform_vectorized\n                    continue\n\n                if stat_name not in vectorizable_stats:\n                    # Custom/Non-vectorized stats only need the last window for bootstrap\n                    # but transform() is also used in transform_batch() for the whole series.\n                    # If it's a batch transform (1D input or many rows), we use the slow path.\n                    # If it's for bootstrapping (2D input, small window), we use the last window.\n\n                    # Bootstrap case or single-step case:\n                    # X_2d is typically (window_size, n_boot) or (window_size, 1)\n                    # We only need the result for the last window\n                    window = col[-ws:]\n                    window = window[~np.isnan(window)]\n                    if len(window) &gt; 0:\n                        rolling_features[i, idx_feature] = self.stats_funcs[k](\n                            window\n                        )\n\n    if array_ndim == 1:\n        # For 1D input, we want (n_samples, n_features)\n        # This logic is slightly different from skforecast because skforecast's\n        # transform() seems optimized for the bootstrap case (returning 1 row per sample)\n        # whereas our original transform() was a batch transform.\n        # Let's align with skforecast's RollingFeatures.transform which returns\n        # (n_samples, n_features) if n_samples == 1? No, skforecast returns (n_boot, n_stats).\n\n        # Re-evaluating: spotforecast2-safe's ForecasterRecursive._create_window_features\n        # calls wf.transform_batch(y) which calls wf.transform(y.to_numpy()).\n        # ForecasterRecursive._recursive_predict calls wf.transform(window_data).\n\n        # In _recursive_predict, window_data is (window_size, n_boot).\n        # The result should be (n_boot, n_features).\n\n        return rolling_features\n    else:\n        return rolling_features\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n    \"\"\"\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n    values = X.to_numpy()\n\n    for j, ws in enumerate(self.window_sizes):\n        for k, stat_name in enumerate(self.stats):\n            idx_feature = j * len(self.stats) + k\n            func = self.stats_funcs[k]\n\n            # Use pandas rolling for batch transformation\n            series = pd.Series(values)\n            # Note: skforecast uses closed='left' for RollingFeatures by default to avoid leakage.\n            # However, RollingFeatures in spotforecast2-safe's original implementation\n            # seemed to be a simple rolling. Let's check skforecast's default again.\n            # skforecast: self.unique_rolling_windows[key]['params'] = {'window': params[0], 'min_periods': params[1], 'center': False, 'closed': 'left'}\n            # Wait, if closed='left', then the current value is NOT included.\n            # Let's align with skforecast's behavior if it's ported.\n\n            # Original spotforecast2-safe implementation used standard rolling (closed='right')\n            # but if we want to be exactly like skforecast, we should use closed='left'.\n            # Actually, ForecasterRecursive handles the lags manually, so if window features are\n            # calculated on the same 'y' as lags, they should probably be shifted too.\n\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n\n    return pd.DataFrame(output, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#differencing","title":"Differencing","text":""},{"location":"api/preprocessing/#_differentiator","title":"_differentiator","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator","title":"<code>spotforecast2_safe.preprocessing._differentiator</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#binning","title":"Binning","text":""},{"location":"api/preprocessing/#_binner","title":"_binner","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner","title":"<code>spotforecast2_safe.preprocessing._binner</code>","text":"<p>QuantileBinner class for binning data into quantile-based bins.</p> <p>This module contains the QuantileBinner class which bins data into quantile-based bins using numpy.percentile with optimized performance using numpy.searchsorted.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner","title":"<code>QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/processing/","title":"Processing Module","text":"<p>End-to-end forecasting pipelines and prediction aggregation.</p>"},{"location":"api/processing/#spotforecast2_safe.processing","title":"<code>spotforecast2_safe.processing</code>","text":"<p>Processing module for end-to-end forecasting pipelines.</p>"},{"location":"api/processing/#n-to-n-prediction","title":"N-to-N Prediction","text":""},{"location":"api/processing/#n2n_predict","title":"n2n_predict","text":""},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict","title":"<code>spotforecast2_safe.processing.n2n_predict</code>","text":"<p>End-to-end baseline forecasting using equivalent date method.</p> <p>This module provides a complete forecasting pipeline using the ForecasterEquivalentDate baseline model. It handles data preparation, outlier detection, imputation, model training, and prediction in a single integrated function.</p> <p>Model persistence follows scikit-learn conventions using joblib for efficient serialization and deserialization of trained forecasters.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.processing.n2n_predict import n2n_predict\n&gt;&gt;&gt; predictions = n2n_predict(forecast_horizon=24, verbose=True)\n</code></pre> <p>Using cached models:</p> <pre><code>&gt;&gt;&gt; # Load existing models if available, or train new ones\n&gt;&gt;&gt; predictions = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre>"},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict.n2n_predict","title":"<code>n2n_predict(data=None, columns=None, forecast_horizon=24, contamination=0.01, window_size=72, force_train=True, model_dir=None, verbose=True, show_progress=True)</code>","text":"<p>End-to-end baseline forecasting using equivalent date method.</p> <p>This function implements a complete forecasting pipeline that: 1. Loads and validates target data 2. Detects and removes outliers 3. Imputes missing values 4. Splits into train/validation/test sets 5. Trains or loads equivalent date forecasters 6. Generates multi-step ahead predictions</p> <p>Models are persisted to disk following scikit-learn conventions using joblib. By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with target time series data. If None, fetches data automatically. Default: None.</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>List of target columns to forecast. If None, uses all available columns. Default: None.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to forecast ahead. Default: 24.</p> <code>24</code> <code>contamination</code> <code>float</code> <p>Contamination parameter for outlier detection. Default: 0.01.</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Rolling window size for gap detection. Default: 72.</p> <code>72</code> <code>force_train</code> <code>bool</code> <p>Force retraining of all models, ignoring cached models. Default: True.</p> <code>True</code> <code>model_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory for saving/loading trained models. If None, uses cache directory from get_cache_home(). Default: None (uses ~/spotforecast2_cache/forecasters).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: True.</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar during training and prediction. Default: True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>Dict</code> <ul> <li>predictions: DataFrame with forecast values for each target variable.</li> </ul> <code>Tuple[DataFrame, Dict]</code> <ul> <li>forecasters: Dictionary of trained ForecasterEquivalentDate objects keyed by target.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data validation fails or required data cannot be retrieved.</p> <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>OSError</code> <p>If models cannot be saved to disk.</p> <p>Examples:</p> <p>Basic usage with automatic model caching:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n&gt;&gt;&gt; print(predictions.shape)\n(24, 11)\n</code></pre> <p>Load cached models (if available):</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./saved_models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./saved_models\",\n...     verbose=True\n... )\n</code></pre> <p>With specific target columns:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     columns=[\"power\", \"energy\"],\n...     forecast_horizon=48,\n...     force_train=False,\n...     verbose=True\n... )\n</code></pre> Notes <ul> <li>Trained models are saved to disk using joblib for fast reuse.</li> <li>When force_train=False, existing models are loaded and prediction   proceeds without retraining. This significantly speeds up prediction   for repeated calls with the same configuration.</li> <li>The model_dir directory is created automatically if it doesn't exist.</li> <li>Default model_dir uses get_cache_home() which respects the   SPOTFORECAST2_CACHE environment variable.</li> </ul> Performance Notes <ul> <li>First run: Full training (~2-5 minutes depending on data size)</li> <li>Subsequent runs (force_train=False): Model loading only (~1-2 seconds)</li> <li>Force retrain (force_train=True): Full training again (~2-5 minutes)</li> </ul> Source code in <code>src/spotforecast2_safe/processing/n2n_predict.py</code> <pre><code>def n2n_predict(\n    data: Optional[pd.DataFrame] = None,\n    columns: Optional[List[str]] = None,\n    forecast_horizon: int = 24,\n    contamination: float = 0.01,\n    window_size: int = 72,\n    force_train: bool = True,\n    model_dir: Optional[Union[str, Path]] = None,\n    verbose: bool = True,\n    show_progress: bool = True,\n) -&gt; Tuple[pd.DataFrame, Dict]:\n    \"\"\"End-to-end baseline forecasting using equivalent date method.\n\n    This function implements a complete forecasting pipeline that:\n    1. Loads and validates target data\n    2. Detects and removes outliers\n    3. Imputes missing values\n    4. Splits into train/validation/test sets\n    5. Trains or loads equivalent date forecasters\n    6. Generates multi-step ahead predictions\n\n    Models are persisted to disk following scikit-learn conventions using joblib.\n    By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.\n\n    Args:\n        data: Optional DataFrame with target time series data. If None, fetches data automatically.\n            Default: None.\n        columns: List of target columns to forecast. If None, uses all available columns.\n            Default: None.\n        forecast_horizon: Number of time steps to forecast ahead. Default: 24.\n        contamination: Contamination parameter for outlier detection. Default: 0.01.\n        window_size: Rolling window size for gap detection. Default: 72.\n        force_train: Force retraining of all models, ignoring cached models.\n            Default: True.\n        model_dir: Directory for saving/loading trained models. If None, uses cache directory from get_cache_home(). Default: None (uses ~/spotforecast2_cache/forecasters).\n        verbose: Print progress messages. Default: True.\n        show_progress: Show progress bar during training and prediction. Default: True.\n\n    Returns:\n        Tuple containing:\n        - predictions: DataFrame with forecast values for each target variable.\n        - forecasters: Dictionary of trained ForecasterEquivalentDate objects keyed by target.\n\n    Raises:\n        ValueError: If data validation fails or required data cannot be retrieved.\n        ImportError: If required dependencies are not installed.\n        OSError: If models cannot be saved to disk.\n\n    Examples:\n        Basic usage with automatic model caching:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(predictions.shape)\n        (24, 11)\n\n        Load cached models (if available):\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     force_train=False,\n        ...     model_dir=\"./saved_models\",\n        ...     verbose=True\n        ... )\n\n        Force retraining and update cache:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     force_train=True,\n        ...     model_dir=\"./saved_models\",\n        ...     verbose=True\n        ... )\n\n        With specific target columns:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     columns=[\"power\", \"energy\"],\n        ...     forecast_horizon=48,\n        ...     force_train=False,\n        ...     verbose=True\n        ... )\n\n    Notes:\n        - Trained models are saved to disk using joblib for fast reuse.\n        - When force_train=False, existing models are loaded and prediction\n          proceeds without retraining. This significantly speeds up prediction\n          for repeated calls with the same configuration.\n        - The model_dir directory is created automatically if it doesn't exist.\n        - Default model_dir uses get_cache_home() which respects the\n          SPOTFORECAST2_CACHE environment variable.\n\n    Performance Notes:\n        - First run: Full training (~2-5 minutes depending on data size)\n        - Subsequent runs (force_train=False): Model loading only (~1-2 seconds)\n        - Force retrain (force_train=True): Full training again (~2-5 minutes)\n    \"\"\"\n    if columns is not None:\n        TARGET = columns\n    else:\n        TARGET = None\n\n    if verbose:\n        print(\"--- Starting n2n_predict ---\")\n\n    # Set default model_dir if not provided\n    if model_dir is None:\n        from spotforecast2_safe.data.fetch_data import get_cache_home\n\n        model_dir = get_cache_home() / \"forecasters\"\n\n    # Handle data input - fetch_data handles both CSV and DataFrame\n    if data is not None:\n        if verbose:\n            print(\"Using provided dataframe...\")\n        data = fetch_data(dataframe=data, columns=TARGET)\n    else:\n        if verbose:\n            print(\"Fetching data from CSV...\")\n        data = fetch_data(filename=\"data_in.csv\", columns=TARGET)\n\n    _, _, _, _ = get_start_end(\n        data=data,\n        forecast_horizon=forecast_horizon,\n        verbose=verbose,\n    )\n\n    basic_ts_checks(data, verbose=verbose)\n\n    data = agg_and_resample_data(data, verbose=verbose)\n\n    # --- Outlier Handling ---\n    if verbose:\n        print(\"Handling outliers...\")\n\n    # data_old = data.copy() # kept in notebook, maybe useful for debugging but not used logic-wise here\n    data, outliers = mark_outliers(\n        data, contamination=contamination, random_state=1234, verbose=verbose\n    )\n\n    # --- Missing Data (Imputation) ---\n    if verbose:\n        print(\"Imputing missing data...\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    if verbose:\n        n_missing = len(missing_indices)\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n\n    data = data.ffill()\n    data = data.bfill()\n\n    # --- Train, Val, Test Split ---\n    if verbose:\n        print(\"Splitting data...\")\n    data_train, data_val, data_test = split_rel_train_val_test(\n        data, perc_train=0.8, perc_val=0.2, verbose=verbose\n    )\n\n    # --- Model Fit ---\n    if verbose:\n        print(\"Fitting models...\")\n\n    end_validation = pd.concat([data_train, data_val]).index[-1]\n\n    baseline_forecasters = {}\n    targets_to_train = list(data.columns)\n\n    # Attempt to load cached models if force_train=False\n    if not force_train and _model_directory_exists(model_dir):\n        if verbose:\n            print(\"  Attempting to load cached models...\")\n        cached_forecasters, missing_targets = _load_forecasters(\n            target_columns=list(data.columns),\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n        baseline_forecasters.update(cached_forecasters)\n        targets_to_train = missing_targets\n\n        if len(cached_forecasters) == len(data.columns):\n            if verbose:\n                print(f\"  \u2713 All {len(data.columns)} forecasters loaded from cache\")\n        elif len(cached_forecasters) &gt; 0:\n            if verbose:\n                print(\n                    f\"  \u2713 Loaded {len(cached_forecasters)} forecasters, \"\n                    f\"will train {len(targets_to_train)} new ones\"\n                )\n\n    # Train missing or forced models\n    if len(targets_to_train) &gt; 0:\n        if force_train and len(baseline_forecasters) &gt; 0:\n            if verbose:\n                print(f\"  Force retraining all {len(data.columns)} forecasters...\")\n            targets_to_train = list(data.columns)\n            baseline_forecasters.clear()\n\n        target_iter = targets_to_train\n        if show_progress and tqdm is not None:\n            target_iter = tqdm(\n                targets_to_train,\n                desc=\"Training forecasters\",\n                unit=\"model\",\n            )\n\n        for target in target_iter:\n            forecaster = ForecasterEquivalentDate(\n                offset=pd.DateOffset(days=1), n_offsets=1\n            )\n\n            forecaster.fit(y=data.loc[:end_validation, target])\n\n            baseline_forecasters[target] = forecaster\n\n        # Save newly trained models to disk\n        if verbose:\n            print(f\"  Saving {len(targets_to_train)} trained forecasters to disk...\")\n        _save_forecasters(\n            forecasters={t: baseline_forecasters[t] for t in targets_to_train},\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n\n    if verbose:\n        print(f\"  \u2713 Total forecasters available: {len(baseline_forecasters)}\")\n\n    # --- Predict ---\n    if verbose:\n        print(\"Generating predictions...\")\n\n    predictions = predict_multivariate(\n        baseline_forecasters,\n        steps_ahead=forecast_horizon,\n        show_progress=show_progress,\n    )\n\n    return predictions, baseline_forecasters\n</code></pre>"},{"location":"api/processing/#n-to-n-prediction-with-covariates","title":"N-to-N Prediction with Covariates","text":""},{"location":"api/processing/#n2n_predict_with_covariates","title":"n2n_predict_with_covariates","text":""},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict_with_covariates","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates</code>","text":"<p>End-to-end recursive forecasting with exogenous covariates.</p> <p>This module provides a complete pipeline for time series forecasting using recursive forecasters with exogenous variables (weather, holidays, calendar features). It handles data preparation, feature engineering, model training, and prediction in a single integrated function.</p> <p>Model persistence follows scikit-learn conventions using joblib for efficient serialization and deserialization of trained forecasters.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.processing.n2n_predict_with_covariates import (\n...     n2n_predict_with_covariates\n... )\n&gt;&gt;&gt; predictions = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n</code></pre> <p>With custom parameters:</p> <pre><code>&gt;&gt;&gt; predictions = n2n_predict_with_covariates(\n...     forecast_horizon=48,\n...     contamination=0.02,\n...     window_size=100,\n...     lags=48,\n...     train_ratio=0.75,\n...     verbose=True\n... )\n</code></pre> <p>Using cached models:</p> <pre><code>&gt;&gt;&gt; # Load existing models if available, or train new ones\n&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre>"},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict_with_covariates.n2n_predict_with_covariates","title":"<code>n2n_predict_with_covariates(data=None, forecast_horizon=24, contamination=0.01, window_size=72, lags=24, train_ratio=0.8, latitude=51.5136, longitude=7.4653, timezone='UTC', country_code='DE', state='NW', estimator=None, include_weather_windows=False, include_holiday_features=False, include_poly_features=False, force_train=True, model_dir=None, verbose=True, show_progress=False)</code>","text":"<p>End-to-end recursive forecasting with exogenous covariates.</p> <p>This function implements a complete forecasting pipeline that: 1. Loads and validates target data 2. Detects and removes outliers 3. Imputes missing values with weighted gaps 4. Creates exogenous features (weather, holidays, calendar, day/night) 5. Performs feature engineering (cyclical encoding, interactions) 6. Merges target and exogenous data 7. Splits into train/validation/test sets 8. Trains or loads recursive forecasters with sample weighting 9. Generates multi-step ahead predictions</p> <p>Models are persisted to disk following scikit-learn conventions using joblib. By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with target time series data. If None, fetches data automatically. Default: None.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to forecast ahead. Default: 24.</p> <code>24</code> <code>contamination</code> <code>float</code> <p>Contamination parameter for outlier detection. Default: 0.01.</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Rolling window size for gap detection. Default: 72.</p> <code>72</code> <code>lags</code> <code>int</code> <p>Number of lags for recursive forecaster. Default: 24.</p> <code>24</code> <code>train_ratio</code> <code>float</code> <p>Fraction of data for training. Default: 0.8.</p> <code>0.8</code> <code>latitude</code> <code>float</code> <p>Location latitude. Default: 51.5136 (Dortmund).</p> <code>51.5136</code> <code>longitude</code> <code>float</code> <p>Location longitude. Default: 7.4653 (Dortmund).</p> <code>7.4653</code> <code>timezone</code> <code>str</code> <p>Timezone for data. Default: \"UTC\".</p> <code>'UTC'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays. Default: \"DE\".</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays. Default: \"NW\".</p> <code>'NW'</code> <code>estimator</code> <code>Optional[object]</code> <p>Base estimator for recursive forecaster. If None, uses LGBMRegressor. Default: None.</p> <code>None</code> <code>include_weather_windows</code> <code>bool</code> <p>Include weather window features. Default: False.</p> <code>False</code> <code>include_holiday_features</code> <code>bool</code> <p>Include holiday features. Default: False.</p> <code>False</code> <code>include_poly_features</code> <code>bool</code> <p>Include polynomial interaction features. Default: False.</p> <code>False</code> <code>force_train</code> <code>bool</code> <p>Force retraining of all models, ignoring cached models. Default: True.</p> <code>True</code> <code>model_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory for saving/loading trained models. If None, uses the spotforecast2 cache directory (~/spotforecast2_cache by default, or SPOTFORECAST2_CACHE environment variable). Default: None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: True.</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar during training. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>Dict</code> <ul> <li>predictions: DataFrame with forecast values for each target variable.</li> </ul> <code>Dict</code> <ul> <li>metadata: Dictionary with forecast metadata (index, shapes, etc.).</li> </ul> <code>Tuple[DataFrame, Dict, Dict]</code> <ul> <li>forecasters: Dictionary of trained ForecasterRecursive objects keyed by target.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data validation fails or required data cannot be retrieved.</p> <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>OSError</code> <p>If models cannot be saved to disk.</p> <p>Examples:</p> <p>Basic usage with automatic model caching:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n&gt;&gt;&gt; print(predictions.shape)\n(24, 11)\n</code></pre> <p>Load cached models (if available):</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./saved_models\"\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./saved_models\"\n... )\n</code></pre> <p>Custom location and features:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=48,\n...     latitude=52.5200,  # Berlin\n...     longitude=13.4050,\n...     lags=48,\n...     include_poly_features=True,\n...     force_train=False,\n...     verbose=True\n... )\n</code></pre> Notes <ul> <li>The function uses cached weather data when available.</li> <li>Missing values are handled via forward/backward fill with downweighting   observations near gaps.</li> <li>Sample weights are passed to the forecaster to penalize observations   near missing data.</li> <li>Train/validation splits are temporal (80/20 by default).</li> <li>All features are cast to float32 for memory efficiency.</li> <li>Trained models are saved to disk using joblib for fast reuse.</li> <li>When force_train=False, existing models are loaded and prediction   proceeds without retraining. This significantly speeds up prediction   for repeated calls with the same configuration.</li> <li>The model_dir directory is created automatically if it doesn't exist.</li> <li>By default, models are cached in ~/spotforecast2_cache, which can be   customized via the SPOTFORECAST2_CACHE environment variable.</li> </ul> Performance Notes <ul> <li>First run: Full training</li> <li>Subsequent runs (force_train=False): Model loading only</li> <li>Force retrain (force_train=True): Full training again</li> </ul> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def n2n_predict_with_covariates(\n    data: Optional[pd.DataFrame] = None,\n    forecast_horizon: int = 24,\n    contamination: float = 0.01,\n    window_size: int = 72,\n    lags: int = 24,\n    train_ratio: float = 0.8,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n    estimator: Optional[object] = None,\n    include_weather_windows: bool = False,\n    include_holiday_features: bool = False,\n    include_poly_features: bool = False,\n    force_train: bool = True,\n    model_dir: Optional[Union[str, Path]] = None,\n    verbose: bool = True,\n    show_progress: bool = False,\n) -&gt; Tuple[pd.DataFrame, Dict, Dict]:\n    \"\"\"End-to-end recursive forecasting with exogenous covariates.\n\n    This function implements a complete forecasting pipeline that:\n    1. Loads and validates target data\n    2. Detects and removes outliers\n    3. Imputes missing values with weighted gaps\n    4. Creates exogenous features (weather, holidays, calendar, day/night)\n    5. Performs feature engineering (cyclical encoding, interactions)\n    6. Merges target and exogenous data\n    7. Splits into train/validation/test sets\n    8. Trains or loads recursive forecasters with sample weighting\n    9. Generates multi-step ahead predictions\n\n    Models are persisted to disk following scikit-learn conventions using joblib.\n    By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.\n\n    Args:\n        data: Optional DataFrame with target time series data. If None, fetches data automatically.\n            Default: None.\n        forecast_horizon: Number of time steps to forecast ahead. Default: 24.\n        contamination: Contamination parameter for outlier detection. Default: 0.01.\n        window_size: Rolling window size for gap detection. Default: 72.\n        lags: Number of lags for recursive forecaster. Default: 24.\n        train_ratio: Fraction of data for training. Default: 0.8.\n        latitude: Location latitude. Default: 51.5136 (Dortmund).\n        longitude: Location longitude. Default: 7.4653 (Dortmund).\n        timezone: Timezone for data. Default: \"UTC\".\n        country_code: Country code for holidays. Default: \"DE\".\n        state: State code for holidays. Default: \"NW\".\n        estimator: Base estimator for recursive forecaster.\n            If None, uses LGBMRegressor. Default: None.\n        include_weather_windows: Include weather window features. Default: False.\n        include_holiday_features: Include holiday features. Default: False.\n        include_poly_features: Include polynomial interaction features. Default: False.\n        force_train: Force retraining of all models, ignoring cached models.\n            Default: True.\n        model_dir: Directory for saving/loading trained models. If None, uses the\n            spotforecast2 cache directory (~/spotforecast2_cache by default, or\n            SPOTFORECAST2_CACHE environment variable). Default: None.\n        verbose: Print progress messages. Default: True.\n        show_progress: Show progress bar during training. Default: False.\n\n    Returns:\n        Tuple containing:\n        - predictions: DataFrame with forecast values for each target variable.\n        - metadata: Dictionary with forecast metadata (index, shapes, etc.).\n        - forecasters: Dictionary of trained ForecasterRecursive objects keyed by target.\n\n    Raises:\n        ValueError: If data validation fails or required data cannot be retrieved.\n        ImportError: If required dependencies are not installed.\n        OSError: If models cannot be saved to disk.\n\n    Examples:\n        Basic usage with automatic model caching:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(predictions.shape)\n        (24, 11)\n\n        Load cached models (if available):\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     force_train=False,\n        ...     model_dir=\"./saved_models\"\n        ... )\n\n        Force retraining and update cache:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     force_train=True,\n        ...     model_dir=\"./saved_models\"\n        ... )\n\n        Custom location and features:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=48,\n        ...     latitude=52.5200,  # Berlin\n        ...     longitude=13.4050,\n        ...     lags=48,\n        ...     include_poly_features=True,\n        ...     force_train=False,\n        ...     verbose=True\n        ... )\n\n    Notes:\n        - The function uses cached weather data when available.\n        - Missing values are handled via forward/backward fill with downweighting\n          observations near gaps.\n        - Sample weights are passed to the forecaster to penalize observations\n          near missing data.\n        - Train/validation splits are temporal (80/20 by default).\n        - All features are cast to float32 for memory efficiency.\n        - Trained models are saved to disk using joblib for fast reuse.\n        - When force_train=False, existing models are loaded and prediction\n          proceeds without retraining. This significantly speeds up prediction\n          for repeated calls with the same configuration.\n        - The model_dir directory is created automatically if it doesn't exist.\n        - By default, models are cached in ~/spotforecast2_cache, which can be\n          customized via the SPOTFORECAST2_CACHE environment variable.\n\n    Performance Notes:\n        - First run: Full training\n        - Subsequent runs (force_train=False): Model loading only\n        - Force retrain (force_train=True): Full training again\n    \"\"\"\n    # Set default model_dir if not provided\n    if model_dir is None:\n        from spotforecast2_safe.data.fetch_data import get_cache_home\n\n        model_dir = get_cache_home() / \"forecasters\"\n\n    # Input Validation\n    if forecast_horizon &lt;= 0:\n        raise ValueError(f\"forecast_horizon must be positive, got {forecast_horizon}\")\n    if not 0 &lt;= contamination &lt;= 0.5:\n        raise ValueError(\n            f\"contamination must be between 0 and 0.5, got {contamination}\"\n        )\n    if window_size &lt;= 0:\n        raise ValueError(f\"window_size must be positive, got {window_size}\")\n    if lags &lt;= 0:\n        raise ValueError(f\"lags must be positive, got {lags}\")\n    if not 0 &lt; train_ratio &lt; 1:\n        raise ValueError(f\"train_ratio must be between 0 and 1, got {train_ratio}\")\n\n    if verbose:\n        print(\"=\" * 80)\n        print(\"N2N Recursive Forecasting with Exogenous Covariates\")\n        print(\"=\" * 80)\n\n    # ========================================================================\n    # 1. DATA PREPARATION\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[1/9] Loading and preparing target data...\")\n\n    # Handle data input - fetch_data handles both CSV and DataFrame\n    if data is None:\n        if verbose:\n            print(\"  Fetching data from CSV...\")\n        data = fetch_data(filename=\"data_in.csv\", timezone=timezone)\n    else:\n        if verbose:\n            print(\"  Using provided dataframe...\")\n        data = fetch_data(dataframe=data, timezone=timezone)\n\n    target_columns = data.columns.tolist()\n\n    if verbose:\n        print(f\"  Target variables: {target_columns}\")\n\n    start, end, cov_start, cov_end = get_start_end(\n        data=data,\n        forecast_horizon=forecast_horizon,\n        verbose=verbose,\n    )\n\n    basic_ts_checks(data, verbose=verbose)\n    data = agg_and_resample_data(data, verbose=verbose)\n\n    # ========================================================================\n    # 2. OUTLIER DETECTION AND REMOVAL\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[2/9] Detecting and marking outliers...\")\n\n    data, outliers = mark_outliers(\n        data,\n        contamination=contamination,\n        random_state=1234,\n        verbose=verbose,\n    )\n\n    # ========================================================================\n    # 3. MISSING VALUE IMPUTATION WITH WEIGHTING\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[3/9] Processing missing values and creating sample weights...\")\n\n    imputed_data, missing_mask = get_missing_weights(\n        data, window_size=window_size, verbose=verbose\n    )\n\n    # Create weight function for forecaster\n    # Invert missing_mask: True (missing) -&gt; 0 (weight), False (valid) -&gt; 1 (weight)\n    weights_series = (~missing_mask).astype(float)\n\n    # Use WeightFunction class which is picklable (unlike local functions with closures)\n    from spotforecast2_safe.preprocessing import WeightFunction\n\n    weight_func = WeightFunction(weights_series)\n\n    # Model persistence enabled: WeightFunction instances can be pickled\n    use_model_persistence = True\n\n    # ========================================================================\n    # 4. EXOGENOUS FEATURES ENGINEERING\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[4/9] Creating exogenous features...\")\n\n    # Location for day/night features\n    location = LocationInfo(\n        latitude=latitude,\n        longitude=longitude,\n        timezone=timezone,\n    )\n\n    # Holidays\n    holiday_features = _get_holiday_features(\n        data=imputed_data,\n        start=start,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        tz=timezone,\n        freq=\"h\",\n        country_code=country_code,\n        state=state,\n    )\n\n    # Weather\n    weather_features, weather_aligned = _get_weather_features(\n        data=imputed_data,\n        start=start,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        latitude=latitude,\n        longitude=longitude,\n        timezone=timezone,\n        freq=\"h\",\n        verbose=verbose,\n    )\n\n    # Calendar\n    calendar_features = _get_calendar_features(\n        start=start,\n        cov_end=cov_end,\n        freq=\"h\",\n        timezone=timezone,\n    )\n\n    # Day/night\n    sun_light_features = _get_day_night_features(\n        start=start,\n        cov_end=cov_end,\n        location=location,\n        freq=\"h\",\n        timezone=timezone,\n    )\n\n    # ========================================================================\n    # 5. COMBINE EXOGENOUS FEATURES\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[5/9] Combining and encoding exogenous features...\")\n\n    exogenous_features = pd.concat(\n        [\n            calendar_features,\n            sun_light_features,\n            weather_features,\n            holiday_features,\n        ],\n        axis=1,\n    )\n\n    missing_count = exogenous_features.isnull().sum().sum()\n    if missing_count != 0:\n        raise ValueError(\n            f\"Missing values in exogenous features: {missing_count} missing entries\"\n        )\n\n    # Apply cyclical encoding\n    exogenous_features = _apply_cyclical_encoding(\n        data=exogenous_features,\n        drop_original=False,\n    )\n\n    # Create interactions\n    exogenous_features = _create_interaction_features(\n        exogenous_features=exogenous_features,\n        weather_aligned=weather_aligned,\n    )\n\n    # ========================================================================\n    # 6. SELECT EXOGENOUS FEATURES\n    # ========================================================================\n\n    exog_features = _select_exogenous_features(\n        exogenous_features=exogenous_features,\n        weather_aligned=weather_aligned,\n        include_weather_windows=include_weather_windows,\n        include_holiday_features=include_holiday_features,\n        include_poly_features=include_poly_features,\n    )\n\n    if verbose:\n        print(f\"  Selected {len(exog_features)} exogenous features\")\n\n    # ========================================================================\n    # 7. MERGE DATA AND COVARIATES\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[6/9] Merging target and exogenous data...\")\n\n    data_with_exog, exo_tmp, exo_pred = _merge_data_and_covariates(\n        data=imputed_data,\n        exogenous_features=exogenous_features,\n        target_columns=target_columns,\n        exog_features=exog_features,\n        start=start,\n        end=end,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        cast_dtype=\"float32\",\n    )\n\n    if verbose:\n        print(f\"  Merged data shape: {data_with_exog.shape}\")\n        print(f\"  Exogenous prediction shape: {exo_pred.shape}\")\n\n    # ========================================================================\n    # 8. TRAIN/VALIDATION/TEST SPLIT\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[7/9] Splitting data into train/validation/test...\")\n\n    perc_val = 1.0 - train_ratio\n    data_train, data_val, data_test = split_rel_train_val_test(\n        data_with_exog,\n        perc_train=train_ratio,\n        perc_val=perc_val,\n        verbose=verbose,\n    )\n\n    # ========================================================================\n    # 9. MODEL TRAINING OR LOADING\n    # ========================================================================\n\n    if verbose:\n        print(\n            \"\\n[8/9] Loading or training recursive forecasters with exogenous variables...\"\n        )\n\n    if estimator is None:\n        estimator = LGBMRegressor(random_state=1234, verbose=-1)\n\n    window_features = RollingFeatures(stats=[\"mean\"], window_sizes=window_size)\n    end_validation = pd.concat([data_train, data_val]).index[-1]\n\n    # Attempt to load cached models if force_train=False and persistence is enabled\n    recursive_forecasters = {}\n    targets_to_train = target_columns\n\n    if use_model_persistence and not force_train and _model_directory_exists(model_dir):\n        if verbose:\n            print(\"  Attempting to load cached models...\")\n        cached_forecasters, missing_targets = _load_forecasters(\n            target_columns=target_columns,\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n        recursive_forecasters.update(cached_forecasters)\n        targets_to_train = missing_targets\n\n        if len(cached_forecasters) == len(target_columns):\n            if verbose:\n                print(f\"  \u2713 All {len(target_columns)} forecasters loaded from cache\")\n        elif len(cached_forecasters) &gt; 0:\n            if verbose:\n                print(\n                    f\"  \u2713 Loaded {len(cached_forecasters)} forecasters, \"\n                    f\"will train {len(targets_to_train)} new ones\"\n                )\n\n    # Train missing or forced models\n    if len(targets_to_train) &gt; 0:\n        if force_train and len(recursive_forecasters) &gt; 0:\n            if verbose:\n                print(f\"  Force retraining all {len(target_columns)} forecasters...\")\n            targets_to_train = target_columns\n            recursive_forecasters.clear()\n\n        target_iter = targets_to_train\n        if show_progress and tqdm is not None:\n            target_iter = tqdm(\n                targets_to_train,\n                desc=\"Training forecasters\",\n                unit=\"model\",\n            )\n\n        for target in target_iter:\n            if verbose:\n                print(f\"  Training forecaster for {target}...\")\n\n            forecaster = ForecasterRecursive(\n                estimator=estimator,\n                lags=lags,\n                window_features=window_features,\n                weight_func=weight_func,\n            )\n\n            forecaster.fit(\n                y=data_with_exog[target].loc[:end_validation].squeeze(),\n                exog=data_with_exog[exog_features].loc[:end_validation],\n            )\n\n            recursive_forecasters[target] = forecaster\n\n            if verbose:\n                print(f\"    \u2713 Forecaster trained for {target}\")\n\n        # Save newly trained models to disk (only if persistence is enabled)\n        if use_model_persistence:\n            if verbose:\n                print(\n                    f\"  Saving {len(targets_to_train)} trained forecasters to disk...\"\n                )\n            _save_forecasters(\n                forecasters={t: recursive_forecasters[t] for t in targets_to_train},\n                model_dir=model_dir,\n                verbose=verbose,\n            )\n\n    if verbose:\n        print(f\"  \u2713 Total forecasters available: {len(recursive_forecasters)}\")\n\n    # ========================================================================\n    # 10. PREDICTION\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[9/9] Generating predictions...\")\n\n    exo_pred_subset = exo_pred[exog_features]\n\n    predictions = predict_multivariate(\n        recursive_forecasters,\n        steps_ahead=forecast_horizon,\n        exog=exo_pred_subset,\n        show_progress=show_progress,\n    )\n\n    if verbose:\n        print(f\"  Predictions shape: {predictions.shape}\")\n        print(\"\\n\" + \"=\" * 80)\n        print(\"Forecasting completed successfully!\")\n        print(\"=\" * 80)\n\n    # ========================================================================\n    # COMPILE METADATA\n    # ========================================================================\n\n    metadata = {\n        \"forecast_horizon\": forecast_horizon,\n        \"target_columns\": target_columns,\n        \"exog_features\": exog_features,\n        \"n_exog_features\": len(exog_features),\n        \"train_size\": len(data_train),\n        \"val_size\": len(data_val),\n        \"test_size\": len(data_test),\n        \"data_shape_original\": data.shape,\n        \"data_shape_merged\": data_with_exog.shape,\n        \"training_end\": end_validation,\n        \"prediction_start\": exo_pred.index[0],\n        \"prediction_end\": exo_pred.index[-1],\n        \"lags\": lags,\n        \"window_size\": window_size,\n        \"contamination\": contamination,\n        \"n_outliers\": (\n            outliers.sum() if isinstance(outliers, pd.Series) else len(outliers)\n        ),\n    }\n\n    return predictions, metadata, recursive_forecasters\n</code></pre>"},{"location":"api/processing/#aggregate-and-prediction-functions","title":"Aggregate and Prediction Functions","text":""},{"location":"api/processing/#agg_predict","title":"agg_predict","text":""},{"location":"api/processing/#spotforecast2_safe.processing.agg_predict","title":"<code>spotforecast2_safe.processing.agg_predict</code>","text":""},{"location":"api/processing/#spotforecast2_safe.processing.agg_predict.agg_predict","title":"<code>agg_predict(predictions, weights=None)</code>","text":"<p>Aggregates multiple prediction columns into a single combined prediction series.</p> <p>The combination is a weighted sum of the prediction columns. If no weights are provided, a default weighting scheme based on specific predefined columns is used.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>DataFrame containing the prediction columns.</p> required <code>weights</code> <code>Optional[Union[Dict[str, float], List[float], ndarray]]</code> <p>Dictionary mapping column names to their weights, or a list/array of weights corresponding to the order of columns in <code>predictions</code>. If None, defaults to summing all columns (weight=1.0 for each column).</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A Series containing the aggregated values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a column specified in weights (or default weights) is missing from predictions.</p> <code>ValueError</code> <p>If weights is a list/array and its length does not match the number of columns in predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.processing import agg_predict\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n&gt;&gt;&gt; agg_predict(df, weights={\"A\": 1.0, \"B\": -1.0})\n0   -2.0\n1   -2.0\ndtype: float64\n&gt;&gt;&gt; agg_predict(df, weights=[0.5, 2.0])\n0    6.5\n1    9.0\ndtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/agg_predict.py</code> <pre><code>def agg_predict(\n    predictions: pd.DataFrame,\n    weights: Optional[Union[Dict[str, float], List[float], np.ndarray]] = None,\n) -&gt; pd.Series:\n    \"\"\"Aggregates multiple prediction columns into a single combined prediction series.\n\n    The combination is a weighted sum of the prediction columns. If no weights are provided,\n    a default weighting scheme based on specific predefined columns is used.\n\n    Args:\n        predictions (pd.DataFrame): DataFrame containing the prediction columns.\n        weights (Optional[Union[Dict[str, float], List[float], np.ndarray]]):\n            Dictionary mapping column names to their weights, or a list/array of weights\n            corresponding to the order of columns in `predictions`.\n            If None, defaults to summing all columns (weight=1.0 for each column).\n\n    Returns:\n        pd.Series: A Series containing the aggregated values.\n\n    Raises:\n        ValueError: If a column specified in weights (or default weights) is missing from predictions.\n        ValueError: If weights is a list/array and its length does not match the number of columns in predictions.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.processing import agg_predict\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        &gt;&gt;&gt; agg_predict(df, weights={\"A\": 1.0, \"B\": -1.0})\n        0   -2.0\n        1   -2.0\n        dtype: float64\n        &gt;&gt;&gt; agg_predict(df, weights=[0.5, 2.0])\n        0    6.5\n        1    9.0\n        dtype: float64\n    \"\"\"\n    if weights is None:\n        # Default to summing all columns\n        weights = {col: 1.0 for col in predictions.columns}\n\n    if isinstance(weights, (list, np.ndarray)):\n        if len(weights) != len(predictions.columns):\n            raise ValueError(\n                f\"Length of weights ({len(weights)}) does not match number of columns in predictions ({len(predictions.columns)})\"\n            )\n        # Convert to dictionary using column order\n        weights = dict(zip(predictions.columns, weights))\n\n    combined = pd.Series(0.0, index=predictions.index)\n\n    missing_cols = [col for col in weights.keys() if col not in predictions.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing columns in predictions dataframe: {missing_cols}\")\n\n    for col, weight in weights.items():\n        combined += predictions[col] * weight\n\n    return combined\n</code></pre>"},{"location":"api/utils/","title":"Utils Module","text":"<p>Utility functions and helpers.</p>"},{"location":"api/utils/#spotforecast2_safe.utils","title":"<code>spotforecast2_safe.utils</code>","text":"<p>Utility functions for spotforecast.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; from spotforecast2_safe.exceptions import MissingValuesWarning\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Args:\n        message (str): The message to display.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; from spotforecast2_safe.exceptions import MissingValuesWarning\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        self.message = message\n\n    def __str__(self) -&gt; str:\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_residuals_input","title":"<code>check_residuals_input(forecaster_name, use_in_sample_residuals, in_sample_residuals_, out_sample_residuals_, use_binned_residuals, in_sample_residuals_by_bin_, out_sample_residuals_by_bin_, levels=None, encoding=None)</code>","text":"<p>Check residuals input arguments in Forecasters.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.check_residuals_input--parameters","title":"Parameters","text":"<p>forecaster_name : str     Forecaster name. use_in_sample_residuals : bool     Indicates if in sample or out sample residuals are used. in_sample_residuals_ : numpy ndarray, dict     Residuals of the model when predicting training data. out_sample_residuals_ : numpy ndarray, dict     Residuals of the model when predicting non training data. use_binned_residuals : bool     Indicates if residuals are binned. in_sample_residuals_by_bin_ : dict     In sample residuals binned according to the predicted value each residual     is associated with. out_sample_residuals_by_bin_ : dict     Out of sample residuals binned according to the predicted value each residual     is associated with. levels : list, default None     Names of the series (levels) to be predicted (Forecasters multiseries). encoding : str, default None     Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p>"},{"location":"api/utils/#spotforecast2_safe.utils.check_residuals_input--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    out_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    levels: list[str] | None = None,\n    encoding: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    use_in_sample_residuals : bool\n        Indicates if in sample or out sample residuals are used.\n    in_sample_residuals_ : numpy ndarray, dict\n        Residuals of the model when predicting training data.\n    out_sample_residuals_ : numpy ndarray, dict\n        Residuals of the model when predicting non training data.\n    use_binned_residuals : bool\n        Indicates if residuals are binned.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with.\n    levels : list, default None\n        Names of the series (levels) to be predicted (Forecasters multiseries).\n    encoding : str, default None\n        Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecasters_multiseries = (\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    )\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        # Check if residuals are empty or None\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`use_in_sample_residuals = True` or the \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels. \"\n                        f\"Otherwise, Use the `set_out_sample_residuals()` method before \"\n                        f\"predicting to set the residuals for these levels.\",\n                        UnknownLevelWarning,\n                    )\n\n    if forecaster_name in forecasters_multiseries:\n        for level in residuals.keys():\n            level_residuals = residuals[level]\n            if level_residuals is None or len(level_residuals) == 0:\n                raise ValueError(\n                    f\"Residuals for level '{level}' are None. Check `forecaster.{literal}`.\"\n                )\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.create_holiday_df","title":"<code>create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.date_to_index_position","title":"<code>date_to_index_position(index, date_input, method='prediction', date_literal='steps', kwargs_pd_to_datetime=None)</code>","text":"<p>Transform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>Original datetime index (must be a pandas DatetimeIndex if <code>date_input</code> is not an int).</p> required <code>date_input</code> <code>Union[int, str, Timestamp]</code> <p>Datetime to transform to integer. - If int, returns the same integer. - If str or pandas Timestamp, it is converted and expanded into the index.</p> required <code>method</code> <code>str</code> <p>Can be 'prediction' or 'validation'. - If 'prediction', the date must be later than the last date in the index. - If 'validation', the date must be within the index range.</p> <code>'prediction'</code> <code>date_literal</code> <code>str</code> <p>Variable name used in error messages. Defaults to 'steps'.</p> <code>'steps'</code> <code>kwargs_pd_to_datetime</code> <code>Optional[dict]</code> <p>Additional keyword arguments to pass to <code>pd.to_datetime()</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p><code>date_input</code> transformed to integer position in the <code>index</code>. - If <code>date_input</code> is an integer, it returns the same integer. - If method is 'prediction', number of steps to predict from the last date in the index. - If method is 'validation', position plus one of the date in the index.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not 'prediction' or 'validation'.</p> <code>TypeError</code> <p>If <code>index</code> is not a DatetimeIndex when <code>date_input</code> is not an integer.</p> <code>ValueError</code> <p>If <code>date_input</code> (as date) does not meet the method's constraints.</p> <code>TypeError</code> <p>If <code>date_input</code> is not an integer, string, or pandas Timestamp.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def date_to_index_position(\n    index: pd.Index,\n    date_input: Union[int, str, pd.Timestamp],\n    method: str = \"prediction\",\n    date_literal: str = \"steps\",\n    kwargs_pd_to_datetime: Optional[dict] = None,\n) -&gt; int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n\n    Args:\n        index:\n            Original datetime index (must be a pandas DatetimeIndex if `date_input` is not an int).\n        date_input:\n            Datetime to transform to integer.\n            - If int, returns the same integer.\n            - If str or pandas Timestamp, it is converted and expanded into the index.\n        method:\n            Can be 'prediction' or 'validation'.\n            - If 'prediction', the date must be later than the last date in the index.\n            - If 'validation', the date must be within the index range.\n        date_literal:\n            Variable name used in error messages. Defaults to 'steps'.\n        kwargs_pd_to_datetime:\n            Additional keyword arguments to pass to `pd.to_datetime()`. Defaults to None.\n\n    Returns:\n        int:\n            `date_input` transformed to integer position in the `index`.\n            - If `date_input` is an integer, it returns the same integer.\n            - If method is 'prediction', number of steps to predict from the last date in the index.\n            - If method is 'validation', position plus one of the date in the index.\n\n    Raises:\n        ValueError: If `method` is not 'prediction' or 'validation'.\n        TypeError: If `index` is not a DatetimeIndex when `date_input` is not an integer.\n        ValueError: If `date_input` (as date) does not meet the method's constraints.\n        TypeError: If `date_input` is not an integer, string, or pandas Timestamp.\n    \"\"\"\n    # Initialize output to satisfy static analyzers; it will be overwritten\n    # on all valid execution paths before being returned.\n    output: int = 0\n\n    if kwargs_pd_to_datetime is None:\n        kwargs_pd_to_datetime = {}\n\n    if method not in [\"prediction\", \"validation\"]:\n        raise ValueError(\"`method` must be 'prediction' or 'validation'.\")\n\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n\n        if method == \"prediction\":\n            if target_date &lt;= last_date:\n                raise ValueError(\n                    \"If `steps` is a date, it must be greater than the last date \"\n                    \"in the index.\"\n                )\n            span_index = pd.date_range(\n                start=last_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index) - 1\n        elif method == \"validation\":\n            first_date = pd.to_datetime(index[0])\n            if target_date &lt; first_date or target_date &gt; last_date:\n                raise ValueError(\n                    \"If `initial_train_size` is a date, it must be greater than \"\n                    \"the first date in the index and less than the last date.\"\n                )\n            span_index = pd.date_range(\n                start=first_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index)\n    elif isinstance(date_input, (int, np.integer)):\n        output = int(date_input)\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#utc-conversion","title":"UTC Conversion","text":""},{"location":"api/utils/#convert_to_utc","title":"convert_to_utc","text":""},{"location":"api/utils/#spotforecast2_safe.utils.convert_to_utc","title":"<code>spotforecast2_safe.utils.convert_to_utc</code>","text":"<p>Utility functions for timezone conversion.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.convert_to_utc.convert_to_utc","title":"<code>convert_to_utc(df, timezone)</code>","text":"<p>Convert DataFrame index timezone to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Optional timezone string. Required if index has no timezone.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with UTC timezone index.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If index is not DatetimeIndex or has no timezone and timezone is None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.utils.convert_to_utc import convert_to_utc\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]}, index=pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n&gt;&gt;&gt; convert_to_utc(df, \"Europe/Berlin\")\n           value\n2022-01-01 00:00:00+01:00\n2022-01-02 00:00:00+01:00\n2022-01-03 00:00:00+01:00\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/convert_to_utc.py</code> <pre><code>def convert_to_utc(df: pd.DataFrame, timezone: Optional[str]) -&gt; pd.DataFrame:\n    \"\"\"Convert DataFrame index timezone to UTC.\n\n    Args:\n        df: DataFrame with DatetimeIndex.\n        timezone: Optional timezone string. Required if index has no timezone.\n\n    Returns:\n        DataFrame with UTC timezone index.\n\n    Raises:\n        ValueError: If index is not DatetimeIndex or has no timezone and\n            timezone is None.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.utils.convert_to_utc import convert_to_utc\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]}, index=pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n        &gt;&gt;&gt; convert_to_utc(df, \"Europe/Berlin\")\n                   value\n        2022-01-01 00:00:00+01:00\n        2022-01-02 00:00:00+01:00\n        2022-01-03 00:00:00+01:00\n    \"\"\"\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\n            \"No DatetimeIndex found. Please specify the time column via 'index_col'\"\n        )\n    if df.index.tz is None:\n        if timezone is not None:\n            df.index = df.index.tz_localize(timezone)\n        else:\n            raise ValueError(\n                \"Index has no timezone information. Please provide a timezone.\"\n            )\n\n    df.index = df.index.tz_convert(\"UTC\")\n\n    return df\n</code></pre>"},{"location":"api/utils/#data-transformation","title":"Data Transformation","text":""},{"location":"api/utils/#data_transform","title":"data_transform","text":""},{"location":"api/utils/#spotforecast2_safe.utils.data_transform","title":"<code>spotforecast2_safe.utils.data_transform</code>","text":"<p>Data transformation utilities for time series forecasting.</p> <p>This module provides functions for normalizing and transforming data formats.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.date_to_index_position","title":"<code>date_to_index_position(index, date_input, method='prediction', date_literal='steps', kwargs_pd_to_datetime=None)</code>","text":"<p>Transform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>Original datetime index (must be a pandas DatetimeIndex if <code>date_input</code> is not an int).</p> required <code>date_input</code> <code>Union[int, str, Timestamp]</code> <p>Datetime to transform to integer. - If int, returns the same integer. - If str or pandas Timestamp, it is converted and expanded into the index.</p> required <code>method</code> <code>str</code> <p>Can be 'prediction' or 'validation'. - If 'prediction', the date must be later than the last date in the index. - If 'validation', the date must be within the index range.</p> <code>'prediction'</code> <code>date_literal</code> <code>str</code> <p>Variable name used in error messages. Defaults to 'steps'.</p> <code>'steps'</code> <code>kwargs_pd_to_datetime</code> <code>Optional[dict]</code> <p>Additional keyword arguments to pass to <code>pd.to_datetime()</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p><code>date_input</code> transformed to integer position in the <code>index</code>. - If <code>date_input</code> is an integer, it returns the same integer. - If method is 'prediction', number of steps to predict from the last date in the index. - If method is 'validation', position plus one of the date in the index.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not 'prediction' or 'validation'.</p> <code>TypeError</code> <p>If <code>index</code> is not a DatetimeIndex when <code>date_input</code> is not an integer.</p> <code>ValueError</code> <p>If <code>date_input</code> (as date) does not meet the method's constraints.</p> <code>TypeError</code> <p>If <code>date_input</code> is not an integer, string, or pandas Timestamp.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def date_to_index_position(\n    index: pd.Index,\n    date_input: Union[int, str, pd.Timestamp],\n    method: str = \"prediction\",\n    date_literal: str = \"steps\",\n    kwargs_pd_to_datetime: Optional[dict] = None,\n) -&gt; int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n\n    Args:\n        index:\n            Original datetime index (must be a pandas DatetimeIndex if `date_input` is not an int).\n        date_input:\n            Datetime to transform to integer.\n            - If int, returns the same integer.\n            - If str or pandas Timestamp, it is converted and expanded into the index.\n        method:\n            Can be 'prediction' or 'validation'.\n            - If 'prediction', the date must be later than the last date in the index.\n            - If 'validation', the date must be within the index range.\n        date_literal:\n            Variable name used in error messages. Defaults to 'steps'.\n        kwargs_pd_to_datetime:\n            Additional keyword arguments to pass to `pd.to_datetime()`. Defaults to None.\n\n    Returns:\n        int:\n            `date_input` transformed to integer position in the `index`.\n            - If `date_input` is an integer, it returns the same integer.\n            - If method is 'prediction', number of steps to predict from the last date in the index.\n            - If method is 'validation', position plus one of the date in the index.\n\n    Raises:\n        ValueError: If `method` is not 'prediction' or 'validation'.\n        TypeError: If `index` is not a DatetimeIndex when `date_input` is not an integer.\n        ValueError: If `date_input` (as date) does not meet the method's constraints.\n        TypeError: If `date_input` is not an integer, string, or pandas Timestamp.\n    \"\"\"\n    # Initialize output to satisfy static analyzers; it will be overwritten\n    # on all valid execution paths before being returned.\n    output: int = 0\n\n    if kwargs_pd_to_datetime is None:\n        kwargs_pd_to_datetime = {}\n\n    if method not in [\"prediction\", \"validation\"]:\n        raise ValueError(\"`method` must be 'prediction' or 'validation'.\")\n\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n\n        if method == \"prediction\":\n            if target_date &lt;= last_date:\n                raise ValueError(\n                    \"If `steps` is a date, it must be greater than the last date \"\n                    \"in the index.\"\n                )\n            span_index = pd.date_range(\n                start=last_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index) - 1\n        elif method == \"validation\":\n            first_date = pd.to_datetime(index[0])\n            if target_date &lt; first_date or target_date &gt; last_date:\n                raise ValueError(\n                    \"If `initial_train_size` is a date, it must be greater than \"\n                    \"the first date in the index and less than the last date.\"\n                )\n            span_index = pd.date_range(\n                start=first_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index)\n    elif isinstance(date_input, (int, np.integer)):\n        output = int(date_input)\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#forecaster-configuration","title":"Forecaster Configuration","text":""},{"location":"api/utils/#forecaster_config","title":"forecaster_config","text":""},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config","title":"<code>spotforecast2_safe.utils.forecaster_config</code>","text":"<p>Forecaster configuration utilities.</p> <p>This module provides functions for initializing and validating forecaster configuration parameters like lags and weights.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2_safe.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#holiday-generation","title":"Holiday Generation","text":""},{"location":"api/utils/#generate_holiday","title":"generate_holiday","text":""},{"location":"api/utils/#spotforecast2_safe.utils.generate_holiday","title":"<code>spotforecast2_safe.utils.generate_holiday</code>","text":"<p>Utilities for generating holiday dataframe as covariate.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.generate_holiday.create_holiday_df","title":"<code>create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/utils/#validation-utilities","title":"Validation Utilities","text":""},{"location":"api/utils/#validation","title":"validation","text":""},{"location":"api/utils/#spotforecast2_safe.utils.validation","title":"<code>spotforecast2_safe.utils.validation</code>","text":"<p>Validation utilities for time series forecasting.</p> <p>This module provides validation functions for time series data and exogenous variables.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_residuals_input","title":"<code>check_residuals_input(forecaster_name, use_in_sample_residuals, in_sample_residuals_, out_sample_residuals_, use_binned_residuals, in_sample_residuals_by_bin_, out_sample_residuals_by_bin_, levels=None, encoding=None)</code>","text":"<p>Check residuals input arguments in Forecasters.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_residuals_input--parameters","title":"Parameters","text":"<p>forecaster_name : str     Forecaster name. use_in_sample_residuals : bool     Indicates if in sample or out sample residuals are used. in_sample_residuals_ : numpy ndarray, dict     Residuals of the model when predicting training data. out_sample_residuals_ : numpy ndarray, dict     Residuals of the model when predicting non training data. use_binned_residuals : bool     Indicates if residuals are binned. in_sample_residuals_by_bin_ : dict     In sample residuals binned according to the predicted value each residual     is associated with. out_sample_residuals_by_bin_ : dict     Out of sample residuals binned according to the predicted value each residual     is associated with. levels : list, default None     Names of the series (levels) to be predicted (Forecasters multiseries). encoding : str, default None     Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_residuals_input--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    out_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    levels: list[str] | None = None,\n    encoding: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    use_in_sample_residuals : bool\n        Indicates if in sample or out sample residuals are used.\n    in_sample_residuals_ : numpy ndarray, dict\n        Residuals of the model when predicting training data.\n    out_sample_residuals_ : numpy ndarray, dict\n        Residuals of the model when predicting non training data.\n    use_binned_residuals : bool\n        Indicates if residuals are binned.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with.\n    levels : list, default None\n        Names of the series (levels) to be predicted (Forecasters multiseries).\n    encoding : str, default None\n        Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecasters_multiseries = (\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    )\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        # Check if residuals are empty or None\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`use_in_sample_residuals = True` or the \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels. \"\n                        f\"Otherwise, Use the `set_out_sample_residuals()` method before \"\n                        f\"predicting to set the residuals for these levels.\",\n                        UnknownLevelWarning,\n                    )\n\n    if forecaster_name in forecasters_multiseries:\n        for level in residuals.keys():\n            level_residuals = residuals[level]\n            if level_residuals is None or len(level_residuals) == 0:\n                raise ValueError(\n                    f\"Residuals for level '{level}' are None. Check `forecaster.{literal}`.\"\n                )\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/weather/","title":"Weather Module","text":"<p>Weather data utilities and integration.</p>"},{"location":"api/weather/#spotforecast2_safe.weather","title":"<code>spotforecast2_safe.weather</code>","text":"<p>Weather data utilities for spotforecast2.</p>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#weather-client","title":"Weather Client","text":""},{"location":"api/weather/#weather_client","title":"weather_client","text":""},{"location":"api/weather/#spotforecast2_safe.weather.weather_client","title":"<code>spotforecast2_safe.weather.weather_client</code>","text":"<p>Weather data fetching and processing using Open-Meteo API.</p>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherService","title":"<code>WeatherService</code>","text":"<p>               Bases: <code>WeatherClient</code></p> <p>High-level service for weather data generation.</p> <p>Extends WeatherClient with caching, hybrid fetching (archive+forecast), and fallback strategies.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherService(WeatherClient):\n    \"\"\"High-level service for weather data generation.\n\n    Extends WeatherClient with caching, hybrid fetching (archive+forecast),\n    and fallback strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        latitude: float,\n        longitude: float,\n        cache_path: Optional[Path] = None,\n        use_forecast: bool = True,\n    ):\n        super().__init__(latitude, longitude)\n        self.cache_path = cache_path\n        self.use_forecast = use_forecast\n\n    def get_dataframe(\n        self,\n        start: Union[str, pd.Timestamp],\n        end: Union[str, pd.Timestamp],\n        timezone: str = \"UTC\",\n        freq: str = \"h\",\n        fallback_on_failure: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n        Refactored from spotpredict.create_weather_df.\n        \"\"\"\n        start_ts = pd.Timestamp(start)\n        end_ts = pd.Timestamp(end)\n\n        # Localize if naive\n        if start_ts.tz is None:\n            start_ts = start_ts.tz_localize(timezone)\n        if end_ts.tz is None:\n            end_ts = end_ts.tz_localize(timezone)\n\n        # Convert to UTC for consistency\n        start_utc = start_ts.tz_convert(\"UTC\")\n        end_utc = end_ts.tz_convert(\"UTC\")\n\n        # 1. Try Cache\n        cached_df = self._load_cache()\n        if cached_df is not None:\n            if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n                self.logger.info(\"Using full cached data.\")\n                return self._finalize_df(\n                    cached_df.loc[start_utc:end_utc], freq, timezone\n                )\n\n        # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n        # (The original logic did partial fills, but full fetch is safer and\n        # simpler for now unless specifically improved).\n        # Actually, strict refactor implies keeping logic. Let's keep it simple:\n        # fetch what's needed.\n\n        try:\n            df = self._fetch_hybrid(start_ts, end_ts, timezone)\n        except Exception as e:\n            self.logger.warning(f\"Fetch failed: {e}\")\n            if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n                df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n            else:\n                raise\n\n        # 3. Merge with cache and save\n        if cached_df is not None:\n            df = pd.concat([cached_df, df])\n            df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n        if self.cache_path:\n            self._save_cache(df)\n\n        # 4. Return slice\n        return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n\n    def _fetch_hybrid(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch from Archive and/or Forecast based on date.\"\"\"\n        now = pd.Timestamp.now(tz=start.tz)\n        archive_cutoff = now - pd.Timedelta(days=5)\n\n        dfs = []\n\n        # Archive part\n        if start &lt; archive_cutoff:\n            arch_end = min(end, archive_cutoff)\n            try:\n                dfs.append(self.fetch_archive(start, arch_end, timezone))\n            except Exception as e:\n                self.logger.warning(f\"Archive fetch warning: {e}\")\n\n        # Forecast part\n        if end &gt; now and self.use_forecast:\n            days = (end - now).days + 2\n            days = min(max(1, days), 16)\n            try:\n                df_fore = self.fetch_forecast(days, timezone)\n                # Filter forecast to needed range to avoid overlap issues\n                dfs.append(df_fore)\n            except Exception as e:\n                self.logger.warning(f\"Forecast fetch warning: {e}\")\n\n        if not dfs:\n            raise ValueError(\"Could not fetch data from Archive or Forecast.\")\n\n        full_df = pd.concat(dfs)\n        full_df = full_df[~full_df.index.duplicated(keep=\"first\")].sort_index()\n\n        # Ensure UTC index\n        if full_df.index.tz is None:\n            full_df.index = full_df.index.tz_localize(timezone)\n        full_df.index = full_df.index.tz_convert(\"UTC\")\n\n        return full_df\n\n    def _create_fallback(\n        self,\n        start: pd.Timestamp,\n        end: pd.Timestamp,\n        source_df: pd.DataFrame,\n        timezone: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Repeat last 24h of data.\"\"\"\n        last_24 = source_df.tail(24)\n        hours = int((end - start).total_seconds() / 3600) + 1\n        repeats = (hours // 24) + 1\n\n        new_data = pd.concat([last_24] * repeats, ignore_index=True)\n        new_data = new_data.iloc[:hours]\n\n        idx = pd.date_range(start, periods=hours, freq=\"h\", tz=\"UTC\")\n        new_data.index = idx\n        return new_data\n\n    def _load_cache(self) -&gt; Optional[pd.DataFrame]:\n        if not self.cache_path or not self.cache_path.exists():\n            return None\n        try:\n            df = pd.read_parquet(self.cache_path)\n            if df.index.tz is None:\n                df.index = df.index.tz_localize(\"UTC\")\n            return df\n        except Exception:\n            return None\n\n    def _save_cache(self, df: pd.DataFrame):\n        if self.cache_path:\n            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n            df.to_parquet(self.cache_path)\n\n    def _finalize_df(self, df: pd.DataFrame, freq: str, timezone: str) -&gt; pd.DataFrame:\n        \"\"\"Resample and localize.\"\"\"\n        # Resample\n        if freq != \"h\":  # Assuming API returns hourly\n            df = df.resample(freq).ffill()  # Forward fill for weather is reasonable\n\n        # Fill gaps\n        df = df.ffill().bfill()\n\n        # Convert to requested timezone if needed (though we keep internal UTC mostly)\n        # User requested specific tz output usually?\n        # Original code returned normalized DF. Let's ensure frequency matches exactly.\n\n        return df\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherService.get_dataframe","title":"<code>get_dataframe(start, end, timezone='UTC', freq='h', fallback_on_failure=True)</code>","text":"<p>Get weather DataFrame for a specified range using best available methods.</p> <p>Refactored from spotpredict.create_weather_df.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def get_dataframe(\n    self,\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n    Refactored from spotpredict.create_weather_df.\n    \"\"\"\n    start_ts = pd.Timestamp(start)\n    end_ts = pd.Timestamp(end)\n\n    # Localize if naive\n    if start_ts.tz is None:\n        start_ts = start_ts.tz_localize(timezone)\n    if end_ts.tz is None:\n        end_ts = end_ts.tz_localize(timezone)\n\n    # Convert to UTC for consistency\n    start_utc = start_ts.tz_convert(\"UTC\")\n    end_utc = end_ts.tz_convert(\"UTC\")\n\n    # 1. Try Cache\n    cached_df = self._load_cache()\n    if cached_df is not None:\n        if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n            self.logger.info(\"Using full cached data.\")\n            return self._finalize_df(\n                cached_df.loc[start_utc:end_utc], freq, timezone\n            )\n\n    # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n    # (The original logic did partial fills, but full fetch is safer and\n    # simpler for now unless specifically improved).\n    # Actually, strict refactor implies keeping logic. Let's keep it simple:\n    # fetch what's needed.\n\n    try:\n        df = self._fetch_hybrid(start_ts, end_ts, timezone)\n    except Exception as e:\n        self.logger.warning(f\"Fetch failed: {e}\")\n        if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n            df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n        else:\n            raise\n\n    # 3. Merge with cache and save\n    if cached_df is not None:\n        df = pd.concat([cached_df, df])\n        df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n    if self.cache_path:\n        self._save_cache(df)\n\n    # 4. Return slice\n    return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n</code></pre>"},{"location":"preprocessing/outliers/","title":"Outlier Detection and Handling","text":"<p>Guide for identifying and handling outliers in time series data.</p>"},{"location":"preprocessing/outliers/#overview","title":"Overview","text":"<p>Outlier detection is crucial for time series forecasting as extreme values can distort model training and predictions. This module provides robust outlier detection and marking capabilities.</p>"},{"location":"preprocessing/outliers/#key-functions","title":"Key Functions","text":""},{"location":"preprocessing/outliers/#mark-outliers","title":"Mark Outliers","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>spotforecast2_safe.preprocessing.outlier.mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"preprocessing/outliers/#outlier-module","title":"Outlier Module","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier","title":"<code>spotforecast2_safe.preprocessing.outlier</code>","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[col].values.reshape(-1, 1))\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"preprocessing/outliers/#examples","title":"Examples","text":"<pre><code>import pandas as pd\nfrom spotforecast2_safe.preprocessing.outlier import mark_outliers\n\n# Create sample time series data\ndata = pd.DataFrame({\n    'value': [1, 2, 100, 4, 5, 6, 7, 8, 9, 10],  # 100 is an outlier\n})\n\n# Mark outliers\nresult_data, outlier_mask = mark_outliers(\n    data,\n    contamination=0.1,  # Expect 10% contamination\n    columns=['value']\n)\n\nprint(f\"Outliers marked: {outlier_mask.sum()} records\")\n</code></pre>"},{"location":"preprocessing/outliers/#detection-methods","title":"Detection Methods","text":"<p>This module uses isolation forest and other statistical methods to detect: - Sudden spikes or drops - Seasonal anomalies - Drift in baseline values - Sudden shifts in variance</p>"},{"location":"processing/model_persistence/","title":"Model Persistence","text":"<p>Guide for saving and loading trained forecasting models.</p>"},{"location":"processing/model_persistence/#overview","title":"Overview","text":"<p>The model persistence functionality enables you to: - Save trained forecasters to disk - Load previously trained models - Manage model caching - Handle batch model operations</p>"},{"location":"processing/model_persistence/#functions","title":"Functions","text":""},{"location":"processing/model_persistence/#saving-models","title":"Saving Models","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.manager.persistence._save_forecasters","title":"<code>spotforecast2_safe.manager.persistence._save_forecasters(forecasters, model_dir, verbose=False)</code>","text":"<p>Save trained forecasters to disk using joblib.</p> <p>Follows scikit-learn persistence conventions using joblib for efficient serialization of sklearn-compatible estimators.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>Dict[str, object]</code> <p>Dictionary mapping target names to trained ForecasterEquivalentDate objects.</p> required <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory to save models. Created if it doesn't exist.</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Path]</code> <p>Dict[str, Path]: Dictionary mapping target names to saved model filepaths.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If models cannot be written to disk.</p> <code>TypeError</code> <p>If forecasters contain non-serializable objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from spotforecast2_safe.manager.persistence import _save_forecasters\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; mock_model = LinearRegression()\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     forecasters = {\"power\": mock_model, \"energy\": mock_model}\n...     paths = _save_forecasters(forecasters, tmpdir, verbose=False)\n...     print(\"power\" in paths)\n...     print(paths[\"power\"].exists())\nTrue\nTrue\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     paths = _save_forecasters({\"demand\": mock_model}, tmpdir)\n...     print(paths[\"demand\"].suffix)\n.joblib\n</code></pre> Source code in <code>src/spotforecast2_safe/manager/persistence.py</code> <pre><code>def _save_forecasters(\n    forecasters: Dict[str, object],\n    model_dir: Union[str, Path],\n    verbose: bool = False,\n) -&gt; Dict[str, Path]:\n    \"\"\"Save trained forecasters to disk using joblib.\n\n    Follows scikit-learn persistence conventions using joblib for efficient\n    serialization of sklearn-compatible estimators.\n\n    Args:\n        forecasters: Dictionary mapping target names to trained ForecasterEquivalentDate objects.\n        model_dir: Directory to save models. Created if it doesn't exist.\n        verbose: Print progress messages. Default: False.\n\n    Returns:\n        Dict[str, Path]: Dictionary mapping target names to saved model filepaths.\n\n    Raises:\n        OSError: If models cannot be written to disk.\n        TypeError: If forecasters contain non-serializable objects.\n\n    Examples:\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from spotforecast2_safe.manager.persistence import _save_forecasters\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; mock_model = LinearRegression()\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     forecasters = {\"power\": mock_model, \"energy\": mock_model}\n        ...     paths = _save_forecasters(forecasters, tmpdir, verbose=False)\n        ...     print(\"power\" in paths)\n        ...     print(paths[\"power\"].exists())\n        True\n        True\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     paths = _save_forecasters({\"demand\": mock_model}, tmpdir)\n        ...     print(paths[\"demand\"].suffix)\n        .joblib\n    \"\"\"\n    model_path = _ensure_model_dir(model_dir)\n    saved_paths = {}\n\n    for target, forecaster in forecasters.items():\n        filepath = _get_model_filepath(model_path, target)\n        try:\n            dump(forecaster, filepath, compress=3)\n            saved_paths[target] = filepath\n            if verbose:\n                print(f\"  \u2713 Saved forecaster for {target} to {filepath}\")\n        except Exception as e:\n            raise OSError(f\"Failed to save model for {target}: {e}\")\n\n    return saved_paths\n</code></pre>"},{"location":"processing/model_persistence/#loading-models","title":"Loading Models","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.manager.persistence._load_forecasters","title":"<code>spotforecast2_safe.manager.persistence._load_forecasters(target_columns, model_dir, verbose=False)</code>","text":"<p>Load trained forecasters from disk using joblib.</p> <p>Attempts to load all forecasters for given targets. Missing models are indicated in the return value for selective retraining.</p> <p>Parameters:</p> Name Type Description Default <code>target_columns</code> <code>List[str]</code> <p>List of target variable names to load.</p> required <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory containing saved models.</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, object]</code> <p>Tuple[Dict[str, object], List[str]]:</p> <code>List[str]</code> <ul> <li>forecasters: Dictionary of successfully loaded ForecasterEquivalentDate objects.</li> </ul> <code>Tuple[Dict[str, object], List[str]]</code> <ul> <li>missing_targets: List of target names without saved models.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from spotforecast2_safe.manager.persistence import (\n...     _save_forecasters,\n...     _load_forecasters,\n... )\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; mock_model = LinearRegression()\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     _ = _save_forecasters({\"power\": mock_model}, tmpdir)\n...     forecasters, missing = _load_forecasters(\n...         [\"power\", \"energy\"],\n...         tmpdir,\n...         verbose=False\n...     )\n...     print(\"power\" in forecasters)\n...     print(\"energy\" in missing)\nTrue\nTrue\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     forecasters, missing = _load_forecasters([\"nonexistent\"], tmpdir)\n...     print(len(forecasters), len(missing))\n0 1\n</code></pre> Source code in <code>src/spotforecast2_safe/manager/persistence.py</code> <pre><code>def _load_forecasters(\n    target_columns: List[str],\n    model_dir: Union[str, Path],\n    verbose: bool = False,\n) -&gt; Tuple[Dict[str, object], List[str]]:\n    \"\"\"Load trained forecasters from disk using joblib.\n\n    Attempts to load all forecasters for given targets. Missing models\n    are indicated in the return value for selective retraining.\n\n    Args:\n        target_columns: List of target variable names to load.\n        model_dir: Directory containing saved models.\n        verbose: Print progress messages. Default: False.\n\n    Returns:\n        Tuple[Dict[str, object], List[str]]:\n        - forecasters: Dictionary of successfully loaded ForecasterEquivalentDate objects.\n        - missing_targets: List of target names without saved models.\n\n    Examples:\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from spotforecast2_safe.manager.persistence import (\n        ...     _save_forecasters,\n        ...     _load_forecasters,\n        ... )\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; mock_model = LinearRegression()\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     _ = _save_forecasters({\"power\": mock_model}, tmpdir)\n        ...     forecasters, missing = _load_forecasters(\n        ...         [\"power\", \"energy\"],\n        ...         tmpdir,\n        ...         verbose=False\n        ...     )\n        ...     print(\"power\" in forecasters)\n        ...     print(\"energy\" in missing)\n        True\n        True\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     forecasters, missing = _load_forecasters([\"nonexistent\"], tmpdir)\n        ...     print(len(forecasters), len(missing))\n        0 1\n    \"\"\"\n    model_path = Path(model_dir)\n    forecasters = {}\n    missing_targets = []\n\n    for target in target_columns:\n        filepath = _get_model_filepath(model_path, target)\n        if filepath.exists():\n            try:\n                forecasters[target] = load(filepath)\n                if verbose:\n                    print(f\"  \u2713 Loaded forecaster for {target} from {filepath}\")\n            except Exception as e:\n                if verbose:\n                    print(f\"  \u2717 Failed to load {target}: {e}\")\n                missing_targets.append(target)\n        else:\n            missing_targets.append(target)\n\n    return forecasters, missing_targets\n</code></pre>"},{"location":"processing/model_persistence/#model-directory-management","title":"Model Directory Management","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.manager.persistence._ensure_model_dir","title":"<code>spotforecast2_safe.manager.persistence._ensure_model_dir(model_dir)</code>","text":"<p>Ensure model directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory path for model storage.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Validated Path object.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If directory cannot be created.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from spotforecast2_safe.manager.persistence import _ensure_model_dir\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     model_path = _ensure_model_dir(Path(tmpdir) / \"models\")\n...     print(model_path.exists())\nTrue\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     nested = _ensure_model_dir(Path(tmpdir) / \"a\" / \"b\" / \"c\")\n...     print(nested.is_dir())\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/manager/persistence.py</code> <pre><code>def _ensure_model_dir(model_dir: Union[str, Path]) -&gt; Path:\n    \"\"\"Ensure model directory exists.\n\n    Args:\n        model_dir: Directory path for model storage.\n\n    Returns:\n        Path: Validated Path object.\n\n    Raises:\n        OSError: If directory cannot be created.\n\n    Examples:\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from spotforecast2_safe.manager.persistence import _ensure_model_dir\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     model_path = _ensure_model_dir(Path(tmpdir) / \"models\")\n        ...     print(model_path.exists())\n        True\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     nested = _ensure_model_dir(Path(tmpdir) / \"a\" / \"b\" / \"c\")\n        ...     print(nested.is_dir())\n        True\n    \"\"\"\n    model_path = Path(model_dir)\n    model_path.mkdir(parents=True, exist_ok=True)\n    return model_path\n</code></pre>"},{"location":"processing/model_persistence/#spotforecast2_safe.manager.persistence._model_directory_exists","title":"<code>spotforecast2_safe.manager.persistence._model_directory_exists(model_dir)</code>","text":"<p>Check if model directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory path to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if directory exists, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from spotforecast2_safe.manager.persistence import _model_directory_exists\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     print(_model_directory_exists(tmpdir))\nTrue\n&gt;&gt;&gt; print(_model_directory_exists(\"/nonexistent/path/to/models\"))\nFalse\n</code></pre> Source code in <code>src/spotforecast2_safe/manager/persistence.py</code> <pre><code>def _model_directory_exists(model_dir: Union[str, Path]) -&gt; bool:\n    \"\"\"Check if model directory exists.\n\n    Args:\n        model_dir: Directory path to check.\n\n    Returns:\n        bool: True if directory exists, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from spotforecast2_safe.manager.persistence import _model_directory_exists\n        &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n        ...     print(_model_directory_exists(tmpdir))\n        True\n        &gt;&gt;&gt; print(_model_directory_exists(\"/nonexistent/path/to/models\"))\n        False\n    \"\"\"\n    return Path(model_dir).exists()\n</code></pre>"},{"location":"processing/model_persistence/#examples","title":"Examples","text":"<pre><code>from spotforecast2_safe.manager.persistence import (\n    _save_forecasters,\n    _load_forecasters,\n)\n\n# Save trained models\ntrained_forecasters = {...}  # Your trained forecasters\n_save_forecasters(trained_forecasters, model_dir=\"models/\")\n\n# Load previously trained models\nforecasters, missing = _load_forecasters(\n    target_columns=[\"power\"], \n    model_dir=\"models/\"\n)\n</code></pre>"},{"location":"recursive/ForecasterEquivalentDate/","title":"ForecasterEquivalentDate","text":""},{"location":"recursive/ForecasterEquivalentDate/#introduction","title":"Introduction","text":"<p>In the realm of time series forecasting, especially within safety-critical systems, robustness and interpretability are paramount. While complex machine learning models like Gradient Boosting or Neural Networks often provide superior accuracy, they can be \"black boxes\" that behave unpredictably when encountering data distributions different from their training set.</p> <p><code>ForecasterEquivalentDate</code> is a specialized forecaster designed to provide a highly interpretable, reliable baseline. It operates on a simple yet powerful principle: history repeats itself. By identifying \"equivalent dates\" in the past (e.g., the same day last week or the same hour yesterday), it generates forecasts that are naturally grounded in observed reality.</p> <p>This class is part of the <code>spotforecast2_safe</code> package, emphasizing its role in building resilient forecasting pipelines where a simple, verifiable fallback is often more valuable than a fragile, complex one.</p>"},{"location":"recursive/ForecasterEquivalentDate/#core-concepts","title":"Core Concepts","text":"<p>The <code>ForecasterEquivalentDate</code> relies on three primary parameters to define its behavior:</p> <ol> <li>Offset: Defines how far back in time to look for the \"equivalent\" data.<ul> <li>Integer: Represents a fixed number of steps (e.g., <code>offset=24</code> for hourly data to look at the same hour yesterday).</li> <li>Pandas DateOffset: Robustly handles calendar logic (e.g., <code>pd.offsets.Week(1)</code> to look at the same day/time last week, even accounting for variable frequencies or business days).</li> </ul> </li> <li>n_offsets: The number of historical equivalent periods to consider. Instead of just looking at one equivalent date, you can look at the last N occurrences (e.g., the last 4 Mondays).</li> <li>agg_func: The function used to aggregate values if <code>n_offsets &gt; 1</code>. Common choices include <code>np.mean</code>, <code>np.median</code>, <code>np.max</code>, or <code>np.min</code>.</li> </ol>"},{"location":"recursive/ForecasterEquivalentDate/#safety-critical-fallback-mechanism","title":"Safety-Critical Fallback Mechanism","text":"<p>In safety-critical environments (such as energy grid management, medical monitoring, or autonomous industrial processes), a failure in the primary forecasting model can have severe consequences. <code>ForecasterEquivalentDate</code> serves as an ideal fallback mechanism due to the following properties:</p> <ul> <li>Low Complexity: It has virtually no \"model risk.\" The forecast is a direct reflection of historical data.</li> <li>High Interpretability: Every prediction can be traced back to specific historical dates and values. If a prediction looks wrong, a human operator can immediately see which historical dates were used.</li> <li>Resilience to Outliers: By using <code>n_offsets &gt; 1</code> with <code>agg_func=np.median</code>, the forecaster becomes extremely robust to historical anomalies.</li> <li>No Training Required: Unlike ML models that require a complex <code>fit</code> process on large datasets, this forecaster essentially \"remembers\" the recent window, making it ready to use almost instantly.</li> <li>Probabilistic Confidence: Through its integration with Conformal Prediction, it provides statistically sound uncertainty intervals, allowing the system to quantify the risk of the baseline forecast.</li> </ul>"},{"location":"recursive/ForecasterEquivalentDate/#functional-examples","title":"Functional Examples","text":""},{"location":"recursive/ForecasterEquivalentDate/#1-basic-baseline-daily-seasonality","title":"1. Basic Baseline (Daily Seasonality)","text":"<p>This example demonstrates the simplest usage: using the value from exactly one week ago as the forecast for today.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n\n# 1. Prepare a synthetic daily series with a weekly pattern\nindex = pd.date_range(start='2023-01-01', periods=21, freq='D')\n# Sine wave with 7-day period + noise\ndata = 10 + 5 * np.sin(2 * np.pi * np.arange(21) / 7) + np.random.normal(0, 0.5, 21)\ny = pd.Series(data, index=index, name=\"energy_consumption\")\n\n# 2. Define the forecaster: look back 7 days\nforecaster = ForecasterEquivalentDate(offset=7)\n\n# 3. \"Fit\" the forecaster (stores the necessary history)\nforecaster.fit(y=y)\n\n# 4. Predict the next 3 days\npredictions = forecaster.predict(steps=3)\n\nprint(\"Predictions for next 3 days:\")\nprint(predictions)\n\n# Verify that the first prediction matches the value from 7 days ago\nlast_known_equivalent = y.iloc[-7]\nprint(f\"\\nValue 7 days ago: {last_known_equivalent:.4f}\")\nprint(f\"Prediction for tomorrow: {predictions.iloc[0]:.4f}\")\n</code></pre>"},{"location":"recursive/ForecasterEquivalentDate/#2-aggregated-multi-offset-robust-weekly-pattern","title":"2. Aggregated Multi-Offset (Robust Weekly Pattern)","text":"<p>For increased safety, we can average the values from the last 3 equivalent dates. This smooths out individual day anomalies.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n\n# Prepare data\nindex = pd.date_range(start='2023-01-01', periods=35, freq='D')\ndata = np.arange(35) % 7 # Repeating 0-6 pattern\ny = pd.Series(data, index=index)\n\n# Aggregated forecaster: Mean of the last 3 weeks (offset 7, 14, 21)\nforecaster = ForecasterEquivalentDate(\n    offset=7, \n    n_offsets=3, \n    agg_func=np.mean\n)\n\nforecaster.fit(y=y)\npredictions = forecaster.predict(steps=7)\n\nprint(\"Forecasted week (should be 0-6):\")\nprint(predictions.values)\n</code></pre>"},{"location":"recursive/ForecasterEquivalentDate/#3-using-calendar-offsets-handling-business-logic","title":"3. Using Calendar Offsets (Handling Business Logic)","text":"<p>In many systems, \"one week ago\" isn't a fixed number of steps but a calendar concept. <code>ForecasterEquivalentDate</code> supports <code>pd.DateOffset</code> for these scenarios.</p> <pre><code>import pandas as pd\nfrom spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n\n# Hourly data with Business Day logic\nindex = pd.date_range(start='2023-01-01', periods=100, freq='h')\ny = pd.Series(range(100), index=index)\n\n# Look back exactly one business week\n# This is more robust than offset=168 (24*7) if the data has gaps or irregular holidays\nforecaster = ForecasterEquivalentDate(offset=pd.offsets.BusinessDay(5))\n\nforecaster.fit(y=y)\npredictions = forecaster.predict(steps=5)\n\nprint(\"Predictions using BusinessDay offset:\")\nprint(predictions)\n</code></pre>"},{"location":"recursive/ForecasterEquivalentDate/#4-advanced-probabilistic-intervals-with-binned-residuals","title":"4. Advanced: Probabilistic Intervals with Binned Residuals","text":"<p>In safety-critical systems, point forecasts are rarely enough. We need to know how much to trust the fallback.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.forecaster.recursive import ForecasterEquivalentDate\n\n# Generate data with increasing noise (heteroscedasticity)\nn = 200\nindex = pd.date_range(start='2023-01-01', periods=n, freq='D')\nnoise = np.random.normal(0, np.linspace(0.1, 2.0, n), n)\ny = pd.Series(10 + noise, index=index)\n\n# Forecaster with Residual Binning\n# This allows the uncertainty intervals to vary depending on the predicted value\nforecaster = ForecasterEquivalentDate(\n    offset=7,\n    n_offsets=1,\n    binner_kwargs={'n_bins': 5}\n)\n\n# Store in-sample residuals to calibrate the intervals\nforecaster.fit(y=y, store_in_sample_residuals=True)\n\n# Predict intervals (90% confidence)\nintervals = forecaster.predict_interval(\n    steps=7, \n    interval=[5, 95] \n)\n\nprint(\"Predictions with Confidence Intervals:\")\nprint(intervals)\n</code></pre>"},{"location":"recursive/ForecasterEquivalentDate/#expert-reference-internal-mechanics","title":"Expert Reference: Internal Mechanics","text":""},{"location":"recursive/ForecasterEquivalentDate/#residual-binning-and-conformal-prediction","title":"Residual Binning and Conformal Prediction","text":"<p>The <code>ForecasterEquivalentDate</code> implements a binned approach to conformal prediction. When <code>fit(store_in_sample_residuals=True)</code> is called: 1.  The forecaster calculates historical \"forecasts\" for the training period. 2.  Residuals ($y_{true} - y_{pred}$) are calculated. 3.  Predicted values are split into bins (quantiles) using the <code>binner</code>. 4.  Residuals are associated with their corresponding bin based on the predicted value. 5.  When <code>predict_interval()</code> is called, the forecaster uses the distribution of residuals from the relevant bin to calculate the bounds. This allows for narrower intervals when the model is historically more accurate and wider intervals when it is less certain.</p>"},{"location":"recursive/ForecasterEquivalentDate/#limitations","title":"Limitations","text":"<p>While excellent as a fallback, this forecaster does not: *   Account for exogenous variables (e.g., weather, holidays). *   Capture trends (it assumes the future level will be similar to the past level). *   Modify its behavior based on recent forecast errors (it's not adaptive in the short term).</p> <p>For these reasons, it should always be used in tandem with more sophisticated models or as a \"sanity check\" boundary.</p>"},{"location":"recursive/ForecasterRecursive/","title":"ForecasterRecursive","text":""},{"location":"recursive/ForecasterRecursive/#introduction","title":"Introduction","text":"<p>In modern time series forecasting, the recursive autoregressive approach is a cornerstone strategy. <code>ForecasterRecursive</code> is a powerful and flexible class in the <code>spotforecast2_safe</code> package that transforms any standard scikit-learn regressor into a multi-step time series forecaster.</p> <p>The core mechanism of <code>ForecasterRecursive</code> involves learning a mapping from past observations (lags) and optional exogenous features to the next value in the series. For multi-step forecasting, it uses its own predictions as inputs for subsequent steps\u2014an approach known as the recursive strategy.</p> <p>This class is designed with safety and reliability in mind, providing extensive validation, support for feature engineering via window functions, and integrated probabilistic forecasting.</p>"},{"location":"recursive/ForecasterRecursive/#core-concepts","title":"Core Concepts","text":"<p>The <code>ForecasterRecursive</code> operates through several key components:</p> <ol> <li>Estimator: Any object compatible with the scikit-learn API (e.g., <code>LinearRegression</code>, <code>RandomForestRegressor</code>, <code>XGBRegressor</code>).</li> <li>Lags: Specific past time steps used as predictors (e.g., <code>lags=7</code> uses the last 7 values; <code>lags=[1, 7, 14]</code> uses specific seasonal lags).</li> <li>Window Features: Automated computation of statistics over rolling windows (e.g., rolling mean, rolling standard deviation) using the <code>RollingFeatures</code> class.</li> <li>Recursive Strategy: During prediction of step $t+k$, the model uses the predicted values from steps $t+1, \\dots, t+k-1$ as if they were real historical data.</li> <li>Exogenous Variables: Support for external factors that influence the target variable (e.g., temperature influencing energy demand).</li> </ol>"},{"location":"recursive/ForecasterRecursive/#safety-critical-design-patterns","title":"Safety-Critical Design Patterns","text":"<p><code>ForecasterRecursive</code> includes several features specifically tailored for safety-critical environments:</p> <ul> <li>Comprehensive Validation: Strict checks on input data types, indices, and frequencies to prevent \"silent failures\" or misaligned data.</li> <li>Resilience to Outliers: By using robust estimators like RandomForestRegressor, the forecaster can handle anomalies in the training data.</li> <li>No Training Required: While the underlying estimator needs fitting, the recursive logic is ready to use as soon as the model is trained.</li> <li>Consistent Interface: It follows the familiar scikit-learn pattern of fit and predict.</li> </ul>"},{"location":"recursive/ForecasterRecursive/#functional-examples","title":"Functional Examples","text":""},{"location":"recursive/ForecasterRecursive/#1-standard-autoregressive-forecast","title":"1. Standard Autoregressive Forecast","text":"<p>A basic example using <code>LinearRegression</code> with the last 7 days as predictors.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n\n# 1. Create a synthetic daily series with a trend and seasonality\nindex = pd.date_range(start='2023-01-01', periods=100, freq='D')\ndata = np.linspace(10, 20, 100) + 5 * np.sin(2 * np.pi * np.arange(100) / 7)\ny = pd.Series(data, index=index, name=\"target\")\n\n# 2. Initialize with 7 lags\nforecaster = ForecasterRecursive(\n    estimator=LinearRegression(),\n    lags=7\n)\n\n# 3. Fit and Predict\nforecaster.fit(y=y)\npredictions = forecaster.predict(steps=14)\n\nprint(\"Forecast for the next 14 days:\")\nprint(predictions.head())\n</code></pre>"},{"location":"recursive/ForecasterRecursive/#2-multi-feature-forecasting-lags-window-statistics","title":"2. Multi-Feature Forecasting (Lags + Window Statistics)","text":"<p>Combining raw lags with rolling means provides the model with both local and global context.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2_safe.preprocessing import RollingFeatures\n\n# Data preparation\nindex = pd.date_range(start='2023-01-01', periods=200, freq='h')\ny = pd.Series(np.random.normal(50, 5, 200), index=index, name=\"sensor_reading\")\n\n# Forecaster with Lags and Rolling Mean\nforecaster = ForecasterRecursive(\n    estimator=RandomForestRegressor(n_estimators=50, random_state=123),\n    lags=24,\n    window_features=RollingFeatures(stats=['mean', 'std'], window_sizes=24)\n)\n\nforecaster.fit(y=y)\npredictions = forecaster.predict(steps=12)\n\nprint(\"Predictions with window features:\")\nprint(predictions)\n</code></pre>"},{"location":"recursive/ForecasterRecursive/#3-handling-exogenous-variables","title":"3. Handling Exogenous Variables","text":"<p>Forecasting energy demand using temperature as an external factor.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n\n# Target and Exogenous data\nindex = pd.date_range(start='2023-01-01', periods=100, freq='D')\ny = pd.Series(np.random.normal(100, 10, 100), index=index, name=\"demand\")\nexog = pd.DataFrame({'temp': np.random.normal(20, 5, 100)}, index=index)\n\n# Define forecaster\nforecaster = ForecasterRecursive(estimator=Ridge(), lags=7)\n\n# Fit with exog\nforecaster.fit(y=y, exog=exog)\n\n# Future exog values must be known for the prediction horizon\nexog_future = pd.DataFrame(\n    {'temp': [22, 21, 23]}, \n    index=pd.date_range(start='2023-04-11', periods=3, freq='D')\n)\n\npredictions = forecaster.predict(steps=3, exog=exog_future)\nprint(\"Demand forecast with temp as exog:\")\nprint(predictions)\n</code></pre>"},{"location":"recursive/ForecasterRecursive/#expert-reference-internal-mechanics","title":"Expert Reference: Internal Mechanics","text":""},{"location":"recursive/ForecasterRecursive/#feature-matrix-construction","title":"Feature Matrix Construction","text":"<p><code>ForecasterRecursive</code> internally transforms the time series into a supervised learning matrix. If <code>lags=[1, 2]</code> is used, the training set $X$ at time $t$ will contain the columns $[y_{t-1}, y_{t-2}]$. If exogenous variables are provided, they are concatenated to this matrix.</p>"},{"location":"recursive/ForecasterRecursive/#robustness-and-interpretability","title":"Robustness and Interpretability","text":"<p>The <code>ForecasterRecursive</code> transforms time series data into a supervised structure. If <code>lags=[1, 2]</code> is used, the training set X at time t will contain columns [y_t-1, y_t-2]. This explicit feature engineering makes it easy to understand what the model is looking at.</p>"},{"location":"recursive/ForecasterRecursive/#differentiation-and-reconstruction","title":"Differentiation and Reconstruction","text":"<p>When <code>differentiation &gt; 0</code> is set: 1.  The training series $y$ is differenced $n$ times. 2.  The model is trained to predict the differenced values. 3.  During prediction, the model generates differenced forecasts. 4.  These forecasts are automatically integrated (undifferenced) using the <code>TimeSeriesDifferentiator</code> and the <code>last_window_</code> to return predictions in the original scale. This process is seamless to the user but critical for handling trends.</p>"},{"location":"recursive/ForecasterRecursive/#4-probabilistic-forecasting-prediction-intervals","title":"4. Probabilistic Forecasting (Prediction Intervals)","text":"<p><code>ForecasterRecursive</code> supports probabilistic forecasting, allowing you to estimate prediction intervals. This is crucial for safety-critical applications where quantifying uncertainty is as important as the point forecast itself.</p> <p>Two methods are available: *   Bootstrapping: Resamples residuals from the training phase to generate a distribution of possible future paths. *   Conformal Prediction: Uses a calibration dataset (out-of-sample residuals) to guarantee a statistical coverage rate (e.g., 95%).</p>"},{"location":"recursive/ForecasterRecursive/#bootstrapping-example","title":"Bootstrapping Example","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n\n# 1. Generate synthetic data with noise\nnp.random.seed(123)\nsteps = 100\nt = np.arange(steps)\ny = pd.Series(\n    data=0.5 * t + 2 * np.sin(2 * np.pi * t / 12) + np.random.normal(0, 1.5, steps),\n    index=pd.date_range(start='2023-01-01', periods=steps, freq='D'),\n    name='y'\n)\n\n# 2. Initialize and Fit\nforecaster = ForecasterRecursive(\n    estimator=LinearRegression(),\n    lags=12\n)\nforecaster.fit(y=y, store_in_sample_residuals=True)\n\n# 3. Predict with Intervals (Bootstrapping)\n# We predict the next 10 days with a 95% confidence interval (2.5% to 97.5%)\nresults = forecaster.predict_interval(\n    steps=10,\n    method='bootstrapping',\n    interval=[5, 95],\n    n_boot=250,\n    random_state=123\n)\n\nprint(\"Bootstrapping Intervals:\")\nprint(results.head())\n</code></pre>"},{"location":"recursive/ForecasterRecursive/#conformal-prediction-example","title":"Conformal Prediction Example","text":"<p>Conformal prediction often requires out-of-sample residuals for calibration to ensure the coverage guarantee holds.</p> <pre><code># ... (Assuming same setup as above)\n\n# 1. Split data into training and calibration sets\n# We use the last 20 points for calibration (out-of-sample residuals)\ny_train = y.iloc[:-20]\ny_calibration = y.iloc[-20:]\n\nforecaster.fit(y=y_train)\n\n# 2. Compute out-of-sample residuals\n# This is a critical step for conformal prediction\ny_pred = forecaster.predict(steps=len(y_calibration))\nforecaster.set_out_sample_residuals(y_true=y_calibration, y_pred=y_pred)\n\n# 3. Predict with Intervals (Conformal)\n# We request a 95% confidence level (nominal coverage)\nresults_conformal = forecaster.predict_interval(\n    steps=10,\n    method='conformal',\n    interval=0.95,  # 0.95 means 95% coverage\n    use_in_sample_residuals=False # Use the calibration residuals we just set\n)\n\nprint(\"\\nConformal Prediction Intervals:\")\nprint(results_conformal.head())\n</code></pre>"},{"location":"safe/MODEL_CARD/","title":"Model/Method Card: spotforecast2-safe","text":""},{"location":"safe/MODEL_CARD/#1-system-details","title":"1. System Details","text":"<ul> <li>Name: spotforecast2-safe</li> <li>Version: 0.8.0-rc.1</li> <li>Type: Deterministic library for time series transformation and feature generation (Preprocessing).</li> <li>License: AGPL-3.0-or-later</li> <li>Developers: bartzbeielstein</li> <li>Repository: https://github.com/sequential-parameter-optimization/spotforecast2-safe</li> <li>CPE Identifier (Wildcard): cpe:2.3:a:sequential_parameter_optimization:spotforecast2_safe::::::::</li> <li>CPE Identifier (Current Release): cpe:2.3:a:sequential_parameter_optimization:spotforecast2_safe:0.8.0-rc.1:::::::*</li> <li>Core Dependencies: numpy, pandas (Minimal Dependency Footprint).</li> <li>Prohibited Dependencies: plotly, matplotlib, spotoptim, optuna, torch, tensorflow.</li> </ul>"},{"location":"safe/MODEL_CARD/#2-intended-use","title":"2. Intended Use","text":""},{"location":"safe/MODEL_CARD/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Safety-Critical Forecasting Systems: Preparation of time series data for regression models in environments requiring auditability (e.g., energy supply, finance).</li> <li>Embedded Systems / Edge AI: Use in resource-constrained environments where large ML frameworks cannot be installed.</li> <li>Reproducible Research: Ensuring exact mathematical reproducibility of N-to-1 transformations without hidden stochastics.</li> </ul>"},{"location":"safe/MODEL_CARD/#out-of-scope","title":"Out-of-Scope","text":"<ul> <li>Interactive Visualization: The package deliberately contains no plotting functions to minimize the attack surface.</li> <li>Automated Hyperparameter Tuning: Optimization (e.g., via spotoptim or Optuna) must take place outside the \"Safe Environment\".</li> <li>Silent Data Cleaning: The package does not perform \"silent\" data imputation. Missing values (NaNs) lead to explicit errors (Fail-Safe), not estimations.</li> </ul>"},{"location":"safe/MODEL_CARD/#3-algorithm-logic","title":"3. Algorithm &amp; Logic","text":"<p>The core task <code>task_n_to_1</code> implements a deterministic sliding-window transformation.</p>"},{"location":"safe/MODEL_CARD/#mathematical-description","title":"Mathematical Description","text":"<p>Given a univariate time series $X = {x_1, x_2, ..., x_T}$, the system transforms this into a feature matrix $X_{feat}$ and a target vector $y$ based on the window size $w$ (lags):</p> <p>$$X_{row, t} = [x_{t-w}, x_{t-w+1}, ..., x_{t-1}] \\rightarrow y_t = x_t$$</p>"},{"location":"safe/MODEL_CARD/#design-objectives","title":"Design Objectives","text":"<ul> <li>Deterministic: The implementation strives to ensure that the same input always generates the exact same bit-level output.</li> <li>Leakage-Free: The implementation aims to ensure that the target value $y_t$ is never contained within the input vector $X_{row, t}$.</li> </ul>"},{"location":"safe/MODEL_CARD/#4-performance-robustness-design-goals","title":"4. Performance &amp; Robustness (Design Goals)","text":"<p>In the absence of \"Accuracy\" (as no model is trained), the following software metrics are design goals intended to support compliance with standards like IEC 61508 / EU AI Act. Users must verify these properties:</p>"},{"location":"safe/MODEL_CARD/#fail-safe-behavior","title":"Fail-Safe Behavior","text":"<ul> <li>Input: DataFrame with <code>NaN</code> or <code>Inf</code>.</li> <li>Behavior: Throws an explicit <code>ValueError</code>. No silent processing (Silent Failure).</li> </ul>"},{"location":"safe/MODEL_CARD/#input-validation","title":"Input Validation","text":"<ul> <li>Strict Checks: Type hinting and runtime checks for <code>pd.DataFrame</code> and <code>np.ndarray</code>.</li> </ul>"},{"location":"safe/MODEL_CARD/#cybersecurity-footprint","title":"Cybersecurity Footprint","text":"<ul> <li>Minimal CVE Surface: By avoiding complex dependencies (like PyTorch or web server components), the Common Vulnerabilities and Exposures (CVE) attack surface is minimized.</li> </ul>"},{"location":"safe/MODEL_CARD/#5-compliance-eu-ai-act-support","title":"5. Compliance &amp; EU AI Act Support","text":"<p>This package is designed to support the development of high-risk AI systems according to the EU AI Act. However, this package itself is not certified.</p> <ul> <li>Transparency (Art. 13): We strive for a fully transparent (\"White Box\") code structure.</li> <li>Accuracy &amp; Robustness (Art. 15): The transformations are designed to be mathematically provable and reproducible, but formal verification is the user's responsibility.</li> <li>Data Governance (Art. 10): The package aims to enforce clean data formats by rejecting \"dirty\" data, assisting in data governance efforts.</li> </ul>"},{"location":"safe/MODEL_CARD/#6-caveats-limitations","title":"6. Caveats &amp; Limitations","text":"<ul> <li>No Extrapolation: The package prepares data; it does not predict by itself. The quality of the forecast depends on the downstream regressor (e.g., <code>scikit-learn</code> LinearRegression).</li> <li>Memory Requirements: Creating the Lag matrix (N-to-1) can be memory-intensive for extremely large time series ($T &gt; 10^7$) as data is duplicated.</li> </ul>"},{"location":"safe/MODEL_CARD/#7-how-to-audit","title":"7. How to Audit","text":"<p>For auditors who need to validate this package:</p> <ol> <li>Check <code>pyproject.toml</code> to confirm the absence of unsafe libraries.</li> <li>Run <code>pytest tests/</code> to verify the functional correctness of the matrix transformation.</li> <li>Run <code>pytest tests/test_cpe.py</code> to verify CPE identifier generation for compliance and SBOM (Software Bill of Materials) tracking.</li> <li>Reference the CPE Identifier from Section 1 to include this package in vulnerability tracking systems and supply chain disclosure documents.</li> <li>Consult the get_cpe_identifier function in <code>src/spotforecast2_safe/utils/cpe.py</code> for CPE generation in automated workflows.</li> </ol>"},{"location":"safe/MODEL_CARD/#8-disclaimer-liability","title":"8. Disclaimer &amp; Liability","text":"<p>LIMITATION OF LIABILITY: While this library is designed with safety principles and deterministic logic in mind, it is provided \"AS IS\" without any warranties. The developers and contributors assume NO LIABILITY for any direct or indirect damages, system failures, or financial losses resulting from the use of this software. </p> <p>It is the sole responsibility of the system integrator to perform a full system-level safety validation (e.g., as per ISO 26262, IEC 61508, or the EU AI Act) before deploying this software in a production or safety-critical environment.</p>"},{"location":"safe/VERSION_MANAGEMENT/","title":"Version Management for spotforecast2-safe","text":""},{"location":"safe/VERSION_MANAGEMENT/#overview","title":"Overview","text":"<p>As a safety-critical system, spotforecast2-safe implements strict version control to ensure auditability and reproducibility across releases. This document outlines the version management strategy for MLOps engineers.</p>"},{"location":"safe/VERSION_MANAGEMENT/#design-principle-single-source-of-truth","title":"Design Principle: Single Source of Truth","text":"<p>Version is defined once, in <code>pyproject.toml</code></p> <pre><code>[project]\nname = \"spotforecast2-safe\"\nversion = \"0.0.5\"  # \u2190 Single source of truth\n</code></pre> <p>All other files (e.g., <code>MODEL_CARD.md</code>, documentation) reference this version programmatically or are automatically synchronized.</p>"},{"location":"safe/VERSION_MANAGEMENT/#architecture","title":"Architecture","text":""},{"location":"safe/VERSION_MANAGEMENT/#1-version-definition-layer-pyprojecttoml","title":"1. Version Definition Layer (<code>pyproject.toml</code>)","text":"<ul> <li>Contains the canonical version</li> <li>Used by <code>pip</code>, <code>uv</code>, and build systems</li> <li>Updated during release workflows</li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#2-version-access-layer-__init__py","title":"2. Version Access Layer (<code>__init__.py</code>)","text":"<ul> <li>Exposes version via <code>__version__</code> attribute</li> <li>Uses <code>importlib.metadata.version()</code> (runtime dynamic)</li> <li>Fallback for development environments</li> <li>Accessible programmatically:   <pre><code>import spotforecast2_safe\nprint(spotforecast2_safe.__version__)  # \u2192 \"0.0.5\"\n</code></pre></li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#3-version-synchronization-layer-scriptsupdate_versionpy","title":"3. Version Synchronization Layer (<code>scripts/update_version.py</code>)","text":"<ul> <li>Automatically updates all documentation</li> <li>Guarantees consistency across files</li> <li>Runs in CI/CD pipelines</li> <li>Can be manually invoked</li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#4-version-verification-layer-cicd-pre-commit-hooks","title":"4. Version Verification Layer (CI/CD, Pre-commit hooks)","text":"<ul> <li>Validates version consistency</li> <li>Prevents accidental mismatches</li> <li>Supports audit compliance</li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#workflow","title":"Workflow","text":""},{"location":"safe/VERSION_MANAGEMENT/#for-mlopsrelease-engineers","title":"For MLOps/Release Engineers","text":""},{"location":"safe/VERSION_MANAGEMENT/#1-during-development","title":"1. During Development","text":"<p>Version stays at current release version in <code>pyproject.toml</code>. No manual updates needed.</p>"},{"location":"safe/VERSION_MANAGEMENT/#2-before-release-eg-from-005-006","title":"2. Before Release (e.g., from 0.0.5 \u2192 0.0.6)","text":"<p>Step 1: Update pyproject.toml <pre><code>[project]\nversion = \"0.0.6\"\n</code></pre></p> <p>Step 2: Run version synchronization script <pre><code># Automatically update all documentation\npython scripts/update_version.py\n\n# Or dry-run first (recommended for safety-critical systems)\npython scripts/update_version.py --dry-run\n</code></pre></p> <p>Step 3: Verify consistency <pre><code># Verify all files are in sync\npython scripts/update_version.py --verify\n</code></pre></p> <p>Step 4: Commit changes <pre><code>git add pyproject.toml docs/safe/MODEL_CARD.md docs/index.md\ngit commit -m \"chore: release version 0.0.6\"\n</code></pre></p>"},{"location":"safe/VERSION_MANAGEMENT/#3-using-pre-commit-hooks-recommended","title":"3. Using Pre-commit Hooks (Recommended)","text":"<p>Pre-commit hooks automatically run version synchronization before each commit:</p> <pre><code># Setup (one-time)\npip install pre-commit\npre-commit install\n\n# Now every commit will automatically synchronize versions\n# (Only if pyproject.toml changed)\n</code></pre>"},{"location":"safe/VERSION_MANAGEMENT/#for-developers","title":"For Developers","text":""},{"location":"safe/VERSION_MANAGEMENT/#i-want-to-check-the-current-version","title":"I want to check the current version:","text":"<p>Option A: From Python <pre><code>import spotforecast2_safe\nprint(spotforecast2_safe.__version__)\n</code></pre></p> <p>Option B: From CLI <pre><code>python scripts/update_version.py --verify\n</code></pre></p> <p>Option C: From pyproject.toml <pre><code>grep 'version = ' pyproject.toml\n</code></pre></p>"},{"location":"safe/VERSION_MANAGEMENT/#i-want-to-verify-everything-is-in-sync","title":"I want to verify everything is in sync:","text":"<pre><code>python scripts/update_version.py --verify\n</code></pre> <p>Exit code: <code>0</code> = version consistent, <code>1</code> = inconsistency detected</p>"},{"location":"safe/VERSION_MANAGEMENT/#script-reference","title":"Script Reference","text":""},{"location":"safe/VERSION_MANAGEMENT/#scriptsupdate_versionpy","title":"<code>scripts/update_version.py</code>","text":"<p>Purpose: Synchronize version information across documentation files.</p> <p>Usage: <pre><code># Normal: Update all files to match pyproject.toml\npython scripts/update_version.py\n\n# Dry-run: Show changes without modifying files\npython scripts/update_version.py --dry-run\n\n# Verify: Only check consistency (no modifications)\npython scripts/update_version.py --verify\n</code></pre></p> <p>Output: <pre><code>======================================================================\nspotforecast2-safe: Version Synchronization Script\n======================================================================\n\n\ud83d\udccb Version Consistency Check:\n  pyproject.toml:  0.0.5\n  MODEL_CARD.md:   0.0.5\n  docs/index.md:   0.0.5\n\u2713 Versions are in sync!\n</code></pre></p>"},{"location":"safe/VERSION_MANAGEMENT/#files-synchronized","title":"Files Synchronized","text":"<ol> <li><code>pyproject.toml</code> - Primary source</li> <li><code>docs/safe/MODEL_CARD.md</code> - Model/Method card (for EU AI Act compliance)</li> <li><code>docs/index.md</code> - Documentation landing page (version badge and banner)</li> <li><code>src/spotforecast2_safe/__init__.py</code> - Package metadata (dynamic)</li> </ol>"},{"location":"safe/VERSION_MANAGEMENT/#safety-compliance-features","title":"Safety &amp; Compliance Features","text":""},{"location":"safe/VERSION_MANAGEMENT/#audit-trail","title":"\u2705 Audit Trail","text":"<pre><code># Git history preserves all version changes\ngit log --oneline -- pyproject.toml\n</code></pre>"},{"location":"safe/VERSION_MANAGEMENT/#consistency-guarantees","title":"\u2705 Consistency Guarantees","text":"<ul> <li>Version synchronization script prevents mismatches</li> <li>Pre-commit hooks enforce synchronization</li> <li>CI/CD pipeline verifies before release</li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#eu-ai-act-compliance","title":"\u2705 EU AI Act Compliance","text":"<ul> <li>Version is documented in MODEL_CARD.md (Article 13 - Transparency)</li> <li>Version history is traceable in git</li> <li>Releases are tagged with version information</li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#reproducibility","title":"\u2705 Reproducibility","text":"<ul> <li>Each version pins exact dependency versions</li> <li><code>uv lock</code> creates lockfile for determinism</li> <li>Users can reproduce results with <code>spotforecast2_safe==0.0.5</code></li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"safe/VERSION_MANAGEMENT/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Release\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      # Verify version consistency before release\n      - name: Verify version consistency\n        run: python scripts/update_version.py --verify\n\n      # Extract version for tagging\n      - name: Get version\n        id: version\n        run: |\n          VERSION=$(grep 'version = ' pyproject.toml | cut -d'\"' -f2)\n          echo \"version=${VERSION}\" &gt;&gt; $GITHUB_OUTPUT\n\n      # Build and publish to PyPI\n      - name: Build and publish\n        run: |\n          uv build\n          uv publish\n        env:\n          UV_PUBLISH_TOKEN: ${{ secrets.PYPI_TOKEN }}\n</code></pre>"},{"location":"safe/VERSION_MANAGEMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"safe/VERSION_MANAGEMENT/#issue-version-mismatch-detected","title":"Issue: Version mismatch detected","text":"<pre><code>\u26a0 Version mismatch detected!\n  pyproject.toml:  0.0.6\n  MODEL_CARD.md:   0.0.5\n</code></pre> <p>Solution: Run synchronization script <pre><code>python scripts/update_version.py\n</code></pre></p>"},{"location":"safe/VERSION_MANAGEMENT/#issue-pre-commit-hook-prevents-commit","title":"Issue: Pre-commit hook prevents commit","text":"<p>This is intentional! The hook detected a version mismatch.</p> <p>Solution: <pre><code># Let the hook automatically fix it\npre-commit run --all-files\n\n# Then commit normally\ngit add .\ngit commit -m \"chore: synchronize versions\"\n</code></pre></p>"},{"location":"safe/VERSION_MANAGEMENT/#issue-importlibmetadata-not-found","title":"Issue: <code>importlib.metadata</code> not found","text":"<p>This should not happen in Python 3.8+. If it does:</p> <ol> <li>Check Python version: <code>python --version</code></li> <li>Ensure package is installed: <code>uv pip install -e .</code></li> <li>Fallback version (0.0.5) will be used automatically</li> </ol>"},{"location":"safe/VERSION_MANAGEMENT/#best-practices","title":"Best Practices","text":""},{"location":"safe/VERSION_MANAGEMENT/#do","title":"\u2705 Do","text":"<ul> <li>Update version only in <code>pyproject.toml</code></li> <li>Run <code>update_version.py</code> after changing version</li> <li>Use pre-commit hooks for automation</li> <li>Verify consistency before releases (<code>--verify</code> flag)</li> <li>Tag releases with version: <code>git tag v0.0.6</code></li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#dont","title":"\u274c Don't","text":"<ul> <li>Manually edit version in MODEL_CARD.md</li> <li>Forget to synchronize after bumping version</li> <li>Skip the verify step before releases</li> <li>Commit version mismatches to main branch</li> </ul>"},{"location":"safe/VERSION_MANAGEMENT/#versioning-scheme","title":"Versioning Scheme","text":"<p>spotforecast2-safe uses Semantic Versioning (MAJOR.MINOR.PATCH):</p> <ul> <li>Major (0.X.X): Breaking API changes (rare for a data transformation library)</li> <li>Minor (X.Y.X): New features, improvements (backward compatible)</li> <li>Patch (X.Y.Z): Bug fixes, documentation updates</li> </ul> <p>Example: - <code>0.0.1</code> \u2192 <code>0.0.2</code>: Patch - bug fix - <code>0.0.5</code> \u2192 <code>0.1.0</code>: Minor - new feature - <code>0.X.X</code> \u2192 <code>1.0.0</code>: Major - breaking change</p>"},{"location":"safe/VERSION_MANAGEMENT/#references","title":"References","text":"<ul> <li>Semantic Versioning</li> <li>EU AI Act - Transparency (Article 13)</li> <li>PEP 440 - Version Identification</li> <li>importlib.metadata Documentation</li> </ul>"},{"location":"safe/spotforecast2-safe/","title":"spotforecast2-safe: Safety-Critical Streamlining","text":"<p>As part of the MLOps engineering for safety-critical systems, the <code>spotforecast2_safe</code> package has been meticulously streamlined. This document outlines the rationale and the specific changes made to ensure the package contains only the necessary components for the defined forecasting tasks.</p>"},{"location":"safe/spotforecast2-safe/#rationale","title":"Rationale","text":"<p>In safety-critical environments, reducing the \"dead code\" and unnecessary dependencies is paramount. By removing unreachable or unused modules, we: 1. Minimize Attack Surface: Fewer lines of code mean fewer potential vulnerabilities. 2. Improve Predictability: Removing complex model search and statistical heuristics ensures the system behaves exactly as expected for its primary workload. 3. Streamline Compliance: Auditing and validating a smaller codebase is more efficient and reliable.</p>"},{"location":"safe/spotforecast2-safe/#positive-list-retained-components","title":"Positive List (Retained Components)","text":"<p>The following files are essential for the execution of the primary workflows: <code>task_n_to_1.py</code> and <code>task_n_to_1_with_covariates_and_dataframe.py</code>.</p>"},{"location":"safe/spotforecast2-safe/#orchestration-pipelines","title":"Orchestration &amp; Pipelines","text":"<ul> <li><code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code></li> <li><code>src/spotforecast2_safe/processing/n2n_predict.py</code></li> <li><code>src/spotforecast2_safe/processing/agg_predict.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#data-environmental-services","title":"Data &amp; Environmental Services","text":"<ul> <li><code>src/spotforecast2_safe/data/data.py</code></li> <li><code>src/spotforecast2_safe/data/fetch_data.py</code></li> <li><code>src/spotforecast2_safe/weather/weather_client.py</code></li> <li><code>src/spotforecast2_safe/utils/generate_holiday.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#forecaster-engine","title":"Forecaster Engine","text":"<ul> <li><code>src/spotforecast2_safe/forecaster/base.py</code></li> <li><code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code></li> <li><code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code></li> <li><code>src/spotforecast2_safe/forecaster/recursive/_warnings.py</code></li> <li><code>src/spotforecast2_safe/forecaster/utils.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#preprocessing-signal-cleaning","title":"Preprocessing &amp; Signal Cleaning","text":"<ul> <li><code>src/spotforecast2_safe/preprocessing/curate_data.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/imputation.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/outlier.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/split.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_rolling.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_differentiator.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_binner.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_common.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#core-utilities","title":"Core Utilities","text":"<ul> <li><code>src/spotforecast2_safe/utils/validation.py</code></li> <li><code>src/spotforecast2_safe/utils/data_transform.py</code></li> <li><code>src/spotforecast2_safe/utils/forecaster_config.py</code></li> <li><code>src/spotforecast2_safe/utils/convert_to_utc.py</code></li> <li><code>src/spotforecast2_safe/exceptions.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#negative-list-removed-components","title":"Negative List (Removed Components)","text":"<p>The following directories and files were present in the original <code>spotforecast2</code> package but have been removed from <code>spotforecast2_safe</code> as they are not required for the target safety-critical tasks.</p>"},{"location":"safe/spotforecast2-safe/#removed-modules","title":"Removed Modules","text":"<ul> <li><code>src/spotforecast2_safe/model_selection/</code> (Entire directory)<ul> <li>Reason: Bayesian, Grid, and Random search are too complex and non-deterministic for these specific safety-critical deployment targets.</li> </ul> </li> <li><code>src/spotforecast2_safe/stats/</code> (Entire directory)<ul> <li>Reason: Contains autocorrelation and secondary statistical tools not used in the automated pipeline.</li> </ul> </li> <li><code>src/spotforecast2_safe/forecaster/metrics.py</code><ul> <li>Reason: Specialized custom metrics that were not invoked by the core pipelines.</li> </ul> </li> <li><code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code><ul> <li>Reason: Headless production environments do not require Plotly or Matplotlib-based visualizations.</li> </ul> </li> </ul>"},{"location":"safe/spotforecast2-safe/#associated-test-deletions","title":"Associated Test Deletions","text":"<p>To maintain a green build and avoid import errors, the following non-essential tests were also removed: - <code>tests/test_model_selection_utils.py</code> - <code>tests/test_time_series_fold.py</code> - <code>tests/test_ts_visualization.py</code></p>"},{"location":"safe/spotforecast2-safe/#conclusion","title":"Conclusion","text":"<p>The resulting <code>spotforecast2_safe</code> project is a hardened version of the original, with $0$ unreachable code paths for the specified tasks and $100\\%$ test coverage on the remaining logic.</p>"},{"location":"safe/spotforecast2-safe/#essential-classes-and-functions-positive-list","title":"Essential Classes and Functions (Positive List)","text":"<p>The following classes and functions (including internal helpers) are strictly required for the execution of <code>task_safe_demo.py</code> and <code>task_safe_n_to_1_with_covariates_and_dataframe.py</code>:</p>"},{"location":"safe/spotforecast2-safe/#orchestration-processing","title":"Orchestration &amp; Processing","text":"<ul> <li><code>agg_predict</code> (Function)</li> <li><code>n2n_predict</code> (Function)</li> <li><code>n2n_predict_with_covariates</code> (Function)</li> </ul>"},{"location":"safe/spotforecast2-safe/#data-environmental-services_1","title":"Data &amp; Environmental Services","text":"<ul> <li><code>fetch_data</code> (Function)</li> <li><code>fetch_holiday_data</code> (Function)</li> <li><code>fetch_weather_data</code> (Function)</li> <li><code>WeatherClient</code> (Class)</li> <li><code>create_holiday_df</code> (Function)</li> <li><code>convert_to_utc</code> (Function)</li> </ul>"},{"location":"safe/spotforecast2-safe/#forecasting-engine","title":"Forecasting Engine","text":"<ul> <li><code>ForecasterRecursive</code> (Class)</li> <li><code>ForecasterEquivalentDate</code> (Class)</li> <li><code>ForecasterBase</code> (Class)</li> <li><code>predict_multivariate</code> (Function)</li> <li><code>initialize_lags</code> (Function)</li> <li><code>initialize_weights</code> (Function)</li> <li><code>initialize_estimator</code> (Function)</li> <li><code>initialize_window_features</code> (Function)</li> </ul>"},{"location":"safe/spotforecast2-safe/#preprocessing-validation","title":"Preprocessing &amp; Validation","text":"<ul> <li><code>agg_and_resample_data</code> (Function)</li> <li><code>basic_ts_checks</code> (Function)</li> <li><code>curate_holidays</code> (Function)</li> <li><code>curate_weather</code> (Function)</li> <li><code>get_start_end</code> (Function)</li> <li><code>mark_outliers</code> (Function)</li> <li><code>get_missing_weights</code> (Function)</li> <li><code>split_rel_train_val_test</code> (Function)</li> <li><code>check_y</code> (Function)</li> <li><code>check_exog</code> (Function)</li> <li><code>check_predict_input</code> (Function)</li> <li><code>check_interval</code> (Function)</li> <li><code>input_to_frame</code> (Function)</li> <li><code>expand_index</code> (Function)</li> <li><code>transform_dataframe</code> (Function)</li> <li><code>check_extract_values_and_index</code> (Function)</li> </ul>"},{"location":"safe/spotforecast2-safe/#feature-engineering","title":"Feature Engineering","text":"<ul> <li><code>RollingFeatures</code> (Class)</li> <li><code>TimeSeriesDifferentiator</code> (Class)</li> <li><code>QuantileBinner</code> (Class)</li> </ul>"},{"location":"safe/spotforecast2-safe/#unused-classes-and-functions-negative-list","title":"Unused Classes and Functions (Negative List)","text":"<p>The following components are present in the <code>spotforecast2_safe</code> codebase but are not invoked by the primary safety-critical tasks mentioned above.</p>"},{"location":"safe/spotforecast2-safe/#utilities","title":"Utilities","text":"<ul> <li><code>check_preprocess_series</code> (Function)</li> <li><code>check_preprocess_exog_multiseries</code> (Function)</li> <li><code>set_skforecast_warnings</code> (Function)</li> <li><code>initialize_transformer_series</code> (Function)</li> <li><code>date_to_index_position</code> (Function)</li> <li><code>prepare_steps_direct</code> (Function)</li> <li><code>exog_to_direct</code> (Function)</li> <li><code>exog_to_direct_numpy</code> (Function)</li> <li><code>transform_numpy</code> (Function)</li> <li><code>select_n_jobs_fit_forecaster</code> (Function)</li> <li><code>get_style_repr_html</code> (Function)</li> </ul>"},{"location":"safe/spotforecast2-safe/#preprocessing","title":"Preprocessing","text":"<ul> <li><code>manual_outlier_removal</code> (Function)</li> <li><code>get_outliers</code> (Function)</li> <li><code>custom_weights</code> (Function)</li> <li><code>WeightFunction</code> (Class)</li> <li><code>split_abs_train_val_test</code> (Function)</li> </ul>"},{"location":"safe/spotforecast2-safe/#internal-details","title":"Internal Details","text":"<ul> <li><code>check_residuals_input</code> (Function)</li> </ul>"}]}