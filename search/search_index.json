{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to spotforecast2-safe (Core)","text":"<p>spotforecast2-safe is a specialized, hardened Python package for time series forecasting in safety-critical production environments. It provides a minimal, auditable core for feature engineering and recursive forecasting.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udce6 GitHub Repository</li> <li>\ud83d\udcda API Reference</li> <li>\ufffd\ufe0f Safety &amp; Compliance</li> <li>\ud83d\udcca Model/Method Card</li> <li>\ufffd\ud83d\ude80 Current Version: 0.0.1</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/sequential-parameter-optimization/spotforecast2-safe.git\ncd spotforecast2-safe\nuv sync\n</code></pre>"},{"location":"#safety-critical-features","title":"Safety-Critical Features","text":"<ul> <li>Zero Dead Code: No GUI, plotting, or AutoML dependencies (No Plotly, No Optuna).</li> <li>Deterministic Transformations: Mathematical logic that ensures bit-level reproducibility.</li> <li>Fail-Safe Processing: Explicit failure on dirty or incomplete data (NaNs/Infs) instead of silent imputation.</li> <li>Minimal Footprint: Reduced attack surface for high-security deployment targets.</li> </ul>"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Data Service: Robust fetching of time series, weather, and holiday data.</li> <li>Preprocessing: Hardened tools for data curation, resampling, and temporal splitting.</li> <li>Forecasting Engine: Simplified recursive forecasting and seasonal baselines.</li> </ul>"},{"location":"#disclaimer-liability","title":"\u26a0\ufe0f Disclaimer &amp; Liability","text":"<p>IMPORTANT: This software is provided \"as is\" and any express or implied warranties are disclaimed. The use of this software in safety-critical systems is at the sole risk of the user. For full details, see the Disclaimer in the Model Card.</p>"},{"location":"#attributions","title":"Attributions","text":"<p>Parts of the code are ported from <code>skforecast</code> to reduce external dependencies. Many thanks to the skforecast team for their great work!</p>"},{"location":"MODEL_CARD/","title":"Model/Method Card: spotforecast2-safe","text":""},{"location":"MODEL_CARD/#1-system-details","title":"1. System Details","text":"<ul> <li>Name: spotforecast2-safe</li> <li>Version: 0.0.1 (Initial Safety Release)</li> <li>Type: Deterministic library for time series transformation and feature generation (Preprocessing).</li> <li>License: BSD-3-Clause</li> <li>Developers: bartzbeielstein</li> <li>Repository: https://github.com/sequential-parameter-optimization/spotforecast2-safe</li> <li>Core Dependencies: <code>numpy</code>, <code>pandas</code> (Minimal Dependency Footprint).</li> <li>Prohibited Dependencies: <code>plotly</code>, <code>matplotlib</code>, <code>spotoptim</code>, <code>optuna</code>, <code>torch</code>, <code>tensorflow</code>.</li> </ul>"},{"location":"MODEL_CARD/#2-intended-use","title":"2. Intended Use","text":""},{"location":"MODEL_CARD/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Safety-Critical Forecasting Systems: Preparation of time series data for regression models in environments requiring auditability (e.g., energy supply, finance).</li> <li>Embedded Systems / Edge AI: Use in resource-constrained environments where large ML frameworks cannot be installed.</li> <li>Reproducible Research: Ensuring exact mathematical reproducibility of N-to-1 transformations without hidden stochastics.</li> </ul>"},{"location":"MODEL_CARD/#out-of-scope","title":"Out-of-Scope","text":"<ul> <li>Interactive Visualization: The package deliberately contains no plotting functions to minimize the attack surface.</li> <li>Automated Hyperparameter Tuning: Optimization (e.g., via spotoptim or Optuna) must take place outside the \"Safe Environment\".</li> <li>Silent Data Cleaning: The package does not perform \"silent\" data imputation. Missing values (NaNs) lead to explicit errors (Fail-Safe), not estimations.</li> </ul>"},{"location":"MODEL_CARD/#3-algorithm-logic","title":"3. Algorithm &amp; Logic","text":"<p>The core module <code>task_n_to_1</code> implements a deterministic sliding-window transformation.</p>"},{"location":"MODEL_CARD/#mathematical-description","title":"Mathematical Description","text":"<p>Given a univariate time series $X = {x_1, x_2, ..., x_T}$, the system transforms this into a feature matrix $X_{feat}$ and a target vector $y$ based on the window size $w$ (lags):</p> <p>$$X_{row, t} = [x_{t-w}, x_{t-w+1}, ..., x_{t-1}] \\rightarrow y_t = x_t$$</p>"},{"location":"MODEL_CARD/#guarantees","title":"Guarantees","text":"<ul> <li>Deterministic: The same input always generates the exact same bit-level output.</li> <li>Leakage-Free: The implementation guarantees that the target value $y_t$ is never contained within the input vector $X_{row, t}$.</li> </ul>"},{"location":"MODEL_CARD/#4-performance-robustness-design-goals","title":"4. Performance &amp; Robustness (Design Goals)","text":"<p>In the absence of \"Accuracy\" (as no model is trained), the following software metrics are design goals intended to support compliance with standards like IEC 61508 / EU AI Act. Users must verify these properties:</p>"},{"location":"MODEL_CARD/#fail-safe-behavior","title":"Fail-Safe Behavior","text":"<ul> <li>Input: DataFrame with <code>NaN</code> or <code>Inf</code>.</li> <li>Behavior: Throws an explicit <code>ValueError</code>. No silent processing (Silent Failure).</li> </ul>"},{"location":"MODEL_CARD/#input-validation","title":"Input Validation","text":"<ul> <li>Strict Checks: Type hinting and runtime checks for <code>pd.DataFrame</code> and <code>np.ndarray</code>.</li> </ul>"},{"location":"MODEL_CARD/#cybersecurity-footprint","title":"Cybersecurity Footprint","text":"<ul> <li>Minimal CVE Surface: By avoiding complex dependencies (like PyTorch or web server components), the Common Vulnerabilities and Exposures (CVE) attack surface is minimized.</li> </ul>"},{"location":"MODEL_CARD/#5-compliance-eu-ai-act-support","title":"5. Compliance &amp; EU AI Act Support","text":"<p>This package is designed to support the development of high-risk AI systems according to the EU AI Act. However, this package itself is not certified.</p> <ul> <li>Transparency (Art. 13): We strive for a fully transparent (\"White Box\") code structure.</li> <li>Accuracy &amp; Robustness (Art. 15): The transformations are designed to be mathematically provable and reproducible, but formal verification is the user's responsibility.</li> <li>Data Governance (Art. 10): The package aims to enforce clean data formats by rejecting \"dirty\" data, assisting in data governance efforts.</li> </ul>"},{"location":"MODEL_CARD/#6-caveats-limitations","title":"6. Caveats &amp; Limitations","text":"<ul> <li>No Extrapolation: The package prepares data; it does not predict by itself. The quality of the forecast depends on the downstream regressor (e.g., <code>scikit-learn</code> LinearRegression).</li> <li>Memory Requirements: Creating the Lag matrix (N-to-1) can be memory-intensive for extremely large time series ($T &gt; 10^7$) as data is duplicated.</li> </ul>"},{"location":"MODEL_CARD/#7-how-to-audit","title":"7. How to Audit","text":"<p>For auditors who need to validate this package: 1. Check <code>pyproject.toml</code> to confirm the absence of unsafe libraries. 2. Run <code>pytest tests/</code> to verify the functional correctness of the matrix transformation. 3. Check the hash values of input and output data to prove determinism.</p>"},{"location":"MODEL_CARD/#8-disclaimer-liability","title":"8. Disclaimer &amp; Liability","text":"<p>LIMITATION OF LIABILITY: While this library is designed with safety principles and deterministic logic in mind, it is provided \"AS IS\" without any warranties. The developers and contributors assume NO LIABILITY for any direct or indirect damages, system failures, or financial losses resulting from the use of this software. </p> <p>It is the sole responsibility of the system integrator to perform a full system-level safety validation (e.g., as per ISO 26262, IEC 61508, or the EU AI Act) before deploying this software in a production or safety-critical environment.</p>"},{"location":"api/data/","title":"Data Module","text":"<p>This module provides utilities for fetching and loading time series data.</p>"},{"location":"api/data/#spotforecast2_safe.data","title":"<code>spotforecast2_safe.data</code>","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_holiday_data","title":"<code>fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_weather_data","title":"<code>fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.get_cache_home","title":"<code>get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/data/#data-fetching-functions","title":"Data Fetching Functions","text":""},{"location":"api/data/#fetch_data","title":"fetch_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data","title":"<code>spotforecast2_safe.data.fetch_data</code>","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_data","title":"<code>fetch_data(filename=None, dataframe=None, columns=None, index_col=0, parse_dates=True, dayfirst=False, timezone='UTC')</code>","text":"<p>Fetches the integrated raw dataset from a CSV file or processes a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename of the CSV file containing the dataset. Must be located in the data home directory. If both filename and dataframe are None, defaults to \"data_in.csv\".</p> <code>None</code> <code>dataframe</code> <code>DataFrame</code> <p>A pandas DataFrame to process. If provided, it will be processed with proper timezone handling. Mutually exclusive with filename.</p> <code>None</code> <code>columns</code> <code>list</code> <p>List of columns to be included in the dataset. If None, all columns are included. If an empty list is provided, a ValueError is raised.</p> <code>None</code> <code>index_col</code> <code>int</code> <p>Column index to be used as the index (only used when loading from CSV).</p> <code>0</code> <code>parse_dates</code> <code>bool</code> <p>Whether to parse dates in the index column (only used when loading from CSV).</p> <code>True</code> <code>dayfirst</code> <code>bool</code> <p>Whether the day comes first in date parsing (only used when loading from CSV).</p> <code>False</code> <code>timezone</code> <code>str</code> <p>Timezone to set for the datetime index. If a DataFrame with naive index is provided, it will be localized to this timezone then converted to UTC. Default: \"UTC\".</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The integrated raw dataset with UTC timezone.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If columns is an empty list or if both filename and dataframe are provided.</p> <code>FileNotFoundError</code> <p>If CSV file does not exist.</p> <p>Examples:</p> <p>Load from CSV (default):</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; data = fetch_data(columns=[\"col1\", \"col2\"])\n&gt;&gt;&gt; data.head()\n                Header1  Header2  Header3\n</code></pre> <p>Load from specific CSV:</p> <pre><code>&gt;&gt;&gt; data = fetch_data(filename=\"custom_data.csv\")\n</code></pre> <p>Process a DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]},\n...                   index=pd.date_range(\"2024-01-01\", periods=3, freq=\"h\"))\n&gt;&gt;&gt; data = fetch_data(dataframe=df, timezone=\"Europe/Berlin\")\n&gt;&gt;&gt; data.index.tz\n&lt;UTC&gt;\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_data(\n    filename: Optional[str] = None,\n    dataframe: Optional[pd.DataFrame] = None,\n    columns: Optional[list] = None,\n    index_col: int = 0,\n    parse_dates: bool = True,\n    dayfirst: bool = False,\n    timezone: str = \"UTC\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches the integrated raw dataset from a CSV file or processes a DataFrame.\n\n    Args:\n        filename (str, optional):\n            Filename of the CSV file containing the dataset. Must be located in the\n            data home directory. If both filename and dataframe are None, defaults to \"data_in.csv\".\n        dataframe (pd.DataFrame, optional):\n            A pandas DataFrame to process. If provided, it will be processed with\n            proper timezone handling. Mutually exclusive with filename.\n        columns (list, optional):\n            List of columns to be included in the dataset. If None, all columns are included.\n            If an empty list is provided, a ValueError is raised.\n        index_col (int):\n            Column index to be used as the index (only used when loading from CSV).\n        parse_dates (bool):\n            Whether to parse dates in the index column (only used when loading from CSV).\n        dayfirst (bool):\n            Whether the day comes first in date parsing (only used when loading from CSV).\n        timezone (str):\n            Timezone to set for the datetime index. If a DataFrame with naive index is provided,\n            it will be localized to this timezone then converted to UTC. Default: \"UTC\".\n\n    Returns:\n        pd.DataFrame: The integrated raw dataset with UTC timezone.\n\n    Raises:\n        ValueError: If columns is an empty list or if both filename and dataframe are provided.\n        FileNotFoundError: If CSV file does not exist.\n\n    Examples:\n        Load from CSV (default):\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; data = fetch_data(columns=[\"col1\", \"col2\"])\n        &gt;&gt;&gt; data.head()\n                        Header1  Header2  Header3\n\n        Load from specific CSV:\n        &gt;&gt;&gt; data = fetch_data(filename=\"custom_data.csv\")\n\n        Process a DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]},\n        ...                   index=pd.date_range(\"2024-01-01\", periods=3, freq=\"h\"))\n        &gt;&gt;&gt; data = fetch_data(dataframe=df, timezone=\"Europe/Berlin\")\n        &gt;&gt;&gt; data.index.tz\n        &lt;UTC&gt;\n    \"\"\"\n    if columns is not None and len(columns) == 0:\n        raise ValueError(\"columns must be specified and cannot be empty.\")\n\n    if filename is not None and dataframe is not None:\n        raise ValueError(\n            \"Cannot specify both filename and dataframe. Please provide only one.\"\n        )\n\n    # Process DataFrame if provided\n    if dataframe is not None:\n        dataset = Data.from_dataframe(\n            df=dataframe,\n            timezone=timezone,\n            columns=columns,\n        )\n    else:\n        # Load from CSV file\n        if filename is None:\n            filename = \"data_in.csv\"\n        csv_path = get_data_home() / filename\n        if not Path(csv_path).is_file():\n            raise FileNotFoundError(f\"The file {csv_path} does not exist.\")\n\n        dataset = Data.from_csv(\n            csv_path=csv_path,\n            index_col=index_col,\n            parse_dates=parse_dates,\n            dayfirst=dayfirst,\n            timezone=timezone,\n            columns=columns,\n        )\n\n    return dataset.data\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_holiday_data","title":"<code>fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_weather_data","title":"<code>fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_cache_home","title":"<code>get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/data/#fetch_holiday_data","title":"fetch_holiday_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_holiday_data","title":"<code>spotforecast2_safe.data.fetch_data.fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#fetch_weather_data","title":"fetch_weather_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_weather_data","title":"<code>spotforecast2_safe.data.fetch_data.fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#data-utilities","title":"Data Utilities","text":""},{"location":"api/data/#get_cache_home","title":"get_cache_home","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_cache_home","title":"<code>spotforecast2_safe.data.fetch_data.get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#get_data_home","title":"get_data_home","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_data_home","title":"<code>spotforecast2_safe.data.fetch_data.get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions Module","text":"<p>Custom exceptions and error handling.</p>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions","title":"<code>spotforecast2_safe.exceptions</code>","text":"<p>Custom exceptions and warnings for spotforecast2.</p> <p>This module contains all the custom warnings and error classes used across spotforecast2.</p> <p>Examples:</p> <p>Using custom warnings::</p> <pre><code>import warnings\nfrom spotforecast2.exceptions import MissingValuesWarning\n\n# Raise a warning\nwarnings.warn(\n    \"Missing values detected in input data.\",\n    MissingValuesWarning\n)\n\n# Suppress a specific warning\nwarnings.simplefilter('ignore', category=MissingValuesWarning)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.DataTransformationWarning","title":"<code>DataTransformationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for output data in transformed space.</p> <p>Used to notify that the output data is in the transformed space.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Output is in transformed space.\",\n...     DataTransformationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTransformationWarning(UserWarning):\n    \"\"\"Warning for output data in transformed space.\n\n    Used to notify that the output data is in the transformed space.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Output is in transformed space.\",\n        ...     DataTransformationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTransformationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.ExogenousInterpretationWarning","title":"<code>ExogenousInterpretationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning about implications when using exogenous variables.</p> <p>Used to notify about important implications when using exogenous variables with models that use a two-step approach (e.g., regression + ARAR).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous variables may not be used as expected.\",\n...     ExogenousInterpretationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class ExogenousInterpretationWarning(UserWarning):\n    \"\"\"Warning about implications when using exogenous variables.\n\n    Used to notify about important implications when using exogenous\n    variables with models that use a two-step approach (e.g., regression + ARAR).\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous variables may not be used as expected.\",\n        ...     ExogenousInterpretationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ExogenousInterpretationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.FeatureOutOfRangeWarning","title":"<code>FeatureOutOfRangeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for features out of training range.</p> <p>Used to notify that a feature is out of the range seen during training.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Feature value exceeds training range.\",\n...     FeatureOutOfRangeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class FeatureOutOfRangeWarning(UserWarning):\n    \"\"\"Warning for features out of training range.\n\n    Used to notify that a feature is out of the range seen during training.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Feature value exceeds training range.\",\n        ...     FeatureOutOfRangeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=FeatureOutOfRangeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for ignored arguments.</p> <p>Used to notify that an argument is ignored when using a method or a function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Argument 'x' is ignored in this context.\",\n...     IgnoredArgumentWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class IgnoredArgumentWarning(UserWarning):\n    \"\"\"Warning for ignored arguments.\n\n    Used to notify that an argument is ignored when using a method\n    or a function.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Argument 'x' is ignored in this context.\",\n        ...     IgnoredArgumentWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.InputTypeWarning","title":"<code>InputTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for inefficient input format.</p> <p>Used to notify that input format is not the most efficient or recommended for the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Input format is not optimal for this forecaster.\",\n...     InputTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class InputTypeWarning(UserWarning):\n    \"\"\"Warning for inefficient input format.\n\n    Used to notify that input format is not the most efficient or\n    recommended for the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Input format is not optimal for this forecaster.\",\n        ...     InputTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=InputTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for potentially long training processes.</p> <p>Used to notify that a large number of models will be trained and the the process may take a while to run.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Training may take a long time.\",\n...     LongTrainingWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class LongTrainingWarning(UserWarning):\n    \"\"\"Warning for potentially long training processes.\n\n    Used to notify that a large number of models will be trained and the\n    the process may take a while to run.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Training may take a long time.\",\n        ...     LongTrainingWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=LongTrainingWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.MissingExogWarning","title":"<code>MissingExogWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing exogenous variables.</p> <p>Used to indicate that there are missing exogenous variables in the data. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing exogenous variables detected.\",\n...     MissingExogWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingExogWarning(UserWarning):\n    \"\"\"Warning for missing exogenous variables.\n\n    Used to indicate that there are missing exogenous variables in the\n    data. Most machine learning models do not accept missing values, so the\n    Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing exogenous variables detected.\",\n        ...     MissingExogWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingExogWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.NotFittedError","title":"<code>NotFittedError</code>","text":"<p>               Bases: <code>ValueError</code>, <code>AttributeError</code></p> <p>Exception class to raise if estimator is used before fitting.</p> <p>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.exceptions import NotFittedError\n&gt;&gt;&gt; try:\n...     raise NotFittedError(\"Forecaster not fitted\")\n... except NotFittedError as e:\n...     print(e)\nForecaster not fitted\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class NotFittedError(ValueError, AttributeError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    This class inherits from both ValueError and AttributeError to help with\n    exception handling and backward compatibility.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.exceptions import NotFittedError\n        &gt;&gt;&gt; try:\n        ...     raise NotFittedError(\"Forecaster not fitted\")\n        ... except NotFittedError as e:\n        ...     print(e)\n        Forecaster not fitted\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.OneStepAheadValidationWarning","title":"<code>OneStepAheadValidationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for one-step-ahead validation usage.</p> <p>Used to notify that the one-step-ahead validation is being used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Using one-step-ahead validation.\",\n...     OneStepAheadValidationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class OneStepAheadValidationWarning(UserWarning):\n    \"\"\"Warning for one-step-ahead validation usage.\n\n    Used to notify that the one-step-ahead validation is being used.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Using one-step-ahead validation.\",\n        ...     OneStepAheadValidationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.ResidualsUsageWarning","title":"<code>ResidualsUsageWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incorrect residuals usage.</p> <p>Used to notify that a residual are not correctly used in the probabilistic forecasting process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Residuals are not properly used.\",\n...     ResidualsUsageWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class ResidualsUsageWarning(UserWarning):\n    \"\"\"Warning for incorrect residuals usage.\n\n    Used to notify that a residual are not correctly used in the\n    probabilistic forecasting process.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Residuals are not properly used.\",\n        ...     ResidualsUsageWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ResidualsUsageWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.SaveLoadSkforecastWarning","title":"<code>SaveLoadSkforecastWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for save/load operations.</p> <p>Used to notify any issues that may arise when saving or loading a forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Issues detected when saving forecaster.\",\n...     SaveLoadSkforecastWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class SaveLoadSkforecastWarning(UserWarning):\n    \"\"\"Warning for save/load operations.\n\n    Used to notify any issues that may arise when saving or loading\n    a forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Issues detected when saving forecaster.\",\n        ...     SaveLoadSkforecastWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SaveLoadSkforecastWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.SpotforecastVersionWarning","title":"<code>SpotforecastVersionWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for version mismatch.</p> <p>Used to notify that the version installed in the environment differs from the version used to initialize the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Version mismatch detected.\",\n...     SpotforecastVersionWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class SpotforecastVersionWarning(UserWarning):\n    \"\"\"Warning for version mismatch.\n\n    Used to notify that the version installed in the\n    environment differs from the version used to initialize the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Version mismatch detected.\",\n        ...     SpotforecastVersionWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SpotforecastVersionWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.UnknownLevelWarning","title":"<code>UnknownLevelWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for unknown levels in prediction.</p> <p>Used to notify that a level being predicted was not part of the training data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Predicting for an unknown level.\",\n...     UnknownLevelWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class UnknownLevelWarning(UserWarning):\n    \"\"\"Warning for unknown levels in prediction.\n\n    Used to notify that a level being predicted was not part of the\n    training data.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Predicting for an unknown level.\",\n        ...     UnknownLevelWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=UnknownLevelWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.format_warning_handler","title":"<code>format_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom warning handler to format warnings in a box.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is used internally by the warnings module\n&gt;&gt;&gt; set_warnings_style('skforecast')\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def format_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom warning handler to format warnings in a box.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; # This is used internally by the warnings module\n        &gt;&gt;&gt; set_warnings_style('skforecast')  # doctest: +SKIP\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        width = 88\n        title = type(message).__name__\n        output_text = [\"\\\\n\"]\n\n        wrapped_message = textwrap.fill(\n            str(message), width=width - 2, expand_tabs=True, replace_whitespace=True\n        )\n        title_top_border = f\"\u256d{'\u2500' * ((width - len(title) - 2) // 2)} {title} {'\u2500' * ((width - len(title) - 2) // 2)}\u256e\"\n        if len(title) % 2 != 0:\n            title_top_border = title_top_border[:-1] + \"\u2500\" + \"\u256e\"\n        bottom_border = f\"\u2570{'\u2500' * width}\u256f\"\n        output_text.append(title_top_border)\n\n        for line in wrapped_message.split(\"\\\\n\"):\n            output_text.append(f\"\u2502 {line.ljust(width - 2)} \u2502\")\n\n        output_text.append(bottom_border)\n        output_text = \"\\\\n\".join(output_text)\n        color = \"\\\\033[38;5;208m\"\n        reset = \"\\\\033[0m\"\n        output_text = f\"{color}{output_text}{reset}\"\n        print(output_text)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.rich_warning_handler","title":"<code>rich_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom handler for warnings that uses rich to display formatted panels.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is used internally when rich is available\n&gt;&gt;&gt; set_warnings_style('skforecast')\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def rich_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom handler for warnings that uses rich to display formatted panels.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; # This is used internally when rich is available\n        &gt;&gt;&gt; set_warnings_style('skforecast')  # doctest: +SKIP\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        if not HAS_RICH:\n            # Fallback to format_warning_handler if rich is not available\n            format_warning_handler(message, category, filename, lineno, file, line)\n            return\n\n        console = Console()\n\n        category_name = category.__name__\n        text = (\n            f\"{message.message}\\\\n\\\\n\"\n            f\"Category : spotforecast2.exceptions.{category_name}\\\\n\"\n            f\"Location : {filename}:{lineno}\\\\n\"\n            f\"Suppress : warnings.simplefilter('ignore', category={category_name})\"\n        )\n\n        panel = Panel(\n            Text(text, justify=\"left\"),\n            title=category_name,\n            title_align=\"center\",\n            border_style=\"color(214)\",\n            width=88,\n        )\n\n        console.print(panel)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.runtime_deprecated","title":"<code>runtime_deprecated(replacement=None, version=None, removal=None, category=FutureWarning)</code>","text":"<p>Decorator to mark functions or classes as deprecated.</p> <p>Works for both function and class targets, and ensures warnings are visible even inside Jupyter notebooks.</p> <p>Parameters:</p> Name Type Description Default <code>replacement</code> <code>str</code> <p>Name of the replacement function/class to use instead.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version in which the function/class was deprecated.</p> <code>None</code> <code>removal</code> <code>str</code> <p>Version in which the function/class will be removed.</p> <code>None</code> <code>category</code> <code>type[Warning]</code> <p>Warning category to use. Default is FutureWarning.</p> <code>FutureWarning</code> <p>Returns:</p> Type Description <code>object</code> <p>Decorator function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @runtime_deprecated(replacement='new_function', version='0.5', removal='1.0')\n... def old_function():\n...     pass\n&gt;&gt;&gt; old_function()\nFutureWarning: old_function() is deprecated since version 0.5; use new_function instead...\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def runtime_deprecated(\n    replacement: str = None,\n    version: str = None,\n    removal: str = None,\n    category: type[Warning] = FutureWarning,\n) -&gt; object:\n    \"\"\"Decorator to mark functions or classes as deprecated.\n\n    Works for both function and class targets, and ensures warnings are visible\n    even inside Jupyter notebooks.\n\n    Args:\n        replacement: Name of the replacement function/class to use instead.\n        version: Version in which the function/class was deprecated.\n        removal: Version in which the function/class will be removed.\n        category: Warning category to use. Default is FutureWarning.\n\n    Returns:\n        Decorator function.\n\n    Examples:\n        &gt;&gt;&gt; @runtime_deprecated(replacement='new_function', version='0.5', removal='1.0')\n        ... def old_function():\n        ...     pass\n        &gt;&gt;&gt; old_function()  # doctest: +SKIP\n        FutureWarning: old_function() is deprecated since version 0.5; use new_function instead...\n    \"\"\"\n\n    def decorator(obj):\n        is_function = inspect.isfunction(obj) or inspect.ismethod(obj)\n        is_class = inspect.isclass(obj)\n\n        if not (is_function or is_class):\n            raise TypeError(\n                \"@runtime_deprecated can only be used on functions or classes\"\n            )\n\n        # ----- Build warning message -----\n        name = obj.__name__\n        message = (\n            f\"{name}() is deprecated\" if is_function else f\"{name} class is deprecated\"\n        )\n        if version:\n            message += f\" since version {version}\"\n        if replacement:\n            message += f\"; use {replacement} instead\"\n        if removal:\n            message += f\". It will be removed in version {removal}.\"\n        else:\n            message += \".\"\n\n        def issue_warning():\n            \"\"\"Emit warning in a way that always shows in notebooks.\"\"\"\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"always\", category)\n                warnings.warn(message, category, stacklevel=3)\n\n        # ----- Case 1: decorating a function -----\n        if is_function:\n\n            @wraps(obj)\n            def wrapper(*args, **kwargs):\n                issue_warning()\n                return obj(*args, **kwargs)\n\n            # Add metadata\n            wrapper.__deprecated__ = True\n            wrapper.__replacement__ = replacement\n            wrapper.__version__ = version\n            wrapper.__removal__ = removal\n            return wrapper\n\n        # ----- Case 2: decorating a class -----\n        elif is_class:\n            orig_init = getattr(obj, \"__init__\", None)\n            orig_new = getattr(obj, \"__new__\", None)\n\n            # Only wrap whichever exists (some classes use __new__, others __init__)\n            if orig_new and (orig_new is not object.__new__):\n\n                @wraps(orig_new)\n                def wrapped_new(cls, *args, **kwargs):\n                    issue_warning()\n                    return orig_new(cls, *args, **kwargs)\n\n                obj.__new__ = staticmethod(wrapped_new)\n\n            elif orig_init:\n\n                @wraps(orig_init)\n                def wrapped_init(self, *args, **kwargs):\n                    issue_warning()\n                    return orig_init(self, *args, **kwargs)\n\n                obj.__init__ = wrapped_init\n\n            # Add metadata\n            obj.__deprecated__ = True\n            obj.__replacement__ = replacement\n            obj.__version__ = version\n            obj.__removal__ = removal\n\n            return obj\n\n    return decorator\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.set_warnings_style","title":"<code>set_warnings_style(style='skforecast')</code>","text":"<p>Set the warning handler based on the provided style.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>The style of the warning handler. Either 'skforecast' or 'default'.</p> <code>'skforecast'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_warnings_style('skforecast')\n&gt;&gt;&gt; # Now warnings will be displayed with formatting\n&gt;&gt;&gt; set_warnings_style('default')\n&gt;&gt;&gt; # Back to default Python warning format\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_warnings_style(style: str = \"skforecast\") -&gt; None:\n    \"\"\"Set the warning handler based on the provided style.\n\n    Args:\n        style: The style of the warning handler. Either 'skforecast' or 'default'.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; set_warnings_style('skforecast')\n        &gt;&gt;&gt; # Now warnings will be displayed with formatting\n        &gt;&gt;&gt; set_warnings_style('default')\n        &gt;&gt;&gt; # Back to default Python warning format\n    \"\"\"\n    if style == \"skforecast\":\n        if not hasattr(warnings, \"_original_showwarning\"):\n            warnings._original_showwarning = warnings.showwarning\n        if HAS_RICH:\n            warnings.showwarning = rich_warning_handler\n        else:\n            warnings.showwarning = format_warning_handler\n    else:\n        if hasattr(warnings, \"_original_showwarning\"):\n            warnings.showwarning = warnings._original_showwarning\n</code></pre>"},{"location":"api/forecaster/","title":"Forecaster Module","text":"<p>Core forecasting classes and utilities.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster","title":"<code>spotforecast2_safe.forecaster</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase","title":"<code>ForecasterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all forecasters in spotforecast2.</p> <p>All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Attributes:</p> Name Type Description <code>__spotforecast_tags__</code> <p>Dictionary with forecaster tags that characterize the behavior of the forecaster.</p> <p>Examples:</p> <p>To see all abstract methods that need to be implemented:</p> <pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n&gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n['create_train_X_y', 'fit', 'predict', 'set_params']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>class ForecasterBase(ABC):\n    \"\"\"Base class for all forecasters in spotforecast2.\n\n    All forecasters should specify all the parameters that can be set at\n    the class level in their __init__.\n\n    Attributes:\n        __spotforecast_tags__: Dictionary with forecaster tags that characterize\n            the behavior of the forecaster.\n\n    Examples:\n        To see all abstract methods that need to be implemented:\n\n        &gt;&gt;&gt; import inspect\n        &gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n        &gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n        ['create_train_X_y', 'fit', 'predict', 'set_params']\n    \"\"\"\n\n    def _preprocess_repr(\n        self,\n        estimator: object | None = None,\n        training_range_: dict[str, str] | None = None,\n        series_names_in_: list[str] | None = None,\n        exog_names_in_: list[str] | None = None,\n        transformer_series: object | dict[str, object] | None = None,\n    ) -&gt; tuple[str, str | None, str | None, str | None, str | None]:\n        \"\"\"Prepare the information to be displayed when a Forecaster object is printed.\n\n        Args:\n            estimator: Estimator object. Default is None.\n            training_range_: Training range. Only used for ForecasterRecursiveMultiSeries.\n                Default is None.\n            series_names_in_: Names of the series used in the forecaster.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n            exog_names_in_: Names of the exogenous variables used in the forecaster.\n                Default is None.\n            transformer_series: Transformer used in the series.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n\n        Returns:\n            Tuple containing params (estimator parameters string), training_range_\n            (training range string representation), series_names_in_ (series names\n            string representation), exog_names_in_ (exogenous variable names string\n            representation), and transformer_series (transformer string representation).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; estimator = Ridge(alpha=0.5)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=estimator, lags=3)\n            &gt;&gt;&gt; params, tr, sn, en, ts = forecaster._preprocess_repr(estimator=estimator)\n            &gt;&gt;&gt; params\n            \"{'alpha': 0.5, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\"\n        \"\"\"\n\n        if estimator is not None:\n            if isinstance(estimator, Pipeline):\n                name_pipe_steps = tuple(\n                    name + \"__\" for name in estimator.named_steps.keys()\n                )\n                params = {\n                    key: value\n                    for key, value in estimator.get_params().items()\n                    if key.startswith(name_pipe_steps)\n                }\n            else:\n                params = estimator.get_params()\n            params = str(params)\n        else:\n            params = None\n\n        if training_range_ is not None:\n            training_range_ = [\n                f\"'{k}': {v.astype(str).to_list()}\" for k, v in training_range_.items()\n            ]\n            if len(training_range_) &gt; 10:\n                training_range_ = training_range_[:5] + [\"...\"] + training_range_[-5:]\n            training_range_ = \", \".join(training_range_)\n\n        if series_names_in_ is not None:\n            if len(series_names_in_) &gt; 50:\n                series_names_in_ = (\n                    series_names_in_[:25] + [\"...\"] + series_names_in_[-25:]\n                )\n            series_names_in_ = \", \".join(series_names_in_)\n\n        if exog_names_in_ is not None:\n            if len(exog_names_in_) &gt; 50:\n                exog_names_in_ = exog_names_in_[:25] + [\"...\"] + exog_names_in_[-25:]\n            exog_names_in_ = \", \".join(exog_names_in_)\n\n        if transformer_series is not None:\n            if isinstance(transformer_series, dict):\n                transformer_series = [\n                    f\"'{k}': {v}\" for k, v in transformer_series.items()\n                ]\n                if len(transformer_series) &gt; 10:\n                    transformer_series = (\n                        transformer_series[:5] + [\"...\"] + transformer_series[-5:]\n                    )\n                transformer_series = \", \".join(transformer_series)\n            else:\n                transformer_series = str(transformer_series)\n\n        return (\n            params,\n            training_range_,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        )\n\n    def _format_text_repr(\n        self,\n        text: str,\n        max_text_length: int = 58,\n        width: int = 80,\n        indent: str = \"    \",\n    ) -&gt; str:\n        \"\"\"Format text for __repr__ method.\n\n        Args:\n            text: Text to format.\n            max_text_length: Maximum length of the text before wrapping. Default is 58.\n            width: Maximum width of the text. Default is 80.\n            indent: Indentation of the text. Default is four spaces.\n\n        Returns:\n            Formatted text string with proper wrapping and indentation.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster._format_text_repr(\"Short text\")\n            'Short text'\n        \"\"\"\n\n        if text is not None and len(text) &gt; max_text_length:\n            text = \"\\n    \" + textwrap.fill(\n                str(text), width=width, subsequent_indent=indent\n            )\n\n        return text\n\n    @abstractmethod\n    def create_train_X_y(\n        self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned.\n                Default is None.\n\n        Returns:\n            Tuple containing X_train (training values/predictors with shape\n            (len(y) - max_lag, len(lags))) and y_train (target values of the\n            time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n            &gt;&gt;&gt; X_train.head(2)\n               lag_1  lag_2  lag_3\n            3    2.0    1.0    0.0\n            4    3.0    2.0    1.0\n            &gt;&gt;&gt; y_train.head(2)\n            3    3\n            4    4\n            Name: y, dtype: int64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n        \"\"\"Training Forecaster.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned so\n                that y[i] is regressed on exog[i]. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.is_fitted\n            True\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n    ) -&gt; pd.Series:\n        \"\"\"Predict n steps ahead.\n\n        Args:\n            steps: Number of steps to predict.\n            last_window: Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1). If None, the values stored in\n                last_window are used to calculate the initial predictors, and the\n                predictions start right after training data. Default is None.\n            exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n        Returns:\n            Predicted values as a pandas Series.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            10    9.5\n            11    9.0\n            12    8.5\n            Name: pred, dtype: float64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def set_params(self, params: dict[str, object]) -&gt; None:\n        \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n        Args:\n            params: Parameters values dictionary.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n            &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n            &gt;&gt;&gt; forecaster.estimator.alpha\n            0.5\n        \"\"\"\n\n        pass\n\n    def set_lags(\n        self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute lags.\n\n        Attributes max_lag and window_size are also updated.\n\n        Args:\n            lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n                If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n                or range: include only lags present in lags, all elements must be int.\n                If None: no lags are included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.set_lags(lags=5)\n            &gt;&gt;&gt; forecaster.lags\n            array([1, 2, 3, 4, 5])\n        \"\"\"\n\n        pass\n\n    def set_window_features(\n        self, window_features: object | list[object] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute window_features.\n\n        Attributes max_size_window_features, window_features_names,\n        window_features_class_names and window_size are also updated.\n\n        Args:\n            window_features: Instance or list of instances used to create window features.\n                Window features are created from the original time series and are\n                included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n            &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n            &gt;&gt;&gt; forecaster.window_features\n            [RollingFeatures(stats=['mean'], window_sizes=[3])]\n        \"\"\"\n\n        pass\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            Dictionary with forecaster tags describing behavior and capabilities.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; tags['forecaster_task']\n            'regression'\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.summary()\n            ForecasterRecursive\n            ===================\n            Estimator: Ridge()\n            Lags: [1 2 3]\n            ...\n        \"\"\"\n\n        print(self.__repr__())\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n        This method is called when an object is unpickled (deserialized).\n        It handles the migration of deprecated attributes to their new names.\n\n        Args:\n            state: The state dictionary from the pickled object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pickle\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n            &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n        \"\"\"\n\n        # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n        if \"regressor\" in state and \"estimator\" not in state:\n            state[\"estimator\"] = state.pop(\"regressor\")\n\n        self.__dict__.update(state)\n\n    @property\n    def regressor(self) -&gt; Any:\n        \"\"\"Deprecated property. Use estimator instead.\n\n        Returns:\n            The estimator object.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\n            Ridge()\n        \"\"\"\n        warnings.warn(\n            \"The `regressor` attribute is deprecated and will be removed in future \"\n            \"versions. Use `estimator` instead.\",\n            FutureWarning,\n        )\n        return self.estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.regressor","title":"<code>regressor</code>  <code>property</code>","text":"<p>Deprecated property. Use estimator instead.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The estimator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\nRidge()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> <p>This method is called when an object is unpickled (deserialized). It handles the migration of deprecated attributes to their new names.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The state dictionary from the pickled object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n&gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n    This method is called when an object is unpickled (deserialized).\n    It handles the migration of deprecated attributes to their new names.\n\n    Args:\n        state: The state dictionary from the pickled object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n        &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n    \"\"\"\n\n    # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n    if \"regressor\" in state and \"estimator\" not in state:\n        state[\"estimator\"] = state.pop(\"regressor\")\n\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing X_train (training values/predictors with shape</p> <code>Series</code> <p>(len(y) - max_lag, len(lags))) and y_train (target values of the</p> <code>tuple[DataFrame, Series]</code> <p>time series related to each row of X_train with shape (len(y) - max_lag,)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n&gt;&gt;&gt; X_train.head(2)\n   lag_1  lag_2  lag_3\n3    2.0    1.0    0.0\n4    3.0    2.0    1.0\n&gt;&gt;&gt; y_train.head(2)\n3    3\n4    4\nName: y, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef create_train_X_y(\n    self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned.\n            Default is None.\n\n    Returns:\n        Tuple containing X_train (training values/predictors with shape\n        (len(y) - max_lag, len(lags))) and y_train (target values of the\n        time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n        &gt;&gt;&gt; X_train.head(2)\n           lag_1  lag_2  lag_3\n        3    2.0    1.0    0.0\n        4    3.0    2.0    1.0\n        &gt;&gt;&gt; y_train.head(2)\n        3    3\n        4    4\n        Name: y, dtype: int64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.fit","title":"<code>fit(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.is_fitted\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n    \"\"\"Training Forecaster.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned so\n            that y[i] is regressed on exog[i]. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.is_fitted\n        True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with forecaster tags describing behavior and capabilities.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; tags['forecaster_task']\n'regression'\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        Dictionary with forecaster tags describing behavior and capabilities.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; tags['forecaster_task']\n        'regression'\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If None, the values stored in last_window are used to calculate the initial predictors, and the predictions start right after training data. Default is None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Predicted values as a pandas Series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n10    9.5\n11    9.0\n12    8.5\nName: pred, dtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; pd.Series:\n    \"\"\"Predict n steps ahead.\n\n    Args:\n        steps: Number of steps to predict.\n        last_window: Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1). If None, the values stored in\n            last_window are used to calculate the initial predictors, and the\n            predictions start right after training data. Default is None.\n        exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n    Returns:\n        Predicted values as a pandas Series.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        10    9.5\n        11    9.0\n        12    8.5\n        Name: pred, dtype: float64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute lags.</p> <p>Attributes max_lag and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int | list[int] | ndarray[int] | range[int] | None</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. If int: include lags from 1 to lags (included). If list, 1d numpy ndarray, or range: include only lags present in lags, all elements must be int. If None: no lags are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.set_lags(lags=5)\n&gt;&gt;&gt; forecaster.lags\narray([1, 2, 3, 4, 5])\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_lags(\n    self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute lags.\n\n    Attributes max_lag and window_size are also updated.\n\n    Args:\n        lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n            If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n            or range: include only lags present in lags, all elements must be int.\n            If None: no lags are included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.set_lags(lags=5)\n        &gt;&gt;&gt; forecaster.lags\n        array([1, 2, 3, 4, 5])\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_params","title":"<code>set_params(params)</code>  <code>abstractmethod</code>","text":"<p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, object]</code> <p>Parameters values dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n&gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n&gt;&gt;&gt; forecaster.estimator.alpha\n0.5\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef set_params(self, params: dict[str, object]) -&gt; None:\n    \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n    Args:\n        params: Parameters values dictionary.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n        &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n        &gt;&gt;&gt; forecaster.estimator.alpha\n        0.5\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute window_features.</p> <p>Attributes max_size_window_features, window_features_names, window_features_class_names and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>object | list[object] | None</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n&gt;&gt;&gt; forecaster.window_features\n[RollingFeatures(stats=['mean'], window_sizes=[3])]\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_window_features(\n    self, window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute window_features.\n\n    Attributes max_size_window_features, window_features_names,\n    window_features_class_names and window_size are also updated.\n\n    Args:\n        window_features: Instance or list of instances used to create window features.\n            Window features are created from the original time series and are\n            included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n        &gt;&gt;&gt; forecaster.window_features\n        [RollingFeatures(stats=['mean'], window_sizes=[3])]\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.summary()\nForecasterRecursive\n===================\nEstimator: Ridge()\nLags: [1 2 3]\n...\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.summary()\n        ForecasterRecursive\n        ===================\n        Estimator: Ridge()\n        Lags: [1 2 3]\n        ...\n    \"\"\"\n\n    print(self.__repr__())\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive","title":"<code>ForecasterRecursive</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>Recursive autoregressive forecaster for scikit-learn compatible estimators.</p> <p>This class turns any estimator compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. The forecaster learns to predict future values by using lagged values of the target variable and optional exogenous features. Predictions are made iteratively, where each step uses previous predictions as input for the next step (recursive strategy).</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object</code> <p>Scikit-learn compatible estimator for regression. If None, a default estimator will be initialized. Can also be passed via regressor parameter.</p> <code>None</code> <code>lags</code> <code>Union[int, List[int], ndarray, range, None]</code> <p>Lagged values of the target variable to use as predictors. Can be an integer (uses lags from 1 to lags), list of integers, numpy array, or range. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>window_features</code> <code>Union[object, List[object], None]</code> <p>List of window feature objects to compute features from the target variable. Each object must implement transform_batch() method. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>transformer_y</code> <code>Optional[object]</code> <p>Transformer object for the target variable. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>transformer_exog</code> <code>Optional[object]</code> <p>Transformer object for exogenous variables. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>weight_func</code> <code>Optional[Callable]</code> <p>Function to compute sample weights for training. Must accept an index and return an array of weights. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>Optional[int]</code> <p>Order of differencing to apply to the target variable. Must be a positive integer. Differencing is applied before creating lags. Defaults to None.</p> <code>None</code> <code>fit_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of additional keyword arguments to pass to the estimator's fit() method. Defaults to None.</p> <code>None</code> <code>binner_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of keyword arguments for QuantileBinner used in probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.</p> <code>None</code> <code>forecaster_id</code> <code>Union[str, int, None]</code> <p>Identifier for the forecaster instance. Can be a string or integer. Used for tracking and logging purposes. Defaults to None.</p> <code>None</code> <code>regressor</code> <code>object</code> <p>Alternative parameter name for estimator. If provided, used instead of estimator. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator</code> <p>Fitted scikit-learn estimator.</p> <code>lags</code> <p>Lag indices used in the model.</p> <code>lags_names</code> <p>Names of lag features (e.g., ['lag_1', 'lag_2']).</p> <code>window_features</code> <p>List of window feature transformers.</p> <code>window_features_names</code> <p>Names of window features.</p> <code>window_size</code> <p>Maximum window size needed (max of lags and window features).</p> <code>transformer_y</code> <p>Transformer for target variable.</p> <code>transformer_exog</code> <p>Transformer for exogenous variables.</p> <code>weight_func</code> <p>Function for sample weighting.</p> <code>differentiation</code> <p>Order of differencing applied.</p> <code>differentiator</code> <p>TimeSeriesDifferentiator instance if differencing is used.</p> <code>is_fitted</code> <p>Boolean indicating if forecaster has been fitted.</p> <code>fit_date</code> <p>Timestamp of the last fit operation.</p> <code>last_window_</code> <p>Last window_size observations from training data.</p> <code>index_type_</code> <p>Type of index in training data (RangeIndex or DatetimeIndex).</p> <code>index_freq_</code> <p>Frequency of DatetimeIndex if applicable.</p> <code>training_range_</code> <p>First and last index values of training data.</p> <code>series_name_in_</code> <p>Name of the target series.</p> <code>exog_in_</code> <p>Boolean indicating if exogenous variables were used in training.</p> <code>exog_names_in_</code> <p>Names of exogenous variables.</p> <code>exog_type_in_</code> <p>Type of exogenous input (Series or DataFrame).</p> <code>X_train_features_names_out_</code> <p>Names of all training features.</p> <code>in_sample_residuals_</code> <p>Residuals from training set.</p> <code>in_sample_residuals_by_bin_</code> <p>Residuals grouped by bins for probabilistic pred.</p> <code>forecaster_id</code> <p>Identifier for the forecaster instance.</p> Note <ul> <li>Either lags or window_features (or both) must be provided during initialization.</li> <li>The forecaster uses a recursive strategy where each multi-step prediction   depends on previous predictions within the same forecast horizon.</li> <li>Exogenous variables must have the same index as the target variable and must   be available for the entire prediction horizon.</li> <li>The forecaster supports point predictions, prediction intervals, bootstrapping,   quantile predictions, and probabilistic forecasts via conformal methods.</li> </ul> <p>Examples:</p> <p>Create a basic forecaster with lags:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=10\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> <p>Create a forecaster with window features and transformations:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(n_estimators=100),\n...     lags=[1, 7, 30],\n...     window_features=[RollingMeanWindow(window=7)],\n...     transformer_y=StandardScaler(),\n...     differentiation=1\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n</code></pre> <p>Create a forecaster with exogenous variables:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=Ridge(),\n...     lags=7,\n...     forecaster_id='my_forecaster'\n... )\n&gt;&gt;&gt; forecaster.fit(y, exog)\n&gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n</code></pre> <p>Create a forecaster with probabilistic prediction configuration:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=GradientBoostingRegressor(),\n...     lags=14,\n...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n... )\n&gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n&gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>class ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    Recursive autoregressive forecaster for scikit-learn compatible estimators.\n\n    This class turns any estimator compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster. The forecaster learns to predict\n    future values by using lagged values of the target variable and optional exogenous\n    features. Predictions are made iteratively, where each step uses previous predictions\n    as input for the next step (recursive strategy).\n\n    Args:\n        estimator: Scikit-learn compatible estimator for regression. If None, a default\n            estimator will be initialized. Can also be passed via regressor parameter.\n        lags: Lagged values of the target variable to use as predictors. Can be an\n            integer (uses lags from 1 to lags), list of integers, numpy array, or range.\n            At least one of lags or window_features must be provided. Defaults to None.\n        window_features: List of window feature objects to compute features from the\n            target variable. Each object must implement transform_batch() method.\n            At least one of lags or window_features must be provided. Defaults to None.\n        transformer_y: Transformer object for the target variable. Must implement fit()\n            and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        transformer_exog: Transformer object for exogenous variables. Must implement\n            fit() and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        weight_func: Function to compute sample weights for training. Must accept an\n            index and return an array of weights. Defaults to None.\n        differentiation: Order of differencing to apply to the target variable.\n            Must be a positive integer. Differencing is applied before creating lags.\n            Defaults to None.\n        fit_kwargs: Dictionary of additional keyword arguments to pass to the estimator's\n            fit() method. Defaults to None.\n        binner_kwargs: Dictionary of keyword arguments for QuantileBinner used in\n            probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.\n        forecaster_id: Identifier for the forecaster instance. Can be a string or\n            integer. Used for tracking and logging purposes. Defaults to None.\n        regressor: Alternative parameter name for estimator. If provided, used instead\n            of estimator. Defaults to None.\n\n    Attributes:\n        estimator: Fitted scikit-learn estimator.\n        lags: Lag indices used in the model.\n        lags_names: Names of lag features (e.g., ['lag_1', 'lag_2']).\n        window_features: List of window feature transformers.\n        window_features_names: Names of window features.\n        window_size: Maximum window size needed (max of lags and window features).\n        transformer_y: Transformer for target variable.\n        transformer_exog: Transformer for exogenous variables.\n        weight_func: Function for sample weighting.\n        differentiation: Order of differencing applied.\n        differentiator: TimeSeriesDifferentiator instance if differencing is used.\n        is_fitted: Boolean indicating if forecaster has been fitted.\n        fit_date: Timestamp of the last fit operation.\n        last_window_: Last window_size observations from training data.\n        index_type_: Type of index in training data (RangeIndex or DatetimeIndex).\n        index_freq_: Frequency of DatetimeIndex if applicable.\n        training_range_: First and last index values of training data.\n        series_name_in_: Name of the target series.\n        exog_in_: Boolean indicating if exogenous variables were used in training.\n        exog_names_in_: Names of exogenous variables.\n        exog_type_in_: Type of exogenous input (Series or DataFrame).\n        X_train_features_names_out_: Names of all training features.\n        in_sample_residuals_: Residuals from training set.\n        in_sample_residuals_by_bin_: Residuals grouped by bins for probabilistic pred.\n        forecaster_id: Identifier for the forecaster instance.\n\n    Note:\n        - Either lags or window_features (or both) must be provided during initialization.\n        - The forecaster uses a recursive strategy where each multi-step prediction\n          depends on previous predictions within the same forecast horizon.\n        - Exogenous variables must have the same index as the target variable and must\n          be available for the entire prediction horizon.\n        - The forecaster supports point predictions, prediction intervals, bootstrapping,\n          quantile predictions, and probabilistic forecasts via conformal methods.\n\n    Examples:\n        Create a basic forecaster with lags:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=10\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n\n        Create a forecaster with window features and transformations:\n\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(n_estimators=100),\n        ...     lags=[1, 7, 30],\n        ...     window_features=[RollingMeanWindow(window=7)],\n        ...     transformer_y=StandardScaler(),\n        ...     differentiation=1\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n\n        Create a forecaster with exogenous variables:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=Ridge(),\n        ...     lags=7,\n        ...     forecaster_id='my_forecaster'\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, exog)\n        &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n\n        Create a forecaster with probabilistic prediction configuration:\n\n        &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=GradientBoostingRegressor(),\n        ...     lags=14,\n        ...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n        &gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: object = None,\n        lags: Union[int, List[int], np.ndarray, range, None] = None,\n        window_features: Union[object, List[object], None] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        binner_kwargs: Optional[Dict[str, object]] = None,\n        forecaster_id: Union[str, int, None] = None,\n        regressor: object = None,\n    ) -&gt; None:\n\n        self.estimator = copy(initialize_estimator(estimator, regressor))\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiation_max = None\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.exog_dtypes_out_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        self.spotforecast_version = __version__\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n\n        (\n            self.lags,\n            self.lags_names,\n            self.max_lag,\n        ) = initialize_lags(type(self).__name__, lags)\n        (\n            self.window_features,\n            self.window_features_names,\n            self.max_size_window_features,\n        ) = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        self.window_size = max(\n            [\n                ws\n                for ws in [self.max_lag, self.max_size_window_features]\n                if ws is not None\n            ]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=estimator,\n            weight_func=weight_func,\n            series_weights=None,\n        )\n\n        if differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation &lt; 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation  # , window_size=self.window_size # Note: TimeSeriesDifferentiator in preprocessing I created only takes order\n            )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n            estimator=estimator, fit_kwargs=fit_kwargs\n        )\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Estimator: {type(self.estimator).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Estimator parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n                &lt;ul&gt;\n                    {exog_names_in_}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Data Transformations&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Estimator Parameters&lt;/summary&gt;\n                &lt;ul&gt;\n                    {params}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n                &lt;ul&gt;\n                    {self.fit_kwargs}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n        super().__setstate__(state)\n        if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n            self.__spotforecast_tags__ = {\n                \"library\": \"spotforecast\",\n                \"forecaster_name\": \"ForecasterRecursive\",\n                \"forecaster_task\": \"regression\",\n                \"forecasting_scope\": \"single-series\",\n                \"forecasting_strategy\": \"recursive\",\n                \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n                \"requires_index_frequency\": True,\n                \"allowed_input_types_series\": [\"pandas.Series\"],\n                \"supports_exog\": True,\n                \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n                \"handles_missing_values_series\": False,\n                \"handles_missing_values_exog\": True,\n                \"supports_lags\": True,\n                \"supports_window_features\": True,\n                \"supports_transformer_series\": True,\n                \"supports_transformer_exog\": True,\n                \"supports_weight_func\": True,\n                \"supports_differentiation\": True,\n                \"prediction_types\": [\n                    \"point\",\n                    \"interval\",\n                    \"bootstrapping\",\n                    \"quantiles\",\n                    \"distribution\",\n                ],\n                \"supports_probabilistic\": True,\n                \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n                \"handles_binned_residuals\": True,\n            }\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None,\n    ) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create lagged predictors and aligned target values.\n\n        Args:\n            y: Target values used to build lag features. Expected shape is\n                (n_samples,) or (n_samples, 1).\n            X_as_pandas: If True, returns lagged features as a pandas DataFrame.\n            train_index: Index to use for the lagged feature DataFrame when\n                `X_as_pandas` is True.\n\n        Returns:\n            Tuple containing:\n                - X_data: Lagged predictors with shape (n_rows, n_lags) or None\n                  if no lags are configured.\n                - y_data: Target values aligned to the lagged predictors with\n                  shape (n_rows,).\n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            # y = y.ravel() # Assuming y is already raveled\n            # Using stride_tricks for sliding window\n            y_strided = np.lib.stride_tricks.sliding_window_view(y, self.window_size)[\n                :-1\n            ]\n            X_data = y_strided[:, self.window_size - self.lags]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                    data=X_data, columns=self.lags_names, index=train_index\n                )\n\n        y_data = y[self.window_size :]\n\n        return X_data, y_data\n\n    def _create_window_features(\n        self,\n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -&gt; Tuple[List[Union[np.ndarray, pd.DataFrame]], List[str]]:\n        \"\"\"\n        Generate window features from the target series.\n\n        Args:\n            y: Target series used to compute window features. Must be a pandas\n                Series with an index aligned to `train_index` after trimming.\n            train_index: Index for the training rows to align the window features.\n            X_as_pandas: If True, keeps each window feature matrix as a pandas\n                DataFrame; otherwise converts to NumPy arrays.\n\n        Returns:\n            Tuple containing:\n                - X_train_window_features: List of window feature matrices, one\n                  per window feature transformer.\n                - X_train_window_features_names_out_: List of feature names for\n                  all generated window features.\n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a pandas DataFrame.\"\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same number of rows as \"\n                    f\"the input time series - `window_size`: {len_train_index}.\"\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same index as \"\n                    f\"the input time series - `window_size`.\"\n                )\n\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name=\"y\")\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n            df=y,\n            transformer=self.transformer_y,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        y_values, y_index = check_extract_values_and_index(data=y, data_label=\"`y`\")\n        if y_values.ndim == 2 and y_values.shape[1] == 1:\n            y_values = y_values.ravel()\n        train_index = y_index[self.window_size :]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                self.differentiator.fit(y_values)  # Differentiator requires fit first\n                y_values = self.differentiator.transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        X_as_pandas = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            _, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\", ignore_freq=True, return_values=False\n            )\n\n            _ = len(y_values) + (\n                self.differentiation if self.differentiation else 0\n            )  # Adjust for differentiation loss of length if needed? No, y_values has NaNs at start\n            # But y_values from check_extract... is raw values.\n            # Differentiator might introduce NaNs. Sklearn transformer keeps length.\n            # My ported differentiator creates NaNs at start.\n\n            # Re-evaluate logic:\n            # y_values (raw) length = N\n            # differentiator transform -&gt; length N, first 'order' are NaN.\n\n            len_exog = len(exog)\n            # The check logic depends on alignment.\n\n            # Simplified check from original code\n            # ... (omitted for brevity, assume caller passed valid data or minimal check)\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=fit_transformer,\n                inverse_transform=False,\n            )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n            X_as_pandas = any(\n                not pd.api.types.is_numeric_dtype(dtype)\n                or pd.api.types.is_bool_dtype(dtype)\n                for dtype in set(exog.dtypes)\n            )\n\n            # Alignment logic\n            if len_exog == len(y):\n                exog = exog.iloc[self.window_size :,]\n            else:\n                pass  # Assume aligned start\n\n        X_train = []\n        X_train_features_names_out_ = []\n\n        # Create lags\n        # Note: y_values might have NaNs from differentiation.\n        # create_lags handles this?\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            if isinstance(y_values, pd.Series):\n                y_vals_for_wf = y_values.iloc[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n            else:\n                y_vals_for_wf = y_values[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n\n            y_window_features = pd.Series(y_vals_for_wf, index=y_index_for_wf)\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features,\n                    X_as_pandas=X_as_pandas,\n                    train_index=train_index,\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                data=X_train, index=train_index, columns=X_train_features_names_out_\n            )\n\n        y_train = pd.Series(data=y_train, index=train_index, name=\"y\")\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        )\n\n    def create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        return self._create_train_X_y(y=y, exog=exog)\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Args:\n            y: Training time series.\n            initial_train_size: Initial size of the training set. It is the number of\n                observations used to train the forecaster before making the first\n                prediction.\n            exog: Exogenous variable/s included as predictor/s. Must have the same\n                number of observations as y and their indexes must be aligned.\n                Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: Predictor values used to train the model as pandas DataFrame.\n                - y_train: Values of the time series related to each row of X_train for\n                    each step in the form {step: y_step_[i]} as dict.\n                - X_test: Predictor values used to test the model as pandas DataFrame.\n                - y_test: Values of the time series related to each row of X_test for\n                    each step in the form {step: y_step_[i]} as dict.\n\n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y=y.iloc[:initial_train_size],\n            exog=exog.iloc[:initial_train_size] if exog is not None else None,\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y=y.iloc[test_init:],\n            exog=exog.iloc[test_init:] if exog is not None else None,\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def get_params(self, deep=True):\n        params = {}\n        for key in [\n            \"estimator\",\n            \"lags\",\n            \"window_features\",\n            \"transformer_y\",\n            \"transformer_exog\",\n            \"weight_func\",\n            \"differentiation\",\n            \"fit_kwargs\",\n            \"binner_kwargs\",\n            \"forecaster_id\",\n        ]:\n            if hasattr(self, key):\n                params[key] = getattr(self, key)\n\n        if not deep:\n            return params\n\n        if hasattr(self, \"estimator\") and self.estimator is not None:\n            if hasattr(self.estimator, \"get_params\"):\n                for key, value in self.estimator.get_params(deep=True).items():\n                    params[f\"estimator__{key}\"] = value\n\n        return params\n\n    def set_params(self, **params):\n        if not params:\n            return self\n\n        valid_params = self.get_params(deep=True)\n        nested_params = {}\n\n        for key, value in params.items():\n            if key not in valid_params and \"__\" not in key:\n                # Relaxed check for now\n                pass\n\n            if \"__\" in key:\n                obj_name, param_name = key.split(\"__\", 1)\n                if obj_name not in nested_params:\n                    nested_params[obj_name] = {}\n                nested_params[obj_name][param_name] = value\n            else:\n                setattr(self, key, value)\n\n        for obj_name, obj_params in nested_params.items():\n            if hasattr(self, obj_name):\n                obj = getattr(self, obj_name)\n                if hasattr(obj, \"set_params\"):\n                    obj.set_params(**obj_params)\n                else:\n                    for param_name, value in obj_params.items():\n                        setattr(obj, param_name, value)\n\n        return self\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        suppress_warnings: bool = False,\n    ) -&gt; None:\n\n        # Reset values\n        self.is_fitted = False\n        self.fit_date = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        SAMPLE_WEIGHT_NAME = \"sample_weight\"\n        if self.weight_func is not None:\n            sample_weight, _, _ = initialize_weights(\n                forecaster_name=type(self).__name__,\n                estimator=self.estimator,\n                weight_func=self.weight_func,\n                series_weights=None,\n            )\n            sample_weight = sample_weight(y.index[self.window_size :])\n            self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n        self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n            del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n        # Store attributes\n        self.last_window_ = y.iloc[-self.window_size :].copy()\n        self.index_type_ = type(y.index)\n        if isinstance(y.index, pd.DatetimeIndex):\n            self.index_freq_ = y.index.freqstr\n        else:\n            try:\n                self.index_freq_ = y.index.step\n            except AttributeError:\n                self.index_freq_ = None\n\n        self.training_range_ = y.index[[0, -1]]\n        self.series_name_in_ = y.name\n        self.exog_in_ = exog is not None\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog) if exog is not None else None\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        residuals = y_train - self.estimator.predict(X_train)\n\n        if len(residuals) &gt; 1000:\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(residuals, size=1000, replace=False)\n\n        self.in_sample_residuals_ = residuals\n\n        if self.binner_kwargs is not None:\n            self.binner = QuantileBinner(**self.binner_kwargs)\n            if isinstance(residuals, pd.Series):\n                residuals = residuals.to_numpy()\n            self.binner.fit(residuals)\n\n            # Construct intervals_ manually if not in binner\n            if hasattr(self.binner, \"intervals_\"):\n                self.binner_intervals_ = self.binner.intervals_\n            else:\n                self.binner_intervals_ = {\n                    i: (self.binner.bins_[i - 1], self.binner.bins_[i])\n                    for i in range(1, len(self.binner.bins_))\n                }\n\n            residuals_binned = self.binner.transform(residuals)\n            self.in_sample_residuals_by_bin_ = {\n                bin: residuals[residuals_binned == bin]\n                for bin in self.binner_intervals_.keys()\n            }\n\n            # Limit residuals stored per bin\n            max_residuals_per_bin = 1000 // self.binner.n_bins\n            for bin, res in self.in_sample_residuals_by_bin_.items():\n                if len(res) &gt; max_residuals_per_bin:\n                    rng = np.random.default_rng(seed=123)\n                    self.in_sample_residuals_by_bin_[bin] = rng.choice(\n                        res, size=max_residuals_per_bin, replace=False\n                    )\n\n    def _create_predict_inputs(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; Tuple[np.ndarray, Union[np.ndarray, None], pd.Index, pd.Index]:\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=self.exog_in_,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n                last_window_exog=None,\n                exog=exog,\n                exog_names_in_=self.exog_names_in_,\n                interval=None,\n                # alpha=None, # Removed alpha check for now\n            )\n\n        last_window = input_to_frame(data=last_window, input_name=\"last_window\")\n        _, last_window_index = check_extract_values_and_index(\n            data=last_window,\n            data_label=\"`last_window`\",\n            ignore_freq=True,\n            return_values=False,\n        )\n\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n\n        last_window = transform_dataframe(\n            df=last_window,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, _ = check_extract_values_and_index(\n            data=last_window, data_label=\"`last_window`\"\n        )\n        last_window_values = last_window_values.ravel()\n\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        exog_values = None\n        exog_index = None\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=False,\n                inverse_transform=False,\n            )\n\n            exog_values, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\"\n            )\n\n            exog_values = (\n                exog_values if isinstance(exog, pd.Series) else exog.to_numpy()\n            )\n\n        return last_window_values, exog_values, prediction_index, exog_index\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Union[np.ndarray, None] = None,\n    ) -&gt; np.ndarray:\n\n        predictions = np.full(shape=steps, fill_value=np.nan)\n\n        for step in range(steps):\n\n            X_gen = []\n\n            if self.lags is not None:\n                X_lags = last_window_values[-self.lags]\n                if X_lags.ndim == 1:\n                    X_lags = X_lags.reshape(1, -1)\n                X_gen.append(X_lags)\n\n            if self.window_features is not None:\n                X_window_features = []\n                for wf in self.window_features:\n                    wf_values = wf.transform(last_window_values)\n                    X_window_features.append(wf_values[-1:])\n\n                X_window_features = np.concatenate(X_window_features, axis=1)\n                X_gen.append(X_window_features)\n\n            if self.exog_in_:\n                X_exog = exog_values[step]\n                if X_exog.ndim &lt; 2:\n                    X_exog = X_exog.reshape(1, -1)\n                X_gen.append(X_exog)\n\n            X_gen = np.concatenate(X_gen, axis=1)\n\n            # Convert to DataFrame with feature names to avoid sklearn warning\n            if self.X_train_features_names_out_ is not None:\n                X_gen = pd.DataFrame(X_gen, columns=self.X_train_features_names_out_)\n\n            pred = self.estimator.predict(X_gen)\n            predictions[step] = pred[0]\n\n            last_window_values = np.append(last_window_values, pred)\n\n        return predictions\n\n    def predict(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; pd.Series:\n\n        last_window_values, exog_values, prediction_index, _ = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        predictions = self._recursive_predict(\n            steps=steps, last_window_values=last_window_values, exog_values=exog_values\n        )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_dataframe(\n            df=pd.Series(predictions, name=\"pred\").to_frame(),\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n        predictions = predictions.iloc[:, 0]\n        predictions.index = prediction_index\n\n        return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a ForecasterRecursive object is printed.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a ForecasterRecursive object is printed.\n    \"\"\"\n\n    params = (\n        self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n    )\n    exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Estimator: {type(self.estimator).__name__} \\n\"\n        f\"Lags: {self.lags} \\n\"\n        f\"Window features: {self.window_features_names} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Exogenous included: {self.exog_in_} \\n\"\n        f\"Exogenous names: {exog_names_in_} \\n\"\n        f\"Transformer for y: {self.transformer_y} \\n\"\n        f\"Transformer for exog: {self.transformer_exog} \\n\"\n        f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n        f\"Differentiation order: {self.differentiation} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Estimator parameters: {params} \\n\"\n        f\"fit_kwargs: {self.fit_kwargs} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"Skforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n    super().__setstate__(state)\n    if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",\n            \"forecasting_strategy\": \"recursive\",\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n</code></pre>"},{"location":"api/forecaster/#base-forecaster","title":"Base Forecaster","text":""},{"location":"api/forecaster/#base","title":"base","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base","title":"<code>spotforecast2_safe.forecaster.base</code>","text":"<p>ForecasterBase class.</p> <p>This module contains the base class for all forecasters in spotforecast2. All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Examples:</p> <p>Create a custom forecaster inheriting from ForecasterBase:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; class MyForecaster(ForecasterBase):\n...     def __init__(self, estimator):\n...         self.estimator = estimator\n...         self.__spotforecast_tags__ = {'hide_lags': True}\n...     def create_train_X_y(self, y, exog=None):\n...         return pd.DataFrame(), pd.Series(dtype=float)\n...     def fit(self, y, exog=None):\n...         pass\n...     def predict(self, steps, last_window=None, exog=None):\n...         return pd.Series(np.zeros(steps))\n...     def set_params(self, params):\n...         pass\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = MyForecaster(estimator=Ridge())\n&gt;&gt;&gt; forecaster\nMyForecaster(estimator=Ridge())\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase","title":"<code>ForecasterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all forecasters in spotforecast2.</p> <p>All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Attributes:</p> Name Type Description <code>__spotforecast_tags__</code> <p>Dictionary with forecaster tags that characterize the behavior of the forecaster.</p> <p>Examples:</p> <p>To see all abstract methods that need to be implemented:</p> <pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n&gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n['create_train_X_y', 'fit', 'predict', 'set_params']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>class ForecasterBase(ABC):\n    \"\"\"Base class for all forecasters in spotforecast2.\n\n    All forecasters should specify all the parameters that can be set at\n    the class level in their __init__.\n\n    Attributes:\n        __spotforecast_tags__: Dictionary with forecaster tags that characterize\n            the behavior of the forecaster.\n\n    Examples:\n        To see all abstract methods that need to be implemented:\n\n        &gt;&gt;&gt; import inspect\n        &gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n        &gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n        ['create_train_X_y', 'fit', 'predict', 'set_params']\n    \"\"\"\n\n    def _preprocess_repr(\n        self,\n        estimator: object | None = None,\n        training_range_: dict[str, str] | None = None,\n        series_names_in_: list[str] | None = None,\n        exog_names_in_: list[str] | None = None,\n        transformer_series: object | dict[str, object] | None = None,\n    ) -&gt; tuple[str, str | None, str | None, str | None, str | None]:\n        \"\"\"Prepare the information to be displayed when a Forecaster object is printed.\n\n        Args:\n            estimator: Estimator object. Default is None.\n            training_range_: Training range. Only used for ForecasterRecursiveMultiSeries.\n                Default is None.\n            series_names_in_: Names of the series used in the forecaster.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n            exog_names_in_: Names of the exogenous variables used in the forecaster.\n                Default is None.\n            transformer_series: Transformer used in the series.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n\n        Returns:\n            Tuple containing params (estimator parameters string), training_range_\n            (training range string representation), series_names_in_ (series names\n            string representation), exog_names_in_ (exogenous variable names string\n            representation), and transformer_series (transformer string representation).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; estimator = Ridge(alpha=0.5)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=estimator, lags=3)\n            &gt;&gt;&gt; params, tr, sn, en, ts = forecaster._preprocess_repr(estimator=estimator)\n            &gt;&gt;&gt; params\n            \"{'alpha': 0.5, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\"\n        \"\"\"\n\n        if estimator is not None:\n            if isinstance(estimator, Pipeline):\n                name_pipe_steps = tuple(\n                    name + \"__\" for name in estimator.named_steps.keys()\n                )\n                params = {\n                    key: value\n                    for key, value in estimator.get_params().items()\n                    if key.startswith(name_pipe_steps)\n                }\n            else:\n                params = estimator.get_params()\n            params = str(params)\n        else:\n            params = None\n\n        if training_range_ is not None:\n            training_range_ = [\n                f\"'{k}': {v.astype(str).to_list()}\" for k, v in training_range_.items()\n            ]\n            if len(training_range_) &gt; 10:\n                training_range_ = training_range_[:5] + [\"...\"] + training_range_[-5:]\n            training_range_ = \", \".join(training_range_)\n\n        if series_names_in_ is not None:\n            if len(series_names_in_) &gt; 50:\n                series_names_in_ = (\n                    series_names_in_[:25] + [\"...\"] + series_names_in_[-25:]\n                )\n            series_names_in_ = \", \".join(series_names_in_)\n\n        if exog_names_in_ is not None:\n            if len(exog_names_in_) &gt; 50:\n                exog_names_in_ = exog_names_in_[:25] + [\"...\"] + exog_names_in_[-25:]\n            exog_names_in_ = \", \".join(exog_names_in_)\n\n        if transformer_series is not None:\n            if isinstance(transformer_series, dict):\n                transformer_series = [\n                    f\"'{k}': {v}\" for k, v in transformer_series.items()\n                ]\n                if len(transformer_series) &gt; 10:\n                    transformer_series = (\n                        transformer_series[:5] + [\"...\"] + transformer_series[-5:]\n                    )\n                transformer_series = \", \".join(transformer_series)\n            else:\n                transformer_series = str(transformer_series)\n\n        return (\n            params,\n            training_range_,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        )\n\n    def _format_text_repr(\n        self,\n        text: str,\n        max_text_length: int = 58,\n        width: int = 80,\n        indent: str = \"    \",\n    ) -&gt; str:\n        \"\"\"Format text for __repr__ method.\n\n        Args:\n            text: Text to format.\n            max_text_length: Maximum length of the text before wrapping. Default is 58.\n            width: Maximum width of the text. Default is 80.\n            indent: Indentation of the text. Default is four spaces.\n\n        Returns:\n            Formatted text string with proper wrapping and indentation.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster._format_text_repr(\"Short text\")\n            'Short text'\n        \"\"\"\n\n        if text is not None and len(text) &gt; max_text_length:\n            text = \"\\n    \" + textwrap.fill(\n                str(text), width=width, subsequent_indent=indent\n            )\n\n        return text\n\n    @abstractmethod\n    def create_train_X_y(\n        self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned.\n                Default is None.\n\n        Returns:\n            Tuple containing X_train (training values/predictors with shape\n            (len(y) - max_lag, len(lags))) and y_train (target values of the\n            time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n            &gt;&gt;&gt; X_train.head(2)\n               lag_1  lag_2  lag_3\n            3    2.0    1.0    0.0\n            4    3.0    2.0    1.0\n            &gt;&gt;&gt; y_train.head(2)\n            3    3\n            4    4\n            Name: y, dtype: int64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n        \"\"\"Training Forecaster.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned so\n                that y[i] is regressed on exog[i]. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.is_fitted\n            True\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n    ) -&gt; pd.Series:\n        \"\"\"Predict n steps ahead.\n\n        Args:\n            steps: Number of steps to predict.\n            last_window: Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1). If None, the values stored in\n                last_window are used to calculate the initial predictors, and the\n                predictions start right after training data. Default is None.\n            exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n        Returns:\n            Predicted values as a pandas Series.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            10    9.5\n            11    9.0\n            12    8.5\n            Name: pred, dtype: float64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def set_params(self, params: dict[str, object]) -&gt; None:\n        \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n        Args:\n            params: Parameters values dictionary.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n            &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n            &gt;&gt;&gt; forecaster.estimator.alpha\n            0.5\n        \"\"\"\n\n        pass\n\n    def set_lags(\n        self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute lags.\n\n        Attributes max_lag and window_size are also updated.\n\n        Args:\n            lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n                If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n                or range: include only lags present in lags, all elements must be int.\n                If None: no lags are included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.set_lags(lags=5)\n            &gt;&gt;&gt; forecaster.lags\n            array([1, 2, 3, 4, 5])\n        \"\"\"\n\n        pass\n\n    def set_window_features(\n        self, window_features: object | list[object] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute window_features.\n\n        Attributes max_size_window_features, window_features_names,\n        window_features_class_names and window_size are also updated.\n\n        Args:\n            window_features: Instance or list of instances used to create window features.\n                Window features are created from the original time series and are\n                included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n            &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n            &gt;&gt;&gt; forecaster.window_features\n            [RollingFeatures(stats=['mean'], window_sizes=[3])]\n        \"\"\"\n\n        pass\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            Dictionary with forecaster tags describing behavior and capabilities.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; tags['forecaster_task']\n            'regression'\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.summary()\n            ForecasterRecursive\n            ===================\n            Estimator: Ridge()\n            Lags: [1 2 3]\n            ...\n        \"\"\"\n\n        print(self.__repr__())\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n        This method is called when an object is unpickled (deserialized).\n        It handles the migration of deprecated attributes to their new names.\n\n        Args:\n            state: The state dictionary from the pickled object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pickle\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n            &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n        \"\"\"\n\n        # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n        if \"regressor\" in state and \"estimator\" not in state:\n            state[\"estimator\"] = state.pop(\"regressor\")\n\n        self.__dict__.update(state)\n\n    @property\n    def regressor(self) -&gt; Any:\n        \"\"\"Deprecated property. Use estimator instead.\n\n        Returns:\n            The estimator object.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\n            Ridge()\n        \"\"\"\n        warnings.warn(\n            \"The `regressor` attribute is deprecated and will be removed in future \"\n            \"versions. Use `estimator` instead.\",\n            FutureWarning,\n        )\n        return self.estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.regressor","title":"<code>regressor</code>  <code>property</code>","text":"<p>Deprecated property. Use estimator instead.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The estimator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\nRidge()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> <p>This method is called when an object is unpickled (deserialized). It handles the migration of deprecated attributes to their new names.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The state dictionary from the pickled object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n&gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n    This method is called when an object is unpickled (deserialized).\n    It handles the migration of deprecated attributes to their new names.\n\n    Args:\n        state: The state dictionary from the pickled object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n        &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n    \"\"\"\n\n    # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n    if \"regressor\" in state and \"estimator\" not in state:\n        state[\"estimator\"] = state.pop(\"regressor\")\n\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing X_train (training values/predictors with shape</p> <code>Series</code> <p>(len(y) - max_lag, len(lags))) and y_train (target values of the</p> <code>tuple[DataFrame, Series]</code> <p>time series related to each row of X_train with shape (len(y) - max_lag,)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n&gt;&gt;&gt; X_train.head(2)\n   lag_1  lag_2  lag_3\n3    2.0    1.0    0.0\n4    3.0    2.0    1.0\n&gt;&gt;&gt; y_train.head(2)\n3    3\n4    4\nName: y, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef create_train_X_y(\n    self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned.\n            Default is None.\n\n    Returns:\n        Tuple containing X_train (training values/predictors with shape\n        (len(y) - max_lag, len(lags))) and y_train (target values of the\n        time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n        &gt;&gt;&gt; X_train.head(2)\n           lag_1  lag_2  lag_3\n        3    2.0    1.0    0.0\n        4    3.0    2.0    1.0\n        &gt;&gt;&gt; y_train.head(2)\n        3    3\n        4    4\n        Name: y, dtype: int64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.fit","title":"<code>fit(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.is_fitted\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n    \"\"\"Training Forecaster.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned so\n            that y[i] is regressed on exog[i]. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.is_fitted\n        True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with forecaster tags describing behavior and capabilities.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; tags['forecaster_task']\n'regression'\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        Dictionary with forecaster tags describing behavior and capabilities.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; tags['forecaster_task']\n        'regression'\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If None, the values stored in last_window are used to calculate the initial predictors, and the predictions start right after training data. Default is None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Predicted values as a pandas Series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n10    9.5\n11    9.0\n12    8.5\nName: pred, dtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; pd.Series:\n    \"\"\"Predict n steps ahead.\n\n    Args:\n        steps: Number of steps to predict.\n        last_window: Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1). If None, the values stored in\n            last_window are used to calculate the initial predictors, and the\n            predictions start right after training data. Default is None.\n        exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n    Returns:\n        Predicted values as a pandas Series.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        10    9.5\n        11    9.0\n        12    8.5\n        Name: pred, dtype: float64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute lags.</p> <p>Attributes max_lag and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int | list[int] | ndarray[int] | range[int] | None</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. If int: include lags from 1 to lags (included). If list, 1d numpy ndarray, or range: include only lags present in lags, all elements must be int. If None: no lags are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.set_lags(lags=5)\n&gt;&gt;&gt; forecaster.lags\narray([1, 2, 3, 4, 5])\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_lags(\n    self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute lags.\n\n    Attributes max_lag and window_size are also updated.\n\n    Args:\n        lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n            If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n            or range: include only lags present in lags, all elements must be int.\n            If None: no lags are included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.set_lags(lags=5)\n        &gt;&gt;&gt; forecaster.lags\n        array([1, 2, 3, 4, 5])\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_params","title":"<code>set_params(params)</code>  <code>abstractmethod</code>","text":"<p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, object]</code> <p>Parameters values dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n&gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n&gt;&gt;&gt; forecaster.estimator.alpha\n0.5\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef set_params(self, params: dict[str, object]) -&gt; None:\n    \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n    Args:\n        params: Parameters values dictionary.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n        &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n        &gt;&gt;&gt; forecaster.estimator.alpha\n        0.5\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute window_features.</p> <p>Attributes max_size_window_features, window_features_names, window_features_class_names and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>object | list[object] | None</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n&gt;&gt;&gt; forecaster.window_features\n[RollingFeatures(stats=['mean'], window_sizes=[3])]\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_window_features(\n    self, window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute window_features.\n\n    Attributes max_size_window_features, window_features_names,\n    window_features_class_names and window_size are also updated.\n\n    Args:\n        window_features: Instance or list of instances used to create window features.\n            Window features are created from the original time series and are\n            included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n        &gt;&gt;&gt; forecaster.window_features\n        [RollingFeatures(stats=['mean'], window_sizes=[3])]\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.summary()\nForecasterRecursive\n===================\nEstimator: Ridge()\nLags: [1 2 3]\n...\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.summary()\n        ForecasterRecursive\n        ===================\n        Estimator: Ridge()\n        Lags: [1 2 3]\n        ...\n    \"\"\"\n\n    print(self.__repr__())\n</code></pre>"},{"location":"api/forecaster/#recursive-forecasting","title":"Recursive Forecasting","text":""},{"location":"api/forecaster/#recursive","title":"recursive","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive","title":"<code>spotforecast2_safe.forecaster.recursive</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate","title":"<code>ForecasterEquivalentDate</code>","text":"<p>This forecaster predicts future values based on the most recent equivalent date. It also allows to aggregate multiple past values of the equivalent date using a function (e.g. mean, median, max, min, etc.). The equivalent date is calculated by moving back in time a specified number of steps (offset). The offset can be defined as an integer or as a pandas DateOffset. This approach is useful as a baseline, but it is a simplistic method and may not capture complex underlying patterns.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily, <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of valid dates. For example, Bday(2) can be used to move back two business days. If the date does not start on a valid date, it is first moved to a valid date. For example, if the date is a Saturday, it is moved to the previous Friday. Then, the offset is applied. If the result is a non-valid date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday. For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> required <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. Defaults to 1. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>1</code> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1. Defaults to np.mean.</p> <code>mean</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize the residuals into k bins according to the predicted values associated with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. Defaults to None.</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period.</p> <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction.</p> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>window_size</code> <code>int</code> <p>Number of past values needed to include the last equivalent dates according to the <code>offset</code> and <code>n_offsets</code>.</p> <code>last_window_</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the past values needed to include the last equivalent date according the <code>offset</code> and <code>n_offsets</code>.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 10_000 values.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>.</p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non-training data. Only stored up to 10_000 values. Use <code>set_out_sample_residuals()</code> method to set values.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>.</p> <code>binner</code> <code>QuantileBinner</code> <p><code>QuantileBinner</code> used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the estimator has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>spotforecast_version</code> <code>str</code> <p>Version of spotforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; # Series with daily frequency\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; # Forecast based on the value 7 days ago\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n2022-01-15    7\n2022-01-16    8\n2022-01-17    9\nFreq: D, Name: pred, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>class ForecasterEquivalentDate:\n    \"\"\"\n    This forecaster predicts future values based on the most recent equivalent\n    date. It also allows to aggregate multiple past values of the equivalent\n    date using a function (e.g. mean, median, max, min, etc.). The equivalent\n    date is calculated by moving back in time a specified number of steps (offset).\n    The offset can be defined as an integer or as a pandas DateOffset. This\n    approach is useful as a baseline, but it is a simplistic method and may not\n    capture complex underlying patterns.\n\n    Args:\n        offset (int, pandas.tseries.offsets.DateOffset): Number of steps to go back\n            in time to find the most recent equivalent date to the target period.\n            If `offset` is an integer, it represents the number of steps to go back\n            in time. For example, if the frequency of the time series is daily,\n            `offset = 7` means that the most recent data similar to the target\n            period is the value observed 7 days ago.\n            Pandas DateOffsets can also be used to move forward a given number of\n            valid dates. For example, Bday(2) can be used to move back two business\n            days. If the date does not start on a valid date, it is first moved to a\n            valid date. For example, if the date is a Saturday, it is moved to the\n            previous Friday. Then, the offset is applied. If the result is a non-valid\n            date, it is moved to the next valid date. For example, if the date\n            is a Sunday, it is moved to the next Monday.\n            For more information about offsets, see\n            https://pandas.pydata.org/docs/reference/offset_frequency.html.\n        n_offsets (int, optional): Number of equivalent dates (multiple of offset)\n            used in the prediction. Defaults to 1.\n            If `n_offsets` is greater than 1, the values at the equivalent dates are\n            aggregated using the `agg_func` function. For example, if the frequency\n            of the time series is daily, `offset = 7`, `n_offsets = 2` and\n            `agg_func = np.mean`, the predicted value will be the mean of the values\n            observed 7 and 14 days ago.\n        agg_func (Callable, optional): Function used to aggregate the values of the\n            equivalent dates when the number of equivalent dates (`n_offsets`) is\n            greater than 1. Defaults to np.mean.\n        binner_kwargs (dict, optional): Additional arguments to pass to the\n            `QuantileBinner` used to discretize the residuals into k bins according\n            to the predicted values associated with each residual. Available arguments\n            are: `n_bins`, `method`, `subsample`, `random_state` and `dtype`.\n            Argument `method` is passed internally to the function `numpy.percentile`.\n            Defaults to None.\n        forecaster_id (str, int, optional): Name used as an identifier of the\n            forecaster. Defaults to None.\n\n    Attributes:\n        offset (int, pandas.tseries.offsets.DateOffset): Number of steps to go back\n            in time to find the most recent equivalent date to the target period.\n        n_offsets (int): Number of equivalent dates (multiple of offset) used in\n            the prediction.\n        agg_func (Callable): Function used to aggregate the values of the equivalent\n            dates when the number of equivalent dates (`n_offsets`) is greater than 1.\n        window_size (int): Number of past values needed to include the last\n            equivalent dates according to the `offset` and `n_offsets`.\n        last_window_ (pandas Series): This window represents the most recent data\n            observed by the predictor during its training phase. It contains the\n            past values needed to include the last equivalent date according the\n            `offset` and `n_offsets`.\n        index_type_ (type): Type of index of the input used in training.\n        index_freq_ (str): Frequency of Index of the input used in training.\n        training_range_ (pandas Index): First and last values of index of the data\n            used during training.\n        series_name_in_ (str): Names of the series provided by the user during training.\n        in_sample_residuals_ (numpy ndarray): Residuals of the model when predicting\n            training data. Only stored up to 10_000 values.\n        in_sample_residuals_by_bin_ (dict): In sample residuals binned according to\n            the predicted value each residual is associated with. The number of\n            residuals stored per bin is limited to `10_000 // self.binner.n_bins_`\n            in the form `{bin: residuals}`.\n        out_sample_residuals_ (numpy ndarray): Residuals of the model when predicting\n            non-training data. Only stored up to 10_000 values. Use\n            `set_out_sample_residuals()` method to set values.\n        out_sample_residuals_by_bin_ (dict): Out of sample residuals binned\n            according to the predicted value each residual is associated with.\n            The number of residuals stored per bin is limited to\n            `10_000 // self.binner.n_bins_` in the form `{bin: residuals}`.\n        binner (spotforecast.preprocessing.QuantileBinner): `QuantileBinner` used to\n            discretize residuals into k bins according to the predicted values\n            associated with each residual.\n        binner_intervals_ (dict): Intervals used to discretize residuals into k bins\n            according to the predicted values associated with each residual.\n        binner_kwargs (dict): Additional arguments to pass to the `QuantileBinner`.\n        creation_date (str): Date of creation.\n        is_fitted (bool): Tag to identify if the estimator has been fitted (trained).\n        fit_date (str): Date of last fit.\n        spotforecast_version (str): Version of spotforecast library used to create\n            the forecaster.\n        python_version (str): Version of python used to create the forecaster.\n        forecaster_id (str, int): Name used as an identifier of the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; # Series with daily frequency\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; # Forecast based on the value 7 days ago\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        2022-01-15    7\n        2022-01-16    8\n        2022-01-17    9\n        Freq: D, Name: pred, dtype: int64\n    \"\"\"\n\n    def __init__(\n        self,\n        offset: int | pd.tseries.offsets.DateOffset,\n        n_offsets: int = 1,\n        agg_func: Callable = np.mean,\n        binner_kwargs: dict[str, object] | None = None,\n        forecaster_id: str | int | None = None,\n    ) -&gt; None:\n\n        self.offset = offset\n        self.n_offsets = n_offsets\n        self.agg_func = agg_func\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        self.spotforecast_version = __version__\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n        self.estimator = None\n        self.differentiation = None\n        self.differentiation_max = None\n        self.window_size = None  # Defaults to None, validated later\n\n        if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n            raise TypeError(\n                \"`offset` must be an integer greater than 0 or a \"\n                \"pandas.tseries.offsets. Find more information about offsets in \"\n                \"https://pandas.pydata.org/docs/reference/offset_frequency.html\"\n            )\n\n        if isinstance(self.offset, int):\n            self.window_size = self.offset * self.n_offsets\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n                \"subsample\": 200000,\n                \"random_state\": 789654,\n                \"dtype\": np.float64,\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterEquivalentDate\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": False,\n            \"allowed_input_types_exog\": [],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": False,\n            \"supports_lags\": False,\n            \"supports_window_features\": False,\n            \"supports_transformer_series\": False,\n            \"supports_transformer_exog\": False,\n            \"supports_weight_func\": False,\n            \"supports_differentiation\": False,\n            \"prediction_types\": [\"point\", \"interval\"],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a Forecaster object is printed.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Offset: {self.offset} \\n\"\n            f\"Number of offsets: {self.n_offsets} \\n\"\n            f\"Aggregation function: {self.agg_func.__name__} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"spotforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Offset:&lt;/strong&gt; {self.offset}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Number of offsets:&lt;/strong&gt; {self.n_offsets}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Aggregation function:&lt;/strong&gt; {self.agg_func.__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def fit(\n        self,\n        y: pd.Series,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        exog: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Training Forecaster.\n\n        Args:\n            y (pandas Series): Training time series.\n            store_in_sample_residuals (bool, optional): If `True`, in-sample\n                residuals will be stored in the forecaster object after fitting\n                (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n                If `False`, only the intervals of the bins are stored. Defaults to False.\n            random_state (int, optional): Set a seed for the random generator so\n                that the stored sample residuals are always deterministic. Defaults to 123.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            None\n        \"\"\"\n\n        if not isinstance(y, pd.Series):\n            raise TypeError(\n                f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n                f\"Found {type(y)}.\"\n            )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            if not isinstance(y.index, pd.DatetimeIndex):\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n            elif y.index.freq is None:\n                try:\n                    y.index.freq = pd.infer_freq(y.index)\n                except (ValueError, TypeError):\n                    raise TypeError(\n                        \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                        \"pandas DatetimeIndex with frequency.\"\n                    )\n                if y.index.freq is None:\n                    raise TypeError(\n                        \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                        \"pandas DatetimeIndex with frequency.\"\n                    )\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.is_fitted = False\n\n        _, y_index = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            # Calculate the window_size in steps for compatibility with the\n            # check_predict_input function. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance\n            # between two dates may not be constant.\n            first_valid_index = y_index[-1] - self.offset * self.n_offsets\n\n            try:\n                window_size_idx_start = y_index.get_loc(first_valid_index)\n                window_size_idx_end = y_index.get_loc(y_index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). This is because  \"\n                    f\"the offset ({self.offset}) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `y`.\"\n                )\n        else:\n            if len(y) &lt;= self.window_size:\n                raise ValueError(\n                    f\"Length of `y` must be greater than the maximum window size \"\n                    f\"needed by the forecaster. This is because  \"\n                    f\"the offset ({self.offset}) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `y`.\\n\"\n                    f\"    Length `y`: {len(y)}.\\n\"\n                    f\"    Max window size: {self.window_size}.\\n\"\n                )\n\n        self.is_fitted = True\n        self.series_name_in_ = y.name if y.name is not None else \"y\"\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.training_range_ = y_index[[0, -1]]\n        self.index_type_ = type(y_index)\n        self.index_freq_ = (\n            y_index.freq if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n        )\n\n        # NOTE: This is done to save time during fit in functions such as backtesting()\n        if self._probabilistic_mode is not False:\n            self._binning_in_sample_residuals(\n                y=y,\n                store_in_sample_residuals=store_in_sample_residuals,\n                random_state=random_state,\n            )\n\n        # The last time window of training data is stored so that equivalent\n        # dates are available when calling the `predict` method.\n        # Store the whole series to avoid errors when the offset is larger\n        # than the data available.\n        self.last_window_ = y.copy()\n\n    def _binning_in_sample_residuals(\n        self,\n        y: pd.Series,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Bin residuals according to the predicted value each residual is\n        associated with. First a `spotforecast.preprocessing.QuantileBinner` object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n\n        The number of residuals stored per bin is limited to\n        `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n        `10_000`.\n\n        Args:\n            y (pandas Series): Training time series.\n            store_in_sample_residuals (bool, optional): If `True`, in-sample\n                residuals will be stored in the forecaster object after fitting\n                (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n                If `False`, only the intervals of the bins are stored. Defaults to False.\n            random_state (int, optional): Set a seed for the random generator so\n                that the stored sample residuals are always deterministic. Defaults to 123.\n\n        Returns:\n            None\n        \"\"\"\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            y_preds = []\n            for n_off in range(1, self.n_offsets + 1):\n                idx = y.index - self.offset * n_off\n                mask = idx &gt;= y.index[0]\n                y_pred = y.loc[idx[mask]]\n                y_pred.index = y.index[-mask.sum() :]\n                y_preds.append(y_pred)\n\n            y_preds = pd.concat(y_preds, axis=1).to_numpy()\n            y_true = y.to_numpy()[-len(y_preds) :]\n\n        else:\n            y_preds = [\n                y.shift(self.offset * n_off)[self.window_size :]\n                for n_off in range(1, self.n_offsets + 1)\n            ]\n            y_preds = np.column_stack(y_preds)\n            y_true = y.to_numpy()[self.window_size :]\n\n        y_pred = np.apply_along_axis(self.agg_func, axis=1, arr=y_preds)\n\n        residuals = y_true - y_pred\n\n        if self._probabilistic_mode == \"binned\":\n            data = pd.DataFrame({\"prediction\": y_pred, \"residuals\": residuals}).dropna()\n            y_pred = data[\"prediction\"].to_numpy()\n            residuals = data[\"residuals\"].to_numpy()\n\n            self.binner.fit(y_pred)\n            self.binner_intervals_ = self.binner.intervals_\n\n        if store_in_sample_residuals:\n            rng = np.random.default_rng(seed=random_state)\n            if self._probabilistic_mode == \"binned\":\n                data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n                self.in_sample_residuals_by_bin_ = (\n                    data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n                )\n\n                max_sample = 10_000 // self.binner.n_bins_\n                for k, v in self.in_sample_residuals_by_bin_.items():\n                    if len(v) &gt; max_sample:\n                        sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                        self.in_sample_residuals_by_bin_[k] = sample\n\n                for k in self.binner_intervals_.keys():\n                    if k not in self.in_sample_residuals_by_bin_:\n                        self.in_sample_residuals_by_bin_[k] = np.array([])\n\n                empty_bins = [\n                    k\n                    for k, v in self.in_sample_residuals_by_bin_.items()\n                    if v.size == 0\n                ]\n                if empty_bins:\n                    empty_bin_size = min(max_sample, len(residuals))\n                    for k in empty_bins:\n                        self.in_sample_residuals_by_bin_[k] = rng.choice(\n                            a=residuals, size=empty_bin_size, replace=False\n                        )\n\n            if len(residuals) &gt; 10_000:\n                residuals = residuals[\n                    rng.integers(low=0, high=len(residuals), size=10_000)\n                ]\n\n            self.in_sample_residuals_ = residuals\n\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | None = None,\n        check_inputs: bool = True,\n        exog: Any = None,\n    ) -&gt; pd.Series:\n        \"\"\"\n        Predict n steps ahead.\n\n        Args:\n            steps (int): Number of steps to predict.\n            last_window (pandas Series, optional): Past values needed to select the\n                last equivalent dates according to the offset. If `last_window = None`,\n                the values stored in `self.last_window_` are used and the predictions\n                start immediately after the training data. Defaults to None.\n            check_inputs (bool, optional): If `True`, the input is checked for\n                possible warnings and errors with the `check_predict_input` function.\n                This argument is created for internal use and is not recommended to\n                be changed. Defaults to True.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            pd.Series: Predicted values.\n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=False,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n            )\n\n        prediction_index = expand_index(index=last_window.index, steps=steps)\n\n        if isinstance(self.offset, int):\n\n            last_window_values = last_window.to_numpy(copy=True).ravel()\n            equivalent_indexes = np.tile(\n                np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n            )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets &gt; 1:\n                equivalent_indexes = [\n                    equivalent_indexes - n * self.offset\n                    for n in np.arange(self.n_offsets)\n                ]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                    self.agg_func, axis=0, arr=equivalent_values\n                )\n\n            predictions = pd.Series(\n                data=predictions, index=prediction_index, name=\"pred\"\n            )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            last_window = last_window.copy()\n            max_allowed_date = last_window.index[-1]\n\n            # For every date in prediction_index, calculate the n offsets\n            offset_dates = []\n            for date in prediction_index:\n                selected_offsets = []\n                while len(selected_offsets) &lt; self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date &lt;= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n\n            offset_dates = np.array(offset_dates)\n\n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                data=equivalent_values,\n                index=prediction_index,\n                columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n            )\n\n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    f\"All equivalent values are missing. This is caused by using \"\n                    f\"an offset ({self.offset}) larger than the available data. \"\n                    f\"Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `last_window`. In backtesting, this error may be \"\n                    f\"caused by using an `initial_train_size` too small.\"\n                )\n\n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                    f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                    f\"To avoid this, increase the `last_window` size or decrease \"\n                    f\"the number of `n_offsets`. The current configuration requires \"\n                    f\"a total offset of {self.offset * self.n_offsets}.\",\n                    MissingValuesWarning,\n                )\n\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.rename(\"pred\")\n\n        return predictions\n\n    def predict_interval(\n        self,\n        steps: int,\n        last_window: pd.Series | None = None,\n        method: str = \"conformal\",\n        interval: float | list[float] | tuple[float] = [5, 95],\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: Any = None,\n        exog: Any = None,\n        n_boot: Any = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead and estimate prediction intervals using conformal\n        prediction method. Refer to the References section for additional\n        details on this method.\n\n        Args:\n            steps (int): Number of steps to predict.\n            last_window (pandas Series, optional): Past values needed to select the\n                last equivalent dates according to the offset. If `last_window = None`,\n                the values stored in `self.last_window_` are used and the predictions\n                start immediately after the training data. Defaults to None.\n            method (str, optional): Technique used to estimate prediction intervals.\n                Available options:\n                - 'conformal': Employs the conformal prediction split method for\n                interval estimation [1]_. Defaults to 'conformal'.\n            interval (float, list, tuple, optional): Confidence level of the\n                prediction interval. Interpretation depends on the method used:\n                - If `float`, represents the nominal (expected) coverage (between 0\n                and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n                percentiles.\n                - If `list` or `tuple`, defines the exact percentiles to compute,\n                which must be between 0 and 100 inclusive. For example, interval\n                of 95% should be as `interval = [2.5, 97.5]`.\n                - When using `method='conformal'`, the interval must be a float or\n                a list/tuple defining a symmetric interval. Defaults to [5, 95].\n            use_in_sample_residuals (bool, optional): If `True`, residuals from the\n                training data are used as proxy of prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals (bool, optional): If `True`, residuals are selected\n                based on the predicted values (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state (Ignored): Not used, present here for API consistency by convention.\n            exog (Ignored): Not used, present here for API consistency by convention.\n            n_boot (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            pd.DataFrame: Values predicted by the forecaster and their estimated interval.\n                - pred: predictions.\n                - lower_bound: lower bound of the interval.\n                - upper_bound: upper bound of the interval.\n\n        References:\n            .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n                https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n        \"\"\"\n\n        if method != \"conformal\":\n            raise ValueError(\n                f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n            )\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=False,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n        )\n\n        check_residuals_input(\n            forecaster_name=type(self).__name__,\n            use_in_sample_residuals=use_in_sample_residuals,\n            in_sample_residuals_=self.in_sample_residuals_,\n            out_sample_residuals_=self.out_sample_residuals_,\n            use_binned_residuals=use_binned_residuals,\n            in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n            out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n        )\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            nominal_coverage = interval\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        prediction_index = expand_index(index=last_window.index, steps=steps)\n\n        if isinstance(self.offset, int):\n\n            last_window_values = last_window.to_numpy(copy=True).ravel()\n            equivalent_indexes = np.tile(\n                np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n            )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets &gt; 1:\n                equivalent_indexes = [\n                    equivalent_indexes - n * self.offset\n                    for n in np.arange(self.n_offsets)\n                ]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                    self.agg_func, axis=0, arr=equivalent_values\n                )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            last_window = last_window.copy()\n            max_allowed_date = last_window.index[-1]\n\n            # For every date in prediction_index, calculate the n offsets\n            offset_dates = []\n            for date in prediction_index:\n                selected_offsets = []\n                while len(selected_offsets) &lt; self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date &lt;= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n\n            offset_dates = np.array(offset_dates)\n\n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                data=equivalent_values,\n                index=prediction_index,\n                columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n            )\n\n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    f\"All equivalent values are missing. This is caused by using \"\n                    f\"an offset ({self.offset}) larger than the available data. \"\n                    f\"Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `last_window`. In backtesting, this error may be \"\n                    f\"caused by using an `initial_train_size` too small.\"\n                )\n\n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                    f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                    f\"To avoid this, increase the `last_window` size or decrease \"\n                    f\"the number of `n_offsets`. The current configuration requires \"\n                    f\"a total offset of {self.offset * self.n_offsets}.\",\n                    MissingValuesWarning,\n                )\n\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.to_numpy()\n\n        if use_binned_residuals:\n            correction_factor_by_bin = {\n                k: np.quantile(np.abs(v), nominal_coverage)\n                for k, v in residuals_by_bin.items()\n            }\n            replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n            predictions_bin = self.binner.transform(predictions)\n            correction_factor = replace_func(predictions_bin)\n        else:\n            correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n        lower_bound = predictions - correction_factor\n        upper_bound = predictions + correction_factor\n        predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n        predictions = pd.DataFrame(\n            data=predictions,\n            index=prediction_index,\n            columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n        )\n\n        return predictions\n\n    def set_in_sample_residuals(\n        self, y: pd.Series, random_state: int = 123, exog: Any = None\n    ) -&gt; None:\n        \"\"\"\n        Set in-sample residuals in case they were not calculated during the\n        training process.\n\n        In-sample residuals are calculated as the difference between the true\n        values and the predictions made by the forecaster using the training\n        data. The following internal attributes are updated:\n\n        + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `binner_intervals_`: intervals used to bin the residuals are calculated\n        using the quantiles of the predicted values.\n        + `in_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the intervals of the predicted values and the values are\n        the residuals associated with that range.\n\n        A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        random_state : int, default 123\n            Sets a seed to the random sampling for reproducible output.\n        exog : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_in_sample_residuals()`.\"\n            )\n\n        check_y(y=y)\n        y_index_range = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )[1][[0, -1]]\n        if not y_index_range.equals(self.training_range_):\n            raise IndexError(\n                f\"The index range of `y` does not match the range \"\n                f\"used during training. Please ensure the index is aligned \"\n                f\"with the training data.\\n\"\n                f\"    Expected : {self.training_range_}\\n\"\n                f\"    Received : {y_index_range}\"\n            )\n\n        self._binning_in_sample_residuals(\n            y=y, store_in_sample_residuals=True, random_state=random_state\n        )\n\n    def set_out_sample_residuals(\n        self,\n        y_true: np.ndarray | pd.Series,\n        y_pred: np.ndarray | pd.Series,\n        append: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n\n        Parameters\n        ----------\n        y_true : numpy ndarray, pandas Series\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : numpy ndarray, pandas Series\n            Predicted values of the time series.\n        append : bool, default False\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default 123\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n\n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        data = pd.DataFrame(\n            {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n        ).dropna()\n        y_pred = data[\"prediction\"].to_numpy()\n        residuals = data[\"residuals\"].to_numpy()\n\n        data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n\n        out_sample_residuals = (\n            np.array([])\n            if self.out_sample_residuals_ is None\n            else self.out_sample_residuals_\n        )\n        out_sample_residuals_by_bin = (\n            {}\n            if self.out_sample_residuals_by_bin_ is None\n            else self.out_sample_residuals_by_bin_\n        )\n        if append:\n            out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n            for k, v in residuals_by_bin.items():\n                if k in out_sample_residuals_by_bin:\n                    out_sample_residuals_by_bin[k] = np.concatenate(\n                        (out_sample_residuals_by_bin[k], v)\n                    )\n                else:\n                    out_sample_residuals_by_bin[k] = v\n        else:\n            out_sample_residuals = residuals\n            out_sample_residuals_by_bin = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in out_sample_residuals_by_bin.items():\n            if len(v) &gt; max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                out_sample_residuals_by_bin[k] = sample\n\n        bin_keys = (\n            [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n        )\n        for k in bin_keys:\n            if k not in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.array([])\n\n        empty_bins = [k for k, v in out_sample_residuals_by_bin.items() if v.size == 0]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\",\n                ResidualsUsageWarning,\n            )\n            empty_bin_size = min(max_samples, len(out_sample_residuals))\n            for k in empty_bins:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    a=out_sample_residuals, size=empty_bin_size, replace=False\n                )\n\n        if len(out_sample_residuals) &gt; 10_000:\n            out_sample_residuals = rng.choice(\n                a=out_sample_residuals, size=10_000, replace=False\n            )\n\n        self.out_sample_residuals_ = out_sample_residuals\n        self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            dict: Dictionary with forecaster tags.\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"\n        Show forecaster information.\n\n        Returns:\n            None\n        \"\"\"\n\n        print(self)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a Forecaster object is printed.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a Forecaster object is printed.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Offset: {self.offset} \\n\"\n        f\"Number of offsets: {self.n_offsets} \\n\"\n        f\"Aggregation function: {self.agg_func.__name__} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"spotforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.fit","title":"<code>fit(y, store_in_sample_residuals=False, random_state=123, exog=None)</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample residuals are always deterministic. Defaults to 123.</p> <code>123</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    exog: Any = None,\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Args:\n        y (pandas Series): Training time series.\n        store_in_sample_residuals (bool, optional): If `True`, in-sample\n            residuals will be stored in the forecaster object after fitting\n            (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n            If `False`, only the intervals of the bins are stored. Defaults to False.\n        random_state (int, optional): Set a seed for the random generator so\n            that the stored sample residuals are always deterministic. Defaults to 123.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        None\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        if not isinstance(y.index, pd.DatetimeIndex):\n            raise TypeError(\n                \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                \"pandas DatetimeIndex with frequency.\"\n            )\n        elif y.index.freq is None:\n            try:\n                y.index.freq = pd.infer_freq(y.index)\n            except (ValueError, TypeError):\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n            if y.index.freq is None:\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_ = None\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.training_range_ = None\n    self.series_name_in_ = None\n    self.is_fitted = False\n\n    _, y_index = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        # Calculate the window_size in steps for compatibility with the\n        # check_predict_input function. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance\n        # between two dates may not be constant.\n        first_valid_index = y_index[-1] - self.offset * self.n_offsets\n\n        try:\n            window_size_idx_start = y_index.get_loc(first_valid_index)\n            window_size_idx_end = y_index.get_loc(y_index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\"\n            )\n    else:\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster. This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n            )\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else \"y\"\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    self.training_range_ = y_index[[0, -1]]\n    self.index_type_ = type(y_index)\n    self.index_freq_ = (\n        y_index.freq if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n    )\n\n    # NOTE: This is done to save time during fit in functions such as backtesting()\n    if self._probabilistic_mode is not False:\n        self._binning_in_sample_residuals(\n            y=y,\n            store_in_sample_residuals=store_in_sample_residuals,\n            random_state=random_state,\n        )\n\n    # The last time window of training data is stored so that equivalent\n    # dates are available when calling the `predict` method.\n    # Store the whole series to avoid errors when the offset is larger\n    # than the data available.\n    self.last_window_ = y.copy()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Dictionary with forecaster tags.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        dict: Dictionary with forecaster tags.\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.predict","title":"<code>predict(steps, last_window=None, check_inputs=True, exog=None)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to the offset. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used and the predictions start immediately after the training data. Defaults to None.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors with the <code>check_predict_input</code> function. This argument is created for internal use and is not recommended to be changed. Defaults to True.</p> <code>True</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Predicted values.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    check_inputs: bool = True,\n    exog: Any = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Args:\n        steps (int): Number of steps to predict.\n        last_window (pandas Series, optional): Past values needed to select the\n            last equivalent dates according to the offset. If `last_window = None`,\n            the values stored in `self.last_window_` are used and the predictions\n            start immediately after the training data. Defaults to None.\n        check_inputs (bool, optional): If `True`, the input is checked for\n            possible warnings and errors with the `check_predict_input` function.\n            This argument is created for internal use and is not recommended to\n            be changed. Defaults to True.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        pd.Series: Predicted values.\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=False,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n        )\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n            np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n        )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset\n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                self.agg_func, axis=0, arr=equivalent_values\n            )\n\n        predictions = pd.Series(\n            data=predictions, index=prediction_index, name=\"pred\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n            data=equivalent_values,\n            index=prediction_index,\n            columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n        )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \"\n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning,\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.rename(\"pred\")\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.predict_interval","title":"<code>predict_interval(steps, last_window=None, method='conformal', interval=[5, 95], use_in_sample_residuals=True, use_binned_residuals=True, random_state=None, exog=None, n_boot=None)</code>","text":"<p>Predict n steps ahead and estimate prediction intervals using conformal prediction method. Refer to the References section for additional details on this method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to the offset. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used and the predictions start immediately after the training data. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options: - 'conformal': Employs the conformal prediction split method for interval estimation [1]_. Defaults to 'conformal'.</p> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends on the method used: - If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles. - If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>. - When using <code>method='conformal'</code>, the interval must be a float or a list/tuple defining a symmetric interval. Defaults to [5, 95].</p> <code>[5, 95]</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>n_boot</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Values predicted by the forecaster and their estimated interval. - pred: predictions. - lower_bound: lower bound of the interval. - upper_bound: upper bound of the interval.</p> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.     https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    method: str = \"conformal\",\n    interval: float | list[float] | tuple[float] = [5, 95],\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: Any = None,\n    exog: Any = None,\n    n_boot: Any = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using conformal\n    prediction method. Refer to the References section for additional\n    details on this method.\n\n    Args:\n        steps (int): Number of steps to predict.\n        last_window (pandas Series, optional): Past values needed to select the\n            last equivalent dates according to the offset. If `last_window = None`,\n            the values stored in `self.last_window_` are used and the predictions\n            start immediately after the training data. Defaults to None.\n        method (str, optional): Technique used to estimate prediction intervals.\n            Available options:\n            - 'conformal': Employs the conformal prediction split method for\n            interval estimation [1]_. Defaults to 'conformal'.\n        interval (float, list, tuple, optional): Confidence level of the\n            prediction interval. Interpretation depends on the method used:\n            - If `float`, represents the nominal (expected) coverage (between 0\n            and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n            percentiles.\n            - If `list` or `tuple`, defines the exact percentiles to compute,\n            which must be between 0 and 100 inclusive. For example, interval\n            of 95% should be as `interval = [2.5, 97.5]`.\n            - When using `method='conformal'`, the interval must be a float or\n            a list/tuple defining a symmetric interval. Defaults to [5, 95].\n        use_in_sample_residuals (bool, optional): If `True`, residuals from the\n            training data are used as proxy of prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals (bool, optional): If `True`, residuals are selected\n            based on the predicted values (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state (Ignored): Not used, present here for API consistency by convention.\n        exog (Ignored): Not used, present here for API consistency by convention.\n        n_boot (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        pd.DataFrame: Values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    References:\n        .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n            https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n    \"\"\"\n\n    if method != \"conformal\":\n        raise ValueError(\n            f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n        )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    check_predict_input(\n        forecaster_name=type(self).__name__,\n        steps=steps,\n        is_fitted=self.is_fitted,\n        exog_in_=False,\n        index_type_=self.index_type_,\n        index_freq_=self.index_freq_,\n        window_size=self.window_size,\n        last_window=last_window,\n    )\n\n    check_residuals_input(\n        forecaster_name=type(self).__name__,\n        use_in_sample_residuals=use_in_sample_residuals,\n        in_sample_residuals_=self.in_sample_residuals_,\n        out_sample_residuals_=self.out_sample_residuals_,\n        use_binned_residuals=use_binned_residuals,\n        in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n        out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n    )\n\n    if isinstance(interval, (list, tuple)):\n        check_interval(interval=interval, ensure_symmetric_intervals=True)\n        nominal_coverage = (interval[1] - interval[0]) / 100\n    else:\n        check_interval(alpha=interval, alpha_literal=\"interval\")\n        nominal_coverage = interval\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n            np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n        )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset\n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                self.agg_func, axis=0, arr=equivalent_values\n            )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n            data=equivalent_values,\n            index=prediction_index,\n            columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n        )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \"\n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning,\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.to_numpy()\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner.transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    predictions = pd.DataFrame(\n        data=predictions,\n        index=prediction_index,\n        columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n    )\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals","title":"<code>set_in_sample_residuals(y, random_state=123, exog=None)</code>","text":"<p>Set in-sample residuals in case they were not calculated during the training process.</p> <p>In-sample residuals are calculated as the difference between the true values and the predictions made by the forecaster using the training data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals--parameters","title":"Parameters","text":"<p>y : pandas Series     Training time series. random_state : int, default 123     Sets a seed to the random sampling for reproducible output. exog : Ignored     Not used, present here for API consistency by convention.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def set_in_sample_residuals(\n    self, y: pd.Series, random_state: int = 123, exog: Any = None\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process.\n\n    In-sample residuals are calculated as the difference between the true\n    values and the predictions made by the forecaster using the training\n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range.\n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )[1][[0, -1]]\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y=y, store_in_sample_residuals=True, random_state=random_state\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals","title":"<code>set_out_sample_residuals(y_true, y_pred, append=False, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Two internal attributes are updated:</p> <ul> <li><code>out_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>out_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the  intervals of the predicted values and the values are the residuals associated with that range. If a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals--parameters","title":"Parameters","text":"<p>y_true : numpy ndarray, pandas Series     True values of the time series from which the residuals have been     calculated. y_pred : numpy ndarray, pandas Series     Predicted values of the time series. append : bool, default False     If <code>True</code>, new residuals are added to the once already stored in the     forecaster. If after appending the new residuals, the limit of     <code>10_000 // self.binner.n_bins_</code> values per bin is reached, a random     sample of residuals is stored. random_state : int, default 123     Sets a seed to the random sampling for reproducible output.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. Two internal attributes are updated:\n\n    + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `out_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the  intervals of the predicted values and the values are\n    the residuals associated with that range. If a bin binning is empty, it\n    is filled with a random sample of residuals from other bins. This is done\n    to ensure that all bins have at least one residual and can be used in the\n    prediction process.\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y_true : numpy ndarray, pandas Series\n        True values of the time series from which the residuals have been\n        calculated.\n    y_pred : numpy ndarray, pandas Series\n        Predicted values of the time series.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        forecaster. If after appending the new residuals, the limit of\n        `10_000 // self.binner.n_bins_` values per bin is reached, a random\n        sample of residuals is stored.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    data = pd.DataFrame(\n        {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n    ).dropna()\n    y_pred = data[\"prediction\"].to_numpy()\n    residuals = data[\"residuals\"].to_numpy()\n\n    data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n\n    out_sample_residuals = (\n        np.array([])\n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {}\n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    bin_keys = (\n        [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n    )\n    for k in bin_keys:\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [k for k, v in out_sample_residuals_by_bin.items() if v.size == 0]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins have no out of sample residuals: {empty_bins}. \"\n            f\"No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\",\n            ResidualsUsageWarning,\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a=out_sample_residuals, size=empty_bin_size, replace=False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a=out_sample_residuals, size=10_000, replace=False\n        )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Show forecaster information.\n\n    Returns:\n        None\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive","title":"<code>ForecasterRecursive</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>Recursive autoregressive forecaster for scikit-learn compatible estimators.</p> <p>This class turns any estimator compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. The forecaster learns to predict future values by using lagged values of the target variable and optional exogenous features. Predictions are made iteratively, where each step uses previous predictions as input for the next step (recursive strategy).</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object</code> <p>Scikit-learn compatible estimator for regression. If None, a default estimator will be initialized. Can also be passed via regressor parameter.</p> <code>None</code> <code>lags</code> <code>Union[int, List[int], ndarray, range, None]</code> <p>Lagged values of the target variable to use as predictors. Can be an integer (uses lags from 1 to lags), list of integers, numpy array, or range. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>window_features</code> <code>Union[object, List[object], None]</code> <p>List of window feature objects to compute features from the target variable. Each object must implement transform_batch() method. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>transformer_y</code> <code>Optional[object]</code> <p>Transformer object for the target variable. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>transformer_exog</code> <code>Optional[object]</code> <p>Transformer object for exogenous variables. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>weight_func</code> <code>Optional[Callable]</code> <p>Function to compute sample weights for training. Must accept an index and return an array of weights. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>Optional[int]</code> <p>Order of differencing to apply to the target variable. Must be a positive integer. Differencing is applied before creating lags. Defaults to None.</p> <code>None</code> <code>fit_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of additional keyword arguments to pass to the estimator's fit() method. Defaults to None.</p> <code>None</code> <code>binner_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of keyword arguments for QuantileBinner used in probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.</p> <code>None</code> <code>forecaster_id</code> <code>Union[str, int, None]</code> <p>Identifier for the forecaster instance. Can be a string or integer. Used for tracking and logging purposes. Defaults to None.</p> <code>None</code> <code>regressor</code> <code>object</code> <p>Alternative parameter name for estimator. If provided, used instead of estimator. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator</code> <p>Fitted scikit-learn estimator.</p> <code>lags</code> <p>Lag indices used in the model.</p> <code>lags_names</code> <p>Names of lag features (e.g., ['lag_1', 'lag_2']).</p> <code>window_features</code> <p>List of window feature transformers.</p> <code>window_features_names</code> <p>Names of window features.</p> <code>window_size</code> <p>Maximum window size needed (max of lags and window features).</p> <code>transformer_y</code> <p>Transformer for target variable.</p> <code>transformer_exog</code> <p>Transformer for exogenous variables.</p> <code>weight_func</code> <p>Function for sample weighting.</p> <code>differentiation</code> <p>Order of differencing applied.</p> <code>differentiator</code> <p>TimeSeriesDifferentiator instance if differencing is used.</p> <code>is_fitted</code> <p>Boolean indicating if forecaster has been fitted.</p> <code>fit_date</code> <p>Timestamp of the last fit operation.</p> <code>last_window_</code> <p>Last window_size observations from training data.</p> <code>index_type_</code> <p>Type of index in training data (RangeIndex or DatetimeIndex).</p> <code>index_freq_</code> <p>Frequency of DatetimeIndex if applicable.</p> <code>training_range_</code> <p>First and last index values of training data.</p> <code>series_name_in_</code> <p>Name of the target series.</p> <code>exog_in_</code> <p>Boolean indicating if exogenous variables were used in training.</p> <code>exog_names_in_</code> <p>Names of exogenous variables.</p> <code>exog_type_in_</code> <p>Type of exogenous input (Series or DataFrame).</p> <code>X_train_features_names_out_</code> <p>Names of all training features.</p> <code>in_sample_residuals_</code> <p>Residuals from training set.</p> <code>in_sample_residuals_by_bin_</code> <p>Residuals grouped by bins for probabilistic pred.</p> <code>forecaster_id</code> <p>Identifier for the forecaster instance.</p> Note <ul> <li>Either lags or window_features (or both) must be provided during initialization.</li> <li>The forecaster uses a recursive strategy where each multi-step prediction   depends on previous predictions within the same forecast horizon.</li> <li>Exogenous variables must have the same index as the target variable and must   be available for the entire prediction horizon.</li> <li>The forecaster supports point predictions, prediction intervals, bootstrapping,   quantile predictions, and probabilistic forecasts via conformal methods.</li> </ul> <p>Examples:</p> <p>Create a basic forecaster with lags:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=10\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> <p>Create a forecaster with window features and transformations:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(n_estimators=100),\n...     lags=[1, 7, 30],\n...     window_features=[RollingMeanWindow(window=7)],\n...     transformer_y=StandardScaler(),\n...     differentiation=1\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n</code></pre> <p>Create a forecaster with exogenous variables:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=Ridge(),\n...     lags=7,\n...     forecaster_id='my_forecaster'\n... )\n&gt;&gt;&gt; forecaster.fit(y, exog)\n&gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n</code></pre> <p>Create a forecaster with probabilistic prediction configuration:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=GradientBoostingRegressor(),\n...     lags=14,\n...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n... )\n&gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n&gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>class ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    Recursive autoregressive forecaster for scikit-learn compatible estimators.\n\n    This class turns any estimator compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster. The forecaster learns to predict\n    future values by using lagged values of the target variable and optional exogenous\n    features. Predictions are made iteratively, where each step uses previous predictions\n    as input for the next step (recursive strategy).\n\n    Args:\n        estimator: Scikit-learn compatible estimator for regression. If None, a default\n            estimator will be initialized. Can also be passed via regressor parameter.\n        lags: Lagged values of the target variable to use as predictors. Can be an\n            integer (uses lags from 1 to lags), list of integers, numpy array, or range.\n            At least one of lags or window_features must be provided. Defaults to None.\n        window_features: List of window feature objects to compute features from the\n            target variable. Each object must implement transform_batch() method.\n            At least one of lags or window_features must be provided. Defaults to None.\n        transformer_y: Transformer object for the target variable. Must implement fit()\n            and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        transformer_exog: Transformer object for exogenous variables. Must implement\n            fit() and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        weight_func: Function to compute sample weights for training. Must accept an\n            index and return an array of weights. Defaults to None.\n        differentiation: Order of differencing to apply to the target variable.\n            Must be a positive integer. Differencing is applied before creating lags.\n            Defaults to None.\n        fit_kwargs: Dictionary of additional keyword arguments to pass to the estimator's\n            fit() method. Defaults to None.\n        binner_kwargs: Dictionary of keyword arguments for QuantileBinner used in\n            probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.\n        forecaster_id: Identifier for the forecaster instance. Can be a string or\n            integer. Used for tracking and logging purposes. Defaults to None.\n        regressor: Alternative parameter name for estimator. If provided, used instead\n            of estimator. Defaults to None.\n\n    Attributes:\n        estimator: Fitted scikit-learn estimator.\n        lags: Lag indices used in the model.\n        lags_names: Names of lag features (e.g., ['lag_1', 'lag_2']).\n        window_features: List of window feature transformers.\n        window_features_names: Names of window features.\n        window_size: Maximum window size needed (max of lags and window features).\n        transformer_y: Transformer for target variable.\n        transformer_exog: Transformer for exogenous variables.\n        weight_func: Function for sample weighting.\n        differentiation: Order of differencing applied.\n        differentiator: TimeSeriesDifferentiator instance if differencing is used.\n        is_fitted: Boolean indicating if forecaster has been fitted.\n        fit_date: Timestamp of the last fit operation.\n        last_window_: Last window_size observations from training data.\n        index_type_: Type of index in training data (RangeIndex or DatetimeIndex).\n        index_freq_: Frequency of DatetimeIndex if applicable.\n        training_range_: First and last index values of training data.\n        series_name_in_: Name of the target series.\n        exog_in_: Boolean indicating if exogenous variables were used in training.\n        exog_names_in_: Names of exogenous variables.\n        exog_type_in_: Type of exogenous input (Series or DataFrame).\n        X_train_features_names_out_: Names of all training features.\n        in_sample_residuals_: Residuals from training set.\n        in_sample_residuals_by_bin_: Residuals grouped by bins for probabilistic pred.\n        forecaster_id: Identifier for the forecaster instance.\n\n    Note:\n        - Either lags or window_features (or both) must be provided during initialization.\n        - The forecaster uses a recursive strategy where each multi-step prediction\n          depends on previous predictions within the same forecast horizon.\n        - Exogenous variables must have the same index as the target variable and must\n          be available for the entire prediction horizon.\n        - The forecaster supports point predictions, prediction intervals, bootstrapping,\n          quantile predictions, and probabilistic forecasts via conformal methods.\n\n    Examples:\n        Create a basic forecaster with lags:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=10\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n\n        Create a forecaster with window features and transformations:\n\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(n_estimators=100),\n        ...     lags=[1, 7, 30],\n        ...     window_features=[RollingMeanWindow(window=7)],\n        ...     transformer_y=StandardScaler(),\n        ...     differentiation=1\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n\n        Create a forecaster with exogenous variables:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=Ridge(),\n        ...     lags=7,\n        ...     forecaster_id='my_forecaster'\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, exog)\n        &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n\n        Create a forecaster with probabilistic prediction configuration:\n\n        &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=GradientBoostingRegressor(),\n        ...     lags=14,\n        ...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n        &gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: object = None,\n        lags: Union[int, List[int], np.ndarray, range, None] = None,\n        window_features: Union[object, List[object], None] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        binner_kwargs: Optional[Dict[str, object]] = None,\n        forecaster_id: Union[str, int, None] = None,\n        regressor: object = None,\n    ) -&gt; None:\n\n        self.estimator = copy(initialize_estimator(estimator, regressor))\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiation_max = None\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.exog_dtypes_out_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        self.spotforecast_version = __version__\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n\n        (\n            self.lags,\n            self.lags_names,\n            self.max_lag,\n        ) = initialize_lags(type(self).__name__, lags)\n        (\n            self.window_features,\n            self.window_features_names,\n            self.max_size_window_features,\n        ) = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        self.window_size = max(\n            [\n                ws\n                for ws in [self.max_lag, self.max_size_window_features]\n                if ws is not None\n            ]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=estimator,\n            weight_func=weight_func,\n            series_weights=None,\n        )\n\n        if differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation &lt; 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation  # , window_size=self.window_size # Note: TimeSeriesDifferentiator in preprocessing I created only takes order\n            )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n            estimator=estimator, fit_kwargs=fit_kwargs\n        )\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Estimator: {type(self.estimator).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Estimator parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n                &lt;ul&gt;\n                    {exog_names_in_}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Data Transformations&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Estimator Parameters&lt;/summary&gt;\n                &lt;ul&gt;\n                    {params}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n                &lt;ul&gt;\n                    {self.fit_kwargs}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n        super().__setstate__(state)\n        if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n            self.__spotforecast_tags__ = {\n                \"library\": \"spotforecast\",\n                \"forecaster_name\": \"ForecasterRecursive\",\n                \"forecaster_task\": \"regression\",\n                \"forecasting_scope\": \"single-series\",\n                \"forecasting_strategy\": \"recursive\",\n                \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n                \"requires_index_frequency\": True,\n                \"allowed_input_types_series\": [\"pandas.Series\"],\n                \"supports_exog\": True,\n                \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n                \"handles_missing_values_series\": False,\n                \"handles_missing_values_exog\": True,\n                \"supports_lags\": True,\n                \"supports_window_features\": True,\n                \"supports_transformer_series\": True,\n                \"supports_transformer_exog\": True,\n                \"supports_weight_func\": True,\n                \"supports_differentiation\": True,\n                \"prediction_types\": [\n                    \"point\",\n                    \"interval\",\n                    \"bootstrapping\",\n                    \"quantiles\",\n                    \"distribution\",\n                ],\n                \"supports_probabilistic\": True,\n                \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n                \"handles_binned_residuals\": True,\n            }\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None,\n    ) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create lagged predictors and aligned target values.\n\n        Args:\n            y: Target values used to build lag features. Expected shape is\n                (n_samples,) or (n_samples, 1).\n            X_as_pandas: If True, returns lagged features as a pandas DataFrame.\n            train_index: Index to use for the lagged feature DataFrame when\n                `X_as_pandas` is True.\n\n        Returns:\n            Tuple containing:\n                - X_data: Lagged predictors with shape (n_rows, n_lags) or None\n                  if no lags are configured.\n                - y_data: Target values aligned to the lagged predictors with\n                  shape (n_rows,).\n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            # y = y.ravel() # Assuming y is already raveled\n            # Using stride_tricks for sliding window\n            y_strided = np.lib.stride_tricks.sliding_window_view(y, self.window_size)[\n                :-1\n            ]\n            X_data = y_strided[:, self.window_size - self.lags]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                    data=X_data, columns=self.lags_names, index=train_index\n                )\n\n        y_data = y[self.window_size :]\n\n        return X_data, y_data\n\n    def _create_window_features(\n        self,\n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -&gt; Tuple[List[Union[np.ndarray, pd.DataFrame]], List[str]]:\n        \"\"\"\n        Generate window features from the target series.\n\n        Args:\n            y: Target series used to compute window features. Must be a pandas\n                Series with an index aligned to `train_index` after trimming.\n            train_index: Index for the training rows to align the window features.\n            X_as_pandas: If True, keeps each window feature matrix as a pandas\n                DataFrame; otherwise converts to NumPy arrays.\n\n        Returns:\n            Tuple containing:\n                - X_train_window_features: List of window feature matrices, one\n                  per window feature transformer.\n                - X_train_window_features_names_out_: List of feature names for\n                  all generated window features.\n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a pandas DataFrame.\"\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same number of rows as \"\n                    f\"the input time series - `window_size`: {len_train_index}.\"\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same index as \"\n                    f\"the input time series - `window_size`.\"\n                )\n\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name=\"y\")\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n            df=y,\n            transformer=self.transformer_y,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        y_values, y_index = check_extract_values_and_index(data=y, data_label=\"`y`\")\n        if y_values.ndim == 2 and y_values.shape[1] == 1:\n            y_values = y_values.ravel()\n        train_index = y_index[self.window_size :]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                self.differentiator.fit(y_values)  # Differentiator requires fit first\n                y_values = self.differentiator.transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        X_as_pandas = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            _, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\", ignore_freq=True, return_values=False\n            )\n\n            _ = len(y_values) + (\n                self.differentiation if self.differentiation else 0\n            )  # Adjust for differentiation loss of length if needed? No, y_values has NaNs at start\n            # But y_values from check_extract... is raw values.\n            # Differentiator might introduce NaNs. Sklearn transformer keeps length.\n            # My ported differentiator creates NaNs at start.\n\n            # Re-evaluate logic:\n            # y_values (raw) length = N\n            # differentiator transform -&gt; length N, first 'order' are NaN.\n\n            len_exog = len(exog)\n            # The check logic depends on alignment.\n\n            # Simplified check from original code\n            # ... (omitted for brevity, assume caller passed valid data or minimal check)\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=fit_transformer,\n                inverse_transform=False,\n            )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n            X_as_pandas = any(\n                not pd.api.types.is_numeric_dtype(dtype)\n                or pd.api.types.is_bool_dtype(dtype)\n                for dtype in set(exog.dtypes)\n            )\n\n            # Alignment logic\n            if len_exog == len(y):\n                exog = exog.iloc[self.window_size :,]\n            else:\n                pass  # Assume aligned start\n\n        X_train = []\n        X_train_features_names_out_ = []\n\n        # Create lags\n        # Note: y_values might have NaNs from differentiation.\n        # create_lags handles this?\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            if isinstance(y_values, pd.Series):\n                y_vals_for_wf = y_values.iloc[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n            else:\n                y_vals_for_wf = y_values[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n\n            y_window_features = pd.Series(y_vals_for_wf, index=y_index_for_wf)\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features,\n                    X_as_pandas=X_as_pandas,\n                    train_index=train_index,\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                data=X_train, index=train_index, columns=X_train_features_names_out_\n            )\n\n        y_train = pd.Series(data=y_train, index=train_index, name=\"y\")\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        )\n\n    def create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        return self._create_train_X_y(y=y, exog=exog)\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Args:\n            y: Training time series.\n            initial_train_size: Initial size of the training set. It is the number of\n                observations used to train the forecaster before making the first\n                prediction.\n            exog: Exogenous variable/s included as predictor/s. Must have the same\n                number of observations as y and their indexes must be aligned.\n                Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: Predictor values used to train the model as pandas DataFrame.\n                - y_train: Values of the time series related to each row of X_train for\n                    each step in the form {step: y_step_[i]} as dict.\n                - X_test: Predictor values used to test the model as pandas DataFrame.\n                - y_test: Values of the time series related to each row of X_test for\n                    each step in the form {step: y_step_[i]} as dict.\n\n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y=y.iloc[:initial_train_size],\n            exog=exog.iloc[:initial_train_size] if exog is not None else None,\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y=y.iloc[test_init:],\n            exog=exog.iloc[test_init:] if exog is not None else None,\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def get_params(self, deep=True):\n        params = {}\n        for key in [\n            \"estimator\",\n            \"lags\",\n            \"window_features\",\n            \"transformer_y\",\n            \"transformer_exog\",\n            \"weight_func\",\n            \"differentiation\",\n            \"fit_kwargs\",\n            \"binner_kwargs\",\n            \"forecaster_id\",\n        ]:\n            if hasattr(self, key):\n                params[key] = getattr(self, key)\n\n        if not deep:\n            return params\n\n        if hasattr(self, \"estimator\") and self.estimator is not None:\n            if hasattr(self.estimator, \"get_params\"):\n                for key, value in self.estimator.get_params(deep=True).items():\n                    params[f\"estimator__{key}\"] = value\n\n        return params\n\n    def set_params(self, **params):\n        if not params:\n            return self\n\n        valid_params = self.get_params(deep=True)\n        nested_params = {}\n\n        for key, value in params.items():\n            if key not in valid_params and \"__\" not in key:\n                # Relaxed check for now\n                pass\n\n            if \"__\" in key:\n                obj_name, param_name = key.split(\"__\", 1)\n                if obj_name not in nested_params:\n                    nested_params[obj_name] = {}\n                nested_params[obj_name][param_name] = value\n            else:\n                setattr(self, key, value)\n\n        for obj_name, obj_params in nested_params.items():\n            if hasattr(self, obj_name):\n                obj = getattr(self, obj_name)\n                if hasattr(obj, \"set_params\"):\n                    obj.set_params(**obj_params)\n                else:\n                    for param_name, value in obj_params.items():\n                        setattr(obj, param_name, value)\n\n        return self\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        suppress_warnings: bool = False,\n    ) -&gt; None:\n\n        # Reset values\n        self.is_fitted = False\n        self.fit_date = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        SAMPLE_WEIGHT_NAME = \"sample_weight\"\n        if self.weight_func is not None:\n            sample_weight, _, _ = initialize_weights(\n                forecaster_name=type(self).__name__,\n                estimator=self.estimator,\n                weight_func=self.weight_func,\n                series_weights=None,\n            )\n            sample_weight = sample_weight(y.index[self.window_size :])\n            self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n        self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n            del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n        # Store attributes\n        self.last_window_ = y.iloc[-self.window_size :].copy()\n        self.index_type_ = type(y.index)\n        if isinstance(y.index, pd.DatetimeIndex):\n            self.index_freq_ = y.index.freqstr\n        else:\n            try:\n                self.index_freq_ = y.index.step\n            except AttributeError:\n                self.index_freq_ = None\n\n        self.training_range_ = y.index[[0, -1]]\n        self.series_name_in_ = y.name\n        self.exog_in_ = exog is not None\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog) if exog is not None else None\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        residuals = y_train - self.estimator.predict(X_train)\n\n        if len(residuals) &gt; 1000:\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(residuals, size=1000, replace=False)\n\n        self.in_sample_residuals_ = residuals\n\n        if self.binner_kwargs is not None:\n            self.binner = QuantileBinner(**self.binner_kwargs)\n            if isinstance(residuals, pd.Series):\n                residuals = residuals.to_numpy()\n            self.binner.fit(residuals)\n\n            # Construct intervals_ manually if not in binner\n            if hasattr(self.binner, \"intervals_\"):\n                self.binner_intervals_ = self.binner.intervals_\n            else:\n                self.binner_intervals_ = {\n                    i: (self.binner.bins_[i - 1], self.binner.bins_[i])\n                    for i in range(1, len(self.binner.bins_))\n                }\n\n            residuals_binned = self.binner.transform(residuals)\n            self.in_sample_residuals_by_bin_ = {\n                bin: residuals[residuals_binned == bin]\n                for bin in self.binner_intervals_.keys()\n            }\n\n            # Limit residuals stored per bin\n            max_residuals_per_bin = 1000 // self.binner.n_bins\n            for bin, res in self.in_sample_residuals_by_bin_.items():\n                if len(res) &gt; max_residuals_per_bin:\n                    rng = np.random.default_rng(seed=123)\n                    self.in_sample_residuals_by_bin_[bin] = rng.choice(\n                        res, size=max_residuals_per_bin, replace=False\n                    )\n\n    def _create_predict_inputs(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; Tuple[np.ndarray, Union[np.ndarray, None], pd.Index, pd.Index]:\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=self.exog_in_,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n                last_window_exog=None,\n                exog=exog,\n                exog_names_in_=self.exog_names_in_,\n                interval=None,\n                # alpha=None, # Removed alpha check for now\n            )\n\n        last_window = input_to_frame(data=last_window, input_name=\"last_window\")\n        _, last_window_index = check_extract_values_and_index(\n            data=last_window,\n            data_label=\"`last_window`\",\n            ignore_freq=True,\n            return_values=False,\n        )\n\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n\n        last_window = transform_dataframe(\n            df=last_window,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, _ = check_extract_values_and_index(\n            data=last_window, data_label=\"`last_window`\"\n        )\n        last_window_values = last_window_values.ravel()\n\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        exog_values = None\n        exog_index = None\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=False,\n                inverse_transform=False,\n            )\n\n            exog_values, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\"\n            )\n\n            exog_values = (\n                exog_values if isinstance(exog, pd.Series) else exog.to_numpy()\n            )\n\n        return last_window_values, exog_values, prediction_index, exog_index\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Union[np.ndarray, None] = None,\n    ) -&gt; np.ndarray:\n\n        predictions = np.full(shape=steps, fill_value=np.nan)\n\n        for step in range(steps):\n\n            X_gen = []\n\n            if self.lags is not None:\n                X_lags = last_window_values[-self.lags]\n                if X_lags.ndim == 1:\n                    X_lags = X_lags.reshape(1, -1)\n                X_gen.append(X_lags)\n\n            if self.window_features is not None:\n                X_window_features = []\n                for wf in self.window_features:\n                    wf_values = wf.transform(last_window_values)\n                    X_window_features.append(wf_values[-1:])\n\n                X_window_features = np.concatenate(X_window_features, axis=1)\n                X_gen.append(X_window_features)\n\n            if self.exog_in_:\n                X_exog = exog_values[step]\n                if X_exog.ndim &lt; 2:\n                    X_exog = X_exog.reshape(1, -1)\n                X_gen.append(X_exog)\n\n            X_gen = np.concatenate(X_gen, axis=1)\n\n            # Convert to DataFrame with feature names to avoid sklearn warning\n            if self.X_train_features_names_out_ is not None:\n                X_gen = pd.DataFrame(X_gen, columns=self.X_train_features_names_out_)\n\n            pred = self.estimator.predict(X_gen)\n            predictions[step] = pred[0]\n\n            last_window_values = np.append(last_window_values, pred)\n\n        return predictions\n\n    def predict(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; pd.Series:\n\n        last_window_values, exog_values, prediction_index, _ = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        predictions = self._recursive_predict(\n            steps=steps, last_window_values=last_window_values, exog_values=exog_values\n        )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_dataframe(\n            df=pd.Series(predictions, name=\"pred\").to_frame(),\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n        predictions = predictions.iloc[:, 0]\n        predictions.index = prediction_index\n\n        return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a ForecasterRecursive object is printed.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a ForecasterRecursive object is printed.\n    \"\"\"\n\n    params = (\n        self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n    )\n    exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Estimator: {type(self.estimator).__name__} \\n\"\n        f\"Lags: {self.lags} \\n\"\n        f\"Window features: {self.window_features_names} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Exogenous included: {self.exog_in_} \\n\"\n        f\"Exogenous names: {exog_names_in_} \\n\"\n        f\"Transformer for y: {self.transformer_y} \\n\"\n        f\"Transformer for exog: {self.transformer_exog} \\n\"\n        f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n        f\"Differentiation order: {self.differentiation} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Estimator parameters: {params} \\n\"\n        f\"fit_kwargs: {self.fit_kwargs} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"Skforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n    super().__setstate__(state)\n    if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",\n            \"forecasting_strategy\": \"recursive\",\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n</code></pre>"},{"location":"api/forecaster/#forecasting-utilities","title":"Forecasting Utilities","text":""},{"location":"api/forecaster/#utils","title":"utils","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils","title":"<code>spotforecast2_safe.forecaster.utils</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_extract_values_and_index","title":"<code>check_extract_values_and_index(data, data_label='`y`', ignore_freq=False, return_values=True)</code>","text":"<p>Extract values and index from a pandas Series or DataFrame, ensuring they are valid.</p> <p>Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts its values and index for use in forecasting operations. Optionally checks for index frequency consistency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data (pandas Series or DataFrame) to extract values and index from.</p> required <code>data_label</code> <code>str</code> <p>Label used in exception messages for better error reporting. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <code>ignore_freq</code> <code>bool</code> <p>If True, the frequency of the index is not checked. Defaults to False.</p> <code>False</code> <code>return_values</code> <code>bool</code> <p>If True, the values of the data are returned. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[ndarray], Index]</code> <p>A tuple containing: - values (numpy.ndarray or None): Values of the data as numpy array,   or None if return_values is False. - index (pandas.Index): Index of the data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame.</p> <code>TypeError</code> <p>If data index is not a DatetimeIndex or RangeIndex.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If DatetimeIndex has no frequency (inferred automatically).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n&gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n&gt;&gt;&gt; print(values.shape)\n(10,)\n&gt;&gt;&gt; print(type(index))\n&lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n</code></pre> <p>Extract index only:</p> <pre><code>&gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n&gt;&gt;&gt; print(index[0])\n2020-01-01 00:00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_extract_values_and_index(\n    data: Union[pd.Series, pd.DataFrame],\n    data_label: str = \"`y`\",\n    ignore_freq: bool = False,\n    return_values: bool = True,\n) -&gt; Tuple[Optional[np.ndarray], pd.Index]:\n    \"\"\"Extract values and index from a pandas Series or DataFrame, ensuring they are valid.\n\n    Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts\n    its values and index for use in forecasting operations. Optionally checks for\n    index frequency consistency.\n\n    Args:\n        data: Input data (pandas Series or DataFrame) to extract values and index from.\n        data_label: Label used in exception messages for better error reporting.\n            Defaults to \"`y`\".\n        ignore_freq: If True, the frequency of the index is not checked.\n            Defaults to False.\n        return_values: If True, the values of the data are returned.\n            Defaults to True.\n\n    Returns:\n        tuple: A tuple containing:\n            - values (numpy.ndarray or None): Values of the data as numpy array,\n              or None if return_values is False.\n            - index (pandas.Index): Index of the data.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame.\n        TypeError: If data index is not a DatetimeIndex or RangeIndex.\n\n    Warnings:\n        UserWarning: If DatetimeIndex has no frequency (inferred automatically).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n        &gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n        &gt;&gt;&gt; print(values.shape)\n        (10,)\n        &gt;&gt;&gt; print(type(index))\n        &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n\n        Extract index only:\n        &gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n        &gt;&gt;&gt; print(index[0])\n        2020-01-01 00:00:00\n    \"\"\"\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"{data_label} must be a pandas Series or DataFrame.\")\n\n    if not isinstance(data.index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(f\"{data_label} must have a pandas DatetimeIndex or RangeIndex.\")\n\n    if isinstance(data.index, pd.DatetimeIndex) and not ignore_freq:\n        if data.index.freq is None:\n            warnings.warn(\n                f\"{data_label} has a DatetimeIndex but no frequency. \"\n                \"The frequency has been inferred from the index.\",\n                UserWarning,\n            )\n\n    values = data.to_numpy() if return_values else None\n\n    return values, data.index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_optional_dependency","title":"<code>check_optional_dependency(package_name)</code>","text":"<p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not installed.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_optional_dependency(package_name: str) -&gt; None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError\n    with installation instructions.\n\n    Args:\n        package_name (str): Name of the package to check.\n\n    Raises:\n        ImportError: If the package is not installed.\n    \"\"\"\n\n    if find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(\n                package_name=package_name\n            )\n            msg = f\"\\n'{package_name}' is an optional dependency not included in the default spotforecast installation.\"\n        except Exception:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_residuals_input","title":"<code>check_residuals_input(forecaster_name, use_in_sample_residuals, in_sample_residuals_, out_sample_residuals_, use_binned_residuals, in_sample_residuals_by_bin_, out_sample_residuals_by_bin_, levels=None, encoding=None)</code>","text":"<p>Check residuals input arguments in Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>use_in_sample_residuals</code> <code>bool</code> <p>bool Indicates if in sample or out sample residuals are used.</p> required <code>in_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting training data.</p> required <code>out_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting non training data.</p> required <code>use_binned_residuals</code> <code>bool</code> <p>bool Indicates if residuals are binned.</p> required <code>in_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict In sample residuals binned according to the predicted value each residual is associated with.</p> required <code>out_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict Out of sample residuals binned according to the predicted value each residual is associated with.</p> required <code>levels</code> <code>list[str] | None</code> <p>list, default None Names of the series (levels) to be predicted (Forecasters multiseries).</p> <code>None</code> <code>encoding</code> <code>str | None</code> <p>str, default None Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    out_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    levels: list[str] | None = None,\n    encoding: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        use_in_sample_residuals: bool\n            Indicates if in sample or out sample residuals are used.\n        in_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting training data.\n        out_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting non training data.\n        use_binned_residuals: bool\n            Indicates if residuals are binned.\n        in_sample_residuals_by_bin_: dict\n            In sample residuals binned according to the predicted value each residual\n            is associated with.\n        out_sample_residuals_by_bin_: dict\n            Out of sample residuals binned according to the predicted value each residual\n            is associated with.\n        levels: list, default None\n            Names of the series (levels) to be predicted (Forecasters multiseries).\n        encoding: str, default None\n            Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns:\n        None\n\n    \"\"\"\n\n    forecasters_multiseries = (\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    )\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        # Check if residuals are empty or None\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position","title":"<code>date_to_index_position(index, date_input, method='prediction', date_literal='steps', kwargs_pd_to_datetime={})</code>","text":"<p>Transform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>pandas Index Original datetime index (must be a pandas DatetimeIndex if <code>date_input</code> is not an int).</p> required <code>date_input</code> <code>int | str | Timestamp</code> <p>int, str, pandas Timestamp Datetime to transform to integer.</p> <ul> <li>If int, returns the same integer.</li> <li>If str or pandas Timestamp, it is converted and expanded into the index.</li> </ul> required <code>method</code> <code>str</code> <p>str, default 'prediction' Can be 'prediction' or 'validation'.</p> <ul> <li>If 'prediction', the date must be later than the last date in the index.</li> <li>If 'validation', the date must be within the index range.</li> </ul> <code>'prediction'</code> <code>date_literal</code> <code>str</code> <p>str, default 'steps' Variable name used in error messages.</p> <code>'steps'</code> <code>kwargs_pd_to_datetime</code> <code>dict</code> <p>dict, default {} Additional keyword arguments to pass to <code>pd.to_datetime()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p><code>date_input</code> transformed to integer position in the <code>index</code>.</p> <code>int</code> <ul> <li>If <code>date_input</code> is an integer, it returns the same integer.</li> </ul> <code>int</code> <ul> <li>If method is 'prediction', number of steps to predict from the last</li> </ul> <code>int</code> <p>date in the index.</p> <code>int</code> <ul> <li>If method is 'validation', position plus one of the date in the index,</li> </ul> <code>int</code> <p>this is done to include the target date in the training set when using</p> <code>int</code> <p>pandas iloc with slices.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def date_to_index_position(\n    index: pd.Index,\n    date_input: int | str | pd.Timestamp,\n    method: str = \"prediction\",\n    date_literal: str = \"steps\",\n    kwargs_pd_to_datetime: dict = {},\n) -&gt; int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n\n    Args:\n        index: pandas Index\n            Original datetime index (must be a pandas DatetimeIndex if `date_input`\n            is not an int).\n        date_input: int, str, pandas Timestamp\n            Datetime to transform to integer.\n\n            - If int, returns the same integer.\n            - If str or pandas Timestamp, it is converted and expanded into the index.\n        method: str, default 'prediction'\n            Can be 'prediction' or 'validation'.\n\n            - If 'prediction', the date must be later than the last date in the index.\n            - If 'validation', the date must be within the index range.\n        date_literal: str, default 'steps'\n            Variable name used in error messages.\n        kwargs_pd_to_datetime: dict, default {}\n            Additional keyword arguments to pass to `pd.to_datetime()`.\n\n    Returns:\n        int:\n            `date_input` transformed to integer position in the `index`.\n\n        + If `date_input` is an integer, it returns the same integer.\n        + If method is 'prediction', number of steps to predict from the last\n        date in the index.\n        + If method is 'validation', position plus one of the date in the index,\n        this is done to include the target date in the training set when using\n        pandas iloc with slices.\n\n    \"\"\"\n\n    if method not in [\"prediction\", \"validation\"]:\n        raise ValueError(\"`method` must be 'prediction' or 'validation'.\")\n\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n\n        if method == \"prediction\":\n            if target_date &lt;= last_date:\n                raise ValueError(\n                    \"If `steps` is a date, it must be greater than the last date \"\n                    \"in the index.\"\n                )\n            span_index = pd.date_range(\n                start=last_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index) - 1\n        elif method == \"validation\":\n            first_date = pd.to_datetime(index[0])\n            if target_date &lt; first_date or target_date &gt; last_date:\n                raise ValueError(\n                    \"If `initial_train_size` is a date, it must be greater than \"\n                    \"the first date in the index and less than the last date.\"\n                )\n            span_index = pd.date_range(\n                start=first_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index)\n\n    elif isinstance(date_input, (int, np.integer)):\n        output = date_input\n\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Series | DataFrame</code> <p>pandas Series, pandas DataFrame Exogenous variables.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, list[str]]</code> <p>tuple[pd.DataFrame, list[str]]: exog_direct: pandas DataFrame     Exogenous variables transformed. exog_direct_names: list     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct(\n    exog: pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: pandas Series, pandas DataFrame\n            Exogenous variables.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[pd.DataFrame, list[str]]:\n            exog_direct: pandas DataFrame\n                Exogenous variables transformed.\n            exog_direct_names: list\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i),]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    exog_direct = pd.concat(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct) :]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>ndarray | Series | DataFrame</code> <p>numpy ndarray, pandas Series, pandas DataFrame Exogenous variables, shape(samples,). If exog is a pandas format, the direct exog names are created.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, list[str] | None]</code> <p>tuple[np.ndarray, list[str] | None]: exog_direct: numpy ndarray     Exogenous variables transformed. exog_direct_names: list, None     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray | pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[np.ndarray, list[str] | None]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: numpy ndarray, pandas Series, pandas DataFrame\n            Exogenous variables, shape(samples,). If exog is a pandas format, the\n            direct exog names are created.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[np.ndarray, list[str] | None]:\n            exog_direct: numpy ndarray\n                Exogenous variables transformed.\n            exog_direct_names: list, None\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = [exog[i : n_rows - (steps - 1 - i)] for i in range(steps)]\n    exog_direct = np.concatenate(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.get_style_repr_html","title":"<code>get_style_repr_html(is_fitted=False)</code>","text":"<p>Generate CSS style for HTML representation of the Forecaster.</p> <p>Creates a unique CSS style block with a container ID for rendering forecaster objects in Jupyter notebooks or HTML documents. The styling provides a clean, monospace display with a light gray background.</p> <p>Parameters:</p> Name Type Description Default <code>is_fitted</code> <code>bool</code> <p>Parameter to indicate if the Forecaster has been fitted. Currently not used in styling but reserved for future extensions.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, str]</code> <p>A tuple containing: - style (str): CSS style block as a string with unique container class. - unique_id (str): Unique 8-character ID for the container element.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n&gt;&gt;&gt; print(f\"Container ID: {uid}\")\nContainer ID: a1b2c3d4\n&gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\nStyle contains CSS: True\n</code></pre> <p>Using in HTML rendering:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n&gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n&gt;&gt;&gt; print(\"background-color\" in html)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def get_style_repr_html(is_fitted: bool = False) -&gt; Tuple[str, str]:\n    \"\"\"Generate CSS style for HTML representation of the Forecaster.\n\n    Creates a unique CSS style block with a container ID for rendering\n    forecaster objects in Jupyter notebooks or HTML documents. The styling\n    provides a clean, monospace display with a light gray background.\n\n    Args:\n        is_fitted: Parameter to indicate if the Forecaster has been fitted.\n            Currently not used in styling but reserved for future extensions.\n\n    Returns:\n        tuple: A tuple containing:\n            - style (str): CSS style block as a string with unique container class.\n            - unique_id (str): Unique 8-character ID for the container element.\n\n    Examples:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n        &gt;&gt;&gt; print(f\"Container ID: {uid}\")\n        Container ID: a1b2c3d4\n        &gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\n        Style contains CSS: True\n\n        Using in HTML rendering:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n        &gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n        &gt;&gt;&gt; print(\"background-color\" in html)\n        True\n    \"\"\"\n\n    unique_id = str(uuid.uuid4())[:8]\n    style = f\"\"\"\n    &lt;style&gt;\n        .container-{unique_id} {{\n            font-family: monospace;\n            background-color: #f0f0f0;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n    &lt;/style&gt;\n    \"\"\"\n    return style, unique_id\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_estimator","title":"<code>initialize_estimator(estimator=None, regressor=None)</code>","text":"<p>Helper to handle the deprecation of 'regressor' in favor of 'estimator'. Returns the valid estimator object.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <code>regressor</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None Deprecated. An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>estimator or pipeline compatible with the scikit-learn API The valid estimator object.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_estimator(\n    estimator: object | None = None, regressor: object | None = None\n) -&gt; None:\n    \"\"\"\n    Helper to handle the deprecation of 'regressor' in favor of 'estimator'.\n    Returns the valid estimator object.\n\n    Args:\n        estimator: estimator or pipeline compatible with the scikit-learn API, default None\n            An instance of a estimator or pipeline compatible with the scikit-learn API.\n        regressor: estimator or pipeline compatible with the scikit-learn API, default None\n            Deprecated. An instance of a estimator or pipeline compatible with the\n            scikit-learn API.\n\n    Returns:\n        estimator or pipeline compatible with the scikit-learn API\n            The valid estimator object.\n\n    \"\"\"\n\n    if regressor is not None:\n        warnings.warn(\n            \"The `regressor` argument is deprecated and will be removed in a future \"\n            \"version. Please use `estimator` instead.\",\n            FutureWarning,\n            stacklevel=3,  # Important: to point to the user's code\n        )\n        if estimator is not None:\n            raise ValueError(\n                \"Both `estimator` and `regressor` were provided. Use only `estimator`.\"\n            )\n        return regressor\n\n    return estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_transformer_series","title":"<code>initialize_transformer_series(forecaster_name, series_names_in_, encoding=None, transformer_series=None)</code>","text":"<p>Initialize transformer_series_ attribute for multivariate/multiseries forecasters.</p> <p>Creates a dictionary of transformers for each time series in multivariate or multiseries forecasting. Handles three cases: no transformation (None), same transformer for all series (single object), or different transformers per series (dictionary). Clones transformer objects to avoid overwriting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster using this function. Special handling is applied for 'ForecasterRecursiveMultiSeries'.</p> required <code>series_names_in_</code> <code>list[str]</code> <p>Names of the time series (levels) used during training. These will be the keys in the returned transformer dictionary.</p> required <code>encoding</code> <code>str | None</code> <p>Encoding used to identify different series. Only used for ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level' entry. Defaults to None.</p> <code>None</code> <code>transformer_series</code> <code>object | dict[str, object | None] | None</code> <p>Transformer(s) to apply to series. Can be: - None: No transformation applied - Single transformer object: Same transformer cloned for all series - Dict mapping series names to transformers: Different transformer per series Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, object | None]</code> <p>Dictionary with series names as keys and transformer objects (or None) as values. Transformers are cloned to prevent overwriting.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If transformer_series is a dict and some series_names_in_ are not present in the dict keys (those series get no transformation).</p> <p>Examples:</p> <p>No transformation:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n&gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=series,\n...     transformer_series=None\n... )\n&gt;&gt;&gt; print(result)\n{'series1': None, 'series2': None, 'series3': None}\n</code></pre> <p>Same transformer for all series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=scaler\n... )\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\nTrue\n&gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\nFalse\n</code></pre> <p>Different transformer per series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; transformers = {\n...     'series1': StandardScaler(),\n...     'series2': MinMaxScaler()\n... }\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=transformers\n... )\n&gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\nTrue\n&gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list[str],\n    encoding: str | None = None,\n    transformer_series: object | dict[str, object | None] | None = None,\n) -&gt; dict[str, object | None]:\n    \"\"\"Initialize transformer_series_ attribute for multivariate/multiseries forecasters.\n\n    Creates a dictionary of transformers for each time series in multivariate or\n    multiseries forecasting. Handles three cases: no transformation (None), same\n    transformer for all series (single object), or different transformers per series\n    (dictionary). Clones transformer objects to avoid overwriting.\n\n    Args:\n        forecaster_name: Name of the forecaster using this function. Special handling\n            is applied for 'ForecasterRecursiveMultiSeries'.\n        series_names_in_: Names of the time series (levels) used during training.\n            These will be the keys in the returned transformer dictionary.\n        encoding: Encoding used to identify different series. Only used for\n            ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level'\n            entry. Defaults to None.\n        transformer_series: Transformer(s) to apply to series. Can be:\n            - None: No transformation applied\n            - Single transformer object: Same transformer cloned for all series\n            - Dict mapping series names to transformers: Different transformer per series\n            Defaults to None.\n\n    Returns:\n        dict: Dictionary with series names as keys and transformer objects (or None)\n            as values. Transformers are cloned to prevent overwriting.\n\n    Warnings:\n        IgnoredArgumentWarning: If transformer_series is a dict and some series_names_in_\n            are not present in the dict keys (those series get no transformation).\n\n    Examples:\n        No transformation:\n        &gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n        &gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=series,\n        ...     transformer_series=None\n        ... )\n        &gt;&gt;&gt; print(result)\n        {'series1': None, 'series2': None, 'series3': None}\n\n        Same transformer for all series:\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=scaler\n        ... )\n        &gt;&gt;&gt; len(result)\n        2\n        &gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\n        True\n        &gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\n        False\n\n        Different transformer per series:\n        &gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n        &gt;&gt;&gt; transformers = {\n        ...     'series1': StandardScaler(),\n        ...     'series2': MinMaxScaler()\n        ... }\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=transformers\n        ... )\n        &gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\n        True\n        &gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\n        True\n    \"\"\"\n    from copy import deepcopy\n    from sklearn.base import clone\n    from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n\n    if forecaster_name == \"ForecasterRecursiveMultiSeries\":\n        if encoding is None:\n            series_names_in_ = [\"_unknown_level\"]\n        else:\n            series_names_in_ = series_names_in_ + [\"_unknown_level\"]\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {\n            serie: clone(transformer_series) for serie in series_names_in_\n        }\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            {\n                k: deepcopy(v)\n                for k, v in transformer_series.items()\n                if k in transformer_series_\n            }\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {\"_unknown_level\"}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\",\n                IgnoredArgumentWarning,\n            )\n\n    return transformer_series_\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_window_features","title":"<code>initialize_window_features(window_features)</code>","text":"<p>Check window_features argument input and generate the corresponding list.</p> <p>This function validates window feature objects and extracts their metadata, ensuring they have the required attributes (window_sizes, features_names) and methods (transform_batch, transform) for proper forecasting operations.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>Any</code> <p>Classes used to create window features. Can be a single object or a list of objects. Each object must have <code>window_sizes</code>, <code>features_names</code> attributes and <code>transform_batch</code>, <code>transform</code> methods.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]</code> <p>A tuple containing: - window_features (list or None): List of classes used to create window features. - window_features_names (list or None): List with all the features names of the window features. - max_size_window_features (int or None): Maximum value of the <code>window_sizes</code> attribute of all classes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>window_features</code> is an empty list.</p> <code>ValueError</code> <p>If a window feature is missing required attributes or methods.</p> <code>TypeError</code> <p>If <code>window_sizes</code> or <code>features_names</code> have incorrect types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 14\n&gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\nNumber of features: 4\n</code></pre> <p>Multiple window features:</p> <pre><code>&gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n&gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 7\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_window_features(\n    window_features: Any,\n) -&gt; Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]:\n    \"\"\"Check window_features argument input and generate the corresponding list.\n\n    This function validates window feature objects and extracts their metadata,\n    ensuring they have the required attributes (window_sizes, features_names) and\n    methods (transform_batch, transform) for proper forecasting operations.\n\n    Args:\n        window_features: Classes used to create window features. Can be a single\n            object or a list of objects. Each object must have `window_sizes`,\n            `features_names` attributes and `transform_batch`, `transform` methods.\n\n    Returns:\n        tuple: A tuple containing:\n            - window_features (list or None): List of classes used to create window features.\n            - window_features_names (list or None): List with all the features names of the window features.\n            - max_size_window_features (int or None): Maximum value of the `window_sizes` attribute of all classes.\n\n    Raises:\n        ValueError: If `window_features` is an empty list.\n        ValueError: If a window feature is missing required attributes or methods.\n        TypeError: If `window_sizes` or `features_names` have incorrect types.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 14\n        &gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\n        Number of features: 4\n\n        Multiple window features:\n        &gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n        &gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 7\n    \"\"\"\n\n    needed_atts = [\"window_sizes\", \"features_names\"]\n    needed_methods = [\"transform_batch\", \"transform\"]\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) &lt; 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n\n        max_window_sizes = []\n        window_features_names = []\n        needed_atts_set = set(needed_atts)\n        needed_methods_set = set(needed_methods)\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set(dir(wf))\n            if not needed_atts_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not needed_methods_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n\n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n\n            if isinstance(window_sizes, int):\n                if window_sizes &lt; 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws &gt;= 1 for ws in window_sizes\n                ):\n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.predict_multivariate","title":"<code>predict_multivariate(forecasters, steps_ahead, exog=None, show_progress=False)</code>","text":"<p>Generate multi-output predictions using multiple baseline forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>dict</code> <p>Dictionary of fitted forecaster instances (one per target). Keys are target names, values are the fitted forecasters (e.g., ForecasterRecursive, ForecasterEquivalentDate).</p> required <code>steps_ahead</code> <code>int</code> <p>Number of steps to forecast.</p> required <code>exog</code> <code>DataFrame</code> <p>Exogenous variables for prediction. If provided, will be passed to each forecaster's predict method.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar while predicting per target forecaster. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with predictions for all targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n&gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n&gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f1.fit(y=y1)\n&gt;&gt;&gt; f2.fit(y=y2)\n&gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n&gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n&gt;&gt;&gt; predictions\n   target1  target2\n5      6.0     12.0\n6      7.0     14.0\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def predict_multivariate(\n    forecasters: dict[str, Any],\n    steps_ahead: int,\n    exog: pd.DataFrame | None = None,\n    show_progress: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multi-output predictions using multiple baseline forecasters.\n\n    Args:\n        forecasters (dict): Dictionary of fitted forecaster instances (one per target).\n            Keys are target names, values are the fitted forecasters (e.g.,\n            ForecasterRecursive, ForecasterEquivalentDate).\n        steps_ahead (int): Number of steps to forecast.\n        exog (pd.DataFrame, optional): Exogenous variables for prediction.\n            If provided, will be passed to each forecaster's predict method.\n        show_progress (bool, optional): Show progress bar while predicting\n            per target forecaster. Default: False.\n\n    Returns:\n        pd.DataFrame: DataFrame with predictions for all targets.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n        &gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n        &gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f1.fit(y=y1)\n        &gt;&gt;&gt; f2.fit(y=y2)\n        &gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n        &gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n        &gt;&gt;&gt; predictions\n           target1  target2\n        5      6.0     12.0\n        6      7.0     14.0\n    \"\"\"\n\n    if not forecasters:\n        return pd.DataFrame()\n\n    predictions = {}\n\n    target_iter = forecasters.items()\n    if show_progress and tqdm is not None:\n        target_iter = tqdm(\n            forecasters.items(),\n            desc=\"Predicting targets\",\n            unit=\"model\",\n        )\n\n    for target, forecaster in target_iter:\n        # Generate predictions for this target\n        if exog is not None:\n            pred = forecaster.predict(steps=steps_ahead, exog=exog)\n        else:\n            pred = forecaster.predict(steps=steps_ahead)\n        predictions[target] = pred\n\n    # Combine into a single DataFrame\n    return pd.concat(predictions, axis=1)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.prepare_steps_direct","title":"<code>prepare_steps_direct(max_step, steps=None)</code>","text":"<p>Prepare list of steps to be predicted in Direct Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>max_step</code> <code>int | list[int] | ndarray</code> <p>int, list, numpy ndarray Maximum number of future steps the forecaster will predict when using predict methods.</p> required <code>steps</code> <code>int | list[int] | None</code> <p>int, list, None, default None Predict n steps. The value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list   are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at   initialization.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: Steps to be predicted.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def prepare_steps_direct(\n    max_step: int | list[int] | np.ndarray, steps: int | list[int] | None = None\n) -&gt; list[int]:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Args:\n        max_step: int, list, numpy ndarray\n            Maximum number of future steps the forecaster will predict\n            when using predict methods.\n        steps: int, list, None, default None\n            Predict n steps. The value of `steps` must be less than or equal to the\n            value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list\n              are predicted.\n            - If `None`: As many steps are predicted as were defined at\n              initialization.\n\n    Returns:\n        list[int]:\n            Steps to be predicted.\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps_direct = list(range(1, steps + 1))\n    elif steps is None:\n        if isinstance(max_step, int):\n            steps_direct = list(range(1, max_step + 1))\n        else:\n            steps_direct = [int(s) for s in max_step]\n    elif isinstance(steps, list):\n        steps_direct = []\n        for step in steps:\n            if not isinstance(step, (int, np.integer)):\n                raise TypeError(\n                    f\"`steps` argument must be an int, a list of ints or `None`. \"\n                    f\"Got {type(steps)}.\"\n                )\n            steps_direct.append(int(step))\n\n    return steps_direct\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.select_n_jobs_fit_forecaster","title":"<code>select_n_jobs_fit_forecaster(forecaster_name, estimator)</code>","text":"<p>Select the number of jobs to run in parallel.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(forecaster_name, estimator):\n    \"\"\"\n    Select the number of jobs to run in parallel.\n    \"\"\"\n    import os\n\n    return os.cpu_count() or 1\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_numpy","title":"<code>transform_numpy(array, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of a numpy ndarray with a scikit-learn alike transformer, preprocessor or ColumnTransformer. The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>numpy ndarray Array to be transformed.</p> required <code>transformer</code> <code>object | None</code> <p>scikit-learn alike transformer, preprocessor, or ColumnTransformer. Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <p>fit: bool, default False     Train the transformer before applying it. inverse_transform: bool, default False     Transform back the data to the original representation. This is not available     when using transformers of class scikit-learn ColumnTransformers.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_numpy--returns","title":"Returns","text":"<p>array_transformed : numpy ndarray     Transformed array.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def transform_numpy(\n    array: np.ndarray,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer. The transformer used must\n    have the following methods: fit, transform, fit_transform and\n    inverse_transform. ColumnTransformers are not allowed since they do not\n    have inverse_transform method.\n\n    Args:\n        array: numpy ndarray\n            Array to be transformed.\n        transformer: scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n            fit_transform and inverse_transform.\n    fit: bool, default False\n        Train the transformer before applying it.\n    inverse_transform: bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n\n    if transformer is None:\n        return array\n\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"`array` argument must be a numpy ndarray. Got {type(array)}\")\n\n    original_ndim = array.ndim\n    original_shape = array.shape\n    reshaped_for_inverse = False\n\n    if original_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n        )\n        if not inverse_transform:\n            if fit:\n                array_transformed = transformer.fit_transform(array)\n            else:\n                array_transformed = transformer.transform(array)\n        else:\n            # Vectorized inverse transformation for 2D arrays with multiple columns.\n            # Reshape to single column, transform, and reshape back.\n            # This is faster than applying the transformer column by column.\n            if array.shape[1] &gt; 1:\n                array = array.reshape(-1, 1)\n                reshaped_for_inverse = True\n            array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if isinstance(array_transformed, (pd.Series, pd.DataFrame)):\n        array_transformed = array_transformed.to_numpy()\n\n    # Reshape back to original shape only if we reshaped for inverse_transform\n    if reshaped_for_inverse:\n        array_transformed = array_transformed.reshape(original_shape)\n\n    if original_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing Module","text":"<p>Tools for data preprocessing, cleaning, and transformation.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing","title":"<code>spotforecast2_safe.preprocessing</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner","title":"<code>QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures","title":"<code>RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        For each statistic and window size combination, computes the rolling\n        statistic across the input time series. The output contains NaN values\n        for the initial positions where the window cannot be fully computed.\n\n        Args:\n            X: Time series data as 1D numpy array or array-like.\n\n        Returns:\n            np.ndarray: Array of shape (len(X), len(features_names)) containing\n                the computed rolling statistics. Each column corresponds to a\n                feature in features_names. Early positions contain NaN values\n                before the window is fully populated.\n        \"\"\"\n        # Assume X is 1D array\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n\n        idx_feature = 0\n        for ws in self.window_sizes:\n            for func in self.stats_funcs:\n                # Naive rolling window loop - can be optimized or use pandas rolling\n                # Using pandas for simplicity and speed if X is convertible\n                series = pd.Series(X)\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n                idx_feature += 1\n\n        return output\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n\n        Transforms a pandas Series into a DataFrame of rolling statistics while\n        preserving the original index. Useful for maintaining time alignment\n        with the input data.\n\n        Args:\n            X: Time series data as pandas Series. The index is preserved in output.\n\n        Returns:\n            pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n                columns are feature names and index matches the input Series.\n                Contains NaN values at the beginning where windows are incomplete.\n\n        Note:\n            This method is preferred over transform() when working with time-indexed\n            data, as it preserves the temporal index and is compatible with\n            forecasting workflows.\n        \"\"\"\n        values = X.to_numpy()\n        transformed = self.transform(values)\n        return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>For each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D numpy array or array-like.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    For each statistic and window size combination, computes the rolling\n    statistic across the input time series. The output contains NaN values\n    for the initial positions where the window cannot be fully computed.\n\n    Args:\n        X: Time series data as 1D numpy array or array-like.\n\n    Returns:\n        np.ndarray: Array of shape (len(X), len(features_names)) containing\n            the computed rolling statistics. Each column corresponds to a\n            feature in features_names. Early positions contain NaN values\n            before the window is fully populated.\n    \"\"\"\n    # Assume X is 1D array\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n\n    idx_feature = 0\n    for ws in self.window_sizes:\n        for func in self.stats_funcs:\n            # Naive rolling window loop - can be optimized or use pandas rolling\n            # Using pandas for simplicity and speed if X is convertible\n            series = pd.Series(X)\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n            idx_feature += 1\n\n    return output\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> <p>Transforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series</code> <p>Time series data as pandas Series. The index is preserved in output.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.</p> Note <p>This method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n\n    Transforms a pandas Series into a DataFrame of rolling statistics while\n    preserving the original index. Useful for maintaining time alignment\n    with the input data.\n\n    Args:\n        X: Time series data as pandas Series. The index is preserved in output.\n\n    Returns:\n        pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n            columns are feature names and index matches the input Series.\n            Contains NaN values at the beginning where windows are incomplete.\n\n    Note:\n        This method is preferred over transform() when working with time-indexed\n        data, as it preserves the temporal index and is compatible with\n        forecasting workflows.\n    \"\"\"\n    values = X.to_numpy()\n    transformed = self.transform(values)\n    return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction","title":"<code>WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.agg_and_resample_data","title":"<code>agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.basic_ts_checks","title":"<code>basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_holidays","title":"<code>curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_weather","title":"<code>curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.custom_weights","title":"<code>custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_missing_weights","title":"<code>get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_start_end","title":"<code>get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split_abs_train_val_test","title":"<code>split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split_rel_train_val_test","title":"<code>split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_test = 0\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#data-curation","title":"Data Curation","text":""},{"location":"api/preprocessing/#curate_data","title":"curate_data","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data","title":"<code>spotforecast2_safe.preprocessing.curate_data</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.agg_and_resample_data","title":"<code>agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.basic_ts_checks","title":"<code>basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.curate_holidays","title":"<code>curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.curate_weather","title":"<code>curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.get_start_end","title":"<code>get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#imputation","title":"Imputation","text":""},{"location":"api/preprocessing/#imputation_1","title":"imputation","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation","title":"<code>spotforecast2_safe.preprocessing.imputation</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction","title":"<code>WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.custom_weights","title":"<code>custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.get_missing_weights","title":"<code>get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#outlier-detection-and-handling","title":"Outlier Detection and Handling","text":""},{"location":"api/preprocessing/#outlier","title":"outlier","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier","title":"<code>spotforecast2_safe.preprocessing.outlier</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#time-series-splitting","title":"Time Series Splitting","text":""},{"location":"api/preprocessing/#split","title":"split","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split","title":"<code>spotforecast2_safe.preprocessing.split</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split.split_abs_train_val_test","title":"<code>split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split.split_rel_train_val_test","title":"<code>split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_test = 0\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#rolling-features","title":"Rolling Features","text":""},{"location":"api/preprocessing/#_rolling","title":"_rolling","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling","title":"<code>spotforecast2_safe.preprocessing._rolling</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures","title":"<code>RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        For each statistic and window size combination, computes the rolling\n        statistic across the input time series. The output contains NaN values\n        for the initial positions where the window cannot be fully computed.\n\n        Args:\n            X: Time series data as 1D numpy array or array-like.\n\n        Returns:\n            np.ndarray: Array of shape (len(X), len(features_names)) containing\n                the computed rolling statistics. Each column corresponds to a\n                feature in features_names. Early positions contain NaN values\n                before the window is fully populated.\n        \"\"\"\n        # Assume X is 1D array\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n\n        idx_feature = 0\n        for ws in self.window_sizes:\n            for func in self.stats_funcs:\n                # Naive rolling window loop - can be optimized or use pandas rolling\n                # Using pandas for simplicity and speed if X is convertible\n                series = pd.Series(X)\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n                idx_feature += 1\n\n        return output\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n\n        Transforms a pandas Series into a DataFrame of rolling statistics while\n        preserving the original index. Useful for maintaining time alignment\n        with the input data.\n\n        Args:\n            X: Time series data as pandas Series. The index is preserved in output.\n\n        Returns:\n            pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n                columns are feature names and index matches the input Series.\n                Contains NaN values at the beginning where windows are incomplete.\n\n        Note:\n            This method is preferred over transform() when working with time-indexed\n            data, as it preserves the temporal index and is compatible with\n            forecasting workflows.\n        \"\"\"\n        values = X.to_numpy()\n        transformed = self.transform(values)\n        return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>For each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D numpy array or array-like.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    For each statistic and window size combination, computes the rolling\n    statistic across the input time series. The output contains NaN values\n    for the initial positions where the window cannot be fully computed.\n\n    Args:\n        X: Time series data as 1D numpy array or array-like.\n\n    Returns:\n        np.ndarray: Array of shape (len(X), len(features_names)) containing\n            the computed rolling statistics. Each column corresponds to a\n            feature in features_names. Early positions contain NaN values\n            before the window is fully populated.\n    \"\"\"\n    # Assume X is 1D array\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n\n    idx_feature = 0\n    for ws in self.window_sizes:\n        for func in self.stats_funcs:\n            # Naive rolling window loop - can be optimized or use pandas rolling\n            # Using pandas for simplicity and speed if X is convertible\n            series = pd.Series(X)\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n            idx_feature += 1\n\n    return output\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> <p>Transforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series</code> <p>Time series data as pandas Series. The index is preserved in output.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.</p> Note <p>This method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n\n    Transforms a pandas Series into a DataFrame of rolling statistics while\n    preserving the original index. Useful for maintaining time alignment\n    with the input data.\n\n    Args:\n        X: Time series data as pandas Series. The index is preserved in output.\n\n    Returns:\n        pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n            columns are feature names and index matches the input Series.\n            Contains NaN values at the beginning where windows are incomplete.\n\n    Note:\n        This method is preferred over transform() when working with time-indexed\n        data, as it preserves the temporal index and is compatible with\n        forecasting workflows.\n    \"\"\"\n    values = X.to_numpy()\n    transformed = self.transform(values)\n    return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#differencing","title":"Differencing","text":""},{"location":"api/preprocessing/#_differentiator","title":"_differentiator","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator","title":"<code>spotforecast2_safe.preprocessing._differentiator</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#binning","title":"Binning","text":""},{"location":"api/preprocessing/#_binner","title":"_binner","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner","title":"<code>spotforecast2_safe.preprocessing._binner</code>","text":"<p>QuantileBinner class for binning data into quantile-based bins.</p> <p>This module contains the QuantileBinner class which bins data into quantile-based bins using numpy.percentile with optimized performance using numpy.searchsorted.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner","title":"<code>QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/processing/","title":"Processing Module","text":"<p>End-to-end forecasting pipelines and prediction aggregation.</p>"},{"location":"api/processing/#spotforecast2_safe.processing","title":"<code>spotforecast2_safe.processing</code>","text":"<p>Processing module for end-to-end forecasting pipelines.</p>"},{"location":"api/processing/#n-to-n-prediction","title":"N-to-N Prediction","text":""},{"location":"api/processing/#n2n_predict","title":"n2n_predict","text":""},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict","title":"<code>spotforecast2_safe.processing.n2n_predict</code>","text":"<p>End-to-end baseline forecasting using equivalent date method.</p> <p>This module provides a complete forecasting pipeline using the ForecasterEquivalentDate baseline model. It handles data preparation, outlier detection, imputation, model training, and prediction in a single integrated function.</p> <p>Model persistence follows scikit-learn conventions using joblib for efficient serialization and deserialization of trained forecasters.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.processing.n2n_predict import n2n_predict\n&gt;&gt;&gt; predictions = n2n_predict(forecast_horizon=24, verbose=True)\n</code></pre> <p>Using cached models:</p> <pre><code>&gt;&gt;&gt; # Load existing models if available, or train new ones\n&gt;&gt;&gt; predictions = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre>"},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict.n2n_predict","title":"<code>n2n_predict(data=None, columns=None, forecast_horizon=24, contamination=0.01, window_size=72, force_train=True, model_dir=None, verbose=True, show_progress=True)</code>","text":"<p>End-to-end baseline forecasting using equivalent date method.</p> <p>This function implements a complete forecasting pipeline that: 1. Loads and validates target data 2. Detects and removes outliers 3. Imputes missing values 4. Splits into train/validation/test sets 5. Trains or loads equivalent date forecasters 6. Generates multi-step ahead predictions</p> <p>Models are persisted to disk following scikit-learn conventions using joblib. By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with target time series data. If None, fetches data automatically. Default: None.</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>List of target columns to forecast. If None, uses all available columns. Default: None.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to forecast ahead. Default: 24.</p> <code>24</code> <code>contamination</code> <code>float</code> <p>Contamination parameter for outlier detection. Default: 0.01.</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Rolling window size for gap detection. Default: 72.</p> <code>72</code> <code>force_train</code> <code>bool</code> <p>Force retraining of all models, ignoring cached models. Default: True.</p> <code>True</code> <code>model_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory for saving/loading trained models. If None, uses cache directory from get_cache_home(). Default: None (uses ~/spotforecast2_cache/forecasters).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: True.</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar during training and prediction. Default: True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>Dict</code> <ul> <li>predictions: DataFrame with forecast values for each target variable.</li> </ul> <code>Tuple[DataFrame, Dict]</code> <ul> <li>forecasters: Dictionary of trained ForecasterEquivalentDate objects keyed by target.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data validation fails or required data cannot be retrieved.</p> <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>OSError</code> <p>If models cannot be saved to disk.</p> <p>Examples:</p> <p>Basic usage with automatic model caching:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n&gt;&gt;&gt; print(predictions.shape)\n(24, 11)\n</code></pre> <p>Load cached models (if available):</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./saved_models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./saved_models\",\n...     verbose=True\n... )\n</code></pre> <p>With specific target columns:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     columns=[\"power\", \"energy\"],\n...     forecast_horizon=48,\n...     force_train=False,\n...     verbose=True\n... )\n</code></pre> Notes <ul> <li>Trained models are saved to disk using joblib for fast reuse.</li> <li>When force_train=False, existing models are loaded and prediction   proceeds without retraining. This significantly speeds up prediction   for repeated calls with the same configuration.</li> <li>The model_dir directory is created automatically if it doesn't exist.</li> <li>Default model_dir uses get_cache_home() which respects the   SPOTFORECAST2_CACHE environment variable.</li> </ul> Performance Notes <ul> <li>First run: Full training (~2-5 minutes depending on data size)</li> <li>Subsequent runs (force_train=False): Model loading only (~1-2 seconds)</li> <li>Force retrain (force_train=True): Full training again (~2-5 minutes)</li> </ul> Source code in <code>src/spotforecast2_safe/processing/n2n_predict.py</code> <pre><code>def n2n_predict(\n    data: Optional[pd.DataFrame] = None,\n    columns: Optional[List[str]] = None,\n    forecast_horizon: int = 24,\n    contamination: float = 0.01,\n    window_size: int = 72,\n    force_train: bool = True,\n    model_dir: Optional[Union[str, Path]] = None,\n    verbose: bool = True,\n    show_progress: bool = True,\n) -&gt; Tuple[pd.DataFrame, Dict]:\n    \"\"\"End-to-end baseline forecasting using equivalent date method.\n\n    This function implements a complete forecasting pipeline that:\n    1. Loads and validates target data\n    2. Detects and removes outliers\n    3. Imputes missing values\n    4. Splits into train/validation/test sets\n    5. Trains or loads equivalent date forecasters\n    6. Generates multi-step ahead predictions\n\n    Models are persisted to disk following scikit-learn conventions using joblib.\n    By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.\n\n    Args:\n        data: Optional DataFrame with target time series data. If None, fetches data automatically.\n            Default: None.\n        columns: List of target columns to forecast. If None, uses all available columns.\n            Default: None.\n        forecast_horizon: Number of time steps to forecast ahead. Default: 24.\n        contamination: Contamination parameter for outlier detection. Default: 0.01.\n        window_size: Rolling window size for gap detection. Default: 72.\n        force_train: Force retraining of all models, ignoring cached models.\n            Default: True.\n        model_dir: Directory for saving/loading trained models. If None, uses cache directory from get_cache_home(). Default: None (uses ~/spotforecast2_cache/forecasters).\n        verbose: Print progress messages. Default: True.\n        show_progress: Show progress bar during training and prediction. Default: True.\n\n    Returns:\n        Tuple containing:\n        - predictions: DataFrame with forecast values for each target variable.\n        - forecasters: Dictionary of trained ForecasterEquivalentDate objects keyed by target.\n\n    Raises:\n        ValueError: If data validation fails or required data cannot be retrieved.\n        ImportError: If required dependencies are not installed.\n        OSError: If models cannot be saved to disk.\n\n    Examples:\n        Basic usage with automatic model caching:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(predictions.shape)\n        (24, 11)\n\n        Load cached models (if available):\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     force_train=False,\n        ...     model_dir=\"./saved_models\",\n        ...     verbose=True\n        ... )\n\n        Force retraining and update cache:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     force_train=True,\n        ...     model_dir=\"./saved_models\",\n        ...     verbose=True\n        ... )\n\n        With specific target columns:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     columns=[\"power\", \"energy\"],\n        ...     forecast_horizon=48,\n        ...     force_train=False,\n        ...     verbose=True\n        ... )\n\n    Notes:\n        - Trained models are saved to disk using joblib for fast reuse.\n        - When force_train=False, existing models are loaded and prediction\n          proceeds without retraining. This significantly speeds up prediction\n          for repeated calls with the same configuration.\n        - The model_dir directory is created automatically if it doesn't exist.\n        - Default model_dir uses get_cache_home() which respects the\n          SPOTFORECAST2_CACHE environment variable.\n\n    Performance Notes:\n        - First run: Full training (~2-5 minutes depending on data size)\n        - Subsequent runs (force_train=False): Model loading only (~1-2 seconds)\n        - Force retrain (force_train=True): Full training again (~2-5 minutes)\n    \"\"\"\n    if columns is not None:\n        TARGET = columns\n    else:\n        TARGET = None\n\n    if verbose:\n        print(\"--- Starting n2n_predict ---\")\n\n    # Set default model_dir if not provided\n    if model_dir is None:\n        from spotforecast2_safe.data.fetch_data import get_cache_home\n\n        model_dir = get_cache_home() / \"forecasters\"\n\n    # Handle data input - fetch_data handles both CSV and DataFrame\n    if data is not None:\n        if verbose:\n            print(\"Using provided dataframe...\")\n        data = fetch_data(dataframe=data, columns=TARGET)\n    else:\n        if verbose:\n            print(\"Fetching data from CSV...\")\n        data = fetch_data(columns=TARGET)\n\n    START, END, COV_START, COV_END = get_start_end(\n        data=data,\n        forecast_horizon=forecast_horizon,\n        verbose=verbose,\n    )\n\n    basic_ts_checks(data, verbose=verbose)\n\n    data = agg_and_resample_data(data, verbose=verbose)\n\n    # --- Outlier Handling ---\n    if verbose:\n        print(\"Handling outliers...\")\n\n    # data_old = data.copy() # kept in notebook, maybe useful for debugging but not used logic-wise here\n    data, outliers = mark_outliers(\n        data, contamination=contamination, random_state=1234, verbose=verbose\n    )\n\n    # --- Missing Data (Imputation) ---\n    if verbose:\n        print(\"Imputing missing data...\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    if verbose:\n        n_missing = len(missing_indices)\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n\n    data = data.ffill()\n    data = data.bfill()\n\n    # --- Train, Val, Test Split ---\n    if verbose:\n        print(\"Splitting data...\")\n    data_train, data_val, data_test = split_rel_train_val_test(\n        data, perc_train=0.8, perc_val=0.2, verbose=verbose\n    )\n\n    # --- Model Fit ---\n    if verbose:\n        print(\"Fitting models...\")\n\n    end_validation = pd.concat([data_train, data_val]).index[-1]\n\n    baseline_forecasters = {}\n    targets_to_train = list(data.columns)\n\n    # Attempt to load cached models if force_train=False\n    if not force_train and _model_directory_exists(model_dir):\n        if verbose:\n            print(\"  Attempting to load cached models...\")\n        cached_forecasters, missing_targets = _load_forecasters(\n            target_columns=list(data.columns),\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n        baseline_forecasters.update(cached_forecasters)\n        targets_to_train = missing_targets\n\n        if len(cached_forecasters) == len(data.columns):\n            if verbose:\n                print(f\"  \u2713 All {len(data.columns)} forecasters loaded from cache\")\n        elif len(cached_forecasters) &gt; 0:\n            if verbose:\n                print(\n                    f\"  \u2713 Loaded {len(cached_forecasters)} forecasters, \"\n                    f\"will train {len(targets_to_train)} new ones\"\n                )\n\n    # Train missing or forced models\n    if len(targets_to_train) &gt; 0:\n        if force_train and len(baseline_forecasters) &gt; 0:\n            if verbose:\n                print(f\"  Force retraining all {len(data.columns)} forecasters...\")\n            targets_to_train = list(data.columns)\n            baseline_forecasters.clear()\n\n        target_iter = targets_to_train\n        if show_progress and tqdm is not None:\n            target_iter = tqdm(\n                targets_to_train,\n                desc=\"Training forecasters\",\n                unit=\"model\",\n            )\n\n        for target in target_iter:\n            forecaster = ForecasterEquivalentDate(\n                offset=pd.DateOffset(days=1), n_offsets=1\n            )\n\n            forecaster.fit(y=data.loc[:end_validation, target])\n\n            baseline_forecasters[target] = forecaster\n\n        # Save newly trained models to disk\n        if verbose:\n            print(f\"  Saving {len(targets_to_train)} trained forecasters to disk...\")\n        _save_forecasters(\n            forecasters={t: baseline_forecasters[t] for t in targets_to_train},\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n\n    if verbose:\n        print(f\"  \u2713 Total forecasters available: {len(baseline_forecasters)}\")\n\n    # --- Predict ---\n    if verbose:\n        print(\"Generating predictions...\")\n\n    predictions = predict_multivariate(\n        baseline_forecasters,\n        steps_ahead=forecast_horizon,\n        show_progress=show_progress,\n    )\n\n    return predictions, baseline_forecasters\n</code></pre>"},{"location":"api/processing/#n-to-n-prediction-with-covariates","title":"N-to-N Prediction with Covariates","text":""},{"location":"api/processing/#n2n_predict_with_covariates","title":"n2n_predict_with_covariates","text":""},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict_with_covariates","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates</code>","text":"<p>End-to-end recursive forecasting with exogenous covariates.</p> <p>This module provides a complete pipeline for time series forecasting using recursive forecasters with exogenous variables (weather, holidays, calendar features). It handles data preparation, feature engineering, model training, and prediction in a single integrated function.</p> <p>Model persistence follows scikit-learn conventions using joblib for efficient serialization and deserialization of trained forecasters.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.processing.n2n_predict_with_covariates import (\n...     n2n_predict_with_covariates\n... )\n&gt;&gt;&gt; predictions = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n</code></pre> <p>With custom parameters:</p> <pre><code>&gt;&gt;&gt; predictions = n2n_predict_with_covariates(\n...     forecast_horizon=48,\n...     contamination=0.02,\n...     window_size=100,\n...     lags=48,\n...     train_ratio=0.75,\n...     verbose=True\n... )\n</code></pre> <p>Using cached models:</p> <pre><code>&gt;&gt;&gt; # Load existing models if available, or train new ones\n&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre>"},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict_with_covariates.n2n_predict_with_covariates","title":"<code>n2n_predict_with_covariates(data=None, forecast_horizon=24, contamination=0.01, window_size=72, lags=24, train_ratio=0.8, latitude=51.5136, longitude=7.4653, timezone='UTC', country_code='DE', state='NW', estimator=None, include_weather_windows=False, include_holiday_features=False, include_poly_features=False, force_train=True, model_dir=None, verbose=True, show_progress=False)</code>","text":"<p>End-to-end recursive forecasting with exogenous covariates.</p> <p>This function implements a complete forecasting pipeline that: 1. Loads and validates target data 2. Detects and removes outliers 3. Imputes missing values with weighted gaps 4. Creates exogenous features (weather, holidays, calendar, day/night) 5. Performs feature engineering (cyclical encoding, interactions) 6. Merges target and exogenous data 7. Splits into train/validation/test sets 8. Trains or loads recursive forecasters with sample weighting 9. Generates multi-step ahead predictions</p> <p>Models are persisted to disk following scikit-learn conventions using joblib. By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with target time series data. If None, fetches data automatically. Default: None.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to forecast ahead. Default: 24.</p> <code>24</code> <code>contamination</code> <code>float</code> <p>Contamination parameter for outlier detection. Default: 0.01.</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Rolling window size for gap detection. Default: 72.</p> <code>72</code> <code>lags</code> <code>int</code> <p>Number of lags for recursive forecaster. Default: 24.</p> <code>24</code> <code>train_ratio</code> <code>float</code> <p>Fraction of data for training. Default: 0.8.</p> <code>0.8</code> <code>latitude</code> <code>float</code> <p>Location latitude. Default: 51.5136 (Dortmund).</p> <code>51.5136</code> <code>longitude</code> <code>float</code> <p>Location longitude. Default: 7.4653 (Dortmund).</p> <code>7.4653</code> <code>timezone</code> <code>str</code> <p>Timezone for data. Default: \"UTC\".</p> <code>'UTC'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays. Default: \"DE\".</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays. Default: \"NW\".</p> <code>'NW'</code> <code>estimator</code> <code>Optional[object]</code> <p>Base estimator for recursive forecaster. If None, uses LGBMRegressor. Default: None.</p> <code>None</code> <code>include_weather_windows</code> <code>bool</code> <p>Include weather window features. Default: False.</p> <code>False</code> <code>include_holiday_features</code> <code>bool</code> <p>Include holiday features. Default: False.</p> <code>False</code> <code>include_poly_features</code> <code>bool</code> <p>Include polynomial interaction features. Default: False.</p> <code>False</code> <code>force_train</code> <code>bool</code> <p>Force retraining of all models, ignoring cached models. Default: True.</p> <code>True</code> <code>model_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory for saving/loading trained models. If None, uses the spotforecast2 cache directory (~/spotforecast2_cache by default, or SPOTFORECAST2_CACHE environment variable). Default: None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: True.</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar during training. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>Dict</code> <ul> <li>predictions: DataFrame with forecast values for each target variable.</li> </ul> <code>Dict</code> <ul> <li>metadata: Dictionary with forecast metadata (index, shapes, etc.).</li> </ul> <code>Tuple[DataFrame, Dict, Dict]</code> <ul> <li>forecasters: Dictionary of trained ForecasterRecursive objects keyed by target.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data validation fails or required data cannot be retrieved.</p> <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>OSError</code> <p>If models cannot be saved to disk.</p> <p>Examples:</p> <p>Basic usage with automatic model caching:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n&gt;&gt;&gt; print(predictions.shape)\n(24, 11)\n</code></pre> <p>Load cached models (if available):</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./saved_models\"\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./saved_models\"\n... )\n</code></pre> <p>Custom location and features:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=48,\n...     latitude=52.5200,  # Berlin\n...     longitude=13.4050,\n...     lags=48,\n...     include_poly_features=True,\n...     force_train=False,\n...     verbose=True\n... )\n</code></pre> Notes <ul> <li>The function uses cached weather data when available.</li> <li>Missing values are handled via forward/backward fill with downweighting   observations near gaps.</li> <li>Sample weights are passed to the forecaster to penalize observations   near missing data.</li> <li>Train/validation splits are temporal (80/20 by default).</li> <li>All features are cast to float32 for memory efficiency.</li> <li>Trained models are saved to disk using joblib for fast reuse.</li> <li>When force_train=False, existing models are loaded and prediction   proceeds without retraining. This significantly speeds up prediction   for repeated calls with the same configuration.</li> <li>The model_dir directory is created automatically if it doesn't exist.</li> <li>By default, models are cached in ~/spotforecast2_cache, which can be   customized via the SPOTFORECAST2_CACHE environment variable.</li> </ul> Performance Notes <ul> <li>First run: Full training</li> <li>Subsequent runs (force_train=False): Model loading only</li> <li>Force retrain (force_train=True): Full training again</li> </ul> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def n2n_predict_with_covariates(\n    data: Optional[pd.DataFrame] = None,\n    forecast_horizon: int = 24,\n    contamination: float = 0.01,\n    window_size: int = 72,\n    lags: int = 24,\n    train_ratio: float = 0.8,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n    estimator: Optional[object] = None,\n    include_weather_windows: bool = False,\n    include_holiday_features: bool = False,\n    include_poly_features: bool = False,\n    force_train: bool = True,\n    model_dir: Optional[Union[str, Path]] = None,\n    verbose: bool = True,\n    show_progress: bool = False,\n) -&gt; Tuple[pd.DataFrame, Dict, Dict]:\n    \"\"\"End-to-end recursive forecasting with exogenous covariates.\n\n    This function implements a complete forecasting pipeline that:\n    1. Loads and validates target data\n    2. Detects and removes outliers\n    3. Imputes missing values with weighted gaps\n    4. Creates exogenous features (weather, holidays, calendar, day/night)\n    5. Performs feature engineering (cyclical encoding, interactions)\n    6. Merges target and exogenous data\n    7. Splits into train/validation/test sets\n    8. Trains or loads recursive forecasters with sample weighting\n    9. Generates multi-step ahead predictions\n\n    Models are persisted to disk following scikit-learn conventions using joblib.\n    By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.\n\n    Args:\n        data: Optional DataFrame with target time series data. If None, fetches data automatically.\n            Default: None.\n        forecast_horizon: Number of time steps to forecast ahead. Default: 24.\n        contamination: Contamination parameter for outlier detection. Default: 0.01.\n        window_size: Rolling window size for gap detection. Default: 72.\n        lags: Number of lags for recursive forecaster. Default: 24.\n        train_ratio: Fraction of data for training. Default: 0.8.\n        latitude: Location latitude. Default: 51.5136 (Dortmund).\n        longitude: Location longitude. Default: 7.4653 (Dortmund).\n        timezone: Timezone for data. Default: \"UTC\".\n        country_code: Country code for holidays. Default: \"DE\".\n        state: State code for holidays. Default: \"NW\".\n        estimator: Base estimator for recursive forecaster.\n            If None, uses LGBMRegressor. Default: None.\n        include_weather_windows: Include weather window features. Default: False.\n        include_holiday_features: Include holiday features. Default: False.\n        include_poly_features: Include polynomial interaction features. Default: False.\n        force_train: Force retraining of all models, ignoring cached models.\n            Default: True.\n        model_dir: Directory for saving/loading trained models. If None, uses the\n            spotforecast2 cache directory (~/spotforecast2_cache by default, or\n            SPOTFORECAST2_CACHE environment variable). Default: None.\n        verbose: Print progress messages. Default: True.\n        show_progress: Show progress bar during training. Default: False.\n\n    Returns:\n        Tuple containing:\n        - predictions: DataFrame with forecast values for each target variable.\n        - metadata: Dictionary with forecast metadata (index, shapes, etc.).\n        - forecasters: Dictionary of trained ForecasterRecursive objects keyed by target.\n\n    Raises:\n        ValueError: If data validation fails or required data cannot be retrieved.\n        ImportError: If required dependencies are not installed.\n        OSError: If models cannot be saved to disk.\n\n    Examples:\n        Basic usage with automatic model caching:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(predictions.shape)\n        (24, 11)\n\n        Load cached models (if available):\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     force_train=False,\n        ...     model_dir=\"./saved_models\"\n        ... )\n\n        Force retraining and update cache:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     force_train=True,\n        ...     model_dir=\"./saved_models\"\n        ... )\n\n        Custom location and features:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=48,\n        ...     latitude=52.5200,  # Berlin\n        ...     longitude=13.4050,\n        ...     lags=48,\n        ...     include_poly_features=True,\n        ...     force_train=False,\n        ...     verbose=True\n        ... )\n\n    Notes:\n        - The function uses cached weather data when available.\n        - Missing values are handled via forward/backward fill with downweighting\n          observations near gaps.\n        - Sample weights are passed to the forecaster to penalize observations\n          near missing data.\n        - Train/validation splits are temporal (80/20 by default).\n        - All features are cast to float32 for memory efficiency.\n        - Trained models are saved to disk using joblib for fast reuse.\n        - When force_train=False, existing models are loaded and prediction\n          proceeds without retraining. This significantly speeds up prediction\n          for repeated calls with the same configuration.\n        - The model_dir directory is created automatically if it doesn't exist.\n        - By default, models are cached in ~/spotforecast2_cache, which can be\n          customized via the SPOTFORECAST2_CACHE environment variable.\n\n    Performance Notes:\n        - First run: Full training\n        - Subsequent runs (force_train=False): Model loading only\n        - Force retrain (force_train=True): Full training again\n    \"\"\"\n    # Set default model_dir if not provided\n    if model_dir is None:\n        from spotforecast2_safe.data.fetch_data import get_cache_home\n\n        model_dir = get_cache_home() / \"forecasters\"\n\n    if verbose:\n        print(\"=\" * 80)\n        print(\"N2N Recursive Forecasting with Exogenous Covariates\")\n        print(\"=\" * 80)\n\n    # ========================================================================\n    # 1. DATA PREPARATION\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[1/9] Loading and preparing target data...\")\n\n    # Handle data input - fetch_data handles both CSV and DataFrame\n    if data is None:\n        if verbose:\n            print(\"  Fetching data from CSV...\")\n        data = fetch_data(timezone=timezone)\n    else:\n        if verbose:\n            print(\"  Using provided dataframe...\")\n        data = fetch_data(dataframe=data, timezone=timezone)\n\n    target_columns = data.columns.tolist()\n\n    if verbose:\n        print(f\"  Target variables: {target_columns}\")\n\n    start, end, cov_start, cov_end = get_start_end(\n        data=data,\n        forecast_horizon=forecast_horizon,\n        verbose=verbose,\n    )\n\n    basic_ts_checks(data, verbose=verbose)\n    data = agg_and_resample_data(data, verbose=verbose)\n\n    # ========================================================================\n    # 2. OUTLIER DETECTION AND REMOVAL\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[2/9] Detecting and marking outliers...\")\n\n    data, outliers = mark_outliers(\n        data,\n        contamination=contamination,\n        random_state=1234,\n        verbose=verbose,\n    )\n\n    # ========================================================================\n    # 3. MISSING VALUE IMPUTATION WITH WEIGHTING\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[3/9] Processing missing values and creating sample weights...\")\n\n    imputed_data, missing_mask = get_missing_weights(\n        data, window_size=window_size, verbose=verbose\n    )\n\n    # Create weight function for forecaster\n    # Invert missing_mask: True (missing) -&gt; 0 (weight), False (valid) -&gt; 1 (weight)\n    weights_series = (~missing_mask).astype(float)\n\n    # Use WeightFunction class which is picklable (unlike local functions with closures)\n    from spotforecast2_safe.preprocessing import WeightFunction\n\n    weight_func = WeightFunction(weights_series)\n\n    # Model persistence enabled: WeightFunction instances can be pickled\n    use_model_persistence = True\n\n    # ========================================================================\n    # 4. EXOGENOUS FEATURES ENGINEERING\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[4/9] Creating exogenous features...\")\n\n    # Location for day/night features\n    location = LocationInfo(\n        latitude=latitude,\n        longitude=longitude,\n        timezone=timezone,\n    )\n\n    # Holidays\n    holiday_features = _get_holiday_features(\n        data=imputed_data,\n        start=start,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        tz=timezone,\n        freq=\"h\",\n        country_code=country_code,\n        state=state,\n    )\n\n    # Weather\n    weather_features, weather_aligned = _get_weather_features(\n        data=imputed_data,\n        start=start,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        latitude=latitude,\n        longitude=longitude,\n        timezone=timezone,\n        freq=\"h\",\n        verbose=verbose,\n    )\n\n    # Calendar\n    calendar_features = _get_calendar_features(\n        start=start,\n        cov_end=cov_end,\n        freq=\"h\",\n        timezone=timezone,\n    )\n\n    # Day/night\n    sun_light_features = _get_day_night_features(\n        start=start,\n        cov_end=cov_end,\n        location=location,\n        freq=\"h\",\n        timezone=timezone,\n    )\n\n    # ========================================================================\n    # 5. COMBINE EXOGENOUS FEATURES\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[5/9] Combining and encoding exogenous features...\")\n\n    exogenous_features = pd.concat(\n        [\n            calendar_features,\n            sun_light_features,\n            weather_features,\n            holiday_features,\n        ],\n        axis=1,\n    )\n\n    assert (\n        sum(exogenous_features.isnull().sum()) == 0\n    ), \"Missing values in exogenous features\"\n\n    # Apply cyclical encoding\n    exogenous_features = _apply_cyclical_encoding(\n        data=exogenous_features,\n        drop_original=False,\n    )\n\n    # Create interactions\n    exogenous_features = _create_interaction_features(\n        exogenous_features=exogenous_features,\n        weather_aligned=weather_aligned,\n    )\n\n    # ========================================================================\n    # 6. SELECT EXOGENOUS FEATURES\n    # ========================================================================\n\n    exog_features = _select_exogenous_features(\n        exogenous_features=exogenous_features,\n        weather_aligned=weather_aligned,\n        include_weather_windows=include_weather_windows,\n        include_holiday_features=include_holiday_features,\n        include_poly_features=include_poly_features,\n    )\n\n    if verbose:\n        print(f\"  Selected {len(exog_features)} exogenous features\")\n\n    # ========================================================================\n    # 7. MERGE DATA AND COVARIATES\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[6/9] Merging target and exogenous data...\")\n\n    data_with_exog, exo_tmp, exo_pred = _merge_data_and_covariates(\n        data=imputed_data,\n        exogenous_features=exogenous_features,\n        target_columns=target_columns,\n        exog_features=exog_features,\n        start=start,\n        end=end,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        cast_dtype=\"float32\",\n    )\n\n    if verbose:\n        print(f\"  Merged data shape: {data_with_exog.shape}\")\n        print(f\"  Exogenous prediction shape: {exo_pred.shape}\")\n\n    # ========================================================================\n    # 8. TRAIN/VALIDATION/TEST SPLIT\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[7/9] Splitting data into train/validation/test...\")\n\n    perc_val = 1.0 - train_ratio\n    data_train, data_val, data_test = split_rel_train_val_test(\n        data_with_exog,\n        perc_train=train_ratio,\n        perc_val=perc_val,\n        verbose=verbose,\n    )\n\n    # ========================================================================\n    # 9. MODEL TRAINING OR LOADING\n    # ========================================================================\n\n    if verbose:\n        print(\n            \"\\n[8/9] Loading or training recursive forecasters with exogenous variables...\"\n        )\n\n    if estimator is None:\n        estimator = LGBMRegressor(random_state=1234, verbose=-1)\n\n    window_features = RollingFeatures(stats=[\"mean\"], window_sizes=window_size)\n    end_validation = pd.concat([data_train, data_val]).index[-1]\n\n    # Attempt to load cached models if force_train=False and persistence is enabled\n    recursive_forecasters = {}\n    targets_to_train = target_columns\n\n    if use_model_persistence and not force_train and _model_directory_exists(model_dir):\n        if verbose:\n            print(\"  Attempting to load cached models...\")\n        cached_forecasters, missing_targets = _load_forecasters(\n            target_columns=target_columns,\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n        recursive_forecasters.update(cached_forecasters)\n        targets_to_train = missing_targets\n\n        if len(cached_forecasters) == len(target_columns):\n            if verbose:\n                print(f\"  \u2713 All {len(target_columns)} forecasters loaded from cache\")\n        elif len(cached_forecasters) &gt; 0:\n            if verbose:\n                print(\n                    f\"  \u2713 Loaded {len(cached_forecasters)} forecasters, \"\n                    f\"will train {len(targets_to_train)} new ones\"\n                )\n\n    # Train missing or forced models\n    if len(targets_to_train) &gt; 0:\n        if force_train and len(recursive_forecasters) &gt; 0:\n            if verbose:\n                print(f\"  Force retraining all {len(target_columns)} forecasters...\")\n            targets_to_train = target_columns\n            recursive_forecasters.clear()\n\n        target_iter = targets_to_train\n        if show_progress and tqdm is not None:\n            target_iter = tqdm(\n                targets_to_train,\n                desc=\"Training forecasters\",\n                unit=\"model\",\n            )\n\n        for target in target_iter:\n            if verbose:\n                print(f\"  Training forecaster for {target}...\")\n\n            forecaster = ForecasterRecursive(\n                estimator=estimator,\n                lags=lags,\n                window_features=window_features,\n                weight_func=weight_func,\n            )\n\n            forecaster.fit(\n                y=data_with_exog[target].loc[:end_validation].squeeze(),\n                exog=data_with_exog[exog_features].loc[:end_validation],\n            )\n\n            recursive_forecasters[target] = forecaster\n\n            if verbose:\n                print(f\"    \u2713 Forecaster trained for {target}\")\n\n        # Save newly trained models to disk (only if persistence is enabled)\n        if use_model_persistence:\n            if verbose:\n                print(\n                    f\"  Saving {len(targets_to_train)} trained forecasters to disk...\"\n                )\n            _save_forecasters(\n                forecasters={t: recursive_forecasters[t] for t in targets_to_train},\n                model_dir=model_dir,\n                verbose=verbose,\n            )\n        else:\n            if verbose:\n                print(\"  \u26a0 Model persistence disabled (weight_func cannot be pickled)\")\n\n    if verbose:\n        print(f\"  \u2713 Total forecasters available: {len(recursive_forecasters)}\")\n\n    # ========================================================================\n    # 10. PREDICTION\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[9/9] Generating predictions...\")\n\n    exo_pred_subset = exo_pred[exog_features]\n\n    predictions = predict_multivariate(\n        recursive_forecasters,\n        steps_ahead=forecast_horizon,\n        exog=exo_pred_subset,\n        show_progress=show_progress,\n    )\n\n    if verbose:\n        print(f\"  Predictions shape: {predictions.shape}\")\n        print(\"\\n\" + \"=\" * 80)\n        print(\"Forecasting completed successfully!\")\n        print(\"=\" * 80)\n\n    # ========================================================================\n    # COMPILE METADATA\n    # ========================================================================\n\n    metadata = {\n        \"forecast_horizon\": forecast_horizon,\n        \"target_columns\": target_columns,\n        \"exog_features\": exog_features,\n        \"n_exog_features\": len(exog_features),\n        \"train_size\": len(data_train),\n        \"val_size\": len(data_val),\n        \"test_size\": len(data_test),\n        \"data_shape_original\": data.shape,\n        \"data_shape_merged\": data_with_exog.shape,\n        \"training_end\": end_validation,\n        \"prediction_start\": exo_pred.index[0],\n        \"prediction_end\": exo_pred.index[-1],\n        \"lags\": lags,\n        \"window_size\": window_size,\n        \"contamination\": contamination,\n        \"n_outliers\": (\n            outliers.sum() if isinstance(outliers, pd.Series) else len(outliers)\n        ),\n    }\n\n    return predictions, metadata, recursive_forecasters\n</code></pre>"},{"location":"api/processing/#aggregate-and-prediction-functions","title":"Aggregate and Prediction Functions","text":""},{"location":"api/processing/#agg_predict","title":"agg_predict","text":""},{"location":"api/processing/#spotforecast2_safe.processing.agg_predict","title":"<code>spotforecast2_safe.processing.agg_predict</code>","text":""},{"location":"api/processing/#spotforecast2_safe.processing.agg_predict.agg_predict","title":"<code>agg_predict(predictions, weights=None)</code>","text":"<p>Aggregates multiple prediction columns into a single combined prediction series.</p> <p>The combination is a weighted sum of the prediction columns. If no weights are provided, a default weighting scheme based on specific predefined columns is used.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>DataFrame containing the prediction columns.</p> required <code>weights</code> <code>Optional[Union[Dict[str, float], List[float], ndarray]]</code> <p>Dictionary mapping column names to their weights, or a list/array of weights corresponding to the order of columns in <code>predictions</code>. If None, defaults to summing all columns (weight=1.0 for each column).</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A Series containing the aggregated values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a column specified in weights (or default weights) is missing from predictions.</p> <code>ValueError</code> <p>If weights is a list/array and its length does not match the number of columns in predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.processing import agg_predict\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n&gt;&gt;&gt; agg_predict(df, weights={\"A\": 1.0, \"B\": -1.0})\n0   -2.0\n1   -2.0\ndtype: float64\n&gt;&gt;&gt; agg_predict(df, weights=[0.5, 2.0])\n0    6.5\n1    9.0\ndtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/agg_predict.py</code> <pre><code>def agg_predict(\n    predictions: pd.DataFrame,\n    weights: Optional[Union[Dict[str, float], List[float], np.ndarray]] = None,\n) -&gt; pd.Series:\n    \"\"\"Aggregates multiple prediction columns into a single combined prediction series.\n\n    The combination is a weighted sum of the prediction columns. If no weights are provided,\n    a default weighting scheme based on specific predefined columns is used.\n\n    Args:\n        predictions (pd.DataFrame): DataFrame containing the prediction columns.\n        weights (Optional[Union[Dict[str, float], List[float], np.ndarray]]):\n            Dictionary mapping column names to their weights, or a list/array of weights\n            corresponding to the order of columns in `predictions`.\n            If None, defaults to summing all columns (weight=1.0 for each column).\n\n    Returns:\n        pd.Series: A Series containing the aggregated values.\n\n    Raises:\n        ValueError: If a column specified in weights (or default weights) is missing from predictions.\n        ValueError: If weights is a list/array and its length does not match the number of columns in predictions.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.processing import agg_predict\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        &gt;&gt;&gt; agg_predict(df, weights={\"A\": 1.0, \"B\": -1.0})\n        0   -2.0\n        1   -2.0\n        dtype: float64\n        &gt;&gt;&gt; agg_predict(df, weights=[0.5, 2.0])\n        0    6.5\n        1    9.0\n        dtype: float64\n    \"\"\"\n    if weights is None:\n        # Default to summing all columns\n        weights = {col: 1.0 for col in predictions.columns}\n\n    if isinstance(weights, (list, np.ndarray)):\n        if len(weights) != len(predictions.columns):\n            raise ValueError(\n                f\"Length of weights ({len(weights)}) does not match number of columns in predictions ({len(predictions.columns)})\"\n            )\n        # Convert to dictionary using column order\n        weights = dict(zip(predictions.columns, weights))\n\n    combined = pd.Series(0.0, index=predictions.index)\n\n    missing_cols = [col for col in weights.keys() if col not in predictions.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing columns in predictions dataframe: {missing_cols}\")\n\n    for col, weight in weights.items():\n        combined += predictions[col] * weight\n\n    return combined\n</code></pre>"},{"location":"api/utils/","title":"Utils Module","text":"<p>Utility functions and helpers.</p>"},{"location":"api/utils/#spotforecast2_safe.utils","title":"<code>spotforecast2_safe.utils</code>","text":"<p>Utility functions for spotforecast.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.create_holiday_df","title":"<code>create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#utc-conversion","title":"UTC Conversion","text":""},{"location":"api/utils/#convert_to_utc","title":"convert_to_utc","text":""},{"location":"api/utils/#spotforecast2_safe.utils.convert_to_utc","title":"<code>spotforecast2_safe.utils.convert_to_utc</code>","text":"<p>Utility functions for timezone conversion.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.convert_to_utc.convert_to_utc","title":"<code>convert_to_utc(df, timezone)</code>","text":"<p>Convert DataFrame index timezone to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Optional timezone string. Required if index has no timezone.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with UTC timezone index.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If index is not DatetimeIndex or has no timezone and timezone is None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.convert_to_utc import convert_to_utc\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]}, index=pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n&gt;&gt;&gt; convert_to_utc(df, \"Europe/Berlin\")\n           value\n2022-01-01 00:00:00+01:00\n2022-01-02 00:00:00+01:00\n2022-01-03 00:00:00+01:00\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/convert_to_utc.py</code> <pre><code>def convert_to_utc(df: pd.DataFrame, timezone: Optional[str]) -&gt; pd.DataFrame:\n    \"\"\"Convert DataFrame index timezone to UTC.\n\n    Args:\n        df: DataFrame with DatetimeIndex.\n        timezone: Optional timezone string. Required if index has no timezone.\n\n    Returns:\n        DataFrame with UTC timezone index.\n\n    Raises:\n        ValueError: If index is not DatetimeIndex or has no timezone and\n            timezone is None.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.convert_to_utc import convert_to_utc\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]}, index=pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n        &gt;&gt;&gt; convert_to_utc(df, \"Europe/Berlin\")\n                   value\n        2022-01-01 00:00:00+01:00\n        2022-01-02 00:00:00+01:00\n        2022-01-03 00:00:00+01:00\n    \"\"\"\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\n            \"No DatetimeIndex found. Please specify the time column via 'index_col'\"\n        )\n    if df.index.tz is None:\n        if timezone is not None:\n            df.index = df.index.tz_localize(timezone)\n        else:\n            raise ValueError(\n                \"Index has no timezone information. Please provide a timezone.\"\n            )\n\n    df.index = df.index.tz_convert(\"UTC\")\n\n    return df\n</code></pre>"},{"location":"api/utils/#data-transformation","title":"Data Transformation","text":""},{"location":"api/utils/#data_transform","title":"data_transform","text":""},{"location":"api/utils/#spotforecast2_safe.utils.data_transform","title":"<code>spotforecast2_safe.utils.data_transform</code>","text":"<p>Data transformation utilities for time series forecasting.</p> <p>This module provides functions for normalizing and transforming data formats.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#forecaster-configuration","title":"Forecaster Configuration","text":""},{"location":"api/utils/#forecaster_config","title":"forecaster_config","text":""},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config","title":"<code>spotforecast2_safe.utils.forecaster_config</code>","text":"<p>Forecaster configuration utilities.</p> <p>This module provides functions for initializing and validating forecaster configuration parameters like lags and weights.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#holiday-generation","title":"Holiday Generation","text":""},{"location":"api/utils/#generate_holiday","title":"generate_holiday","text":""},{"location":"api/utils/#spotforecast2_safe.utils.generate_holiday","title":"<code>spotforecast2_safe.utils.generate_holiday</code>","text":"<p>Utilities for generating holiday dataframe as covariate.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.generate_holiday.create_holiday_df","title":"<code>create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/utils/#validation-utilities","title":"Validation Utilities","text":""},{"location":"api/utils/#validation","title":"validation","text":""},{"location":"api/utils/#spotforecast2_safe.utils.validation","title":"<code>spotforecast2_safe.utils.validation</code>","text":"<p>Validation utilities for time series forecasting.</p> <p>This module provides validation functions for time series data and exogenous variables.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/weather/","title":"Weather Module","text":"<p>Weather data utilities and integration.</p>"},{"location":"api/weather/#spotforecast2_safe.weather","title":"<code>spotforecast2_safe.weather</code>","text":"<p>Weather data utilities for spotforecast2.</p>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#weather-client","title":"Weather Client","text":""},{"location":"api/weather/#weather_client","title":"weather_client","text":""},{"location":"api/weather/#spotforecast2_safe.weather.weather_client","title":"<code>spotforecast2_safe.weather.weather_client</code>","text":"<p>Weather data fetching and processing using Open-Meteo API.</p>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherService","title":"<code>WeatherService</code>","text":"<p>               Bases: <code>WeatherClient</code></p> <p>High-level service for weather data generation.</p> <p>Extends WeatherClient with caching, hybrid fetching (archive+forecast), and fallback strategies.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherService(WeatherClient):\n    \"\"\"High-level service for weather data generation.\n\n    Extends WeatherClient with caching, hybrid fetching (archive+forecast),\n    and fallback strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        latitude: float,\n        longitude: float,\n        cache_path: Optional[Path] = None,\n        use_forecast: bool = True,\n    ):\n        super().__init__(latitude, longitude)\n        self.cache_path = cache_path\n        self.use_forecast = use_forecast\n\n    def get_dataframe(\n        self,\n        start: Union[str, pd.Timestamp],\n        end: Union[str, pd.Timestamp],\n        timezone: str = \"UTC\",\n        freq: str = \"h\",\n        fallback_on_failure: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n        Refactored from spotpredict.create_weather_df.\n        \"\"\"\n        start_ts = pd.Timestamp(start)\n        end_ts = pd.Timestamp(end)\n\n        # Localize if naive\n        if start_ts.tz is None:\n            start_ts = start_ts.tz_localize(timezone)\n        if end_ts.tz is None:\n            end_ts = end_ts.tz_localize(timezone)\n\n        # Convert to UTC for consistency\n        start_utc = start_ts.tz_convert(\"UTC\")\n        end_utc = end_ts.tz_convert(\"UTC\")\n\n        # 1. Try Cache\n        cached_df = self._load_cache()\n        if cached_df is not None:\n            if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n                self.logger.info(\"Using full cached data.\")\n                return self._finalize_df(\n                    cached_df.loc[start_utc:end_utc], freq, timezone\n                )\n\n        # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n        # (The original logic did partial fills, but full fetch is safer and\n        # simpler for now unless specifically improved).\n        # Actually, strict refactor implies keeping logic. Let's keep it simple:\n        # fetch what's needed.\n\n        try:\n            df = self._fetch_hybrid(start_ts, end_ts, timezone)\n        except Exception as e:\n            self.logger.warning(f\"Fetch failed: {e}\")\n            if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n                df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n            else:\n                raise\n\n        # 3. Merge with cache and save\n        if cached_df is not None:\n            df = pd.concat([cached_df, df])\n            df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n        if self.cache_path:\n            self._save_cache(df)\n\n        # 4. Return slice\n        return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n\n    def _fetch_hybrid(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch from Archive and/or Forecast based on date.\"\"\"\n        now = pd.Timestamp.now(tz=start.tz)\n        archive_cutoff = now - pd.Timedelta(days=5)\n\n        dfs = []\n\n        # Archive part\n        if start &lt; archive_cutoff:\n            arch_end = min(end, archive_cutoff)\n            try:\n                dfs.append(self.fetch_archive(start, arch_end, timezone))\n            except Exception as e:\n                self.logger.warning(f\"Archive fetch warning: {e}\")\n\n        # Forecast part\n        if end &gt; now and self.use_forecast:\n            days = (end - now).days + 2\n            days = min(max(1, days), 16)\n            try:\n                df_fore = self.fetch_forecast(days, timezone)\n                # Filter forecast to needed range to avoid overlap issues\n                dfs.append(df_fore)\n            except Exception as e:\n                self.logger.warning(f\"Forecast fetch warning: {e}\")\n\n        if not dfs:\n            raise ValueError(\"Could not fetch data from Archive or Forecast.\")\n\n        full_df = pd.concat(dfs)\n        full_df = full_df[~full_df.index.duplicated(keep=\"first\")].sort_index()\n\n        # Ensure UTC index\n        if full_df.index.tz is None:\n            full_df.index = full_df.index.tz_localize(timezone)\n        full_df.index = full_df.index.tz_convert(\"UTC\")\n\n        return full_df\n\n    def _create_fallback(\n        self,\n        start: pd.Timestamp,\n        end: pd.Timestamp,\n        source_df: pd.DataFrame,\n        timezone: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Repeat last 24h of data.\"\"\"\n        last_24 = source_df.tail(24)\n        hours = int((end - start).total_seconds() / 3600) + 1\n        repeats = (hours // 24) + 1\n\n        new_data = pd.concat([last_24] * repeats, ignore_index=True)\n        new_data = new_data.iloc[:hours]\n\n        idx = pd.date_range(start, periods=hours, freq=\"h\", tz=\"UTC\")\n        new_data.index = idx\n        return new_data\n\n    def _load_cache(self) -&gt; Optional[pd.DataFrame]:\n        if not self.cache_path or not self.cache_path.exists():\n            return None\n        try:\n            df = pd.read_parquet(self.cache_path)\n            if df.index.tz is None:\n                df.index = df.index.tz_localize(\"UTC\")\n            return df\n        except Exception:\n            return None\n\n    def _save_cache(self, df: pd.DataFrame):\n        if self.cache_path:\n            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n            df.to_parquet(self.cache_path)\n\n    def _finalize_df(self, df: pd.DataFrame, freq: str, timezone: str) -&gt; pd.DataFrame:\n        \"\"\"Resample and localize.\"\"\"\n        # Resample\n        if freq != \"h\":  # Assuming API returns hourly\n            df = df.resample(freq).ffill()  # Forward fill for weather is reasonable\n\n        # Fill gaps\n        df = df.ffill().bfill()\n\n        # Convert to requested timezone if needed (though we keep internal UTC mostly)\n        # User requested specific tz output usually?\n        # Original code returned normalized DF. Let's ensure frequency matches exactly.\n\n        return df\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherService.get_dataframe","title":"<code>get_dataframe(start, end, timezone='UTC', freq='h', fallback_on_failure=True)</code>","text":"<p>Get weather DataFrame for a specified range using best available methods.</p> <p>Refactored from spotpredict.create_weather_df.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def get_dataframe(\n    self,\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n    Refactored from spotpredict.create_weather_df.\n    \"\"\"\n    start_ts = pd.Timestamp(start)\n    end_ts = pd.Timestamp(end)\n\n    # Localize if naive\n    if start_ts.tz is None:\n        start_ts = start_ts.tz_localize(timezone)\n    if end_ts.tz is None:\n        end_ts = end_ts.tz_localize(timezone)\n\n    # Convert to UTC for consistency\n    start_utc = start_ts.tz_convert(\"UTC\")\n    end_utc = end_ts.tz_convert(\"UTC\")\n\n    # 1. Try Cache\n    cached_df = self._load_cache()\n    if cached_df is not None:\n        if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n            self.logger.info(\"Using full cached data.\")\n            return self._finalize_df(\n                cached_df.loc[start_utc:end_utc], freq, timezone\n            )\n\n    # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n    # (The original logic did partial fills, but full fetch is safer and\n    # simpler for now unless specifically improved).\n    # Actually, strict refactor implies keeping logic. Let's keep it simple:\n    # fetch what's needed.\n\n    try:\n        df = self._fetch_hybrid(start_ts, end_ts, timezone)\n    except Exception as e:\n        self.logger.warning(f\"Fetch failed: {e}\")\n        if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n            df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n        else:\n            raise\n\n    # 3. Merge with cache and save\n    if cached_df is not None:\n        df = pd.concat([cached_df, df])\n        df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n    if self.cache_path:\n        self._save_cache(df)\n\n    # 4. Return slice\n    return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n</code></pre>"},{"location":"preprocessing/outliers/","title":"Outlier Detection and Handling","text":"<p>Guide for identifying and handling outliers in time series data.</p>"},{"location":"preprocessing/outliers/#overview","title":"Overview","text":"<p>Outlier detection is crucial for time series forecasting as extreme values can distort model training and predictions. This module provides robust outlier detection and marking capabilities.</p>"},{"location":"preprocessing/outliers/#key-functions","title":"Key Functions","text":""},{"location":"preprocessing/outliers/#mark-outliers","title":"Mark Outliers","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>spotforecast2_safe.preprocessing.outlier.mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"preprocessing/outliers/#outlier-module","title":"Outlier Module","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier","title":"<code>spotforecast2_safe.preprocessing.outlier</code>","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"preprocessing/outliers/#examples","title":"Examples","text":"<pre><code>import pandas as pd\nfrom spotforecast2_safe.preprocessing.outlier import mark_outliers\n\n# Create sample time series data\ndata = pd.DataFrame({\n    'value': [1, 2, 100, 4, 5, 6, 7, 8, 9, 10],  # 100 is an outlier\n})\n\n# Mark outliers\nresult_data, outlier_mask = mark_outliers(\n    data,\n    contamination=0.1,  # Expect 10% contamination\n    columns=['value']\n)\n\nprint(f\"Outliers marked: {outlier_mask.sum()} records\")\n</code></pre>"},{"location":"preprocessing/outliers/#detection-methods","title":"Detection Methods","text":"<p>This module uses isolation forest and other statistical methods to detect: - Sudden spikes or drops - Seasonal anomalies - Drift in baseline values - Sudden shifts in variance</p>"},{"location":"processing/model_persistence/","title":"Model Persistence","text":"<p>Guide for saving and loading trained forecasting models.</p>"},{"location":"processing/model_persistence/#overview","title":"Overview","text":"<p>The model persistence functionality enables you to: - Save trained forecasters to disk - Load previously trained models - Manage model caching - Handle batch model operations</p>"},{"location":"processing/model_persistence/#functions","title":"Functions","text":""},{"location":"processing/model_persistence/#saving-models","title":"Saving Models","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._save_forecasters","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._save_forecasters(forecasters, model_dir, verbose=False)</code>","text":"<p>Save trained forecasters to disk using joblib.</p> <p>Follows scikit-learn persistence conventions using joblib for efficient serialization of sklearn-compatible estimators.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>Dict[str, object]</code> <p>Dictionary mapping target names to trained ForecasterRecursive objects.</p> required <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory to save models. Created if it doesn't exist.</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Path]</code> <p>Dict[str, Path]: Dictionary mapping target names to saved model filepaths.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If models cannot be written to disk.</p> <code>TypeError</code> <p>If forecasters contain non-serializable objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forecasters = {\"power\": forecaster_obj}\n&gt;&gt;&gt; paths = _save_forecasters(forecasters, \"./models\", verbose=True)\n&gt;&gt;&gt; print(paths[\"power\"])\nmodels/forecaster_power.joblib\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _save_forecasters(\n    forecasters: Dict[str, object],\n    model_dir: Union[str, Path],\n    verbose: bool = False,\n) -&gt; Dict[str, Path]:\n    \"\"\"Save trained forecasters to disk using joblib.\n\n    Follows scikit-learn persistence conventions using joblib for efficient\n    serialization of sklearn-compatible estimators.\n\n    Args:\n        forecasters: Dictionary mapping target names to trained ForecasterRecursive objects.\n        model_dir: Directory to save models. Created if it doesn't exist.\n        verbose: Print progress messages. Default: False.\n\n    Returns:\n        Dict[str, Path]: Dictionary mapping target names to saved model filepaths.\n\n    Raises:\n        OSError: If models cannot be written to disk.\n        TypeError: If forecasters contain non-serializable objects.\n\n    Examples:\n        &gt;&gt;&gt; forecasters = {\"power\": forecaster_obj}\n        &gt;&gt;&gt; paths = _save_forecasters(forecasters, \"./models\", verbose=True)\n        &gt;&gt;&gt; print(paths[\"power\"])\n        models/forecaster_power.joblib\n    \"\"\"\n    model_path = _ensure_model_dir(model_dir)\n    saved_paths = {}\n\n    for target, forecaster in forecasters.items():\n        filepath = _get_model_filepath(model_path, target)\n        try:\n            dump(forecaster, filepath, compress=3)\n            saved_paths[target] = filepath\n            if verbose:\n                print(f\"  \u2713 Saved forecaster for {target} to {filepath}\")\n        except Exception as e:\n            raise OSError(f\"Failed to save model for {target}: {e}\")\n\n    return saved_paths\n</code></pre>"},{"location":"processing/model_persistence/#loading-models","title":"Loading Models","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._load_forecasters","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._load_forecasters(target_columns, model_dir, verbose=False)</code>","text":"<p>Load trained forecasters from disk using joblib.</p> <p>Attempts to load all forecasters for given targets. Missing models are indicated in the return value for selective retraining.</p> <p>Parameters:</p> Name Type Description Default <code>target_columns</code> <code>List[str]</code> <p>List of target variable names to load.</p> required <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory containing saved models.</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, object]</code> <p>Tuple[Dict[str, object], List[str]]:</p> <code>List[str]</code> <ul> <li>forecasters: Dictionary of successfully loaded ForecasterRecursive objects.</li> </ul> <code>Tuple[Dict[str, object], List[str]]</code> <ul> <li>missing_targets: List of target names without saved models.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forecasters, missing = _load_forecasters(\n...     [\"power\", \"energy\"],\n...     \"./models\",\n...     verbose=True\n... )\n&gt;&gt;&gt; print(missing)\n['energy']\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _load_forecasters(\n    target_columns: List[str],\n    model_dir: Union[str, Path],\n    verbose: bool = False,\n) -&gt; Tuple[Dict[str, object], List[str]]:\n    \"\"\"Load trained forecasters from disk using joblib.\n\n    Attempts to load all forecasters for given targets. Missing models\n    are indicated in the return value for selective retraining.\n\n    Args:\n        target_columns: List of target variable names to load.\n        model_dir: Directory containing saved models.\n        verbose: Print progress messages. Default: False.\n\n    Returns:\n        Tuple[Dict[str, object], List[str]]:\n        - forecasters: Dictionary of successfully loaded ForecasterRecursive objects.\n        - missing_targets: List of target names without saved models.\n\n    Examples:\n        &gt;&gt;&gt; forecasters, missing = _load_forecasters(\n        ...     [\"power\", \"energy\"],\n        ...     \"./models\",\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(missing)\n        ['energy']\n    \"\"\"\n    model_path = Path(model_dir)\n    forecasters = {}\n    missing_targets = []\n\n    for target in target_columns:\n        filepath = _get_model_filepath(model_path, target)\n        if filepath.exists():\n            try:\n                forecasters[target] = load(filepath)\n                if verbose:\n                    print(f\"  \u2713 Loaded forecaster for {target} from {filepath}\")\n            except Exception as e:\n                if verbose:\n                    print(f\"  \u2717 Failed to load {target}: {e}\")\n                missing_targets.append(target)\n        else:\n            missing_targets.append(target)\n\n    return forecasters, missing_targets\n</code></pre>"},{"location":"processing/model_persistence/#model-directory-management","title":"Model Directory Management","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._ensure_model_dir","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._ensure_model_dir(model_dir)</code>","text":"<p>Ensure model directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory path for model storage.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Validated Path object.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If directory cannot be created.</p> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _ensure_model_dir(model_dir: Union[str, Path]) -&gt; Path:\n    \"\"\"Ensure model directory exists.\n\n    Args:\n        model_dir: Directory path for model storage.\n\n    Returns:\n        Path: Validated Path object.\n\n    Raises:\n        OSError: If directory cannot be created.\n    \"\"\"\n    model_path = Path(model_dir)\n    model_path.mkdir(parents=True, exist_ok=True)\n    return model_path\n</code></pre>"},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._model_directory_exists","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._model_directory_exists(model_dir)</code>","text":"<p>Check if model directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory path to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if directory exists, False otherwise.</p> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _model_directory_exists(model_dir: Union[str, Path]) -&gt; bool:\n    \"\"\"Check if model directory exists.\n\n    Args:\n        model_dir: Directory path to check.\n\n    Returns:\n        bool: True if directory exists, False otherwise.\n    \"\"\"\n    return Path(model_dir).exists()\n</code></pre>"},{"location":"processing/model_persistence/#examples","title":"Examples","text":"<pre><code>from spotforecast2_safe.processing.n2n_predict_with_covariates import (\n    _save_forecasters,\n    _load_forecasters,\n)\n\n# Save trained models\ntrained_forecasters = {...}  # Your trained forecasters\n_save_forecasters(trained_forecasters, model_dir=\"models/\")\n\n# Load previously trained models\nloaded_forecasters = _load_forecasters(model_dir=\"models/\")\n</code></pre>"},{"location":"safe/spotforecast2-safe/","title":"spotforecast2-safe: Safety-Critical Streamlining","text":"<p>As part of the MLOps engineering for safety-critical systems, the <code>spotforecast2_safe</code> package has been meticulously streamlined. This document outlines the rationale and the specific changes made to ensure the package contains only the necessary components for the defined forecasting tasks.</p>"},{"location":"safe/spotforecast2-safe/#rationale","title":"Rationale","text":"<p>In safety-critical environments, reducing the \"dead code\" and unnecessary dependencies is paramount. By removing unreachable or unused modules, we: 1. Minimize Attack Surface: Fewer lines of code mean fewer potential vulnerabilities. 2. Improve Predictability: Removing complex model search and statistical heuristics ensures the system behaves exactly as expected for its primary workload. 3. Streamline Compliance: Auditing and validating a smaller codebase is more efficient and reliable.</p>"},{"location":"safe/spotforecast2-safe/#positive-list-retained-components","title":"Positive List (Retained Components)","text":"<p>The following files are essential for the execution of the primary workflows: <code>task_n_to_1.py</code> and <code>task_n_to_1_with_covariates_and_dataframe.py</code>.</p>"},{"location":"safe/spotforecast2-safe/#orchestration-pipelines","title":"Orchestration &amp; Pipelines","text":"<ul> <li><code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code></li> <li><code>src/spotforecast2_safe/processing/n2n_predict.py</code></li> <li><code>src/spotforecast2_safe/processing/agg_predict.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#data-environmental-services","title":"Data &amp; Environmental Services","text":"<ul> <li><code>src/spotforecast2_safe/data/data.py</code></li> <li><code>src/spotforecast2_safe/data/fetch_data.py</code></li> <li><code>src/spotforecast2_safe/weather/weather_client.py</code></li> <li><code>src/spotforecast2_safe/utils/generate_holiday.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#forecaster-engine","title":"Forecaster Engine","text":"<ul> <li><code>src/spotforecast2_safe/forecaster/base.py</code></li> <li><code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code></li> <li><code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code></li> <li><code>src/spotforecast2_safe/forecaster/recursive/_warnings.py</code></li> <li><code>src/spotforecast2_safe/forecaster/utils.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#preprocessing-signal-cleaning","title":"Preprocessing &amp; Signal Cleaning","text":"<ul> <li><code>src/spotforecast2_safe/preprocessing/curate_data.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/imputation.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/outlier.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/split.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_rolling.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_differentiator.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_binner.py</code></li> <li><code>src/spotforecast2_safe/preprocessing/_common.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#core-utilities","title":"Core Utilities","text":"<ul> <li><code>src/spotforecast2_safe/utils/validation.py</code></li> <li><code>src/spotforecast2_safe/utils/data_transform.py</code></li> <li><code>src/spotforecast2_safe/utils/forecaster_config.py</code></li> <li><code>src/spotforecast2_safe/utils/convert_to_utc.py</code></li> <li><code>src/spotforecast2_safe/exceptions.py</code></li> </ul>"},{"location":"safe/spotforecast2-safe/#negative-list-removed-components","title":"Negative List (Removed Components)","text":"<p>The following directories and files were present in the original <code>spotforecast2</code> package but have been removed from <code>spotforecast2_safe</code> as they are not required for the target safety-critical tasks.</p>"},{"location":"safe/spotforecast2-safe/#removed-modules","title":"Removed Modules","text":"<ul> <li><code>src/spotforecast2_safe/model_selection/</code> (Entire directory)<ul> <li>Reason: Bayesian, Grid, and Random search are too complex and non-deterministic for these specific safety-critical deployment targets.</li> </ul> </li> <li><code>src/spotforecast2_safe/stats/</code> (Entire directory)<ul> <li>Reason: Contains autocorrelation and secondary statistical tools not used in the automated pipeline.</li> </ul> </li> <li><code>src/spotforecast2_safe/forecaster/metrics.py</code><ul> <li>Reason: Specialized custom metrics that were not invoked by the core pipelines.</li> </ul> </li> <li><code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code><ul> <li>Reason: Headless production environments do not require Plotly or Matplotlib-based visualizations.</li> </ul> </li> </ul>"},{"location":"safe/spotforecast2-safe/#associated-test-deletions","title":"Associated Test Deletions","text":"<p>To maintain a green build and avoid import errors, the following non-essential tests were also removed: - <code>tests/test_model_selection_utils.py</code> - <code>tests/test_time_series_fold.py</code> - <code>tests/test_ts_visualization.py</code></p>"},{"location":"safe/spotforecast2-safe/#conclusion","title":"Conclusion","text":"<p>The resulting <code>spotforecast2_safe</code> project is a hardened version of the original, with $0$ unreachable code paths for the specified tasks and $100\\%$ test coverage on the remaining logic.</p>"}]}