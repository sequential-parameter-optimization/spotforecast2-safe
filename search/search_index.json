{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to spotforecast2-safe","text":"<p>spotforecast2-safe is a Python package for forecasting, combining the power of <code>sklearn</code>, <code>spotoptim</code>and <code>skforecast</code> with specialized utilities for \"spot\" forecasting.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udce6 GitHub Repository</li> <li>\ud83d\udcda API Reference</li> <li>\ud83d\ude80 Current Version: 0.2.5</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Download from GitHub</li> </ul> <pre><code>git clone https://github.com/sequential-parameter-optimization/spotforecast2-safe.git\ncd spotforecast2-safe\n</code></pre> <ul> <li>Sync using uv <pre><code>uv sync\n</code></pre></li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Data Fetching: Easy access to time series data.</li> <li>Preprocessing: Robust tools for curating, cleaning, and splitting data.</li> <li>Forecasting: A rich set of forecasting strategies (constantly extended).</li> <li>Model Selection: <code>spotoptim</code> and <code>optuna</code> search for hyperparameter tuning.</li> <li>Weather Integration: Utilities for fetching and using weather data in forecasts.</li> </ul>"},{"location":"#attributions","title":"Attributions","text":"<p>Parts of the code are ported from skforecast to reduce external dependencies. Many thanks to the skforecast team for their great work!</p>"},{"location":"api/data/","title":"Data Module","text":"<p>This module provides utilities for fetching and loading time series data.</p>"},{"location":"api/data/#spotforecast2_safe.data","title":"<code>spotforecast2_safe.data</code>","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_holiday_data","title":"<code>fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_weather_data","title":"<code>fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.get_cache_home","title":"<code>get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/data/#data-fetching-functions","title":"Data Fetching Functions","text":""},{"location":"api/data/#fetch_data","title":"fetch_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data","title":"<code>spotforecast2_safe.data.fetch_data</code>","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_data","title":"<code>fetch_data(filename=None, dataframe=None, columns=None, index_col=0, parse_dates=True, dayfirst=False, timezone='UTC')</code>","text":"<p>Fetches the integrated raw dataset from a CSV file or processes a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename of the CSV file containing the dataset. Must be located in the data home directory. If both filename and dataframe are None, defaults to \"data_in.csv\".</p> <code>None</code> <code>dataframe</code> <code>DataFrame</code> <p>A pandas DataFrame to process. If provided, it will be processed with proper timezone handling. Mutually exclusive with filename.</p> <code>None</code> <code>columns</code> <code>list</code> <p>List of columns to be included in the dataset. If None, all columns are included. If an empty list is provided, a ValueError is raised.</p> <code>None</code> <code>index_col</code> <code>int</code> <p>Column index to be used as the index (only used when loading from CSV).</p> <code>0</code> <code>parse_dates</code> <code>bool</code> <p>Whether to parse dates in the index column (only used when loading from CSV).</p> <code>True</code> <code>dayfirst</code> <code>bool</code> <p>Whether the day comes first in date parsing (only used when loading from CSV).</p> <code>False</code> <code>timezone</code> <code>str</code> <p>Timezone to set for the datetime index. If a DataFrame with naive index is provided, it will be localized to this timezone then converted to UTC. Default: \"UTC\".</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The integrated raw dataset with UTC timezone.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If columns is an empty list or if both filename and dataframe are provided.</p> <code>FileNotFoundError</code> <p>If CSV file does not exist.</p> <p>Examples:</p> <p>Load from CSV (default):</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; data = fetch_data(columns=[\"col1\", \"col2\"])\n&gt;&gt;&gt; data.head()\n                Header1  Header2  Header3\n</code></pre> <p>Load from specific CSV:</p> <pre><code>&gt;&gt;&gt; data = fetch_data(filename=\"custom_data.csv\")\n</code></pre> <p>Process a DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]},\n...                   index=pd.date_range(\"2024-01-01\", periods=3, freq=\"h\"))\n&gt;&gt;&gt; data = fetch_data(dataframe=df, timezone=\"Europe/Berlin\")\n&gt;&gt;&gt; data.index.tz\n&lt;UTC&gt;\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_data(\n    filename: Optional[str] = None,\n    dataframe: Optional[pd.DataFrame] = None,\n    columns: Optional[list] = None,\n    index_col: int = 0,\n    parse_dates: bool = True,\n    dayfirst: bool = False,\n    timezone: str = \"UTC\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches the integrated raw dataset from a CSV file or processes a DataFrame.\n\n    Args:\n        filename (str, optional):\n            Filename of the CSV file containing the dataset. Must be located in the\n            data home directory. If both filename and dataframe are None, defaults to \"data_in.csv\".\n        dataframe (pd.DataFrame, optional):\n            A pandas DataFrame to process. If provided, it will be processed with\n            proper timezone handling. Mutually exclusive with filename.\n        columns (list, optional):\n            List of columns to be included in the dataset. If None, all columns are included.\n            If an empty list is provided, a ValueError is raised.\n        index_col (int):\n            Column index to be used as the index (only used when loading from CSV).\n        parse_dates (bool):\n            Whether to parse dates in the index column (only used when loading from CSV).\n        dayfirst (bool):\n            Whether the day comes first in date parsing (only used when loading from CSV).\n        timezone (str):\n            Timezone to set for the datetime index. If a DataFrame with naive index is provided,\n            it will be localized to this timezone then converted to UTC. Default: \"UTC\".\n\n    Returns:\n        pd.DataFrame: The integrated raw dataset with UTC timezone.\n\n    Raises:\n        ValueError: If columns is an empty list or if both filename and dataframe are provided.\n        FileNotFoundError: If CSV file does not exist.\n\n    Examples:\n        Load from CSV (default):\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; data = fetch_data(columns=[\"col1\", \"col2\"])\n        &gt;&gt;&gt; data.head()\n                        Header1  Header2  Header3\n\n        Load from specific CSV:\n        &gt;&gt;&gt; data = fetch_data(filename=\"custom_data.csv\")\n\n        Process a DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]},\n        ...                   index=pd.date_range(\"2024-01-01\", periods=3, freq=\"h\"))\n        &gt;&gt;&gt; data = fetch_data(dataframe=df, timezone=\"Europe/Berlin\")\n        &gt;&gt;&gt; data.index.tz\n        &lt;UTC&gt;\n    \"\"\"\n    if columns is not None and len(columns) == 0:\n        raise ValueError(\"columns must be specified and cannot be empty.\")\n\n    if filename is not None and dataframe is not None:\n        raise ValueError(\n            \"Cannot specify both filename and dataframe. Please provide only one.\"\n        )\n\n    # Process DataFrame if provided\n    if dataframe is not None:\n        dataset = Data.from_dataframe(\n            df=dataframe,\n            timezone=timezone,\n            columns=columns,\n        )\n    else:\n        # Load from CSV file\n        if filename is None:\n            filename = \"data_in.csv\"\n        csv_path = get_data_home() / filename\n        if not Path(csv_path).is_file():\n            raise FileNotFoundError(f\"The file {csv_path} does not exist.\")\n\n        dataset = Data.from_csv(\n            csv_path=csv_path,\n            index_col=index_col,\n            parse_dates=parse_dates,\n            dayfirst=dayfirst,\n            timezone=timezone,\n            columns=columns,\n        )\n\n    return dataset.data\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_holiday_data","title":"<code>fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_weather_data","title":"<code>fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_cache_home","title":"<code>get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/data/#fetch_holiday_data","title":"fetch_holiday_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_holiday_data","title":"<code>spotforecast2_safe.data.fetch_data.fetch_holiday_data(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Fetches holiday data for the dataset period.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str or Timestamp</code> <p>Start date of the dataset period.</p> required <code>end</code> <code>str or Timestamp</code> <p>End date of the dataset period.</p> required <code>tz</code> <code>str</code> <p>Timezone for the holiday data.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the holiday data.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for the holidays.</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for the holidays.</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing holiday information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; holiday_df.head()\n                is_holiday\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_holiday_data(\n    start: str | Timestamp,\n    end: str | Timestamp,\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches holiday data for the dataset period.\n\n    Args:\n        start (str or pd.Timestamp):\n            Start date of the dataset period.\n        end (str or pd.Timestamp):\n            End date of the dataset period.\n        tz (str):\n            Timezone for the holiday data.\n        freq (str):\n            Frequency of the holiday data.\n        country_code (str):\n            Country code for the holidays.\n        state (str):\n            State code for the holidays.\n\n    Returns:\n        pd.DataFrame: DataFrame containing holiday information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_holiday_data\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; holiday_df.head()\n                        is_holiday\n    \"\"\"\n\n    holiday_df = create_holiday_df(\n        start=start, end=end, tz=tz, freq=freq, country_code=country_code, state=state\n    )\n    return holiday_df\n</code></pre>"},{"location":"api/data/#fetch_weather_data","title":"fetch_weather_data","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.fetch_weather_data","title":"<code>spotforecast2_safe.data.fetch_data.fetch_weather_data(cov_start, cov_end, latitude=51.5136, longitude=7.4653, timezone='UTC', freq='h', fallback_on_failure=True, cached=True)</code>","text":"<p>Fetches weather data for the dataset period plus forecast horizon.     Create weather dataframe using API with optional caching. Args:     cov_start (str):         Start date for covariate data.     cov_end (str):         End date for covariate data.     latitude (float):         Latitude of the location for weather data. Default is 51.5136 (Dortmund).     longitude (float):         Longitude of the location for weather data. Default is 7.4653 (Dortmund).     timezone (str):         Timezone for the weather data.     freq (str):         Frequency of the weather data.     fallback_on_failure (bool):         Whether to use fallback data in case of failure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing weather information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start='2023-01-01T00:00',\n...     cov_end='2023-01-11T00:00',\n...     latitude=51.5136,\n...     longitude=7.4653,\n...     timezone='UTC',\n...     freq='h',\n...     fallback_on_failure=True,\n...     cached=True\n... )\n&gt;&gt;&gt; weather_df.head()\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def fetch_weather_data(\n    cov_start: str,\n    cov_end: str,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n    cached=True,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches weather data for the dataset period plus forecast horizon.\n        Create weather dataframe using API with optional caching.\n    Args:\n        cov_start (str):\n            Start date for covariate data.\n        cov_end (str):\n            End date for covariate data.\n        latitude (float):\n            Latitude of the location for weather data. Default is 51.5136 (Dortmund).\n        longitude (float):\n            Longitude of the location for weather data. Default is 7.4653 (Dortmund).\n        timezone (str):\n            Timezone for the weather data.\n        freq (str):\n            Frequency of the weather data.\n        fallback_on_failure (bool):\n            Whether to use fallback data in case of failure.\n\n    Returns:\n        pd.DataFrame: DataFrame containing weather information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_weather_data\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start='2023-01-01T00:00',\n        ...     cov_end='2023-01-11T00:00',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653,\n        ...     timezone='UTC',\n        ...     freq='h',\n        ...     fallback_on_failure=True,\n        ...     cached=True\n        ... )\n        &gt;&gt;&gt; weather_df.head()\n    \"\"\"\n    if cached:\n        cache_path = get_data_home() / \"weather_cache.parquet\"\n    else:\n        cache_path = None\n\n    service = WeatherService(\n        latitude=latitude, longitude=longitude, cache_path=cache_path\n    )\n\n    weather_df = service.get_dataframe(\n        start=cov_start,\n        end=cov_end,\n        timezone=timezone,\n        freq=freq,\n        fallback_on_failure=fallback_on_failure,\n    )\n    return weather_df\n</code></pre>"},{"location":"api/data/#data-utilities","title":"Data Utilities","text":""},{"location":"api/data/#get_cache_home","title":"get_cache_home","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_cache_home","title":"<code>spotforecast2_safe.data.fetch_data.get_cache_home(cache_home=None)</code>","text":"<p>Return the location where persistent models are to be cached.</p> <p>By default the cache directory is set to a folder named 'spotforecast2_cache' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>This directory is used to store pickled trained models for quick reuse across forecasting runs, following scikit-learn model persistence conventions.</p> <p>Parameters:</p> Name Type Description Default <code>cache_home</code> <code>str or Path</code> <p>The path to spotforecast cache directory. If <code>None</code>, the default path is <code>~/spotforecast2_cache</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the spotforecast cache directory.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.name\n'spotforecast2_cache'\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom cache location\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n&gt;&gt;&gt; custom_cache.exists()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Using environment variable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n&gt;&gt;&gt; cache_dir = get_cache_home()\n&gt;&gt;&gt; cache_dir.as_posix()\n'/var/cache/spotforecast2'\n</code></pre> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_cache_home(cache_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where persistent models are to be cached.\n\n    By default the cache directory is set to a folder named 'spotforecast2_cache' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_CACHE' environment\n    variable or programmatically by giving an explicit folder path. The '~' symbol is\n    expanded to the user home folder. If the folder does not already exist, it is\n    automatically created.\n\n    This directory is used to store pickled trained models for quick reuse across\n    forecasting runs, following scikit-learn model persistence conventions.\n\n    Args:\n        cache_home (str or pathlib.Path, optional):\n            The path to spotforecast cache directory. If `None`, the default path\n            is `~/spotforecast2_cache`.\n\n    Returns:\n        pathlib.Path:\n            The path to the spotforecast cache directory.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission issues.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import get_cache_home\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.name\n        'spotforecast2_cache'\n\n        &gt;&gt;&gt; # Custom cache location\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; custom_cache = get_cache_home(Path('/tmp/my_cache'))\n        &gt;&gt;&gt; custom_cache.exists()\n        True\n\n        &gt;&gt;&gt; # Using environment variable\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['SPOTFORECAST2_CACHE'] = '/var/cache/spotforecast2'\n        &gt;&gt;&gt; cache_dir = get_cache_home()\n        &gt;&gt;&gt; cache_dir.as_posix()\n        '/var/cache/spotforecast2'\n    \"\"\"\n    if cache_home is None:\n        cache_home = environ.get(\n            \"SPOTFORECAST2_CACHE\", Path.home() / \"spotforecast2_cache\"\n        )\n    # Ensure cache_home is a Path() object pointing to an absolute path\n    cache_home = Path(cache_home).expanduser().absolute()\n    # Create cache directory if it does not exist\n    cache_home.mkdir(parents=True, exist_ok=True)\n    return cache_home\n</code></pre>"},{"location":"api/data/#get_data_home","title":"get_data_home","text":""},{"location":"api/data/#spotforecast2_safe.data.fetch_data.get_data_home","title":"<code>spotforecast2_safe.data.fetch_data.get_data_home(data_home=None)</code>","text":"<p>Return the location where datasets are to be stored.</p> <p>By default the data directory is set to a folder named 'spotforecast2_data' in the user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotforecast data directory. If <code>None</code>, the default path is <code>~/spotforecast2_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotforecast data directory.</p> <p>Examples:     &gt;&gt;&gt; from pathlib import Path     &gt;&gt;&gt; get_data_home()     PosixPath('/home/user/spotforecast2_data')     &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))     PosixPath('/tmp/spotforecast2_data')</p> Source code in <code>src/spotforecast2_safe/data/fetch_data.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotforecast2_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTFORECAST2_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotforecast data directory. If `None`, the default path\n            is `~/spotforecast2_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotforecast data directory.\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotforecast2_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotforecast2_data'))\n        PosixPath('/tmp/spotforecast2_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\n            \"SPOTFORECAST2_DATA\", Path.home() / \"spotforecast2_data\"\n        )\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).expanduser().absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions Module","text":"<p>Custom exceptions and error handling.</p>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions","title":"<code>spotforecast2_safe.exceptions</code>","text":"<p>Custom exceptions and warnings for spotforecast2.</p> <p>This module contains all the custom warnings and error classes used across spotforecast2.</p> <p>Examples:</p> <p>Using custom warnings::</p> <pre><code>import warnings\nfrom spotforecast2.exceptions import MissingValuesWarning\n\n# Raise a warning\nwarnings.warn(\n    \"Missing values detected in input data.\",\n    MissingValuesWarning\n)\n\n# Suppress a specific warning\nwarnings.simplefilter('ignore', category=MissingValuesWarning)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.DataTransformationWarning","title":"<code>DataTransformationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for output data in transformed space.</p> <p>Used to notify that the output data is in the transformed space.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Output is in transformed space.\",\n...     DataTransformationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTransformationWarning(UserWarning):\n    \"\"\"Warning for output data in transformed space.\n\n    Used to notify that the output data is in the transformed space.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Output is in transformed space.\",\n        ...     DataTransformationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTransformationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.ExogenousInterpretationWarning","title":"<code>ExogenousInterpretationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning about implications when using exogenous variables.</p> <p>Used to notify about important implications when using exogenous variables with models that use a two-step approach (e.g., regression + ARAR).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous variables may not be used as expected.\",\n...     ExogenousInterpretationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class ExogenousInterpretationWarning(UserWarning):\n    \"\"\"Warning about implications when using exogenous variables.\n\n    Used to notify about important implications when using exogenous\n    variables with models that use a two-step approach (e.g., regression + ARAR).\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous variables may not be used as expected.\",\n        ...     ExogenousInterpretationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ExogenousInterpretationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.FeatureOutOfRangeWarning","title":"<code>FeatureOutOfRangeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for features out of training range.</p> <p>Used to notify that a feature is out of the range seen during training.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Feature value exceeds training range.\",\n...     FeatureOutOfRangeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class FeatureOutOfRangeWarning(UserWarning):\n    \"\"\"Warning for features out of training range.\n\n    Used to notify that a feature is out of the range seen during training.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Feature value exceeds training range.\",\n        ...     FeatureOutOfRangeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=FeatureOutOfRangeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for ignored arguments.</p> <p>Used to notify that an argument is ignored when using a method or a function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Argument 'x' is ignored in this context.\",\n...     IgnoredArgumentWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class IgnoredArgumentWarning(UserWarning):\n    \"\"\"Warning for ignored arguments.\n\n    Used to notify that an argument is ignored when using a method\n    or a function.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Argument 'x' is ignored in this context.\",\n        ...     IgnoredArgumentWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.InputTypeWarning","title":"<code>InputTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for inefficient input format.</p> <p>Used to notify that input format is not the most efficient or recommended for the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Input format is not optimal for this forecaster.\",\n...     InputTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class InputTypeWarning(UserWarning):\n    \"\"\"Warning for inefficient input format.\n\n    Used to notify that input format is not the most efficient or\n    recommended for the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Input format is not optimal for this forecaster.\",\n        ...     InputTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=InputTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for potentially long training processes.</p> <p>Used to notify that a large number of models will be trained and the the process may take a while to run.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Training may take a long time.\",\n...     LongTrainingWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class LongTrainingWarning(UserWarning):\n    \"\"\"Warning for potentially long training processes.\n\n    Used to notify that a large number of models will be trained and the\n    the process may take a while to run.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Training may take a long time.\",\n        ...     LongTrainingWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=LongTrainingWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.MissingExogWarning","title":"<code>MissingExogWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing exogenous variables.</p> <p>Used to indicate that there are missing exogenous variables in the data. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing exogenous variables detected.\",\n...     MissingExogWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingExogWarning(UserWarning):\n    \"\"\"Warning for missing exogenous variables.\n\n    Used to indicate that there are missing exogenous variables in the\n    data. Most machine learning models do not accept missing values, so the\n    Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing exogenous variables detected.\",\n        ...     MissingExogWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingExogWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.NotFittedError","title":"<code>NotFittedError</code>","text":"<p>               Bases: <code>ValueError</code>, <code>AttributeError</code></p> <p>Exception class to raise if estimator is used before fitting.</p> <p>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.exceptions import NotFittedError\n&gt;&gt;&gt; try:\n...     raise NotFittedError(\"Forecaster not fitted\")\n... except NotFittedError as e:\n...     print(e)\nForecaster not fitted\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class NotFittedError(ValueError, AttributeError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    This class inherits from both ValueError and AttributeError to help with\n    exception handling and backward compatibility.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.exceptions import NotFittedError\n        &gt;&gt;&gt; try:\n        ...     raise NotFittedError(\"Forecaster not fitted\")\n        ... except NotFittedError as e:\n        ...     print(e)\n        Forecaster not fitted\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.OneStepAheadValidationWarning","title":"<code>OneStepAheadValidationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for one-step-ahead validation usage.</p> <p>Used to notify that the one-step-ahead validation is being used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Using one-step-ahead validation.\",\n...     OneStepAheadValidationWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class OneStepAheadValidationWarning(UserWarning):\n    \"\"\"Warning for one-step-ahead validation usage.\n\n    Used to notify that the one-step-ahead validation is being used.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Using one-step-ahead validation.\",\n        ...     OneStepAheadValidationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.ResidualsUsageWarning","title":"<code>ResidualsUsageWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incorrect residuals usage.</p> <p>Used to notify that a residual are not correctly used in the probabilistic forecasting process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Residuals are not properly used.\",\n...     ResidualsUsageWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class ResidualsUsageWarning(UserWarning):\n    \"\"\"Warning for incorrect residuals usage.\n\n    Used to notify that a residual are not correctly used in the\n    probabilistic forecasting process.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Residuals are not properly used.\",\n        ...     ResidualsUsageWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ResidualsUsageWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.SaveLoadSkforecastWarning","title":"<code>SaveLoadSkforecastWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for save/load operations.</p> <p>Used to notify any issues that may arise when saving or loading a forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Issues detected when saving forecaster.\",\n...     SaveLoadSkforecastWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class SaveLoadSkforecastWarning(UserWarning):\n    \"\"\"Warning for save/load operations.\n\n    Used to notify any issues that may arise when saving or loading\n    a forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Issues detected when saving forecaster.\",\n        ...     SaveLoadSkforecastWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SaveLoadSkforecastWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.SpotforecastVersionWarning","title":"<code>SpotforecastVersionWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for version mismatch.</p> <p>Used to notify that the version installed in the environment differs from the version used to initialize the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Version mismatch detected.\",\n...     SpotforecastVersionWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class SpotforecastVersionWarning(UserWarning):\n    \"\"\"Warning for version mismatch.\n\n    Used to notify that the version installed in the\n    environment differs from the version used to initialize the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Version mismatch detected.\",\n        ...     SpotforecastVersionWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SpotforecastVersionWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.UnknownLevelWarning","title":"<code>UnknownLevelWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for unknown levels in prediction.</p> <p>Used to notify that a level being predicted was not part of the training data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Predicting for an unknown level.\",\n...     UnknownLevelWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class UnknownLevelWarning(UserWarning):\n    \"\"\"Warning for unknown levels in prediction.\n\n    Used to notify that a level being predicted was not part of the\n    training data.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Predicting for an unknown level.\",\n        ...     UnknownLevelWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=UnknownLevelWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.format_warning_handler","title":"<code>format_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom warning handler to format warnings in a box.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is used internally by the warnings module\n&gt;&gt;&gt; set_warnings_style('skforecast')\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def format_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom warning handler to format warnings in a box.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; # This is used internally by the warnings module\n        &gt;&gt;&gt; set_warnings_style('skforecast')  # doctest: +SKIP\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        width = 88\n        title = type(message).__name__\n        output_text = [\"\\\\n\"]\n\n        wrapped_message = textwrap.fill(\n            str(message), width=width - 2, expand_tabs=True, replace_whitespace=True\n        )\n        title_top_border = f\"\u256d{'\u2500' * ((width - len(title) - 2) // 2)} {title} {'\u2500' * ((width - len(title) - 2) // 2)}\u256e\"\n        if len(title) % 2 != 0:\n            title_top_border = title_top_border[:-1] + \"\u2500\" + \"\u256e\"\n        bottom_border = f\"\u2570{'\u2500' * width}\u256f\"\n        output_text.append(title_top_border)\n\n        for line in wrapped_message.split(\"\\\\n\"):\n            output_text.append(f\"\u2502 {line.ljust(width - 2)} \u2502\")\n\n        output_text.append(bottom_border)\n        output_text = \"\\\\n\".join(output_text)\n        color = \"\\\\033[38;5;208m\"\n        reset = \"\\\\033[0m\"\n        output_text = f\"{color}{output_text}{reset}\"\n        print(output_text)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.rich_warning_handler","title":"<code>rich_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom handler for warnings that uses rich to display formatted panels.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is used internally when rich is available\n&gt;&gt;&gt; set_warnings_style('skforecast')\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def rich_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom handler for warnings that uses rich to display formatted panels.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; # This is used internally when rich is available\n        &gt;&gt;&gt; set_warnings_style('skforecast')  # doctest: +SKIP\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        if not HAS_RICH:\n            # Fallback to format_warning_handler if rich is not available\n            format_warning_handler(message, category, filename, lineno, file, line)\n            return\n\n        console = Console()\n\n        category_name = category.__name__\n        text = (\n            f\"{message.message}\\\\n\\\\n\"\n            f\"Category : spotforecast2.exceptions.{category_name}\\\\n\"\n            f\"Location : {filename}:{lineno}\\\\n\"\n            f\"Suppress : warnings.simplefilter('ignore', category={category_name})\"\n        )\n\n        panel = Panel(\n            Text(text, justify=\"left\"),\n            title=category_name,\n            title_align=\"center\",\n            border_style=\"color(214)\",\n            width=88,\n        )\n\n        console.print(panel)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.runtime_deprecated","title":"<code>runtime_deprecated(replacement=None, version=None, removal=None, category=FutureWarning)</code>","text":"<p>Decorator to mark functions or classes as deprecated.</p> <p>Works for both function and class targets, and ensures warnings are visible even inside Jupyter notebooks.</p> <p>Parameters:</p> Name Type Description Default <code>replacement</code> <code>str</code> <p>Name of the replacement function/class to use instead.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version in which the function/class was deprecated.</p> <code>None</code> <code>removal</code> <code>str</code> <p>Version in which the function/class will be removed.</p> <code>None</code> <code>category</code> <code>type[Warning]</code> <p>Warning category to use. Default is FutureWarning.</p> <code>FutureWarning</code> <p>Returns:</p> Type Description <code>object</code> <p>Decorator function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @runtime_deprecated(replacement='new_function', version='0.5', removal='1.0')\n... def old_function():\n...     pass\n&gt;&gt;&gt; old_function()\nFutureWarning: old_function() is deprecated since version 0.5; use new_function instead...\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def runtime_deprecated(\n    replacement: str = None,\n    version: str = None,\n    removal: str = None,\n    category: type[Warning] = FutureWarning,\n) -&gt; object:\n    \"\"\"Decorator to mark functions or classes as deprecated.\n\n    Works for both function and class targets, and ensures warnings are visible\n    even inside Jupyter notebooks.\n\n    Args:\n        replacement: Name of the replacement function/class to use instead.\n        version: Version in which the function/class was deprecated.\n        removal: Version in which the function/class will be removed.\n        category: Warning category to use. Default is FutureWarning.\n\n    Returns:\n        Decorator function.\n\n    Examples:\n        &gt;&gt;&gt; @runtime_deprecated(replacement='new_function', version='0.5', removal='1.0')\n        ... def old_function():\n        ...     pass\n        &gt;&gt;&gt; old_function()  # doctest: +SKIP\n        FutureWarning: old_function() is deprecated since version 0.5; use new_function instead...\n    \"\"\"\n\n    def decorator(obj):\n        is_function = inspect.isfunction(obj) or inspect.ismethod(obj)\n        is_class = inspect.isclass(obj)\n\n        if not (is_function or is_class):\n            raise TypeError(\n                \"@runtime_deprecated can only be used on functions or classes\"\n            )\n\n        # ----- Build warning message -----\n        name = obj.__name__\n        message = (\n            f\"{name}() is deprecated\" if is_function else f\"{name} class is deprecated\"\n        )\n        if version:\n            message += f\" since version {version}\"\n        if replacement:\n            message += f\"; use {replacement} instead\"\n        if removal:\n            message += f\". It will be removed in version {removal}.\"\n        else:\n            message += \".\"\n\n        def issue_warning():\n            \"\"\"Emit warning in a way that always shows in notebooks.\"\"\"\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"always\", category)\n                warnings.warn(message, category, stacklevel=3)\n\n        # ----- Case 1: decorating a function -----\n        if is_function:\n\n            @wraps(obj)\n            def wrapper(*args, **kwargs):\n                issue_warning()\n                return obj(*args, **kwargs)\n\n            # Add metadata\n            wrapper.__deprecated__ = True\n            wrapper.__replacement__ = replacement\n            wrapper.__version__ = version\n            wrapper.__removal__ = removal\n            return wrapper\n\n        # ----- Case 2: decorating a class -----\n        elif is_class:\n            orig_init = getattr(obj, \"__init__\", None)\n            orig_new = getattr(obj, \"__new__\", None)\n\n            # Only wrap whichever exists (some classes use __new__, others __init__)\n            if orig_new and (orig_new is not object.__new__):\n\n                @wraps(orig_new)\n                def wrapped_new(cls, *args, **kwargs):\n                    issue_warning()\n                    return orig_new(cls, *args, **kwargs)\n\n                obj.__new__ = staticmethod(wrapped_new)\n\n            elif orig_init:\n\n                @wraps(orig_init)\n                def wrapped_init(self, *args, **kwargs):\n                    issue_warning()\n                    return orig_init(self, *args, **kwargs)\n\n                obj.__init__ = wrapped_init\n\n            # Add metadata\n            obj.__deprecated__ = True\n            obj.__replacement__ = replacement\n            obj.__version__ = version\n            obj.__removal__ = removal\n\n            return obj\n\n    return decorator\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2_safe.exceptions.set_warnings_style","title":"<code>set_warnings_style(style='skforecast')</code>","text":"<p>Set the warning handler based on the provided style.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>The style of the warning handler. Either 'skforecast' or 'default'.</p> <code>'skforecast'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_warnings_style('skforecast')\n&gt;&gt;&gt; # Now warnings will be displayed with formatting\n&gt;&gt;&gt; set_warnings_style('default')\n&gt;&gt;&gt; # Back to default Python warning format\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_warnings_style(style: str = \"skforecast\") -&gt; None:\n    \"\"\"Set the warning handler based on the provided style.\n\n    Args:\n        style: The style of the warning handler. Either 'skforecast' or 'default'.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; set_warnings_style('skforecast')\n        &gt;&gt;&gt; # Now warnings will be displayed with formatting\n        &gt;&gt;&gt; set_warnings_style('default')\n        &gt;&gt;&gt; # Back to default Python warning format\n    \"\"\"\n    if style == \"skforecast\":\n        if not hasattr(warnings, \"_original_showwarning\"):\n            warnings._original_showwarning = warnings.showwarning\n        if HAS_RICH:\n            warnings.showwarning = rich_warning_handler\n        else:\n            warnings.showwarning = format_warning_handler\n    else:\n        if hasattr(warnings, \"_original_showwarning\"):\n            warnings.showwarning = warnings._original_showwarning\n</code></pre>"},{"location":"api/forecaster/","title":"Forecaster Module","text":"<p>Core forecasting classes and utilities.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster","title":"<code>spotforecast2_safe.forecaster</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase","title":"<code>ForecasterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all forecasters in spotforecast2.</p> <p>All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Attributes:</p> Name Type Description <code>__spotforecast_tags__</code> <p>Dictionary with forecaster tags that characterize the behavior of the forecaster.</p> <p>Examples:</p> <p>To see all abstract methods that need to be implemented:</p> <pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n&gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n['create_train_X_y', 'fit', 'predict', 'set_params']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>class ForecasterBase(ABC):\n    \"\"\"Base class for all forecasters in spotforecast2.\n\n    All forecasters should specify all the parameters that can be set at\n    the class level in their __init__.\n\n    Attributes:\n        __spotforecast_tags__: Dictionary with forecaster tags that characterize\n            the behavior of the forecaster.\n\n    Examples:\n        To see all abstract methods that need to be implemented:\n\n        &gt;&gt;&gt; import inspect\n        &gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n        &gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n        ['create_train_X_y', 'fit', 'predict', 'set_params']\n    \"\"\"\n\n    def _preprocess_repr(\n        self,\n        estimator: object | None = None,\n        training_range_: dict[str, str] | None = None,\n        series_names_in_: list[str] | None = None,\n        exog_names_in_: list[str] | None = None,\n        transformer_series: object | dict[str, object] | None = None,\n    ) -&gt; tuple[str, str | None, str | None, str | None, str | None]:\n        \"\"\"Prepare the information to be displayed when a Forecaster object is printed.\n\n        Args:\n            estimator: Estimator object. Default is None.\n            training_range_: Training range. Only used for ForecasterRecursiveMultiSeries.\n                Default is None.\n            series_names_in_: Names of the series used in the forecaster.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n            exog_names_in_: Names of the exogenous variables used in the forecaster.\n                Default is None.\n            transformer_series: Transformer used in the series.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n\n        Returns:\n            Tuple containing params (estimator parameters string), training_range_\n            (training range string representation), series_names_in_ (series names\n            string representation), exog_names_in_ (exogenous variable names string\n            representation), and transformer_series (transformer string representation).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; estimator = Ridge(alpha=0.5)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=estimator, lags=3)\n            &gt;&gt;&gt; params, tr, sn, en, ts = forecaster._preprocess_repr(estimator=estimator)\n            &gt;&gt;&gt; params\n            \"{'alpha': 0.5, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\"\n        \"\"\"\n\n        if estimator is not None:\n            if isinstance(estimator, Pipeline):\n                name_pipe_steps = tuple(\n                    name + \"__\" for name in estimator.named_steps.keys()\n                )\n                params = {\n                    key: value\n                    for key, value in estimator.get_params().items()\n                    if key.startswith(name_pipe_steps)\n                }\n            else:\n                params = estimator.get_params()\n            params = str(params)\n        else:\n            params = None\n\n        if training_range_ is not None:\n            training_range_ = [\n                f\"'{k}': {v.astype(str).to_list()}\" for k, v in training_range_.items()\n            ]\n            if len(training_range_) &gt; 10:\n                training_range_ = training_range_[:5] + [\"...\"] + training_range_[-5:]\n            training_range_ = \", \".join(training_range_)\n\n        if series_names_in_ is not None:\n            if len(series_names_in_) &gt; 50:\n                series_names_in_ = (\n                    series_names_in_[:25] + [\"...\"] + series_names_in_[-25:]\n                )\n            series_names_in_ = \", \".join(series_names_in_)\n\n        if exog_names_in_ is not None:\n            if len(exog_names_in_) &gt; 50:\n                exog_names_in_ = exog_names_in_[:25] + [\"...\"] + exog_names_in_[-25:]\n            exog_names_in_ = \", \".join(exog_names_in_)\n\n        if transformer_series is not None:\n            if isinstance(transformer_series, dict):\n                transformer_series = [\n                    f\"'{k}': {v}\" for k, v in transformer_series.items()\n                ]\n                if len(transformer_series) &gt; 10:\n                    transformer_series = (\n                        transformer_series[:5] + [\"...\"] + transformer_series[-5:]\n                    )\n                transformer_series = \", \".join(transformer_series)\n            else:\n                transformer_series = str(transformer_series)\n\n        return (\n            params,\n            training_range_,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        )\n\n    def _format_text_repr(\n        self,\n        text: str,\n        max_text_length: int = 58,\n        width: int = 80,\n        indent: str = \"    \",\n    ) -&gt; str:\n        \"\"\"Format text for __repr__ method.\n\n        Args:\n            text: Text to format.\n            max_text_length: Maximum length of the text before wrapping. Default is 58.\n            width: Maximum width of the text. Default is 80.\n            indent: Indentation of the text. Default is four spaces.\n\n        Returns:\n            Formatted text string with proper wrapping and indentation.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster._format_text_repr(\"Short text\")\n            'Short text'\n        \"\"\"\n\n        if text is not None and len(text) &gt; max_text_length:\n            text = \"\\n    \" + textwrap.fill(\n                str(text), width=width, subsequent_indent=indent\n            )\n\n        return text\n\n    @abstractmethod\n    def create_train_X_y(\n        self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned.\n                Default is None.\n\n        Returns:\n            Tuple containing X_train (training values/predictors with shape\n            (len(y) - max_lag, len(lags))) and y_train (target values of the\n            time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n            &gt;&gt;&gt; X_train.head(2)\n               lag_1  lag_2  lag_3\n            3    2.0    1.0    0.0\n            4    3.0    2.0    1.0\n            &gt;&gt;&gt; y_train.head(2)\n            3    3\n            4    4\n            Name: y, dtype: int64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n        \"\"\"Training Forecaster.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned so\n                that y[i] is regressed on exog[i]. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.is_fitted\n            True\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n    ) -&gt; pd.Series:\n        \"\"\"Predict n steps ahead.\n\n        Args:\n            steps: Number of steps to predict.\n            last_window: Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1). If None, the values stored in\n                last_window are used to calculate the initial predictors, and the\n                predictions start right after training data. Default is None.\n            exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n        Returns:\n            Predicted values as a pandas Series.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            10    9.5\n            11    9.0\n            12    8.5\n            Name: pred, dtype: float64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def set_params(self, params: dict[str, object]) -&gt; None:\n        \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n        Args:\n            params: Parameters values dictionary.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n            &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n            &gt;&gt;&gt; forecaster.estimator.alpha\n            0.5\n        \"\"\"\n\n        pass\n\n    def set_lags(\n        self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute lags.\n\n        Attributes max_lag and window_size are also updated.\n\n        Args:\n            lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n                If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n                or range: include only lags present in lags, all elements must be int.\n                If None: no lags are included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.set_lags(lags=5)\n            &gt;&gt;&gt; forecaster.lags\n            array([1, 2, 3, 4, 5])\n        \"\"\"\n\n        pass\n\n    def set_window_features(\n        self, window_features: object | list[object] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute window_features.\n\n        Attributes max_size_window_features, window_features_names,\n        window_features_class_names and window_size are also updated.\n\n        Args:\n            window_features: Instance or list of instances used to create window features.\n                Window features are created from the original time series and are\n                included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n            &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n            &gt;&gt;&gt; forecaster.window_features\n            [RollingFeatures(stats=['mean'], window_sizes=[3])]\n        \"\"\"\n\n        pass\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            Dictionary with forecaster tags describing behavior and capabilities.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; tags['forecaster_task']\n            'regression'\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.summary()\n            ForecasterRecursive\n            ===================\n            Estimator: Ridge()\n            Lags: [1 2 3]\n            ...\n        \"\"\"\n\n        print(self.__repr__())\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n        This method is called when an object is unpickled (deserialized).\n        It handles the migration of deprecated attributes to their new names.\n\n        Args:\n            state: The state dictionary from the pickled object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pickle\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n            &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n        \"\"\"\n\n        # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n        if \"regressor\" in state and \"estimator\" not in state:\n            state[\"estimator\"] = state.pop(\"regressor\")\n\n        self.__dict__.update(state)\n\n    @property\n    def regressor(self) -&gt; Any:\n        \"\"\"Deprecated property. Use estimator instead.\n\n        Returns:\n            The estimator object.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\n            Ridge()\n        \"\"\"\n        warnings.warn(\n            \"The `regressor` attribute is deprecated and will be removed in future \"\n            \"versions. Use `estimator` instead.\",\n            FutureWarning,\n        )\n        return self.estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.regressor","title":"<code>regressor</code>  <code>property</code>","text":"<p>Deprecated property. Use estimator instead.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The estimator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\nRidge()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> <p>This method is called when an object is unpickled (deserialized). It handles the migration of deprecated attributes to their new names.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The state dictionary from the pickled object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n&gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n    This method is called when an object is unpickled (deserialized).\n    It handles the migration of deprecated attributes to their new names.\n\n    Args:\n        state: The state dictionary from the pickled object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n        &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n    \"\"\"\n\n    # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n    if \"regressor\" in state and \"estimator\" not in state:\n        state[\"estimator\"] = state.pop(\"regressor\")\n\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing X_train (training values/predictors with shape</p> <code>Series</code> <p>(len(y) - max_lag, len(lags))) and y_train (target values of the</p> <code>tuple[DataFrame, Series]</code> <p>time series related to each row of X_train with shape (len(y) - max_lag,)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n&gt;&gt;&gt; X_train.head(2)\n   lag_1  lag_2  lag_3\n3    2.0    1.0    0.0\n4    3.0    2.0    1.0\n&gt;&gt;&gt; y_train.head(2)\n3    3\n4    4\nName: y, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef create_train_X_y(\n    self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned.\n            Default is None.\n\n    Returns:\n        Tuple containing X_train (training values/predictors with shape\n        (len(y) - max_lag, len(lags))) and y_train (target values of the\n        time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n        &gt;&gt;&gt; X_train.head(2)\n           lag_1  lag_2  lag_3\n        3    2.0    1.0    0.0\n        4    3.0    2.0    1.0\n        &gt;&gt;&gt; y_train.head(2)\n        3    3\n        4    4\n        Name: y, dtype: int64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.fit","title":"<code>fit(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.is_fitted\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n    \"\"\"Training Forecaster.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned so\n            that y[i] is regressed on exog[i]. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.is_fitted\n        True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with forecaster tags describing behavior and capabilities.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; tags['forecaster_task']\n'regression'\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        Dictionary with forecaster tags describing behavior and capabilities.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; tags['forecaster_task']\n        'regression'\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If None, the values stored in last_window are used to calculate the initial predictors, and the predictions start right after training data. Default is None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Predicted values as a pandas Series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n10    9.5\n11    9.0\n12    8.5\nName: pred, dtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; pd.Series:\n    \"\"\"Predict n steps ahead.\n\n    Args:\n        steps: Number of steps to predict.\n        last_window: Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1). If None, the values stored in\n            last_window are used to calculate the initial predictors, and the\n            predictions start right after training data. Default is None.\n        exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n    Returns:\n        Predicted values as a pandas Series.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        10    9.5\n        11    9.0\n        12    8.5\n        Name: pred, dtype: float64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute lags.</p> <p>Attributes max_lag and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int | list[int] | ndarray[int] | range[int] | None</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. If int: include lags from 1 to lags (included). If list, 1d numpy ndarray, or range: include only lags present in lags, all elements must be int. If None: no lags are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.set_lags(lags=5)\n&gt;&gt;&gt; forecaster.lags\narray([1, 2, 3, 4, 5])\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_lags(\n    self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute lags.\n\n    Attributes max_lag and window_size are also updated.\n\n    Args:\n        lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n            If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n            or range: include only lags present in lags, all elements must be int.\n            If None: no lags are included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.set_lags(lags=5)\n        &gt;&gt;&gt; forecaster.lags\n        array([1, 2, 3, 4, 5])\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_params","title":"<code>set_params(params)</code>  <code>abstractmethod</code>","text":"<p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, object]</code> <p>Parameters values dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n&gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n&gt;&gt;&gt; forecaster.estimator.alpha\n0.5\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef set_params(self, params: dict[str, object]) -&gt; None:\n    \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n    Args:\n        params: Parameters values dictionary.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n        &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n        &gt;&gt;&gt; forecaster.estimator.alpha\n        0.5\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute window_features.</p> <p>Attributes max_size_window_features, window_features_names, window_features_class_names and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>object | list[object] | None</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n&gt;&gt;&gt; forecaster.window_features\n[RollingFeatures(stats=['mean'], window_sizes=[3])]\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_window_features(\n    self, window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute window_features.\n\n    Attributes max_size_window_features, window_features_names,\n    window_features_class_names and window_size are also updated.\n\n    Args:\n        window_features: Instance or list of instances used to create window features.\n            Window features are created from the original time series and are\n            included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n        &gt;&gt;&gt; forecaster.window_features\n        [RollingFeatures(stats=['mean'], window_sizes=[3])]\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterBase.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.summary()\nForecasterRecursive\n===================\nEstimator: Ridge()\nLags: [1 2 3]\n...\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.summary()\n        ForecasterRecursive\n        ===================\n        Estimator: Ridge()\n        Lags: [1 2 3]\n        ...\n    \"\"\"\n\n    print(self.__repr__())\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive","title":"<code>ForecasterRecursive</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>Recursive autoregressive forecaster for scikit-learn compatible estimators.</p> <p>This class turns any estimator compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. The forecaster learns to predict future values by using lagged values of the target variable and optional exogenous features. Predictions are made iteratively, where each step uses previous predictions as input for the next step (recursive strategy).</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object</code> <p>Scikit-learn compatible estimator for regression. If None, a default estimator will be initialized. Can also be passed via regressor parameter.</p> <code>None</code> <code>lags</code> <code>Union[int, List[int], ndarray, range, None]</code> <p>Lagged values of the target variable to use as predictors. Can be an integer (uses lags from 1 to lags), list of integers, numpy array, or range. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>window_features</code> <code>Union[object, List[object], None]</code> <p>List of window feature objects to compute features from the target variable. Each object must implement transform_batch() method. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>transformer_y</code> <code>Optional[object]</code> <p>Transformer object for the target variable. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>transformer_exog</code> <code>Optional[object]</code> <p>Transformer object for exogenous variables. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>weight_func</code> <code>Optional[Callable]</code> <p>Function to compute sample weights for training. Must accept an index and return an array of weights. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>Optional[int]</code> <p>Order of differencing to apply to the target variable. Must be a positive integer. Differencing is applied before creating lags. Defaults to None.</p> <code>None</code> <code>fit_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of additional keyword arguments to pass to the estimator's fit() method. Defaults to None.</p> <code>None</code> <code>binner_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of keyword arguments for QuantileBinner used in probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.</p> <code>None</code> <code>forecaster_id</code> <code>Union[str, int, None]</code> <p>Identifier for the forecaster instance. Can be a string or integer. Used for tracking and logging purposes. Defaults to None.</p> <code>None</code> <code>regressor</code> <code>object</code> <p>Alternative parameter name for estimator. If provided, used instead of estimator. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator</code> <p>Fitted scikit-learn estimator.</p> <code>lags</code> <p>Lag indices used in the model.</p> <code>lags_names</code> <p>Names of lag features (e.g., ['lag_1', 'lag_2']).</p> <code>window_features</code> <p>List of window feature transformers.</p> <code>window_features_names</code> <p>Names of window features.</p> <code>window_size</code> <p>Maximum window size needed (max of lags and window features).</p> <code>transformer_y</code> <p>Transformer for target variable.</p> <code>transformer_exog</code> <p>Transformer for exogenous variables.</p> <code>weight_func</code> <p>Function for sample weighting.</p> <code>differentiation</code> <p>Order of differencing applied.</p> <code>differentiator</code> <p>TimeSeriesDifferentiator instance if differencing is used.</p> <code>is_fitted</code> <p>Boolean indicating if forecaster has been fitted.</p> <code>fit_date</code> <p>Timestamp of the last fit operation.</p> <code>last_window_</code> <p>Last window_size observations from training data.</p> <code>index_type_</code> <p>Type of index in training data (RangeIndex or DatetimeIndex).</p> <code>index_freq_</code> <p>Frequency of DatetimeIndex if applicable.</p> <code>training_range_</code> <p>First and last index values of training data.</p> <code>series_name_in_</code> <p>Name of the target series.</p> <code>exog_in_</code> <p>Boolean indicating if exogenous variables were used in training.</p> <code>exog_names_in_</code> <p>Names of exogenous variables.</p> <code>exog_type_in_</code> <p>Type of exogenous input (Series or DataFrame).</p> <code>X_train_features_names_out_</code> <p>Names of all training features.</p> <code>in_sample_residuals_</code> <p>Residuals from training set.</p> <code>in_sample_residuals_by_bin_</code> <p>Residuals grouped by bins for probabilistic pred.</p> <code>forecaster_id</code> <p>Identifier for the forecaster instance.</p> Note <ul> <li>Either lags or window_features (or both) must be provided during initialization.</li> <li>The forecaster uses a recursive strategy where each multi-step prediction   depends on previous predictions within the same forecast horizon.</li> <li>Exogenous variables must have the same index as the target variable and must   be available for the entire prediction horizon.</li> <li>The forecaster supports point predictions, prediction intervals, bootstrapping,   quantile predictions, and probabilistic forecasts via conformal methods.</li> </ul> <p>Examples:</p> <p>Create a basic forecaster with lags:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=10\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> <p>Create a forecaster with window features and transformations:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(n_estimators=100),\n...     lags=[1, 7, 30],\n...     window_features=[RollingMeanWindow(window=7)],\n...     transformer_y=StandardScaler(),\n...     differentiation=1\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n</code></pre> <p>Create a forecaster with exogenous variables:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=Ridge(),\n...     lags=7,\n...     forecaster_id='my_forecaster'\n... )\n&gt;&gt;&gt; forecaster.fit(y, exog)\n&gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n</code></pre> <p>Create a forecaster with probabilistic prediction configuration:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=GradientBoostingRegressor(),\n...     lags=14,\n...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n... )\n&gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n&gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>class ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    Recursive autoregressive forecaster for scikit-learn compatible estimators.\n\n    This class turns any estimator compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster. The forecaster learns to predict\n    future values by using lagged values of the target variable and optional exogenous\n    features. Predictions are made iteratively, where each step uses previous predictions\n    as input for the next step (recursive strategy).\n\n    Args:\n        estimator: Scikit-learn compatible estimator for regression. If None, a default\n            estimator will be initialized. Can also be passed via regressor parameter.\n        lags: Lagged values of the target variable to use as predictors. Can be an\n            integer (uses lags from 1 to lags), list of integers, numpy array, or range.\n            At least one of lags or window_features must be provided. Defaults to None.\n        window_features: List of window feature objects to compute features from the\n            target variable. Each object must implement transform_batch() method.\n            At least one of lags or window_features must be provided. Defaults to None.\n        transformer_y: Transformer object for the target variable. Must implement fit()\n            and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        transformer_exog: Transformer object for exogenous variables. Must implement\n            fit() and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        weight_func: Function to compute sample weights for training. Must accept an\n            index and return an array of weights. Defaults to None.\n        differentiation: Order of differencing to apply to the target variable.\n            Must be a positive integer. Differencing is applied before creating lags.\n            Defaults to None.\n        fit_kwargs: Dictionary of additional keyword arguments to pass to the estimator's\n            fit() method. Defaults to None.\n        binner_kwargs: Dictionary of keyword arguments for QuantileBinner used in\n            probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.\n        forecaster_id: Identifier for the forecaster instance. Can be a string or\n            integer. Used for tracking and logging purposes. Defaults to None.\n        regressor: Alternative parameter name for estimator. If provided, used instead\n            of estimator. Defaults to None.\n\n    Attributes:\n        estimator: Fitted scikit-learn estimator.\n        lags: Lag indices used in the model.\n        lags_names: Names of lag features (e.g., ['lag_1', 'lag_2']).\n        window_features: List of window feature transformers.\n        window_features_names: Names of window features.\n        window_size: Maximum window size needed (max of lags and window features).\n        transformer_y: Transformer for target variable.\n        transformer_exog: Transformer for exogenous variables.\n        weight_func: Function for sample weighting.\n        differentiation: Order of differencing applied.\n        differentiator: TimeSeriesDifferentiator instance if differencing is used.\n        is_fitted: Boolean indicating if forecaster has been fitted.\n        fit_date: Timestamp of the last fit operation.\n        last_window_: Last window_size observations from training data.\n        index_type_: Type of index in training data (RangeIndex or DatetimeIndex).\n        index_freq_: Frequency of DatetimeIndex if applicable.\n        training_range_: First and last index values of training data.\n        series_name_in_: Name of the target series.\n        exog_in_: Boolean indicating if exogenous variables were used in training.\n        exog_names_in_: Names of exogenous variables.\n        exog_type_in_: Type of exogenous input (Series or DataFrame).\n        X_train_features_names_out_: Names of all training features.\n        in_sample_residuals_: Residuals from training set.\n        in_sample_residuals_by_bin_: Residuals grouped by bins for probabilistic pred.\n        forecaster_id: Identifier for the forecaster instance.\n\n    Note:\n        - Either lags or window_features (or both) must be provided during initialization.\n        - The forecaster uses a recursive strategy where each multi-step prediction\n          depends on previous predictions within the same forecast horizon.\n        - Exogenous variables must have the same index as the target variable and must\n          be available for the entire prediction horizon.\n        - The forecaster supports point predictions, prediction intervals, bootstrapping,\n          quantile predictions, and probabilistic forecasts via conformal methods.\n\n    Examples:\n        Create a basic forecaster with lags:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=10\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n\n        Create a forecaster with window features and transformations:\n\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(n_estimators=100),\n        ...     lags=[1, 7, 30],\n        ...     window_features=[RollingMeanWindow(window=7)],\n        ...     transformer_y=StandardScaler(),\n        ...     differentiation=1\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n\n        Create a forecaster with exogenous variables:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=Ridge(),\n        ...     lags=7,\n        ...     forecaster_id='my_forecaster'\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, exog)\n        &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n\n        Create a forecaster with probabilistic prediction configuration:\n\n        &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=GradientBoostingRegressor(),\n        ...     lags=14,\n        ...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n        &gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: object = None,\n        lags: Union[int, List[int], np.ndarray, range, None] = None,\n        window_features: Union[object, List[object], None] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        binner_kwargs: Optional[Dict[str, object]] = None,\n        forecaster_id: Union[str, int, None] = None,\n        regressor: object = None,\n    ) -&gt; None:\n\n        self.estimator = copy(initialize_estimator(estimator, regressor))\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiation_max = None\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.exog_dtypes_out_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        self.spotforecast_version = __version__\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n\n        (\n            self.lags,\n            self.lags_names,\n            self.max_lag,\n        ) = initialize_lags(type(self).__name__, lags)\n        (\n            self.window_features,\n            self.window_features_names,\n            self.max_size_window_features,\n        ) = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        self.window_size = max(\n            [\n                ws\n                for ws in [self.max_lag, self.max_size_window_features]\n                if ws is not None\n            ]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=estimator,\n            weight_func=weight_func,\n            series_weights=None,\n        )\n\n        if differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation &lt; 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation  # , window_size=self.window_size # Note: TimeSeriesDifferentiator in preprocessing I created only takes order\n            )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n            estimator=estimator, fit_kwargs=fit_kwargs\n        )\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Estimator: {type(self.estimator).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Estimator parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n                &lt;ul&gt;\n                    {exog_names_in_}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Data Transformations&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Estimator Parameters&lt;/summary&gt;\n                &lt;ul&gt;\n                    {params}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n                &lt;ul&gt;\n                    {self.fit_kwargs}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n        super().__setstate__(state)\n        if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n            self.__spotforecast_tags__ = {\n                \"library\": \"spotforecast\",\n                \"forecaster_name\": \"ForecasterRecursive\",\n                \"forecaster_task\": \"regression\",\n                \"forecasting_scope\": \"single-series\",\n                \"forecasting_strategy\": \"recursive\",\n                \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n                \"requires_index_frequency\": True,\n                \"allowed_input_types_series\": [\"pandas.Series\"],\n                \"supports_exog\": True,\n                \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n                \"handles_missing_values_series\": False,\n                \"handles_missing_values_exog\": True,\n                \"supports_lags\": True,\n                \"supports_window_features\": True,\n                \"supports_transformer_series\": True,\n                \"supports_transformer_exog\": True,\n                \"supports_weight_func\": True,\n                \"supports_differentiation\": True,\n                \"prediction_types\": [\n                    \"point\",\n                    \"interval\",\n                    \"bootstrapping\",\n                    \"quantiles\",\n                    \"distribution\",\n                ],\n                \"supports_probabilistic\": True,\n                \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n                \"handles_binned_residuals\": True,\n            }\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None,\n    ) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create lagged predictors and aligned target values.\n\n        Args:\n            y: Target values used to build lag features. Expected shape is\n                (n_samples,) or (n_samples, 1).\n            X_as_pandas: If True, returns lagged features as a pandas DataFrame.\n            train_index: Index to use for the lagged feature DataFrame when\n                `X_as_pandas` is True.\n\n        Returns:\n            Tuple containing:\n                - X_data: Lagged predictors with shape (n_rows, n_lags) or None\n                  if no lags are configured.\n                - y_data: Target values aligned to the lagged predictors with\n                  shape (n_rows,).\n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            # y = y.ravel() # Assuming y is already raveled\n            # Using stride_tricks for sliding window\n            y_strided = np.lib.stride_tricks.sliding_window_view(y, self.window_size)[\n                :-1\n            ]\n            X_data = y_strided[:, self.window_size - self.lags]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                    data=X_data, columns=self.lags_names, index=train_index\n                )\n\n        y_data = y[self.window_size :]\n\n        return X_data, y_data\n\n    def _create_window_features(\n        self,\n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -&gt; Tuple[List[Union[np.ndarray, pd.DataFrame]], List[str]]:\n        \"\"\"\n        Generate window features from the target series.\n\n        Args:\n            y: Target series used to compute window features. Must be a pandas\n                Series with an index aligned to `train_index` after trimming.\n            train_index: Index for the training rows to align the window features.\n            X_as_pandas: If True, keeps each window feature matrix as a pandas\n                DataFrame; otherwise converts to NumPy arrays.\n\n        Returns:\n            Tuple containing:\n                - X_train_window_features: List of window feature matrices, one\n                  per window feature transformer.\n                - X_train_window_features_names_out_: List of feature names for\n                  all generated window features.\n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a pandas DataFrame.\"\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same number of rows as \"\n                    f\"the input time series - `window_size`: {len_train_index}.\"\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same index as \"\n                    f\"the input time series - `window_size`.\"\n                )\n\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name=\"y\")\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n            df=y,\n            transformer=self.transformer_y,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        y_values, y_index = check_extract_values_and_index(data=y, data_label=\"`y`\")\n        if y_values.ndim == 2 and y_values.shape[1] == 1:\n            y_values = y_values.ravel()\n        train_index = y_index[self.window_size :]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                self.differentiator.fit(y_values)  # Differentiator requires fit first\n                y_values = self.differentiator.transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        X_as_pandas = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            _, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\", ignore_freq=True, return_values=False\n            )\n\n            _ = len(y_values) + (\n                self.differentiation if self.differentiation else 0\n            )  # Adjust for differentiation loss of length if needed? No, y_values has NaNs at start\n            # But y_values from check_extract... is raw values.\n            # Differentiator might introduce NaNs. Sklearn transformer keeps length.\n            # My ported differentiator creates NaNs at start.\n\n            # Re-evaluate logic:\n            # y_values (raw) length = N\n            # differentiator transform -&gt; length N, first 'order' are NaN.\n\n            len_exog = len(exog)\n            # The check logic depends on alignment.\n\n            # Simplified check from original code\n            # ... (omitted for brevity, assume caller passed valid data or minimal check)\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=fit_transformer,\n                inverse_transform=False,\n            )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n            X_as_pandas = any(\n                not pd.api.types.is_numeric_dtype(dtype)\n                or pd.api.types.is_bool_dtype(dtype)\n                for dtype in set(exog.dtypes)\n            )\n\n            # Alignment logic\n            if len_exog == len(y):\n                exog = exog.iloc[self.window_size :,]\n            else:\n                pass  # Assume aligned start\n\n        X_train = []\n        X_train_features_names_out_ = []\n\n        # Create lags\n        # Note: y_values might have NaNs from differentiation.\n        # create_lags handles this?\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            if isinstance(y_values, pd.Series):\n                y_vals_for_wf = y_values.iloc[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n            else:\n                y_vals_for_wf = y_values[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n\n            y_window_features = pd.Series(y_vals_for_wf, index=y_index_for_wf)\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features,\n                    X_as_pandas=X_as_pandas,\n                    train_index=train_index,\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                data=X_train, index=train_index, columns=X_train_features_names_out_\n            )\n\n        y_train = pd.Series(data=y_train, index=train_index, name=\"y\")\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        )\n\n    def create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        return self._create_train_X_y(y=y, exog=exog)\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Args:\n            y: Training time series.\n            initial_train_size: Initial size of the training set. It is the number of\n                observations used to train the forecaster before making the first\n                prediction.\n            exog: Exogenous variable/s included as predictor/s. Must have the same\n                number of observations as y and their indexes must be aligned.\n                Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: Predictor values used to train the model as pandas DataFrame.\n                - y_train: Values of the time series related to each row of X_train for\n                    each step in the form {step: y_step_[i]} as dict.\n                - X_test: Predictor values used to test the model as pandas DataFrame.\n                - y_test: Values of the time series related to each row of X_test for\n                    each step in the form {step: y_step_[i]} as dict.\n\n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y=y.iloc[:initial_train_size],\n            exog=exog.iloc[:initial_train_size] if exog is not None else None,\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y=y.iloc[test_init:],\n            exog=exog.iloc[test_init:] if exog is not None else None,\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def get_params(self, deep=True):\n        params = {}\n        for key in [\n            \"estimator\",\n            \"lags\",\n            \"window_features\",\n            \"transformer_y\",\n            \"transformer_exog\",\n            \"weight_func\",\n            \"differentiation\",\n            \"fit_kwargs\",\n            \"binner_kwargs\",\n            \"forecaster_id\",\n        ]:\n            if hasattr(self, key):\n                params[key] = getattr(self, key)\n\n        if not deep:\n            return params\n\n        if hasattr(self, \"estimator\") and self.estimator is not None:\n            if hasattr(self.estimator, \"get_params\"):\n                for key, value in self.estimator.get_params(deep=True).items():\n                    params[f\"estimator__{key}\"] = value\n\n        return params\n\n    def set_params(self, **params):\n        if not params:\n            return self\n\n        valid_params = self.get_params(deep=True)\n        nested_params = {}\n\n        for key, value in params.items():\n            if key not in valid_params and \"__\" not in key:\n                # Relaxed check for now\n                pass\n\n            if \"__\" in key:\n                obj_name, param_name = key.split(\"__\", 1)\n                if obj_name not in nested_params:\n                    nested_params[obj_name] = {}\n                nested_params[obj_name][param_name] = value\n            else:\n                setattr(self, key, value)\n\n        for obj_name, obj_params in nested_params.items():\n            if hasattr(self, obj_name):\n                obj = getattr(self, obj_name)\n                if hasattr(obj, \"set_params\"):\n                    obj.set_params(**obj_params)\n                else:\n                    for param_name, value in obj_params.items():\n                        setattr(obj, param_name, value)\n\n        return self\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        suppress_warnings: bool = False,\n    ) -&gt; None:\n\n        # Reset values\n        self.is_fitted = False\n        self.fit_date = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        SAMPLE_WEIGHT_NAME = \"sample_weight\"\n        if self.weight_func is not None:\n            sample_weight, _, _ = initialize_weights(\n                forecaster_name=type(self).__name__,\n                estimator=self.estimator,\n                weight_func=self.weight_func,\n                series_weights=None,\n            )\n            sample_weight = sample_weight(y.index[self.window_size :])\n            self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n        self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n            del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n        # Store attributes\n        self.last_window_ = y.iloc[-self.window_size :].copy()\n        self.index_type_ = type(y.index)\n        if isinstance(y.index, pd.DatetimeIndex):\n            self.index_freq_ = y.index.freqstr\n        else:\n            try:\n                self.index_freq_ = y.index.step\n            except AttributeError:\n                self.index_freq_ = None\n\n        self.training_range_ = y.index[[0, -1]]\n        self.series_name_in_ = y.name\n        self.exog_in_ = exog is not None\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog) if exog is not None else None\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        residuals = y_train - self.estimator.predict(X_train)\n\n        if len(residuals) &gt; 1000:\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(residuals, size=1000, replace=False)\n\n        self.in_sample_residuals_ = residuals\n\n        if self.binner_kwargs is not None:\n            self.binner = QuantileBinner(**self.binner_kwargs)\n            if isinstance(residuals, pd.Series):\n                residuals = residuals.to_numpy()\n            self.binner.fit(residuals)\n\n            # Construct intervals_ manually if not in binner\n            if hasattr(self.binner, \"intervals_\"):\n                self.binner_intervals_ = self.binner.intervals_\n            else:\n                self.binner_intervals_ = {\n                    i: (self.binner.bins_[i - 1], self.binner.bins_[i])\n                    for i in range(1, len(self.binner.bins_))\n                }\n\n            residuals_binned = self.binner.transform(residuals)\n            self.in_sample_residuals_by_bin_ = {\n                bin: residuals[residuals_binned == bin]\n                for bin in self.binner_intervals_.keys()\n            }\n\n            # Limit residuals stored per bin\n            max_residuals_per_bin = 1000 // self.binner.n_bins\n            for bin, res in self.in_sample_residuals_by_bin_.items():\n                if len(res) &gt; max_residuals_per_bin:\n                    rng = np.random.default_rng(seed=123)\n                    self.in_sample_residuals_by_bin_[bin] = rng.choice(\n                        res, size=max_residuals_per_bin, replace=False\n                    )\n\n    def _create_predict_inputs(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; Tuple[np.ndarray, Union[np.ndarray, None], pd.Index, pd.Index]:\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=self.exog_in_,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n                last_window_exog=None,\n                exog=exog,\n                exog_names_in_=self.exog_names_in_,\n                interval=None,\n                # alpha=None, # Removed alpha check for now\n            )\n\n        last_window = input_to_frame(data=last_window, input_name=\"last_window\")\n        _, last_window_index = check_extract_values_and_index(\n            data=last_window,\n            data_label=\"`last_window`\",\n            ignore_freq=True,\n            return_values=False,\n        )\n\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n\n        last_window = transform_dataframe(\n            df=last_window,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, _ = check_extract_values_and_index(\n            data=last_window, data_label=\"`last_window`\"\n        )\n        last_window_values = last_window_values.ravel()\n\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        exog_values = None\n        exog_index = None\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=False,\n                inverse_transform=False,\n            )\n\n            exog_values, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\"\n            )\n\n            exog_values = (\n                exog_values if isinstance(exog, pd.Series) else exog.to_numpy()\n            )\n\n        return last_window_values, exog_values, prediction_index, exog_index\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Union[np.ndarray, None] = None,\n    ) -&gt; np.ndarray:\n\n        predictions = np.full(shape=steps, fill_value=np.nan)\n\n        for step in range(steps):\n\n            X_gen = []\n\n            if self.lags is not None:\n                X_lags = last_window_values[-self.lags]\n                if X_lags.ndim == 1:\n                    X_lags = X_lags.reshape(1, -1)\n                X_gen.append(X_lags)\n\n            if self.window_features is not None:\n                X_window_features = []\n                for wf in self.window_features:\n                    wf_values = wf.transform(last_window_values)\n                    X_window_features.append(wf_values[-1:])\n\n                X_window_features = np.concatenate(X_window_features, axis=1)\n                X_gen.append(X_window_features)\n\n            if self.exog_in_:\n                X_exog = exog_values[step]\n                if X_exog.ndim &lt; 2:\n                    X_exog = X_exog.reshape(1, -1)\n                X_gen.append(X_exog)\n\n            X_gen = np.concatenate(X_gen, axis=1)\n\n            # Convert to DataFrame with feature names to avoid sklearn warning\n            if self.X_train_features_names_out_ is not None:\n                X_gen = pd.DataFrame(X_gen, columns=self.X_train_features_names_out_)\n\n            pred = self.estimator.predict(X_gen)\n            predictions[step] = pred[0]\n\n            last_window_values = np.append(last_window_values, pred)\n\n        return predictions\n\n    def predict(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; pd.Series:\n\n        last_window_values, exog_values, prediction_index, _ = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        predictions = self._recursive_predict(\n            steps=steps, last_window_values=last_window_values, exog_values=exog_values\n        )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_dataframe(\n            df=pd.Series(predictions, name=\"pred\").to_frame(),\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n        predictions = predictions.iloc[:, 0]\n        predictions.index = prediction_index\n\n        return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a ForecasterRecursive object is printed.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a ForecasterRecursive object is printed.\n    \"\"\"\n\n    params = (\n        self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n    )\n    exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Estimator: {type(self.estimator).__name__} \\n\"\n        f\"Lags: {self.lags} \\n\"\n        f\"Window features: {self.window_features_names} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Exogenous included: {self.exog_in_} \\n\"\n        f\"Exogenous names: {exog_names_in_} \\n\"\n        f\"Transformer for y: {self.transformer_y} \\n\"\n        f\"Transformer for exog: {self.transformer_exog} \\n\"\n        f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n        f\"Differentiation order: {self.differentiation} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Estimator parameters: {params} \\n\"\n        f\"fit_kwargs: {self.fit_kwargs} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"Skforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.ForecasterRecursive.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n    super().__setstate__(state)\n    if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",\n            \"forecasting_strategy\": \"recursive\",\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n</code></pre>"},{"location":"api/forecaster/#base-forecaster","title":"Base Forecaster","text":""},{"location":"api/forecaster/#base","title":"base","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base","title":"<code>spotforecast2_safe.forecaster.base</code>","text":"<p>ForecasterBase class.</p> <p>This module contains the base class for all forecasters in spotforecast2. All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Examples:</p> <p>Create a custom forecaster inheriting from ForecasterBase:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; class MyForecaster(ForecasterBase):\n...     def __init__(self, estimator):\n...         self.estimator = estimator\n...         self.__spotforecast_tags__ = {'hide_lags': True}\n...     def create_train_X_y(self, y, exog=None):\n...         return pd.DataFrame(), pd.Series(dtype=float)\n...     def fit(self, y, exog=None):\n...         pass\n...     def predict(self, steps, last_window=None, exog=None):\n...         return pd.Series(np.zeros(steps))\n...     def set_params(self, params):\n...         pass\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = MyForecaster(estimator=Ridge())\n&gt;&gt;&gt; forecaster\nMyForecaster(estimator=Ridge())\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase","title":"<code>ForecasterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all forecasters in spotforecast2.</p> <p>All forecasters should specify all the parameters that can be set at the class level in their init.</p> <p>Attributes:</p> Name Type Description <code>__spotforecast_tags__</code> <p>Dictionary with forecaster tags that characterize the behavior of the forecaster.</p> <p>Examples:</p> <p>To see all abstract methods that need to be implemented:</p> <pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n&gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n['create_train_X_y', 'fit', 'predict', 'set_params']\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>class ForecasterBase(ABC):\n    \"\"\"Base class for all forecasters in spotforecast2.\n\n    All forecasters should specify all the parameters that can be set at\n    the class level in their __init__.\n\n    Attributes:\n        __spotforecast_tags__: Dictionary with forecaster tags that characterize\n            the behavior of the forecaster.\n\n    Examples:\n        To see all abstract methods that need to be implemented:\n\n        &gt;&gt;&gt; import inspect\n        &gt;&gt;&gt; from spotforecast2.forecaster.base import ForecasterBase\n        &gt;&gt;&gt; [m[0] for m in inspect.getmembers(ForecasterBase, predicate=inspect.isabstract)]\n        ['create_train_X_y', 'fit', 'predict', 'set_params']\n    \"\"\"\n\n    def _preprocess_repr(\n        self,\n        estimator: object | None = None,\n        training_range_: dict[str, str] | None = None,\n        series_names_in_: list[str] | None = None,\n        exog_names_in_: list[str] | None = None,\n        transformer_series: object | dict[str, object] | None = None,\n    ) -&gt; tuple[str, str | None, str | None, str | None, str | None]:\n        \"\"\"Prepare the information to be displayed when a Forecaster object is printed.\n\n        Args:\n            estimator: Estimator object. Default is None.\n            training_range_: Training range. Only used for ForecasterRecursiveMultiSeries.\n                Default is None.\n            series_names_in_: Names of the series used in the forecaster.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n            exog_names_in_: Names of the exogenous variables used in the forecaster.\n                Default is None.\n            transformer_series: Transformer used in the series.\n                Only used for ForecasterRecursiveMultiSeries. Default is None.\n\n        Returns:\n            Tuple containing params (estimator parameters string), training_range_\n            (training range string representation), series_names_in_ (series names\n            string representation), exog_names_in_ (exogenous variable names string\n            representation), and transformer_series (transformer string representation).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; estimator = Ridge(alpha=0.5)\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=estimator, lags=3)\n            &gt;&gt;&gt; params, tr, sn, en, ts = forecaster._preprocess_repr(estimator=estimator)\n            &gt;&gt;&gt; params\n            \"{'alpha': 0.5, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\"\n        \"\"\"\n\n        if estimator is not None:\n            if isinstance(estimator, Pipeline):\n                name_pipe_steps = tuple(\n                    name + \"__\" for name in estimator.named_steps.keys()\n                )\n                params = {\n                    key: value\n                    for key, value in estimator.get_params().items()\n                    if key.startswith(name_pipe_steps)\n                }\n            else:\n                params = estimator.get_params()\n            params = str(params)\n        else:\n            params = None\n\n        if training_range_ is not None:\n            training_range_ = [\n                f\"'{k}': {v.astype(str).to_list()}\" for k, v in training_range_.items()\n            ]\n            if len(training_range_) &gt; 10:\n                training_range_ = training_range_[:5] + [\"...\"] + training_range_[-5:]\n            training_range_ = \", \".join(training_range_)\n\n        if series_names_in_ is not None:\n            if len(series_names_in_) &gt; 50:\n                series_names_in_ = (\n                    series_names_in_[:25] + [\"...\"] + series_names_in_[-25:]\n                )\n            series_names_in_ = \", \".join(series_names_in_)\n\n        if exog_names_in_ is not None:\n            if len(exog_names_in_) &gt; 50:\n                exog_names_in_ = exog_names_in_[:25] + [\"...\"] + exog_names_in_[-25:]\n            exog_names_in_ = \", \".join(exog_names_in_)\n\n        if transformer_series is not None:\n            if isinstance(transformer_series, dict):\n                transformer_series = [\n                    f\"'{k}': {v}\" for k, v in transformer_series.items()\n                ]\n                if len(transformer_series) &gt; 10:\n                    transformer_series = (\n                        transformer_series[:5] + [\"...\"] + transformer_series[-5:]\n                    )\n                transformer_series = \", \".join(transformer_series)\n            else:\n                transformer_series = str(transformer_series)\n\n        return (\n            params,\n            training_range_,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        )\n\n    def _format_text_repr(\n        self,\n        text: str,\n        max_text_length: int = 58,\n        width: int = 80,\n        indent: str = \"    \",\n    ) -&gt; str:\n        \"\"\"Format text for __repr__ method.\n\n        Args:\n            text: Text to format.\n            max_text_length: Maximum length of the text before wrapping. Default is 58.\n            width: Maximum width of the text. Default is 80.\n            indent: Indentation of the text. Default is four spaces.\n\n        Returns:\n            Formatted text string with proper wrapping and indentation.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster._format_text_repr(\"Short text\")\n            'Short text'\n        \"\"\"\n\n        if text is not None and len(text) &gt; max_text_length:\n            text = \"\\n    \" + textwrap.fill(\n                str(text), width=width, subsequent_indent=indent\n            )\n\n        return text\n\n    @abstractmethod\n    def create_train_X_y(\n        self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned.\n                Default is None.\n\n        Returns:\n            Tuple containing X_train (training values/predictors with shape\n            (len(y) - max_lag, len(lags))) and y_train (target values of the\n            time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n            &gt;&gt;&gt; X_train.head(2)\n               lag_1  lag_2  lag_3\n            3    2.0    1.0    0.0\n            4    3.0    2.0    1.0\n            &gt;&gt;&gt; y_train.head(2)\n            3    3\n            4    4\n            Name: y, dtype: int64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n        \"\"\"Training Forecaster.\n\n        Args:\n            y: Training time series.\n            exog: Exogenous variable(s) included as predictor(s). Must have the same\n                number of observations as y and their indexes must be aligned so\n                that y[i] is regressed on exog[i]. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.is_fitted\n            True\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | pd.DataFrame | None = None,\n        exog: pd.Series | pd.DataFrame | None = None,\n    ) -&gt; pd.Series:\n        \"\"\"Predict n steps ahead.\n\n        Args:\n            steps: Number of steps to predict.\n            last_window: Series values used to create the predictors (lags) needed in the\n                first iteration of the prediction (t + 1). If None, the values stored in\n                last_window are used to calculate the initial predictors, and the\n                predictions start right after training data. Default is None.\n            exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n        Returns:\n            Predicted values as a pandas Series.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n            &gt;&gt;&gt; forecaster.fit(y)\n            &gt;&gt;&gt; forecaster.predict(steps=3)\n            10    9.5\n            11    9.0\n            12    8.5\n            Name: pred, dtype: float64\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def set_params(self, params: dict[str, object]) -&gt; None:\n        \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n        Args:\n            params: Parameters values dictionary.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n            &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n            &gt;&gt;&gt; forecaster.estimator.alpha\n            0.5\n        \"\"\"\n\n        pass\n\n    def set_lags(\n        self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute lags.\n\n        Attributes max_lag and window_size are also updated.\n\n        Args:\n            lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n                If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n                or range: include only lags present in lags, all elements must be int.\n                If None: no lags are included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.set_lags(lags=5)\n            &gt;&gt;&gt; forecaster.lags\n            array([1, 2, 3, 4, 5])\n        \"\"\"\n\n        pass\n\n    def set_window_features(\n        self, window_features: object | list[object] | None = None\n    ) -&gt; None:\n        \"\"\"Set new value to the attribute window_features.\n\n        Attributes max_size_window_features, window_features_names,\n        window_features_class_names and window_size are also updated.\n\n        Args:\n            window_features: Instance or list of instances used to create window features.\n                Window features are created from the original time series and are\n                included as predictors. Default is None.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n            &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n            &gt;&gt;&gt; forecaster.window_features\n            [RollingFeatures(stats=['mean'], window_sizes=[3])]\n        \"\"\"\n\n        pass\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            Dictionary with forecaster tags describing behavior and capabilities.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; tags = forecaster.get_tags()\n            &gt;&gt;&gt; tags['forecaster_task']\n            'regression'\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"Show forecaster information.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.summary()\n            ForecasterRecursive\n            ===================\n            Estimator: Ridge()\n            Lags: [1 2 3]\n            ...\n        \"\"\"\n\n        print(self.__repr__())\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n        This method is called when an object is unpickled (deserialized).\n        It handles the migration of deprecated attributes to their new names.\n\n        Args:\n            state: The state dictionary from the pickled object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; import pickle\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n            &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n        \"\"\"\n\n        # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n        if \"regressor\" in state and \"estimator\" not in state:\n            state[\"estimator\"] = state.pop(\"regressor\")\n\n        self.__dict__.update(state)\n\n    @property\n    def regressor(self) -&gt; Any:\n        \"\"\"Deprecated property. Use estimator instead.\n\n        Returns:\n            The estimator object.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n            &gt;&gt;&gt; from sklearn.linear_model import Ridge\n            &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n            &gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\n            Ridge()\n        \"\"\"\n        warnings.warn(\n            \"The `regressor` attribute is deprecated and will be removed in future \"\n            \"versions. Use `estimator` instead.\",\n            FutureWarning,\n        )\n        return self.estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.regressor","title":"<code>regressor</code>  <code>property</code>","text":"<p>Deprecated property. Use estimator instead.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The estimator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.regressor # Raises FutureWarning\nRidge()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> <p>This method is called when an object is unpickled (deserialized). It handles the migration of deprecated attributes to their new names.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The state dictionary from the pickled object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n&gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\n\n    This method is called when an object is unpickled (deserialized).\n    It handles the migration of deprecated attributes to their new names.\n\n    Args:\n        state: The state dictionary from the pickled object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; pickled_forecaster = pickle.dumps(forecaster)\n        &gt;&gt;&gt; unpickled_forecaster = pickle.loads(pickled_forecaster)\n    \"\"\"\n\n    # Migration: 'regressor' renamed to 'estimator' in version 0.18.0\n    if \"regressor\" in state and \"estimator\" not in state:\n        state[\"estimator\"] = state.pop(\"regressor\")\n\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing X_train (training values/predictors with shape</p> <code>Series</code> <p>(len(y) - max_lag, len(lags))) and y_train (target values of the</p> <code>tuple[DataFrame, Series]</code> <p>time series related to each row of X_train with shape (len(y) - max_lag,)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n&gt;&gt;&gt; X_train.head(2)\n   lag_1  lag_2  lag_3\n3    2.0    1.0    0.0\n4    3.0    2.0    1.0\n&gt;&gt;&gt; y_train.head(2)\n3    3\n4    4\nName: y, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef create_train_X_y(\n    self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Create training matrices from univariate time series and exogenous variables.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned.\n            Default is None.\n\n    Returns:\n        Tuple containing X_train (training values/predictors with shape\n        (len(y) - max_lag, len(lags))) and y_train (target values of the\n        time series related to each row of X_train with shape (len(y) - max_lag,)).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; X_train, y_train = forecaster.create_train_X_y(y)\n        &gt;&gt;&gt; X_train.head(2)\n           lag_1  lag_2  lag_3\n        3    2.0    1.0    0.0\n        4    3.0    2.0    1.0\n        &gt;&gt;&gt; y_train.head(2)\n        3    3\n        4    4\n        Name: y, dtype: int64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.fit","title":"<code>fit(y, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.is_fitted\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef fit(self, y: pd.Series, exog: pd.Series | pd.DataFrame | None = None) -&gt; None:\n    \"\"\"Training Forecaster.\n\n    Args:\n        y: Training time series.\n        exog: Exogenous variable(s) included as predictor(s). Must have the same\n            number of observations as y and their indexes must be aligned so\n            that y[i] is regressed on exog[i]. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.is_fitted\n        True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with forecaster tags describing behavior and capabilities.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; tags = forecaster.get_tags()\n&gt;&gt;&gt; tags['forecaster_task']\n'regression'\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        Dictionary with forecaster tags describing behavior and capabilities.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; tags = forecaster.get_tags()\n        &gt;&gt;&gt; tags['forecaster_task']\n        'regression'\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>  <code>abstractmethod</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>Series | DataFrame | None</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If None, the values stored in last_window are used to calculate the initial predictors, and the predictions start right after training data. Default is None.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictor(s). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Predicted values as a pandas Series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n10    9.5\n11    9.0\n12    8.5\nName: pred, dtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; pd.Series:\n    \"\"\"Predict n steps ahead.\n\n    Args:\n        steps: Number of steps to predict.\n        last_window: Series values used to create the predictors (lags) needed in the\n            first iteration of the prediction (t + 1). If None, the values stored in\n            last_window are used to calculate the initial predictors, and the\n            predictions start right after training data. Default is None.\n        exog: Exogenous variable(s) included as predictor(s). Default is None.\n\n    Returns:\n        Predicted values as a pandas Series.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), name='y')\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        10    9.5\n        11    9.0\n        12    8.5\n        Name: pred, dtype: float64\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute lags.</p> <p>Attributes max_lag and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int | list[int] | ndarray[int] | range[int] | None</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. If int: include lags from 1 to lags (included). If list, 1d numpy ndarray, or range: include only lags present in lags, all elements must be int. If None: no lags are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.set_lags(lags=5)\n&gt;&gt;&gt; forecaster.lags\narray([1, 2, 3, 4, 5])\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_lags(\n    self, lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute lags.\n\n    Attributes max_lag and window_size are also updated.\n\n    Args:\n        lags: Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n            If int: include lags from 1 to lags (included). If list, 1d numpy ndarray,\n            or range: include only lags present in lags, all elements must be int.\n            If None: no lags are included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.set_lags(lags=5)\n        &gt;&gt;&gt; forecaster.lags\n        array([1, 2, 3, 4, 5])\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_params","title":"<code>set_params(params)</code>  <code>abstractmethod</code>","text":"<p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, object]</code> <p>Parameters values dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n&gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n&gt;&gt;&gt; forecaster.estimator.alpha\n0.5\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>@abstractmethod\ndef set_params(self, params: dict[str, object]) -&gt; None:\n    \"\"\"Set new values to the parameters of the scikit-learn model stored in the forecaster.\n\n    Args:\n        params: Parameters values dictionary.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(alpha=1.0), lags=3)\n        &gt;&gt;&gt; forecaster.set_params({'estimator__alpha': 0.5})\n        &gt;&gt;&gt; forecaster.estimator.alpha\n        0.5\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute window_features.</p> <p>Attributes max_size_window_features, window_features_names, window_features_class_names and window_size are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>object | list[object] | None</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n&gt;&gt;&gt; forecaster.window_features\n[RollingFeatures(stats=['mean'], window_sizes=[3])]\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def set_window_features(\n    self, window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"Set new value to the attribute window_features.\n\n    Attributes max_size_window_features, window_features_names,\n    window_features_class_names and window_size are also updated.\n\n    Args:\n        window_features: Instance or list of instances used to create window features.\n            Window features are created from the original time series and are\n            included as predictors. Default is None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; window_feat = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; forecaster.set_window_features(window_features=window_feat)\n        &gt;&gt;&gt; forecaster.window_features\n        [RollingFeatures(stats=['mean'], window_sizes=[3])]\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.base.ForecasterBase.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; forecaster.summary()\nForecasterRecursive\n===================\nEstimator: Ridge()\nLags: [1 2 3]\n...\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Show forecaster information.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; forecaster.summary()\n        ForecasterRecursive\n        ===================\n        Estimator: Ridge()\n        Lags: [1 2 3]\n        ...\n    \"\"\"\n\n    print(self.__repr__())\n</code></pre>"},{"location":"api/forecaster/#recursive-forecasting","title":"Recursive Forecasting","text":""},{"location":"api/forecaster/#recursive","title":"recursive","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive","title":"<code>spotforecast2_safe.forecaster.recursive</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate","title":"<code>ForecasterEquivalentDate</code>","text":"<p>This forecaster predicts future values based on the most recent equivalent date. It also allows to aggregate multiple past values of the equivalent date using a function (e.g. mean, median, max, min, etc.). The equivalent date is calculated by moving back in time a specified number of steps (offset). The offset can be defined as an integer or as a pandas DateOffset. This approach is useful as a baseline, but it is a simplistic method and may not capture complex underlying patterns.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily, <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of valid dates. For example, Bday(2) can be used to move back two business days. If the date does not start on a valid date, it is first moved to a valid date. For example, if the date is a Saturday, it is moved to the previous Friday. Then, the offset is applied. If the result is a non-valid date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday. For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> required <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. Defaults to 1. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>1</code> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1. Defaults to np.mean.</p> <code>mean</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize the residuals into k bins according to the predicted values associated with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. Defaults to None.</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period.</p> <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction.</p> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>window_size</code> <code>int</code> <p>Number of past values needed to include the last equivalent dates according to the <code>offset</code> and <code>n_offsets</code>.</p> <code>last_window_</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the past values needed to include the last equivalent date according the <code>offset</code> and <code>n_offsets</code>.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 10_000 values.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>.</p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non-training data. Only stored up to 10_000 values. Use <code>set_out_sample_residuals()</code> method to set values.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>.</p> <code>binner</code> <code>QuantileBinner</code> <p><code>QuantileBinner</code> used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the estimator has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>spotforecast_version</code> <code>str</code> <p>Version of spotforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterEquivalentDate\n&gt;&gt;&gt; # Series with daily frequency\n&gt;&gt;&gt; data = pd.Series(\n...     data = np.arange(14),\n...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n... )\n&gt;&gt;&gt; # Forecast based on the value 7 days ago\n&gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n&gt;&gt;&gt; forecaster.fit(y=data)\n&gt;&gt;&gt; forecaster.predict(steps=3)\n2022-01-15    7\n2022-01-16    8\n2022-01-17    9\nFreq: D, Name: pred, dtype: int64\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>class ForecasterEquivalentDate:\n    \"\"\"\n    This forecaster predicts future values based on the most recent equivalent\n    date. It also allows to aggregate multiple past values of the equivalent\n    date using a function (e.g. mean, median, max, min, etc.). The equivalent\n    date is calculated by moving back in time a specified number of steps (offset).\n    The offset can be defined as an integer or as a pandas DateOffset. This\n    approach is useful as a baseline, but it is a simplistic method and may not\n    capture complex underlying patterns.\n\n    Args:\n        offset (int, pandas.tseries.offsets.DateOffset): Number of steps to go back\n            in time to find the most recent equivalent date to the target period.\n            If `offset` is an integer, it represents the number of steps to go back\n            in time. For example, if the frequency of the time series is daily,\n            `offset = 7` means that the most recent data similar to the target\n            period is the value observed 7 days ago.\n            Pandas DateOffsets can also be used to move forward a given number of\n            valid dates. For example, Bday(2) can be used to move back two business\n            days. If the date does not start on a valid date, it is first moved to a\n            valid date. For example, if the date is a Saturday, it is moved to the\n            previous Friday. Then, the offset is applied. If the result is a non-valid\n            date, it is moved to the next valid date. For example, if the date\n            is a Sunday, it is moved to the next Monday.\n            For more information about offsets, see\n            https://pandas.pydata.org/docs/reference/offset_frequency.html.\n        n_offsets (int, optional): Number of equivalent dates (multiple of offset)\n            used in the prediction. Defaults to 1.\n            If `n_offsets` is greater than 1, the values at the equivalent dates are\n            aggregated using the `agg_func` function. For example, if the frequency\n            of the time series is daily, `offset = 7`, `n_offsets = 2` and\n            `agg_func = np.mean`, the predicted value will be the mean of the values\n            observed 7 and 14 days ago.\n        agg_func (Callable, optional): Function used to aggregate the values of the\n            equivalent dates when the number of equivalent dates (`n_offsets`) is\n            greater than 1. Defaults to np.mean.\n        binner_kwargs (dict, optional): Additional arguments to pass to the\n            `QuantileBinner` used to discretize the residuals into k bins according\n            to the predicted values associated with each residual. Available arguments\n            are: `n_bins`, `method`, `subsample`, `random_state` and `dtype`.\n            Argument `method` is passed internally to the function `numpy.percentile`.\n            Defaults to None.\n        forecaster_id (str, int, optional): Name used as an identifier of the\n            forecaster. Defaults to None.\n\n    Attributes:\n        offset (int, pandas.tseries.offsets.DateOffset): Number of steps to go back\n            in time to find the most recent equivalent date to the target period.\n        n_offsets (int): Number of equivalent dates (multiple of offset) used in\n            the prediction.\n        agg_func (Callable): Function used to aggregate the values of the equivalent\n            dates when the number of equivalent dates (`n_offsets`) is greater than 1.\n        window_size (int): Number of past values needed to include the last\n            equivalent dates according to the `offset` and `n_offsets`.\n        last_window_ (pandas Series): This window represents the most recent data\n            observed by the predictor during its training phase. It contains the\n            past values needed to include the last equivalent date according the\n            `offset` and `n_offsets`.\n        index_type_ (type): Type of index of the input used in training.\n        index_freq_ (str): Frequency of Index of the input used in training.\n        training_range_ (pandas Index): First and last values of index of the data\n            used during training.\n        series_name_in_ (str): Names of the series provided by the user during training.\n        in_sample_residuals_ (numpy ndarray): Residuals of the model when predicting\n            training data. Only stored up to 10_000 values.\n        in_sample_residuals_by_bin_ (dict): In sample residuals binned according to\n            the predicted value each residual is associated with. The number of\n            residuals stored per bin is limited to `10_000 // self.binner.n_bins_`\n            in the form `{bin: residuals}`.\n        out_sample_residuals_ (numpy ndarray): Residuals of the model when predicting\n            non-training data. Only stored up to 10_000 values. Use\n            `set_out_sample_residuals()` method to set values.\n        out_sample_residuals_by_bin_ (dict): Out of sample residuals binned\n            according to the predicted value each residual is associated with.\n            The number of residuals stored per bin is limited to\n            `10_000 // self.binner.n_bins_` in the form `{bin: residuals}`.\n        binner (spotforecast.preprocessing.QuantileBinner): `QuantileBinner` used to\n            discretize residuals into k bins according to the predicted values\n            associated with each residual.\n        binner_intervals_ (dict): Intervals used to discretize residuals into k bins\n            according to the predicted values associated with each residual.\n        binner_kwargs (dict): Additional arguments to pass to the `QuantileBinner`.\n        creation_date (str): Date of creation.\n        is_fitted (bool): Tag to identify if the estimator has been fitted (trained).\n        fit_date (str): Date of last fit.\n        spotforecast_version (str): Version of spotforecast library used to create\n            the forecaster.\n        python_version (str): Version of python used to create the forecaster.\n        forecaster_id (str, int): Name used as an identifier of the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterEquivalentDate\n        &gt;&gt;&gt; # Series with daily frequency\n        &gt;&gt;&gt; data = pd.Series(\n        ...     data = np.arange(14),\n        ...     index = pd.date_range(start='2022-01-01', periods=14, freq='D')\n        ... )\n        &gt;&gt;&gt; # Forecast based on the value 7 days ago\n        &gt;&gt;&gt; forecaster = ForecasterEquivalentDate(offset=7)\n        &gt;&gt;&gt; forecaster.fit(y=data)\n        &gt;&gt;&gt; forecaster.predict(steps=3)\n        2022-01-15    7\n        2022-01-16    8\n        2022-01-17    9\n        Freq: D, Name: pred, dtype: int64\n    \"\"\"\n\n    def __init__(\n        self,\n        offset: int | pd.tseries.offsets.DateOffset,\n        n_offsets: int = 1,\n        agg_func: Callable = np.mean,\n        binner_kwargs: dict[str, object] | None = None,\n        forecaster_id: str | int | None = None,\n    ) -&gt; None:\n\n        self.offset = offset\n        self.n_offsets = n_offsets\n        self.agg_func = agg_func\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        self.spotforecast_version = __version__\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n        self.estimator = None\n        self.differentiation = None\n        self.differentiation_max = None\n        self.window_size = None  # Defaults to None, validated later\n\n        if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n            raise TypeError(\n                \"`offset` must be an integer greater than 0 or a \"\n                \"pandas.tseries.offsets. Find more information about offsets in \"\n                \"https://pandas.pydata.org/docs/reference/offset_frequency.html\"\n            )\n\n        if isinstance(self.offset, int):\n            self.window_size = self.offset * self.n_offsets\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n                \"subsample\": 200000,\n                \"random_state\": 789654,\n                \"dtype\": np.float64,\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterEquivalentDate\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": False,\n            \"allowed_input_types_exog\": [],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": False,\n            \"supports_lags\": False,\n            \"supports_window_features\": False,\n            \"supports_transformer_series\": False,\n            \"supports_transformer_exog\": False,\n            \"supports_weight_func\": False,\n            \"supports_differentiation\": False,\n            \"prediction_types\": [\"point\", \"interval\"],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a Forecaster object is printed.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Offset: {self.offset} \\n\"\n            f\"Number of offsets: {self.n_offsets} \\n\"\n            f\"Aggregation function: {self.agg_func.__name__} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"spotforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Offset:&lt;/strong&gt; {self.offset}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Number of offsets:&lt;/strong&gt; {self.n_offsets}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Aggregation function:&lt;/strong&gt; {self.agg_func.__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def fit(\n        self,\n        y: pd.Series,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        exog: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Training Forecaster.\n\n        Args:\n            y (pandas Series): Training time series.\n            store_in_sample_residuals (bool, optional): If `True`, in-sample\n                residuals will be stored in the forecaster object after fitting\n                (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n                If `False`, only the intervals of the bins are stored. Defaults to False.\n            random_state (int, optional): Set a seed for the random generator so\n                that the stored sample residuals are always deterministic. Defaults to 123.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            None\n        \"\"\"\n\n        if not isinstance(y, pd.Series):\n            raise TypeError(\n                f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n                f\"Found {type(y)}.\"\n            )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            if not isinstance(y.index, pd.DatetimeIndex):\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n            elif y.index.freq is None:\n                try:\n                    y.index.freq = pd.infer_freq(y.index)\n                except (ValueError, TypeError):\n                    raise TypeError(\n                        \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                        \"pandas DatetimeIndex with frequency.\"\n                    )\n                if y.index.freq is None:\n                    raise TypeError(\n                        \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                        \"pandas DatetimeIndex with frequency.\"\n                    )\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.is_fitted = False\n\n        _, y_index = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            # Calculate the window_size in steps for compatibility with the\n            # check_predict_input function. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance\n            # between two dates may not be constant.\n            first_valid_index = y_index[-1] - self.offset * self.n_offsets\n\n            try:\n                window_size_idx_start = y_index.get_loc(first_valid_index)\n                window_size_idx_end = y_index.get_loc(y_index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). This is because  \"\n                    f\"the offset ({self.offset}) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `y`.\"\n                )\n        else:\n            if len(y) &lt;= self.window_size:\n                raise ValueError(\n                    f\"Length of `y` must be greater than the maximum window size \"\n                    f\"needed by the forecaster. This is because  \"\n                    f\"the offset ({self.offset}) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `y`.\\n\"\n                    f\"    Length `y`: {len(y)}.\\n\"\n                    f\"    Max window size: {self.window_size}.\\n\"\n                )\n\n        self.is_fitted = True\n        self.series_name_in_ = y.name if y.name is not None else \"y\"\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.training_range_ = y_index[[0, -1]]\n        self.index_type_ = type(y_index)\n        self.index_freq_ = (\n            y_index.freq if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n        )\n\n        # NOTE: This is done to save time during fit in functions such as backtesting()\n        if self._probabilistic_mode is not False:\n            self._binning_in_sample_residuals(\n                y=y,\n                store_in_sample_residuals=store_in_sample_residuals,\n                random_state=random_state,\n            )\n\n        # The last time window of training data is stored so that equivalent\n        # dates are available when calling the `predict` method.\n        # Store the whole series to avoid errors when the offset is larger\n        # than the data available.\n        self.last_window_ = y.copy()\n\n    def _binning_in_sample_residuals(\n        self,\n        y: pd.Series,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Bin residuals according to the predicted value each residual is\n        associated with. First a `spotforecast.preprocessing.QuantileBinner` object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n\n        The number of residuals stored per bin is limited to\n        `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n        `10_000`.\n\n        Args:\n            y (pandas Series): Training time series.\n            store_in_sample_residuals (bool, optional): If `True`, in-sample\n                residuals will be stored in the forecaster object after fitting\n                (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n                If `False`, only the intervals of the bins are stored. Defaults to False.\n            random_state (int, optional): Set a seed for the random generator so\n                that the stored sample residuals are always deterministic. Defaults to 123.\n\n        Returns:\n            None\n        \"\"\"\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            y_preds = []\n            for n_off in range(1, self.n_offsets + 1):\n                idx = y.index - self.offset * n_off\n                mask = idx &gt;= y.index[0]\n                y_pred = y.loc[idx[mask]]\n                y_pred.index = y.index[-mask.sum() :]\n                y_preds.append(y_pred)\n\n            y_preds = pd.concat(y_preds, axis=1).to_numpy()\n            y_true = y.to_numpy()[-len(y_preds) :]\n\n        else:\n            y_preds = [\n                y.shift(self.offset * n_off)[self.window_size :]\n                for n_off in range(1, self.n_offsets + 1)\n            ]\n            y_preds = np.column_stack(y_preds)\n            y_true = y.to_numpy()[self.window_size :]\n\n        y_pred = np.apply_along_axis(self.agg_func, axis=1, arr=y_preds)\n\n        residuals = y_true - y_pred\n\n        if self._probabilistic_mode == \"binned\":\n            data = pd.DataFrame({\"prediction\": y_pred, \"residuals\": residuals}).dropna()\n            y_pred = data[\"prediction\"].to_numpy()\n            residuals = data[\"residuals\"].to_numpy()\n\n            self.binner.fit(y_pred)\n            self.binner_intervals_ = self.binner.intervals_\n\n        if store_in_sample_residuals:\n            rng = np.random.default_rng(seed=random_state)\n            if self._probabilistic_mode == \"binned\":\n                data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n                self.in_sample_residuals_by_bin_ = (\n                    data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n                )\n\n                max_sample = 10_000 // self.binner.n_bins_\n                for k, v in self.in_sample_residuals_by_bin_.items():\n                    if len(v) &gt; max_sample:\n                        sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                        self.in_sample_residuals_by_bin_[k] = sample\n\n                for k in self.binner_intervals_.keys():\n                    if k not in self.in_sample_residuals_by_bin_:\n                        self.in_sample_residuals_by_bin_[k] = np.array([])\n\n                empty_bins = [\n                    k\n                    for k, v in self.in_sample_residuals_by_bin_.items()\n                    if v.size == 0\n                ]\n                if empty_bins:\n                    empty_bin_size = min(max_sample, len(residuals))\n                    for k in empty_bins:\n                        self.in_sample_residuals_by_bin_[k] = rng.choice(\n                            a=residuals, size=empty_bin_size, replace=False\n                        )\n\n            if len(residuals) &gt; 10_000:\n                residuals = residuals[\n                    rng.integers(low=0, high=len(residuals), size=10_000)\n                ]\n\n            self.in_sample_residuals_ = residuals\n\n    def predict(\n        self,\n        steps: int,\n        last_window: pd.Series | None = None,\n        check_inputs: bool = True,\n        exog: Any = None,\n    ) -&gt; pd.Series:\n        \"\"\"\n        Predict n steps ahead.\n\n        Args:\n            steps (int): Number of steps to predict.\n            last_window (pandas Series, optional): Past values needed to select the\n                last equivalent dates according to the offset. If `last_window = None`,\n                the values stored in `self.last_window_` are used and the predictions\n                start immediately after the training data. Defaults to None.\n            check_inputs (bool, optional): If `True`, the input is checked for\n                possible warnings and errors with the `check_predict_input` function.\n                This argument is created for internal use and is not recommended to\n                be changed. Defaults to True.\n            exog (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            pd.Series: Predicted values.\n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=False,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n            )\n\n        prediction_index = expand_index(index=last_window.index, steps=steps)\n\n        if isinstance(self.offset, int):\n\n            last_window_values = last_window.to_numpy(copy=True).ravel()\n            equivalent_indexes = np.tile(\n                np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n            )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets &gt; 1:\n                equivalent_indexes = [\n                    equivalent_indexes - n * self.offset\n                    for n in np.arange(self.n_offsets)\n                ]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                    self.agg_func, axis=0, arr=equivalent_values\n                )\n\n            predictions = pd.Series(\n                data=predictions, index=prediction_index, name=\"pred\"\n            )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            last_window = last_window.copy()\n            max_allowed_date = last_window.index[-1]\n\n            # For every date in prediction_index, calculate the n offsets\n            offset_dates = []\n            for date in prediction_index:\n                selected_offsets = []\n                while len(selected_offsets) &lt; self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date &lt;= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n\n            offset_dates = np.array(offset_dates)\n\n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                data=equivalent_values,\n                index=prediction_index,\n                columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n            )\n\n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    f\"All equivalent values are missing. This is caused by using \"\n                    f\"an offset ({self.offset}) larger than the available data. \"\n                    f\"Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `last_window`. In backtesting, this error may be \"\n                    f\"caused by using an `initial_train_size` too small.\"\n                )\n\n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                    f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                    f\"To avoid this, increase the `last_window` size or decrease \"\n                    f\"the number of `n_offsets`. The current configuration requires \"\n                    f\"a total offset of {self.offset * self.n_offsets}.\",\n                    MissingValuesWarning,\n                )\n\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.rename(\"pred\")\n\n        return predictions\n\n    def predict_interval(\n        self,\n        steps: int,\n        last_window: pd.Series | None = None,\n        method: str = \"conformal\",\n        interval: float | list[float] | tuple[float] = [5, 95],\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = True,\n        random_state: Any = None,\n        exog: Any = None,\n        n_boot: Any = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead and estimate prediction intervals using conformal\n        prediction method. Refer to the References section for additional\n        details on this method.\n\n        Args:\n            steps (int): Number of steps to predict.\n            last_window (pandas Series, optional): Past values needed to select the\n                last equivalent dates according to the offset. If `last_window = None`,\n                the values stored in `self.last_window_` are used and the predictions\n                start immediately after the training data. Defaults to None.\n            method (str, optional): Technique used to estimate prediction intervals.\n                Available options:\n                - 'conformal': Employs the conformal prediction split method for\n                interval estimation [1]_. Defaults to 'conformal'.\n            interval (float, list, tuple, optional): Confidence level of the\n                prediction interval. Interpretation depends on the method used:\n                - If `float`, represents the nominal (expected) coverage (between 0\n                and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n                percentiles.\n                - If `list` or `tuple`, defines the exact percentiles to compute,\n                which must be between 0 and 100 inclusive. For example, interval\n                of 95% should be as `interval = [2.5, 97.5]`.\n                - When using `method='conformal'`, the interval must be a float or\n                a list/tuple defining a symmetric interval. Defaults to [5, 95].\n            use_in_sample_residuals (bool, optional): If `True`, residuals from the\n                training data are used as proxy of prediction error to create predictions.\n                If `False`, out of sample residuals (calibration) are used.\n                Out-of-sample residuals must be precomputed using Forecaster's\n                `set_out_sample_residuals()` method. Defaults to True.\n            use_binned_residuals (bool, optional): If `True`, residuals are selected\n                based on the predicted values (binned selection).\n                If `False`, residuals are selected randomly. Defaults to True.\n            random_state (Ignored): Not used, present here for API consistency by convention.\n            exog (Ignored): Not used, present here for API consistency by convention.\n            n_boot (Ignored): Not used, present here for API consistency by convention.\n\n        Returns:\n            pd.DataFrame: Values predicted by the forecaster and their estimated interval.\n                - pred: predictions.\n                - lower_bound: lower bound of the interval.\n                - upper_bound: upper bound of the interval.\n\n        References:\n            .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n                https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n        \"\"\"\n\n        if method != \"conformal\":\n            raise ValueError(\n                f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n            )\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=False,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n        )\n\n        check_residuals_input(\n            forecaster_name=type(self).__name__,\n            use_in_sample_residuals=use_in_sample_residuals,\n            in_sample_residuals_=self.in_sample_residuals_,\n            out_sample_residuals_=self.out_sample_residuals_,\n            use_binned_residuals=use_binned_residuals,\n            in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n            out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n        )\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal=\"interval\")\n            nominal_coverage = interval\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        prediction_index = expand_index(index=last_window.index, steps=steps)\n\n        if isinstance(self.offset, int):\n\n            last_window_values = last_window.to_numpy(copy=True).ravel()\n            equivalent_indexes = np.tile(\n                np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n            )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets &gt; 1:\n                equivalent_indexes = [\n                    equivalent_indexes - n * self.offset\n                    for n in np.arange(self.n_offsets)\n                ]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                    self.agg_func, axis=0, arr=equivalent_values\n                )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            last_window = last_window.copy()\n            max_allowed_date = last_window.index[-1]\n\n            # For every date in prediction_index, calculate the n offsets\n            offset_dates = []\n            for date in prediction_index:\n                selected_offsets = []\n                while len(selected_offsets) &lt; self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date &lt;= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n\n            offset_dates = np.array(offset_dates)\n\n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                data=equivalent_values,\n                index=prediction_index,\n                columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n            )\n\n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    f\"All equivalent values are missing. This is caused by using \"\n                    f\"an offset ({self.offset}) larger than the available data. \"\n                    f\"Try to decrease the size of the offset ({self.offset}), \"\n                    f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                    f\"size of `last_window`. In backtesting, this error may be \"\n                    f\"caused by using an `initial_train_size` too small.\"\n                )\n\n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                    f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                    f\"To avoid this, increase the `last_window` size or decrease \"\n                    f\"the number of `n_offsets`. The current configuration requires \"\n                    f\"a total offset of {self.offset * self.n_offsets}.\",\n                    MissingValuesWarning,\n                )\n\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.to_numpy()\n\n        if use_binned_residuals:\n            correction_factor_by_bin = {\n                k: np.quantile(np.abs(v), nominal_coverage)\n                for k, v in residuals_by_bin.items()\n            }\n            replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n            predictions_bin = self.binner.transform(predictions)\n            correction_factor = replace_func(predictions_bin)\n        else:\n            correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n        lower_bound = predictions - correction_factor\n        upper_bound = predictions + correction_factor\n        predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n        predictions = pd.DataFrame(\n            data=predictions,\n            index=prediction_index,\n            columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n        )\n\n        return predictions\n\n    def set_in_sample_residuals(\n        self, y: pd.Series, random_state: int = 123, exog: Any = None\n    ) -&gt; None:\n        \"\"\"\n        Set in-sample residuals in case they were not calculated during the\n        training process.\n\n        In-sample residuals are calculated as the difference between the true\n        values and the predictions made by the forecaster using the training\n        data. The following internal attributes are updated:\n\n        + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `binner_intervals_`: intervals used to bin the residuals are calculated\n        using the quantiles of the predicted values.\n        + `in_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the intervals of the predicted values and the values are\n        the residuals associated with that range.\n\n        A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        random_state : int, default 123\n            Sets a seed to the random sampling for reproducible output.\n        exog : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_in_sample_residuals()`.\"\n            )\n\n        check_y(y=y)\n        y_index_range = check_extract_values_and_index(\n            data=y, data_label=\"`y`\", return_values=False\n        )[1][[0, -1]]\n        if not y_index_range.equals(self.training_range_):\n            raise IndexError(\n                f\"The index range of `y` does not match the range \"\n                f\"used during training. Please ensure the index is aligned \"\n                f\"with the training data.\\n\"\n                f\"    Expected : {self.training_range_}\\n\"\n                f\"    Received : {y_index_range}\"\n            )\n\n        self._binning_in_sample_residuals(\n            y=y, store_in_sample_residuals=True, random_state=random_state\n        )\n\n    def set_out_sample_residuals(\n        self,\n        y_true: np.ndarray | pd.Series,\n        y_pred: np.ndarray | pd.Series,\n        append: bool = False,\n        random_state: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n\n        Parameters\n        ----------\n        y_true : numpy ndarray, pandas Series\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : numpy ndarray, pandas Series\n            Predicted values of the time series.\n        append : bool, default False\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default 123\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n\n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        data = pd.DataFrame(\n            {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n        ).dropna()\n        y_pred = data[\"prediction\"].to_numpy()\n        residuals = data[\"residuals\"].to_numpy()\n\n        data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n\n        out_sample_residuals = (\n            np.array([])\n            if self.out_sample_residuals_ is None\n            else self.out_sample_residuals_\n        )\n        out_sample_residuals_by_bin = (\n            {}\n            if self.out_sample_residuals_by_bin_ is None\n            else self.out_sample_residuals_by_bin_\n        )\n        if append:\n            out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n            for k, v in residuals_by_bin.items():\n                if k in out_sample_residuals_by_bin:\n                    out_sample_residuals_by_bin[k] = np.concatenate(\n                        (out_sample_residuals_by_bin[k], v)\n                    )\n                else:\n                    out_sample_residuals_by_bin[k] = v\n        else:\n            out_sample_residuals = residuals\n            out_sample_residuals_by_bin = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in out_sample_residuals_by_bin.items():\n            if len(v) &gt; max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                out_sample_residuals_by_bin[k] = sample\n\n        bin_keys = (\n            [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n        )\n        for k in bin_keys:\n            if k not in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.array([])\n\n        empty_bins = [k for k, v in out_sample_residuals_by_bin.items() if v.size == 0]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\",\n                ResidualsUsageWarning,\n            )\n            empty_bin_size = min(max_samples, len(out_sample_residuals))\n            for k in empty_bins:\n                out_sample_residuals_by_bin[k] = rng.choice(\n                    a=out_sample_residuals, size=empty_bin_size, replace=False\n                )\n\n        if len(out_sample_residuals) &gt; 10_000:\n            out_sample_residuals = rng.choice(\n                a=out_sample_residuals, size=10_000, replace=False\n            )\n\n        self.out_sample_residuals_ = out_sample_residuals\n        self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n\n    def get_tags(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return the tags that characterize the behavior of the forecaster.\n\n        Returns:\n            dict: Dictionary with forecaster tags.\n        \"\"\"\n\n        return self.__spotforecast_tags__\n\n    def summary(self) -&gt; None:\n        \"\"\"\n        Show forecaster information.\n\n        Returns:\n            None\n        \"\"\"\n\n        print(self)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a Forecaster object is printed.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a Forecaster object is printed.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Offset: {self.offset} \\n\"\n        f\"Number of offsets: {self.n_offsets} \\n\"\n        f\"Aggregation function: {self.agg_func.__name__} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"spotforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.fit","title":"<code>fit(y, store_in_sample_residuals=False, random_state=123, exog=None)</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample residuals are always deterministic. Defaults to 123.</p> <code>123</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    exog: Any = None,\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Args:\n        y (pandas Series): Training time series.\n        store_in_sample_residuals (bool, optional): If `True`, in-sample\n            residuals will be stored in the forecaster object after fitting\n            (`in_sample_residuals_` and `in_sample_residuals_by_bin_` attributes).\n            If `False`, only the intervals of the bins are stored. Defaults to False.\n        random_state (int, optional): Set a seed for the random generator so\n            that the stored sample residuals are always deterministic. Defaults to 123.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        None\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        if not isinstance(y.index, pd.DatetimeIndex):\n            raise TypeError(\n                \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                \"pandas DatetimeIndex with frequency.\"\n            )\n        elif y.index.freq is None:\n            try:\n                y.index.freq = pd.infer_freq(y.index)\n            except (ValueError, TypeError):\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n            if y.index.freq is None:\n                raise TypeError(\n                    \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                    \"pandas DatetimeIndex with frequency.\"\n                )\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_ = None\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.training_range_ = None\n    self.series_name_in_ = None\n    self.is_fitted = False\n\n    _, y_index = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        # Calculate the window_size in steps for compatibility with the\n        # check_predict_input function. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance\n        # between two dates may not be constant.\n        first_valid_index = y_index[-1] - self.offset * self.n_offsets\n\n        try:\n            window_size_idx_start = y_index.get_loc(first_valid_index)\n            window_size_idx_end = y_index.get_loc(y_index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\"\n            )\n    else:\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster. This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n            )\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else \"y\"\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    self.training_range_ = y_index[[0, -1]]\n    self.index_type_ = type(y_index)\n    self.index_freq_ = (\n        y_index.freq if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n    )\n\n    # NOTE: This is done to save time during fit in functions such as backtesting()\n    if self._probabilistic_mode is not False:\n        self._binning_in_sample_residuals(\n            y=y,\n            store_in_sample_residuals=store_in_sample_residuals,\n            random_state=random_state,\n        )\n\n    # The last time window of training data is stored so that equivalent\n    # dates are available when calling the `predict` method.\n    # Store the whole series to avoid errors when the offset is larger\n    # than the data available.\n    self.last_window_ = y.copy()\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.get_tags","title":"<code>get_tags()</code>","text":"<p>Return the tags that characterize the behavior of the forecaster.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Dictionary with forecaster tags.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def get_tags(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the tags that characterize the behavior of the forecaster.\n\n    Returns:\n        dict: Dictionary with forecaster tags.\n    \"\"\"\n\n    return self.__spotforecast_tags__\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.predict","title":"<code>predict(steps, last_window=None, check_inputs=True, exog=None)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to the offset. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used and the predictions start immediately after the training data. Defaults to None.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors with the <code>check_predict_input</code> function. This argument is created for internal use and is not recommended to be changed. Defaults to True.</p> <code>True</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Predicted values.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    check_inputs: bool = True,\n    exog: Any = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Args:\n        steps (int): Number of steps to predict.\n        last_window (pandas Series, optional): Past values needed to select the\n            last equivalent dates according to the offset. If `last_window = None`,\n            the values stored in `self.last_window_` are used and the predictions\n            start immediately after the training data. Defaults to None.\n        check_inputs (bool, optional): If `True`, the input is checked for\n            possible warnings and errors with the `check_predict_input` function.\n            This argument is created for internal use and is not recommended to\n            be changed. Defaults to True.\n        exog (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        pd.Series: Predicted values.\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=False,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n        )\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n            np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n        )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset\n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                self.agg_func, axis=0, arr=equivalent_values\n            )\n\n        predictions = pd.Series(\n            data=predictions, index=prediction_index, name=\"pred\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n            data=equivalent_values,\n            index=prediction_index,\n            columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n        )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \"\n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning,\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.rename(\"pred\")\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.predict_interval","title":"<code>predict_interval(steps, last_window=None, method='conformal', interval=[5, 95], use_in_sample_residuals=True, use_binned_residuals=True, random_state=None, exog=None, n_boot=None)</code>","text":"<p>Predict n steps ahead and estimate prediction intervals using conformal prediction method. Refer to the References section for additional details on this method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to the offset. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used and the predictions start immediately after the training data. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options: - 'conformal': Employs the conformal prediction split method for interval estimation [1]_. Defaults to 'conformal'.</p> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends on the method used: - If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles. - If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>. - When using <code>method='conformal'</code>, the interval must be a float or a list/tuple defining a symmetric interval. Defaults to [5, 95].</p> <code>[5, 95]</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>n_boot</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Values predicted by the forecaster and their estimated interval. - pred: predictions. - lower_bound: lower bound of the interval. - upper_bound: upper bound of the interval.</p> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.     https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    method: str = \"conformal\",\n    interval: float | list[float] | tuple[float] = [5, 95],\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: Any = None,\n    exog: Any = None,\n    n_boot: Any = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using conformal\n    prediction method. Refer to the References section for additional\n    details on this method.\n\n    Args:\n        steps (int): Number of steps to predict.\n        last_window (pandas Series, optional): Past values needed to select the\n            last equivalent dates according to the offset. If `last_window = None`,\n            the values stored in `self.last_window_` are used and the predictions\n            start immediately after the training data. Defaults to None.\n        method (str, optional): Technique used to estimate prediction intervals.\n            Available options:\n            - 'conformal': Employs the conformal prediction split method for\n            interval estimation [1]_. Defaults to 'conformal'.\n        interval (float, list, tuple, optional): Confidence level of the\n            prediction interval. Interpretation depends on the method used:\n            - If `float`, represents the nominal (expected) coverage (between 0\n            and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]`\n            percentiles.\n            - If `list` or `tuple`, defines the exact percentiles to compute,\n            which must be between 0 and 100 inclusive. For example, interval\n            of 95% should be as `interval = [2.5, 97.5]`.\n            - When using `method='conformal'`, the interval must be a float or\n            a list/tuple defining a symmetric interval. Defaults to [5, 95].\n        use_in_sample_residuals (bool, optional): If `True`, residuals from the\n            training data are used as proxy of prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals (bool, optional): If `True`, residuals are selected\n            based on the predicted values (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state (Ignored): Not used, present here for API consistency by convention.\n        exog (Ignored): Not used, present here for API consistency by convention.\n        n_boot (Ignored): Not used, present here for API consistency by convention.\n\n    Returns:\n        pd.DataFrame: Values predicted by the forecaster and their estimated interval.\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    References:\n        .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n            https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n    \"\"\"\n\n    if method != \"conformal\":\n        raise ValueError(\n            f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n        )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    check_predict_input(\n        forecaster_name=type(self).__name__,\n        steps=steps,\n        is_fitted=self.is_fitted,\n        exog_in_=False,\n        index_type_=self.index_type_,\n        index_freq_=self.index_freq_,\n        window_size=self.window_size,\n        last_window=last_window,\n    )\n\n    check_residuals_input(\n        forecaster_name=type(self).__name__,\n        use_in_sample_residuals=use_in_sample_residuals,\n        in_sample_residuals_=self.in_sample_residuals_,\n        out_sample_residuals_=self.out_sample_residuals_,\n        use_binned_residuals=use_binned_residuals,\n        in_sample_residuals_by_bin_=self.in_sample_residuals_by_bin_,\n        out_sample_residuals_by_bin_=self.out_sample_residuals_by_bin_,\n    )\n\n    if isinstance(interval, (list, tuple)):\n        check_interval(interval=interval, ensure_symmetric_intervals=True)\n        nominal_coverage = (interval[1] - interval[0]) / 100\n    else:\n        check_interval(alpha=interval, alpha_literal=\"interval\")\n        nominal_coverage = interval\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n            np.arange(-self.offset, 0), int(np.ceil(steps / self.offset))\n        )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset\n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                self.agg_func, axis=0, arr=equivalent_values\n            )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n            data=equivalent_values,\n            index=prediction_index,\n            columns=[f\"offset_{i}\" for i in range(self.n_offsets)],\n        )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \"\n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning,\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.to_numpy()\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner.transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    predictions = pd.DataFrame(\n        data=predictions,\n        index=prediction_index,\n        columns=[\"pred\", \"lower_bound\", \"upper_bound\"],\n    )\n\n    return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals","title":"<code>set_in_sample_residuals(y, random_state=123, exog=None)</code>","text":"<p>Set in-sample residuals in case they were not calculated during the training process.</p> <p>In-sample residuals are calculated as the difference between the true values and the predictions made by the forecaster using the training data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals--parameters","title":"Parameters","text":"<p>y : pandas Series     Training time series. random_state : int, default 123     Sets a seed to the random sampling for reproducible output. exog : Ignored     Not used, present here for API consistency by convention.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_in_sample_residuals--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def set_in_sample_residuals(\n    self, y: pd.Series, random_state: int = 123, exog: Any = None\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process.\n\n    In-sample residuals are calculated as the difference between the true\n    values and the predictions made by the forecaster using the training\n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range.\n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label=\"`y`\", return_values=False\n    )[1][[0, -1]]\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y=y, store_in_sample_residuals=True, random_state=random_state\n    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals","title":"<code>set_out_sample_residuals(y_true, y_pred, append=False, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Two internal attributes are updated:</p> <ul> <li><code>out_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>out_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the  intervals of the predicted values and the values are the residuals associated with that range. If a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals--parameters","title":"Parameters","text":"<p>y_true : numpy ndarray, pandas Series     True values of the time series from which the residuals have been     calculated. y_pred : numpy ndarray, pandas Series     Predicted values of the time series. append : bool, default False     If <code>True</code>, new residuals are added to the once already stored in the     forecaster. If after appending the new residuals, the limit of     <code>10_000 // self.binner.n_bins_</code> values per bin is reached, a random     sample of residuals is stored. random_state : int, default 123     Sets a seed to the random sampling for reproducible output.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.set_out_sample_residuals--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123,\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. Two internal attributes are updated:\n\n    + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `out_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the  intervals of the predicted values and the values are\n    the residuals associated with that range. If a bin binning is empty, it\n    is filled with a random sample of residuals from other bins. This is done\n    to ensure that all bins have at least one residual and can be used in the\n    prediction process.\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y_true : numpy ndarray, pandas Series\n        True values of the time series from which the residuals have been\n        calculated.\n    y_pred : numpy ndarray, pandas Series\n        Predicted values of the time series.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        forecaster. If after appending the new residuals, the limit of\n        `10_000 // self.binner.n_bins_` values per bin is reached, a random\n        sample of residuals is stored.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\"`y_true` and `y_pred` must have the same index.\")\n\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    data = pd.DataFrame(\n        {\"prediction\": y_pred, \"residuals\": y_true - y_pred}\n    ).dropna()\n    y_pred = data[\"prediction\"].to_numpy()\n    residuals = data[\"residuals\"].to_numpy()\n\n    data[\"bin\"] = self.binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby(\"bin\")[\"residuals\"].apply(np.array).to_dict()\n\n    out_sample_residuals = (\n        np.array([])\n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {}\n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    bin_keys = (\n        [] if self.binner_intervals_ is None else self.binner_intervals_.keys()\n    )\n    for k in bin_keys:\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [k for k, v in out_sample_residuals_by_bin.items() if v.size == 0]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins have no out of sample residuals: {empty_bins}. \"\n            f\"No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\",\n            ResidualsUsageWarning,\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a=out_sample_residuals, size=empty_bin_size, replace=False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a=out_sample_residuals, size=10_000, replace=False\n        )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterEquivalentDate.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_equivalent_date.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Show forecaster information.\n\n    Returns:\n        None\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive","title":"<code>ForecasterRecursive</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>Recursive autoregressive forecaster for scikit-learn compatible estimators.</p> <p>This class turns any estimator compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. The forecaster learns to predict future values by using lagged values of the target variable and optional exogenous features. Predictions are made iteratively, where each step uses previous predictions as input for the next step (recursive strategy).</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object</code> <p>Scikit-learn compatible estimator for regression. If None, a default estimator will be initialized. Can also be passed via regressor parameter.</p> <code>None</code> <code>lags</code> <code>Union[int, List[int], ndarray, range, None]</code> <p>Lagged values of the target variable to use as predictors. Can be an integer (uses lags from 1 to lags), list of integers, numpy array, or range. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>window_features</code> <code>Union[object, List[object], None]</code> <p>List of window feature objects to compute features from the target variable. Each object must implement transform_batch() method. At least one of lags or window_features must be provided. Defaults to None.</p> <code>None</code> <code>transformer_y</code> <code>Optional[object]</code> <p>Transformer object for the target variable. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>transformer_exog</code> <code>Optional[object]</code> <p>Transformer object for exogenous variables. Must implement fit() and transform() methods. Applied before training and predictions. Defaults to None.</p> <code>None</code> <code>weight_func</code> <code>Optional[Callable]</code> <p>Function to compute sample weights for training. Must accept an index and return an array of weights. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>Optional[int]</code> <p>Order of differencing to apply to the target variable. Must be a positive integer. Differencing is applied before creating lags. Defaults to None.</p> <code>None</code> <code>fit_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of additional keyword arguments to pass to the estimator's fit() method. Defaults to None.</p> <code>None</code> <code>binner_kwargs</code> <code>Optional[Dict[str, object]]</code> <p>Dictionary of keyword arguments for QuantileBinner used in probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.</p> <code>None</code> <code>forecaster_id</code> <code>Union[str, int, None]</code> <p>Identifier for the forecaster instance. Can be a string or integer. Used for tracking and logging purposes. Defaults to None.</p> <code>None</code> <code>regressor</code> <code>object</code> <p>Alternative parameter name for estimator. If provided, used instead of estimator. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator</code> <p>Fitted scikit-learn estimator.</p> <code>lags</code> <p>Lag indices used in the model.</p> <code>lags_names</code> <p>Names of lag features (e.g., ['lag_1', 'lag_2']).</p> <code>window_features</code> <p>List of window feature transformers.</p> <code>window_features_names</code> <p>Names of window features.</p> <code>window_size</code> <p>Maximum window size needed (max of lags and window features).</p> <code>transformer_y</code> <p>Transformer for target variable.</p> <code>transformer_exog</code> <p>Transformer for exogenous variables.</p> <code>weight_func</code> <p>Function for sample weighting.</p> <code>differentiation</code> <p>Order of differencing applied.</p> <code>differentiator</code> <p>TimeSeriesDifferentiator instance if differencing is used.</p> <code>is_fitted</code> <p>Boolean indicating if forecaster has been fitted.</p> <code>fit_date</code> <p>Timestamp of the last fit operation.</p> <code>last_window_</code> <p>Last window_size observations from training data.</p> <code>index_type_</code> <p>Type of index in training data (RangeIndex or DatetimeIndex).</p> <code>index_freq_</code> <p>Frequency of DatetimeIndex if applicable.</p> <code>training_range_</code> <p>First and last index values of training data.</p> <code>series_name_in_</code> <p>Name of the target series.</p> <code>exog_in_</code> <p>Boolean indicating if exogenous variables were used in training.</p> <code>exog_names_in_</code> <p>Names of exogenous variables.</p> <code>exog_type_in_</code> <p>Type of exogenous input (Series or DataFrame).</p> <code>X_train_features_names_out_</code> <p>Names of all training features.</p> <code>in_sample_residuals_</code> <p>Residuals from training set.</p> <code>in_sample_residuals_by_bin_</code> <p>Residuals grouped by bins for probabilistic pred.</p> <code>forecaster_id</code> <p>Identifier for the forecaster instance.</p> Note <ul> <li>Either lags or window_features (or both) must be provided during initialization.</li> <li>The forecaster uses a recursive strategy where each multi-step prediction   depends on previous predictions within the same forecast horizon.</li> <li>Exogenous variables must have the same index as the target variable and must   be available for the entire prediction horizon.</li> <li>The forecaster supports point predictions, prediction intervals, bootstrapping,   quantile predictions, and probabilistic forecasts via conformal methods.</li> </ul> <p>Examples:</p> <p>Create a basic forecaster with lags:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=LinearRegression(),\n...     lags=10\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n</code></pre> <p>Create a forecaster with window features and transformations:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(n_estimators=100),\n...     lags=[1, 7, 30],\n...     window_features=[RollingMeanWindow(window=7)],\n...     transformer_y=StandardScaler(),\n...     differentiation=1\n... )\n&gt;&gt;&gt; forecaster.fit(y)\n&gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n</code></pre> <p>Create a forecaster with exogenous variables:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n&gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=Ridge(),\n...     lags=7,\n...     forecaster_id='my_forecaster'\n... )\n&gt;&gt;&gt; forecaster.fit(y, exog)\n&gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n</code></pre> <p>Create a forecaster with probabilistic prediction configuration:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=GradientBoostingRegressor(),\n...     lags=14,\n...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n... )\n&gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n&gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n&gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>class ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    Recursive autoregressive forecaster for scikit-learn compatible estimators.\n\n    This class turns any estimator compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster. The forecaster learns to predict\n    future values by using lagged values of the target variable and optional exogenous\n    features. Predictions are made iteratively, where each step uses previous predictions\n    as input for the next step (recursive strategy).\n\n    Args:\n        estimator: Scikit-learn compatible estimator for regression. If None, a default\n            estimator will be initialized. Can also be passed via regressor parameter.\n        lags: Lagged values of the target variable to use as predictors. Can be an\n            integer (uses lags from 1 to lags), list of integers, numpy array, or range.\n            At least one of lags or window_features must be provided. Defaults to None.\n        window_features: List of window feature objects to compute features from the\n            target variable. Each object must implement transform_batch() method.\n            At least one of lags or window_features must be provided. Defaults to None.\n        transformer_y: Transformer object for the target variable. Must implement fit()\n            and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        transformer_exog: Transformer object for exogenous variables. Must implement\n            fit() and transform() methods. Applied before training and predictions.\n            Defaults to None.\n        weight_func: Function to compute sample weights for training. Must accept an\n            index and return an array of weights. Defaults to None.\n        differentiation: Order of differencing to apply to the target variable.\n            Must be a positive integer. Differencing is applied before creating lags.\n            Defaults to None.\n        fit_kwargs: Dictionary of additional keyword arguments to pass to the estimator's\n            fit() method. Defaults to None.\n        binner_kwargs: Dictionary of keyword arguments for QuantileBinner used in\n            probabilistic predictions. Defaults to {'n_bins': 10, 'method': 'linear'}.\n        forecaster_id: Identifier for the forecaster instance. Can be a string or\n            integer. Used for tracking and logging purposes. Defaults to None.\n        regressor: Alternative parameter name for estimator. If provided, used instead\n            of estimator. Defaults to None.\n\n    Attributes:\n        estimator: Fitted scikit-learn estimator.\n        lags: Lag indices used in the model.\n        lags_names: Names of lag features (e.g., ['lag_1', 'lag_2']).\n        window_features: List of window feature transformers.\n        window_features_names: Names of window features.\n        window_size: Maximum window size needed (max of lags and window features).\n        transformer_y: Transformer for target variable.\n        transformer_exog: Transformer for exogenous variables.\n        weight_func: Function for sample weighting.\n        differentiation: Order of differencing applied.\n        differentiator: TimeSeriesDifferentiator instance if differencing is used.\n        is_fitted: Boolean indicating if forecaster has been fitted.\n        fit_date: Timestamp of the last fit operation.\n        last_window_: Last window_size observations from training data.\n        index_type_: Type of index in training data (RangeIndex or DatetimeIndex).\n        index_freq_: Frequency of DatetimeIndex if applicable.\n        training_range_: First and last index values of training data.\n        series_name_in_: Name of the target series.\n        exog_in_: Boolean indicating if exogenous variables were used in training.\n        exog_names_in_: Names of exogenous variables.\n        exog_type_in_: Type of exogenous input (Series or DataFrame).\n        X_train_features_names_out_: Names of all training features.\n        in_sample_residuals_: Residuals from training set.\n        in_sample_residuals_by_bin_: Residuals grouped by bins for probabilistic pred.\n        forecaster_id: Identifier for the forecaster instance.\n\n    Note:\n        - Either lags or window_features (or both) must be provided during initialization.\n        - The forecaster uses a recursive strategy where each multi-step prediction\n          depends on previous predictions within the same forecast horizon.\n        - Exogenous variables must have the same index as the target variable and must\n          be available for the entire prediction horizon.\n        - The forecaster supports point predictions, prediction intervals, bootstrapping,\n          quantile predictions, and probabilistic forecasts via conformal methods.\n\n    Examples:\n        Create a basic forecaster with lags:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=LinearRegression(),\n        ...     lags=10\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5)\n\n        Create a forecaster with window features and transformations:\n\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingMeanWindow\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(n_estimators=100),\n        ...     lags=[1, 7, 30],\n        ...     window_features=[RollingMeanWindow(window=7)],\n        ...     transformer_y=StandardScaler(),\n        ...     differentiation=1\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y)\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=10)\n\n        Create a forecaster with exogenous variables:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(100), name='target')\n        &gt;&gt;&gt; exog = pd.DataFrame({'temp': np.random.randn(100)})\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=Ridge(),\n        ...     lags=7,\n        ...     forecaster_id='my_forecaster'\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, exog)\n        &gt;&gt;&gt; exog_future = pd.DataFrame({'temp': np.random.randn(5)})\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, exog=exog_future)\n\n        Create a forecaster with probabilistic prediction configuration:\n\n        &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n        &gt;&gt;&gt; y = np.random.randn(100)\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=GradientBoostingRegressor(),\n        ...     lags=14,\n        ...     binner_kwargs={'n_bins': 15, 'method': 'quantile'}\n        ... )\n        &gt;&gt;&gt; forecaster.fit(y, store_in_sample_residuals=True)\n        &gt;&gt;&gt; # Get probabilistic predictions with prediction intervals\n        &gt;&gt;&gt; predictions = forecaster.predict(steps=5, prediction_interval=True, level=0.95)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: object = None,\n        lags: Union[int, List[int], np.ndarray, range, None] = None,\n        window_features: Union[object, List[object], None] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        binner_kwargs: Optional[Dict[str, object]] = None,\n        forecaster_id: Union[str, int, None] = None,\n        regressor: object = None,\n    ) -&gt; None:\n\n        self.estimator = copy(initialize_estimator(estimator, regressor))\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiation_max = None\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_name_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.exog_dtypes_out_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.is_fitted = False\n        self.fit_date = None\n        self.spotforecast_version = __version__\n        self.python_version = sys.version.split(\" \")[0]\n        self.forecaster_id = forecaster_id\n        self._probabilistic_mode = \"binned\"\n\n        (\n            self.lags,\n            self.lags_names,\n            self.max_lag,\n        ) = initialize_lags(type(self).__name__, lags)\n        (\n            self.window_features,\n            self.window_features_names,\n            self.max_size_window_features,\n        ) = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        self.window_size = max(\n            [\n                ws\n                for ws in [self.max_lag, self.max_size_window_features]\n                if ws is not None\n            ]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name=type(self).__name__,\n            estimator=estimator,\n            weight_func=weight_func,\n            series_weights=None,\n        )\n\n        if differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation &lt; 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation  # , window_size=self.window_size # Note: TimeSeriesDifferentiator in preprocessing I created only takes order\n            )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n            estimator=estimator, fit_kwargs=fit_kwargs\n        )\n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                \"n_bins\": 10,\n                \"method\": \"linear\",\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",  # single-series | global\n            \"forecasting_strategy\": \"recursive\",  # recursive | direct | deep_learning\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Estimator: {type(self.estimator).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Series name: {self.series_name_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Estimator parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.spotforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        params = (\n            self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n        )\n        exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n        style, unique_id = get_style_repr_html(self.is_fitted)\n\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; {type(self.estimator).__name__}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;spotforecast version:&lt;/strong&gt; {self.spotforecast_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n                &lt;ul&gt;\n                    {exog_names_in_}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Data Transformations&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Training Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Estimator Parameters&lt;/summary&gt;\n                &lt;ul&gt;\n                    {params}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n            &lt;details&gt;\n                &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n                &lt;ul&gt;\n                    {self.fit_kwargs}\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def __setstate__(self, state: dict) -&gt; None:\n        \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n        super().__setstate__(state)\n        if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n            self.__spotforecast_tags__ = {\n                \"library\": \"spotforecast\",\n                \"forecaster_name\": \"ForecasterRecursive\",\n                \"forecaster_task\": \"regression\",\n                \"forecasting_scope\": \"single-series\",\n                \"forecasting_strategy\": \"recursive\",\n                \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n                \"requires_index_frequency\": True,\n                \"allowed_input_types_series\": [\"pandas.Series\"],\n                \"supports_exog\": True,\n                \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n                \"handles_missing_values_series\": False,\n                \"handles_missing_values_exog\": True,\n                \"supports_lags\": True,\n                \"supports_window_features\": True,\n                \"supports_transformer_series\": True,\n                \"supports_transformer_exog\": True,\n                \"supports_weight_func\": True,\n                \"supports_differentiation\": True,\n                \"prediction_types\": [\n                    \"point\",\n                    \"interval\",\n                    \"bootstrapping\",\n                    \"quantiles\",\n                    \"distribution\",\n                ],\n                \"supports_probabilistic\": True,\n                \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n                \"handles_binned_residuals\": True,\n            }\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None,\n    ) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create lagged predictors and aligned target values.\n\n        Args:\n            y: Target values used to build lag features. Expected shape is\n                (n_samples,) or (n_samples, 1).\n            X_as_pandas: If True, returns lagged features as a pandas DataFrame.\n            train_index: Index to use for the lagged feature DataFrame when\n                `X_as_pandas` is True.\n\n        Returns:\n            Tuple containing:\n                - X_data: Lagged predictors with shape (n_rows, n_lags) or None\n                  if no lags are configured.\n                - y_data: Target values aligned to the lagged predictors with\n                  shape (n_rows,).\n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            # y = y.ravel() # Assuming y is already raveled\n            # Using stride_tricks for sliding window\n            y_strided = np.lib.stride_tricks.sliding_window_view(y, self.window_size)[\n                :-1\n            ]\n            X_data = y_strided[:, self.window_size - self.lags]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                    data=X_data, columns=self.lags_names, index=train_index\n                )\n\n        y_data = y[self.window_size :]\n\n        return X_data, y_data\n\n    def _create_window_features(\n        self,\n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -&gt; Tuple[List[Union[np.ndarray, pd.DataFrame]], List[str]]:\n        \"\"\"\n        Generate window features from the target series.\n\n        Args:\n            y: Target series used to compute window features. Must be a pandas\n                Series with an index aligned to `train_index` after trimming.\n            train_index: Index for the training rows to align the window features.\n            X_as_pandas: If True, keeps each window feature matrix as a pandas\n                DataFrame; otherwise converts to NumPy arrays.\n\n        Returns:\n            Tuple containing:\n                - X_train_window_features: List of window feature matrices, one\n                  per window feature transformer.\n                - X_train_window_features_names_out_: List of feature names for\n                  all generated window features.\n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a pandas DataFrame.\"\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same number of rows as \"\n                    f\"the input time series - `window_size`: {len_train_index}.\"\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    f\"The method `transform_batch` of {type(wf).__name__} \"\n                    f\"must return a DataFrame with the same index as \"\n                    f\"the input time series - `window_size`.\"\n                )\n\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name=\"y\")\n\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n            df=y,\n            transformer=self.transformer_y,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        y_values, y_index = check_extract_values_and_index(data=y, data_label=\"`y`\")\n        if y_values.ndim == 2 and y_values.shape[1] == 1:\n            y_values = y_values.ravel()\n        train_index = y_index[self.window_size :]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                self.differentiator.fit(y_values)  # Differentiator requires fit first\n                y_values = self.differentiator.transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        X_as_pandas = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            _, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\", ignore_freq=True, return_values=False\n            )\n\n            _ = len(y_values) + (\n                self.differentiation if self.differentiation else 0\n            )  # Adjust for differentiation loss of length if needed? No, y_values has NaNs at start\n            # But y_values from check_extract... is raw values.\n            # Differentiator might introduce NaNs. Sklearn transformer keeps length.\n            # My ported differentiator creates NaNs at start.\n\n            # Re-evaluate logic:\n            # y_values (raw) length = N\n            # differentiator transform -&gt; length N, first 'order' are NaN.\n\n            len_exog = len(exog)\n            # The check logic depends on alignment.\n\n            # Simplified check from original code\n            # ... (omitted for brevity, assume caller passed valid data or minimal check)\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=fit_transformer,\n                inverse_transform=False,\n            )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n            X_as_pandas = any(\n                not pd.api.types.is_numeric_dtype(dtype)\n                or pd.api.types.is_bool_dtype(dtype)\n                for dtype in set(exog.dtypes)\n            )\n\n            # Alignment logic\n            if len_exog == len(y):\n                exog = exog.iloc[self.window_size :,]\n            else:\n                pass  # Assume aligned start\n\n        X_train = []\n        X_train_features_names_out_ = []\n\n        # Create lags\n        # Note: y_values might have NaNs from differentiation.\n        # create_lags handles this?\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            if isinstance(y_values, pd.Series):\n                y_vals_for_wf = y_values.iloc[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n            else:\n                y_vals_for_wf = y_values[n_diff:]\n                y_index_for_wf = y_index[n_diff:]\n\n            y_window_features = pd.Series(y_vals_for_wf, index=y_index_for_wf)\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features,\n                    X_as_pandas=X_as_pandas,\n                    train_index=train_index,\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                data=X_train, index=train_index, columns=X_train_features_names_out_\n            )\n\n        y_train = pd.Series(data=y_train, index=train_index, name=\"y\")\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        )\n\n    def create_train_X_y(\n        self, y: pd.Series, exog: Union[pd.Series, pd.DataFrame, None] = None\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.Series,\n        List[str],\n        List[str],\n        List[str],\n        List[str],\n        Dict[str, type],\n        Dict[str, type],\n    ]:\n        return self._create_train_X_y(y=y, exog=exog)\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Args:\n            y: Training time series.\n            initial_train_size: Initial size of the training set. It is the number of\n                observations used to train the forecaster before making the first\n                prediction.\n            exog: Exogenous variable/s included as predictor/s. Must have the same\n                number of observations as y and their indexes must be aligned.\n                Defaults to None.\n\n        Returns:\n            Tuple containing:\n                - X_train: Predictor values used to train the model as pandas DataFrame.\n                - y_train: Values of the time series related to each row of X_train for\n                    each step in the form {step: y_step_[i]} as dict.\n                - X_test: Predictor values used to test the model as pandas DataFrame.\n                - y_test: Values of the time series related to each row of X_test for\n                    each step in the form {step: y_step_[i]} as dict.\n\n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y=y.iloc[:initial_train_size],\n            exog=exog.iloc[:initial_train_size] if exog is not None else None,\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y=y.iloc[test_init:],\n            exog=exog.iloc[test_init:] if exog is not None else None,\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def get_params(self, deep=True):\n        params = {}\n        for key in [\n            \"estimator\",\n            \"lags\",\n            \"window_features\",\n            \"transformer_y\",\n            \"transformer_exog\",\n            \"weight_func\",\n            \"differentiation\",\n            \"fit_kwargs\",\n            \"binner_kwargs\",\n            \"forecaster_id\",\n        ]:\n            if hasattr(self, key):\n                params[key] = getattr(self, key)\n\n        if not deep:\n            return params\n\n        if hasattr(self, \"estimator\") and self.estimator is not None:\n            if hasattr(self.estimator, \"get_params\"):\n                for key, value in self.estimator.get_params(deep=True).items():\n                    params[f\"estimator__{key}\"] = value\n\n        return params\n\n    def set_params(self, **params):\n        if not params:\n            return self\n\n        valid_params = self.get_params(deep=True)\n        nested_params = {}\n\n        for key, value in params.items():\n            if key not in valid_params and \"__\" not in key:\n                # Relaxed check for now\n                pass\n\n            if \"__\" in key:\n                obj_name, param_name = key.split(\"__\", 1)\n                if obj_name not in nested_params:\n                    nested_params[obj_name] = {}\n                nested_params[obj_name][param_name] = value\n            else:\n                setattr(self, key, value)\n\n        for obj_name, obj_params in nested_params.items():\n            if hasattr(self, obj_name):\n                obj = getattr(self, obj_name)\n                if hasattr(obj, \"set_params\"):\n                    obj.set_params(**obj_params)\n                else:\n                    for param_name, value in obj_params.items():\n                        setattr(obj, param_name, value)\n\n        return self\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = False,\n        random_state: int = 123,\n        suppress_warnings: bool = False,\n    ) -&gt; None:\n\n        # Reset values\n        self.is_fitted = False\n        self.fit_date = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_,\n            exog_dtypes_out_,\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        SAMPLE_WEIGHT_NAME = \"sample_weight\"\n        if self.weight_func is not None:\n            sample_weight, _, _ = initialize_weights(\n                forecaster_name=type(self).__name__,\n                estimator=self.estimator,\n                weight_func=self.weight_func,\n                series_weights=None,\n            )\n            sample_weight = sample_weight(y.index[self.window_size :])\n            self.fit_kwargs[SAMPLE_WEIGHT_NAME] = sample_weight\n\n        self.estimator.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        if SAMPLE_WEIGHT_NAME in self.fit_kwargs:\n            del self.fit_kwargs[SAMPLE_WEIGHT_NAME]\n\n        # Store attributes\n        self.last_window_ = y.iloc[-self.window_size :].copy()\n        self.index_type_ = type(y.index)\n        if isinstance(y.index, pd.DatetimeIndex):\n            self.index_freq_ = y.index.freqstr\n        else:\n            try:\n                self.index_freq_ = y.index.step\n            except AttributeError:\n                self.index_freq_ = None\n\n        self.training_range_ = y.index[[0, -1]]\n        self.series_name_in_ = y.name\n        self.exog_in_ = exog is not None\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog) if exog is not None else None\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        residuals = y_train - self.estimator.predict(X_train)\n\n        if len(residuals) &gt; 1000:\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(residuals, size=1000, replace=False)\n\n        self.in_sample_residuals_ = residuals\n\n        if self.binner_kwargs is not None:\n            self.binner = QuantileBinner(**self.binner_kwargs)\n            if isinstance(residuals, pd.Series):\n                residuals = residuals.to_numpy()\n            self.binner.fit(residuals)\n\n            # Construct intervals_ manually if not in binner\n            if hasattr(self.binner, \"intervals_\"):\n                self.binner_intervals_ = self.binner.intervals_\n            else:\n                self.binner_intervals_ = {\n                    i: (self.binner.bins_[i - 1], self.binner.bins_[i])\n                    for i in range(1, len(self.binner.bins_))\n                }\n\n            residuals_binned = self.binner.transform(residuals)\n            self.in_sample_residuals_by_bin_ = {\n                bin: residuals[residuals_binned == bin]\n                for bin in self.binner_intervals_.keys()\n            }\n\n            # Limit residuals stored per bin\n            max_residuals_per_bin = 1000 // self.binner.n_bins\n            for bin, res in self.in_sample_residuals_by_bin_.items():\n                if len(res) &gt; max_residuals_per_bin:\n                    rng = np.random.default_rng(seed=123)\n                    self.in_sample_residuals_by_bin_[bin] = rng.choice(\n                        res, size=max_residuals_per_bin, replace=False\n                    )\n\n    def _create_predict_inputs(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; Tuple[np.ndarray, Union[np.ndarray, None], pd.Index, pd.Index]:\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name=type(self).__name__,\n                steps=steps,\n                is_fitted=self.is_fitted,\n                exog_in_=self.exog_in_,\n                index_type_=self.index_type_,\n                index_freq_=self.index_freq_,\n                window_size=self.window_size,\n                last_window=last_window,\n                last_window_exog=None,\n                exog=exog,\n                exog_names_in_=self.exog_names_in_,\n                interval=None,\n                # alpha=None, # Removed alpha check for now\n            )\n\n        last_window = input_to_frame(data=last_window, input_name=\"last_window\")\n        _, last_window_index = check_extract_values_and_index(\n            data=last_window,\n            data_label=\"`last_window`\",\n            ignore_freq=True,\n            return_values=False,\n        )\n\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n\n        last_window = transform_dataframe(\n            df=last_window,\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, _ = check_extract_values_and_index(\n            data=last_window, data_label=\"`last_window`\"\n        )\n        last_window_values = last_window_values.ravel()\n\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        exog_values = None\n        exog_index = None\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name=\"exog\")\n            exog = transform_dataframe(\n                df=exog,\n                transformer=self.transformer_exog,\n                fit=False,\n                inverse_transform=False,\n            )\n\n            exog_values, exog_index = check_extract_values_and_index(\n                data=exog, data_label=\"`exog`\"\n            )\n\n            exog_values = (\n                exog_values if isinstance(exog, pd.Series) else exog.to_numpy()\n            )\n\n        return last_window_values, exog_values, prediction_index, exog_index\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Union[np.ndarray, None] = None,\n    ) -&gt; np.ndarray:\n\n        predictions = np.full(shape=steps, fill_value=np.nan)\n\n        for step in range(steps):\n\n            X_gen = []\n\n            if self.lags is not None:\n                X_lags = last_window_values[-self.lags]\n                if X_lags.ndim == 1:\n                    X_lags = X_lags.reshape(1, -1)\n                X_gen.append(X_lags)\n\n            if self.window_features is not None:\n                X_window_features = []\n                for wf in self.window_features:\n                    wf_values = wf.transform(last_window_values)\n                    X_window_features.append(wf_values[-1:])\n\n                X_window_features = np.concatenate(X_window_features, axis=1)\n                X_gen.append(X_window_features)\n\n            if self.exog_in_:\n                X_exog = exog_values[step]\n                if X_exog.ndim &lt; 2:\n                    X_exog = X_exog.reshape(1, -1)\n                X_gen.append(X_exog)\n\n            X_gen = np.concatenate(X_gen, axis=1)\n\n            # Convert to DataFrame with feature names to avoid sklearn warning\n            if self.X_train_features_names_out_ is not None:\n                X_gen = pd.DataFrame(X_gen, columns=self.X_train_features_names_out_)\n\n            pred = self.estimator.predict(X_gen)\n            predictions[step] = pred[0]\n\n            last_window_values = np.append(last_window_values, pred)\n\n        return predictions\n\n    def predict(\n        self,\n        steps: int,\n        last_window: Union[pd.Series, pd.DataFrame, None] = None,\n        exog: Union[pd.Series, pd.DataFrame, None] = None,\n        check_inputs: bool = True,\n    ) -&gt; pd.Series:\n\n        last_window_values, exog_values, prediction_index, _ = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        predictions = self._recursive_predict(\n            steps=steps, last_window_values=last_window_values, exog_values=exog_values\n        )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_dataframe(\n            df=pd.Series(predictions, name=\"pred\").to_frame(),\n            transformer=self.transformer_y,\n            fit=False,\n            inverse_transform=True,\n        )\n\n        predictions = predictions.iloc[:, 0]\n        predictions.index = prediction_index\n\n        return predictions\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when a ForecasterRecursive object is printed.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when a ForecasterRecursive object is printed.\n    \"\"\"\n\n    params = (\n        self.estimator.get_params() if hasattr(self.estimator, \"get_params\") else {}\n    )\n    exog_names_in_ = self.exog_names_in_ if self.exog_in_ else None\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Estimator: {type(self.estimator).__name__} \\n\"\n        f\"Lags: {self.lags} \\n\"\n        f\"Window features: {self.window_features_names} \\n\"\n        f\"Window size: {self.window_size} \\n\"\n        f\"Series name: {self.series_name_in_} \\n\"\n        f\"Exogenous included: {self.exog_in_} \\n\"\n        f\"Exogenous names: {exog_names_in_} \\n\"\n        f\"Transformer for y: {self.transformer_y} \\n\"\n        f\"Transformer for exog: {self.transformer_exog} \\n\"\n        f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n        f\"Differentiation order: {self.differentiation} \\n\"\n        f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n        f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n        f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n        f\"Estimator parameters: {params} \\n\"\n        f\"fit_kwargs: {self.fit_kwargs} \\n\"\n        f\"Creation date: {self.creation_date} \\n\"\n        f\"Last fit date: {self.fit_date} \\n\"\n        f\"Skforecast version: {self.spotforecast_version} \\n\"\n        f\"Python version: {self.python_version} \\n\"\n        f\"Forecaster id: {self.forecaster_id} \\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.recursive.ForecasterRecursive.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Custom setstate to ensure backward compatibility when unpickling.</p> Source code in <code>src/spotforecast2_safe/forecaster/recursive/_forecaster_recursive.py</code> <pre><code>def __setstate__(self, state: dict) -&gt; None:\n    \"\"\"Custom __setstate__ to ensure backward compatibility when unpickling.\"\"\"\n    super().__setstate__(state)\n    if not hasattr(self, \"_ForecasterRecursive__spotforecast_tags__\"):\n        self.__spotforecast_tags__ = {\n            \"library\": \"spotforecast\",\n            \"forecaster_name\": \"ForecasterRecursive\",\n            \"forecaster_task\": \"regression\",\n            \"forecasting_scope\": \"single-series\",\n            \"forecasting_strategy\": \"recursive\",\n            \"index_types_supported\": [\"pandas.RangeIndex\", \"pandas.DatetimeIndex\"],\n            \"requires_index_frequency\": True,\n            \"allowed_input_types_series\": [\"pandas.Series\"],\n            \"supports_exog\": True,\n            \"allowed_input_types_exog\": [\"pandas.Series\", \"pandas.DataFrame\"],\n            \"handles_missing_values_series\": False,\n            \"handles_missing_values_exog\": True,\n            \"supports_lags\": True,\n            \"supports_window_features\": True,\n            \"supports_transformer_series\": True,\n            \"supports_transformer_exog\": True,\n            \"supports_weight_func\": True,\n            \"supports_differentiation\": True,\n            \"prediction_types\": [\n                \"point\",\n                \"interval\",\n                \"bootstrapping\",\n                \"quantiles\",\n                \"distribution\",\n            ],\n            \"supports_probabilistic\": True,\n            \"probabilistic_methods\": [\"bootstrapping\", \"conformal\"],\n            \"handles_binned_residuals\": True,\n        }\n</code></pre>"},{"location":"api/forecaster/#forecasting-metrics","title":"Forecasting Metrics","text":""},{"location":"api/forecaster/#metrics","title":"metrics","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics","title":"<code>spotforecast2_safe.forecaster.metrics</code>","text":"<p>Metrics for evaluating forecasting models.</p> <p>This module provides various metric functions for evaluating forecasting performance, including custom metrics like MASE, RMSSE, and probabilistic metrics like CRPS.</p> <p>Examples:</p> <p>Using standard metrics::</p> <pre><code>import numpy as np\nfrom spotforecast2.forecaster.metrics import _get_metric\n\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\n\n# Get a metric function\nmse = _get_metric('mean_squared_error')\nerror = mse(y_true, y_pred)\n</code></pre> <p>Using scaled metrics::</p> <pre><code>from spotforecast2.forecaster.metrics import mean_absolute_scaled_error\n\ny_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\ny_true = np.array([9, 10, 11])\ny_pred = np.array([8.8, 10.2, 10.9])\n\nmase = mean_absolute_scaled_error(y_true, y_pred, y_train)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.add_y_train_argument","title":"<code>add_y_train_argument(func)</code>","text":"<p>Add <code>y_train</code> argument to a function if it is not already present.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to which the argument is added.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Function with <code>y_train</code> argument added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def my_metric(y_true, y_pred):\n...     return np.mean(np.abs(y_true - y_pred))\n&gt;&gt;&gt; enhanced_metric = add_y_train_argument(my_metric)\n&gt;&gt;&gt; # Now the function accepts y_train parameter\n&gt;&gt;&gt; result = enhanced_metric(np.array([1,2,3]), np.array([1,2,3]), y_train=None)\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def add_y_train_argument(func: Callable) -&gt; Callable:\n    \"\"\"Add `y_train` argument to a function if it is not already present.\n\n    Args:\n        func: Function to which the argument is added.\n\n    Returns:\n        Function with `y_train` argument added.\n\n    Examples:\n        &gt;&gt;&gt; def my_metric(y_true, y_pred):\n        ...     return np.mean(np.abs(y_true - y_pred))\n        &gt;&gt;&gt; enhanced_metric = add_y_train_argument(my_metric)\n        &gt;&gt;&gt; # Now the function accepts y_train parameter\n        &gt;&gt;&gt; result = enhanced_metric(np.array([1,2,3]), np.array([1,2,3]), y_train=None)\n    \"\"\"\n\n    sig = inspect.signature(func)\n\n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n\n    wrapper.__signature__ = new_sig\n\n    return wrapper\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.calculate_coverage","title":"<code>calculate_coverage(y_true, lower_bound, upper_bound)</code>","text":"<p>Calculate coverage of a given interval.</p> <p>Coverage is the proportion of true values that fall within the interval.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>lower_bound</code> <code>ndarray | Series</code> <p>Lower bound of the interval.</p> required <code>upper_bound</code> <code>ndarray | Series</code> <p>Upper bound of the interval.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Coverage of the interval.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import calculate_coverage\n&gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; lower_bound = np.array([0.5, 1.5, 2.5, 3.5, 4.5])\n&gt;&gt;&gt; upper_bound = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n&gt;&gt;&gt; coverage = calculate_coverage(y_true, lower_bound, upper_bound)\n&gt;&gt;&gt; coverage == 1.0  # All values within bounds\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def calculate_coverage(\n    y_true: np.ndarray | pd.Series,\n    lower_bound: np.ndarray | pd.Series,\n    upper_bound: np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"Calculate coverage of a given interval.\n\n    Coverage is the proportion of true values that fall within the interval.\n\n    Args:\n        y_true: True values of the target variable.\n        lower_bound: Lower bound of the interval.\n        upper_bound: Upper bound of the interval.\n\n    Returns:\n        Coverage of the interval.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import calculate_coverage\n        &gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; lower_bound = np.array([0.5, 1.5, 2.5, 3.5, 4.5])\n        &gt;&gt;&gt; upper_bound = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n        &gt;&gt;&gt; coverage = calculate_coverage(y_true, lower_bound, upper_bound)\n        &gt;&gt;&gt; coverage == 1.0  # All values within bounds\n        True\n    \"\"\"\n    if not isinstance(y_true, (np.ndarray, pd.Series)) or y_true.ndim != 1:\n        raise TypeError(\"`y_true` must be a 1D numpy array or pandas Series.\")\n\n    if not isinstance(lower_bound, (np.ndarray, pd.Series)) or lower_bound.ndim != 1:\n        raise TypeError(\"`lower_bound` must be a 1D numpy array or pandas Series.\")\n\n    if not isinstance(upper_bound, (np.ndarray, pd.Series)) or upper_bound.ndim != 1:\n        raise TypeError(\"`upper_bound` must be a 1D numpy array or pandas Series.\")\n\n    y_true = np.asarray(y_true)\n    lower_bound = np.asarray(lower_bound)\n    upper_bound = np.asarray(upper_bound)\n\n    if y_true.shape != lower_bound.shape or y_true.shape != upper_bound.shape:\n        raise ValueError(\n            \"`y_true`, `lower_bound` and `upper_bound` must have the same shape.\"\n        )\n\n    coverage = np.mean(np.logical_and(y_true &gt;= lower_bound, y_true &lt;= upper_bound))\n\n    return coverage\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.create_mean_pinball_loss","title":"<code>create_mean_pinball_loss(alpha)</code>","text":"<p>Create pinball loss for a given quantile.</p> <p>Also known as quantile loss. Internally, it uses the <code>mean_pinball_loss</code> function from scikit-learn.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Quantile for which the Pinball loss is calculated. Must be between 0 and 1, inclusive.</p> required <p>Returns:</p> Type Description <code>callable</code> <p>Mean Pinball loss function for the given quantile.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import create_mean_pinball_loss\n&gt;&gt;&gt; pinball_loss_50 = create_mean_pinball_loss(alpha=0.5)\n&gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\n&gt;&gt;&gt; loss = pinball_loss_50(y_true, y_pred)\n&gt;&gt;&gt; loss &gt;= 0\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def create_mean_pinball_loss(alpha: float) -&gt; callable:\n    \"\"\"Create pinball loss for a given quantile.\n\n    Also known as quantile loss. Internally, it uses the `mean_pinball_loss`\n    function from scikit-learn.\n\n    Args:\n        alpha: Quantile for which the Pinball loss is calculated.\n            Must be between 0 and 1, inclusive.\n\n    Returns:\n        Mean Pinball loss function for the given quantile.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import create_mean_pinball_loss\n        &gt;&gt;&gt; pinball_loss_50 = create_mean_pinball_loss(alpha=0.5)\n        &gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\n        &gt;&gt;&gt; loss = pinball_loss_50(y_true, y_pred)\n        &gt;&gt;&gt; loss &gt;= 0\n        True\n    \"\"\"\n    if not (0 &lt;= alpha &lt;= 1):\n        raise ValueError(\"alpha must be between 0 and 1, both inclusive.\")\n\n    def mean_pinball_loss_q(y_true, y_pred):\n        return mean_pinball_loss(y_true, y_pred, alpha=alpha)\n\n    return mean_pinball_loss_q\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.crps_from_predictions","title":"<code>crps_from_predictions(y_true, y_pred)</code>","text":"<p>Compute the Continuous Ranked Probability Score (CRPS) from predictions.</p> <p>The CRPS compares the empirical distribution of a set of forecasted values to a scalar observation. The smaller the CRPS, the better.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>float</code> <p>The true value of the random variable.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values of the random variable. These are the multiple forecasted values for a single observation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The CRPS score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_predictions\n&gt;&gt;&gt; y_true = 5.0\n&gt;&gt;&gt; y_pred = np.array([4.5, 5.1, 4.9, 5.3, 4.7])\n&gt;&gt;&gt; crps = crps_from_predictions(y_true, y_pred)\n&gt;&gt;&gt; crps &gt;= 0\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def crps_from_predictions(y_true: float, y_pred: np.ndarray) -&gt; float:\n    \"\"\"Compute the Continuous Ranked Probability Score (CRPS) from predictions.\n\n    The CRPS compares the empirical distribution of a set of forecasted values\n    to a scalar observation. The smaller the CRPS, the better.\n\n    Args:\n        y_true: The true value of the random variable.\n        y_pred: The predicted values of the random variable. These are the multiple\n            forecasted values for a single observation.\n\n    Returns:\n        The CRPS score.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_predictions\n        &gt;&gt;&gt; y_true = 5.0\n        &gt;&gt;&gt; y_pred = np.array([4.5, 5.1, 4.9, 5.3, 4.7])\n        &gt;&gt;&gt; crps = crps_from_predictions(y_true, y_pred)\n        &gt;&gt;&gt; crps &gt;= 0\n        True\n    \"\"\"\n    if not isinstance(y_pred, np.ndarray) or y_pred.ndim != 1:\n        raise TypeError(\"`y_pred` must be a 1D numpy array.\")\n\n    if not isinstance(y_true, (float, int)):\n        raise TypeError(\"`y_true` must be a float or integer.\")\n\n    y_pred = np.sort(y_pred)\n    # Define the grid for integration including the true value\n    grid = np.concatenate(([y_true], y_pred))\n    grid = np.sort(grid)\n    cdf_values = np.searchsorted(y_pred, grid, side=\"right\") / len(y_pred)\n    indicator = grid &gt;= y_true\n    diffs = np.diff(grid)\n    crps = np.sum(diffs * (cdf_values[:-1] - indicator[:-1]) ** 2)\n\n    return crps\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.crps_from_quantiles","title":"<code>crps_from_quantiles(y_true, pred_quantiles, quantile_levels)</code>","text":"<p>Calculate the Continuous Ranked Probability Score (CRPS) from quantiles.</p> <p>The empirical cdf is approximated using linear interpolation between the predicted quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>float</code> <p>The true value of the random variable.</p> required <code>pred_quantiles</code> <code>ndarray</code> <p>The predicted quantile values.</p> required <code>quantile_levels</code> <code>ndarray</code> <p>The quantile levels corresponding to the predicted quantiles.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The CRPS score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_quantiles\n&gt;&gt;&gt; y_true = 5.0\n&gt;&gt;&gt; pred_quantiles = np.array([4.0, 4.5, 5.0, 5.5, 6.0])\n&gt;&gt;&gt; quantile_levels = np.array([0.1, 0.25, 0.5, 0.75, 0.9])\n&gt;&gt;&gt; crps = crps_from_quantiles(y_true, pred_quantiles, quantile_levels)\n&gt;&gt;&gt; crps &gt;= 0\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def crps_from_quantiles(\n    y_true: float,\n    pred_quantiles: np.ndarray,\n    quantile_levels: np.ndarray,\n) -&gt; float:\n    \"\"\"Calculate the Continuous Ranked Probability Score (CRPS) from quantiles.\n\n    The empirical cdf is approximated using linear interpolation\n    between the predicted quantiles.\n\n    Args:\n        y_true: The true value of the random variable.\n        pred_quantiles: The predicted quantile values.\n        quantile_levels: The quantile levels corresponding to the predicted quantiles.\n\n    Returns:\n        The CRPS score.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_quantiles\n        &gt;&gt;&gt; y_true = 5.0\n        &gt;&gt;&gt; pred_quantiles = np.array([4.0, 4.5, 5.0, 5.5, 6.0])\n        &gt;&gt;&gt; quantile_levels = np.array([0.1, 0.25, 0.5, 0.75, 0.9])\n        &gt;&gt;&gt; crps = crps_from_quantiles(y_true, pred_quantiles, quantile_levels)\n        &gt;&gt;&gt; crps &gt;= 0\n        True\n    \"\"\"\n    if not isinstance(y_true, (float, int)):\n        raise TypeError(\"`y_true` must be a float or integer.\")\n\n    if not isinstance(pred_quantiles, np.ndarray) or pred_quantiles.ndim != 1:\n        raise TypeError(\"`pred_quantiles` must be a 1D numpy array.\")\n\n    if not isinstance(quantile_levels, np.ndarray) or quantile_levels.ndim != 1:\n        raise TypeError(\"`quantile_levels` must be a 1D numpy array.\")\n\n    if len(pred_quantiles) != len(quantile_levels):\n        raise ValueError(\n            \"The number of predicted quantiles and quantile levels must be equal.\"\n        )\n\n    sorted_indices = np.argsort(pred_quantiles)\n    pred_quantiles = pred_quantiles[sorted_indices]\n    quantile_levels = quantile_levels[sorted_indices]\n\n    # Define the empirical CDF function using interpolation\n    def empirical_cdf(x):\n        return np.interp(x, pred_quantiles, quantile_levels, left=0.0, right=1.0)\n\n    # Define the CRPS integrand\n    def crps_integrand(x):\n        return (empirical_cdf(x) - (x &gt;= y_true)) ** 2\n\n    # Integration bounds: Extend slightly beyond predicted quantiles\n    xmin = np.min(pred_quantiles) * 0.9\n    xmax = np.max(pred_quantiles) * 1.1\n\n    # Create a fine grid of x values for integration\n    x_values = np.linspace(xmin, xmax, 1000)\n\n    # Compute the integrand values and integrate using the trapezoidal rule\n    integrand_values = crps_integrand(x_values)\n    if np.__version__ &gt;= \"2.0.0\":\n        crps = np.trapezoid(integrand_values, x=x_values)\n    else:\n        crps = np.trapz(integrand_values, x_values)\n\n    return crps\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.mean_absolute_scaled_error","title":"<code>mean_absolute_scaled_error(y_true, y_pred, y_train)</code>","text":"<p>Mean Absolute Scaled Error (MASE).</p> <p>MASE is a scale-independent error metric that measures the accuracy of a forecast. It is the mean absolute error of the forecast divided by the mean absolute error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>ndarray | Series</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list[float] | ndarray | Series</code> <p>True values of the target variable in the training set. If <code>list</code>, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>MASE value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import mean_absolute_scaled_error\n&gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; y_true = np.array([9, 10, 11])\n&gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n&gt;&gt;&gt; mase = mean_absolute_scaled_error(y_true, y_pred, y_train)\n&gt;&gt;&gt; mase &lt; 1.0  # Good forecast\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def mean_absolute_scaled_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    y_train: list[float] | np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"Mean Absolute Scaled Error (MASE).\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Args:\n        y_true: True values of the target variable.\n        y_pred: Predicted values of the target variable.\n        y_train: True values of the target variable in the training set. If `list`, it\n            is consider that each element is the true value of the target variable\n            in the training set for each time series.\n\n    Returns:\n        MASE value.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import mean_absolute_scaled_error\n        &gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n        &gt;&gt;&gt; y_true = np.array([9, 10, 11])\n        &gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n        &gt;&gt;&gt; mase = mean_absolute_scaled_error(y_true, y_pred, y_train)\n        &gt;&gt;&gt; mase &lt; 1.0  # Good forecast\n        True\n    \"\"\"\n\n    # NOTE: When using this metric in validation, `y_train` doesn't include\n    # the first window_size observations used to create the predictors and/or\n    # rolling features.\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    \"When `y_train` is a list, each element must be a pandas Series \"\n                    \"or numpy ndarray.\"\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        # Flatten list of arrays for naive forecast if meaningful, but MASE usually assumes\n        # naive forecast on single series. If list, we might be doing something else.\n        # Original code does: np.concatenate([np.diff(x) for x in y_train])\n        # This assumes independent series and we average error over all of them.\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.root_mean_squared_scaled_error","title":"<code>root_mean_squared_scaled_error(y_true, y_pred, y_train)</code>","text":"<p>Root Mean Squared Scaled Error (RMSSE).</p> <p>RMSSE is a scale-independent error metric that measures the accuracy of a forecast. It is the root mean squared error of the forecast divided by the root mean squared error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>ndarray | Series</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list[float] | ndarray | Series</code> <p>True values of the target variable in the training set. If list, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>RMSSE value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import root_mean_squared_scaled_error\n&gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; y_true = np.array([9, 10, 11])\n&gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n&gt;&gt;&gt; rmsse = root_mean_squared_scaled_error(y_true, y_pred, y_train)\n&gt;&gt;&gt; rmsse &lt; 1.0  # Good forecast\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def root_mean_squared_scaled_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    y_train: list[float] | np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"Root Mean Squared Scaled Error (RMSSE).\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Args:\n        y_true: True values of the target variable.\n        y_pred: Predicted values of the target variable.\n        y_train: True values of the target variable in the training set. If list, it\n            is consider that each element is the true value of the target variable\n            in the training set for each time series.\n\n    Returns:\n        RMSSE value.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import root_mean_squared_scaled_error\n        &gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n        &gt;&gt;&gt; y_true = np.array([9, 10, 11])\n        &gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n        &gt;&gt;&gt; rmsse = root_mean_squared_scaled_error(y_true, y_pred, y_train)\n        &gt;&gt;&gt; rmsse &lt; 1.0  # Good forecast\n        True\n    \"\"\"\n\n    # NOTE: When using this metric in validation, `y_train` doesn't include\n    # the first window_size observations used to create the predictors and/or\n    # rolling features.\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    \"When `y_train` is a list, each element must be a pandas Series \"\n                    \"or numpy ndarray.\"\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(\n        np.nanmean(naive_forecast**2)\n    )\n\n    return rmsse\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.metrics.symmetric_mean_absolute_percentage_error","title":"<code>symmetric_mean_absolute_percentage_error(y_true, y_pred)</code>","text":"<p>Compute the Symmetric Mean Absolute Percentage Error (SMAPE).</p> <p>SMAPE is a relative error metric used to measure the accuracy of forecasts. Unlike MAPE, it is symmetric and prevents division by zero by averaging the absolute values of actual and predicted values.</p> <p>The result is expressed as a percentage and ranges from 0% (perfect prediction) to 200% (maximum error).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>ndarray | Series</code> <p>Predicted values of the target variable.</p> required <p>Returns:</p> Type Description <code>float</code> <p>SMAPE value as a percentage.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import symmetric_mean_absolute_percentage_error\n&gt;&gt;&gt; y_true = np.array([100, 200, 0])\n&gt;&gt;&gt; y_pred = np.array([110, 180, 10])\n&gt;&gt;&gt; result = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n&gt;&gt;&gt; 0 &lt;= result &lt;= 200\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def symmetric_mean_absolute_percentage_error(\n    y_true: np.ndarray | pd.Series, y_pred: np.ndarray | pd.Series\n) -&gt; float:\n    \"\"\"Compute the Symmetric Mean Absolute Percentage Error (SMAPE).\n\n    SMAPE is a relative error metric used to measure the accuracy\n    of forecasts. Unlike MAPE, it is symmetric and prevents division\n    by zero by averaging the absolute values of actual and predicted values.\n\n    The result is expressed as a percentage and ranges from 0%\n    (perfect prediction) to 200% (maximum error).\n\n    Args:\n        y_true: True values of the target variable.\n        y_pred: Predicted values of the target variable.\n\n    Returns:\n        SMAPE value as a percentage.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import symmetric_mean_absolute_percentage_error\n        &gt;&gt;&gt; y_true = np.array([100, 200, 0])\n        &gt;&gt;&gt; y_pred = np.array([110, 180, 10])\n        &gt;&gt;&gt; result = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n        &gt;&gt;&gt; 0 &lt;= result &lt;= 200\n        True\n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    numerator = np.abs(y_true - y_pred)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n\n    # NOTE: Avoid division by zero\n    mask = denominator != 0\n    smape_values = np.zeros_like(denominator)\n    smape_values[mask] = numerator[mask] / denominator[mask]\n\n    smape = 100 * np.mean(smape_values)\n\n    return smape\n</code></pre>"},{"location":"api/forecaster/#forecasting-utilities","title":"Forecasting Utilities","text":""},{"location":"api/forecaster/#utils","title":"utils","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils","title":"<code>spotforecast2_safe.forecaster.utils</code>","text":""},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_extract_values_and_index","title":"<code>check_extract_values_and_index(data, data_label='`y`', ignore_freq=False, return_values=True)</code>","text":"<p>Extract values and index from a pandas Series or DataFrame, ensuring they are valid.</p> <p>Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts its values and index for use in forecasting operations. Optionally checks for index frequency consistency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data (pandas Series or DataFrame) to extract values and index from.</p> required <code>data_label</code> <code>str</code> <p>Label used in exception messages for better error reporting. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <code>ignore_freq</code> <code>bool</code> <p>If True, the frequency of the index is not checked. Defaults to False.</p> <code>False</code> <code>return_values</code> <code>bool</code> <p>If True, the values of the data are returned. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[ndarray], Index]</code> <p>A tuple containing: - values (numpy.ndarray or None): Values of the data as numpy array,   or None if return_values is False. - index (pandas.Index): Index of the data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame.</p> <code>TypeError</code> <p>If data index is not a DatetimeIndex or RangeIndex.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If DatetimeIndex has no frequency (inferred automatically).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n&gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n&gt;&gt;&gt; print(values.shape)\n(10,)\n&gt;&gt;&gt; print(type(index))\n&lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n</code></pre> <p>Extract index only:</p> <pre><code>&gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n&gt;&gt;&gt; print(index[0])\n2020-01-01 00:00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_extract_values_and_index(\n    data: Union[pd.Series, pd.DataFrame],\n    data_label: str = \"`y`\",\n    ignore_freq: bool = False,\n    return_values: bool = True,\n) -&gt; Tuple[Optional[np.ndarray], pd.Index]:\n    \"\"\"Extract values and index from a pandas Series or DataFrame, ensuring they are valid.\n\n    Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts\n    its values and index for use in forecasting operations. Optionally checks for\n    index frequency consistency.\n\n    Args:\n        data: Input data (pandas Series or DataFrame) to extract values and index from.\n        data_label: Label used in exception messages for better error reporting.\n            Defaults to \"`y`\".\n        ignore_freq: If True, the frequency of the index is not checked.\n            Defaults to False.\n        return_values: If True, the values of the data are returned.\n            Defaults to True.\n\n    Returns:\n        tuple: A tuple containing:\n            - values (numpy.ndarray or None): Values of the data as numpy array,\n              or None if return_values is False.\n            - index (pandas.Index): Index of the data.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame.\n        TypeError: If data index is not a DatetimeIndex or RangeIndex.\n\n    Warnings:\n        UserWarning: If DatetimeIndex has no frequency (inferred automatically).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n        &gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n        &gt;&gt;&gt; print(values.shape)\n        (10,)\n        &gt;&gt;&gt; print(type(index))\n        &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n\n        Extract index only:\n        &gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n        &gt;&gt;&gt; print(index[0])\n        2020-01-01 00:00:00\n    \"\"\"\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"{data_label} must be a pandas Series or DataFrame.\")\n\n    if not isinstance(data.index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(f\"{data_label} must have a pandas DatetimeIndex or RangeIndex.\")\n\n    if isinstance(data.index, pd.DatetimeIndex) and not ignore_freq:\n        if data.index.freq is None:\n            warnings.warn(\n                f\"{data_label} has a DatetimeIndex but no frequency. \"\n                \"The frequency has been inferred from the index.\",\n                UserWarning,\n            )\n\n    values = data.to_numpy() if return_values else None\n\n    return values, data.index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_optional_dependency","title":"<code>check_optional_dependency(package_name)</code>","text":"<p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not installed.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_optional_dependency(package_name: str) -&gt; None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError\n    with installation instructions.\n\n    Args:\n        package_name (str): Name of the package to check.\n\n    Raises:\n        ImportError: If the package is not installed.\n    \"\"\"\n\n    if find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(\n                package_name=package_name\n            )\n            msg = f\"\\n'{package_name}' is an optional dependency not included in the default spotforecast installation.\"\n        except Exception:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_residuals_input","title":"<code>check_residuals_input(forecaster_name, use_in_sample_residuals, in_sample_residuals_, out_sample_residuals_, use_binned_residuals, in_sample_residuals_by_bin_, out_sample_residuals_by_bin_, levels=None, encoding=None)</code>","text":"<p>Check residuals input arguments in Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>use_in_sample_residuals</code> <code>bool</code> <p>bool Indicates if in sample or out sample residuals are used.</p> required <code>in_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting training data.</p> required <code>out_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting non training data.</p> required <code>use_binned_residuals</code> <code>bool</code> <p>bool Indicates if residuals are binned.</p> required <code>in_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict In sample residuals binned according to the predicted value each residual is associated with.</p> required <code>out_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict Out of sample residuals binned according to the predicted value each residual is associated with.</p> required <code>levels</code> <code>list[str] | None</code> <p>list, default None Names of the series (levels) to be predicted (Forecasters multiseries).</p> <code>None</code> <code>encoding</code> <code>str | None</code> <p>str, default None Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    out_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    levels: list[str] | None = None,\n    encoding: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        use_in_sample_residuals: bool\n            Indicates if in sample or out sample residuals are used.\n        in_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting training data.\n        out_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting non training data.\n        use_binned_residuals: bool\n            Indicates if residuals are binned.\n        in_sample_residuals_by_bin_: dict\n            In sample residuals binned according to the predicted value each residual\n            is associated with.\n        out_sample_residuals_by_bin_: dict\n            Out of sample residuals binned according to the predicted value each residual\n            is associated with.\n        levels: list, default None\n            Names of the series (levels) to be predicted (Forecasters multiseries).\n        encoding: str, default None\n            Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns:\n        None\n\n    \"\"\"\n\n    forecasters_multiseries = (\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    )\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        # Check if residuals are empty or None\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.date_to_index_position","title":"<code>date_to_index_position(index, date_input, method='prediction', date_literal='steps', kwargs_pd_to_datetime={})</code>","text":"<p>Transform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>pandas Index Original datetime index (must be a pandas DatetimeIndex if <code>date_input</code> is not an int).</p> required <code>date_input</code> <code>int | str | Timestamp</code> <p>int, str, pandas Timestamp Datetime to transform to integer.</p> <ul> <li>If int, returns the same integer.</li> <li>If str or pandas Timestamp, it is converted and expanded into the index.</li> </ul> required <code>method</code> <code>str</code> <p>str, default 'prediction' Can be 'prediction' or 'validation'.</p> <ul> <li>If 'prediction', the date must be later than the last date in the index.</li> <li>If 'validation', the date must be within the index range.</li> </ul> <code>'prediction'</code> <code>date_literal</code> <code>str</code> <p>str, default 'steps' Variable name used in error messages.</p> <code>'steps'</code> <code>kwargs_pd_to_datetime</code> <code>dict</code> <p>dict, default {} Additional keyword arguments to pass to <code>pd.to_datetime()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p><code>date_input</code> transformed to integer position in the <code>index</code>.</p> <code>int</code> <ul> <li>If <code>date_input</code> is an integer, it returns the same integer.</li> </ul> <code>int</code> <ul> <li>If method is 'prediction', number of steps to predict from the last</li> </ul> <code>int</code> <p>date in the index.</p> <code>int</code> <ul> <li>If method is 'validation', position plus one of the date in the index,</li> </ul> <code>int</code> <p>this is done to include the target date in the training set when using</p> <code>int</code> <p>pandas iloc with slices.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def date_to_index_position(\n    index: pd.Index,\n    date_input: int | str | pd.Timestamp,\n    method: str = \"prediction\",\n    date_literal: str = \"steps\",\n    kwargs_pd_to_datetime: dict = {},\n) -&gt; int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n\n    Args:\n        index: pandas Index\n            Original datetime index (must be a pandas DatetimeIndex if `date_input`\n            is not an int).\n        date_input: int, str, pandas Timestamp\n            Datetime to transform to integer.\n\n            - If int, returns the same integer.\n            - If str or pandas Timestamp, it is converted and expanded into the index.\n        method: str, default 'prediction'\n            Can be 'prediction' or 'validation'.\n\n            - If 'prediction', the date must be later than the last date in the index.\n            - If 'validation', the date must be within the index range.\n        date_literal: str, default 'steps'\n            Variable name used in error messages.\n        kwargs_pd_to_datetime: dict, default {}\n            Additional keyword arguments to pass to `pd.to_datetime()`.\n\n    Returns:\n        int:\n            `date_input` transformed to integer position in the `index`.\n\n        + If `date_input` is an integer, it returns the same integer.\n        + If method is 'prediction', number of steps to predict from the last\n        date in the index.\n        + If method is 'validation', position plus one of the date in the index,\n        this is done to include the target date in the training set when using\n        pandas iloc with slices.\n\n    \"\"\"\n\n    if method not in [\"prediction\", \"validation\"]:\n        raise ValueError(\"`method` must be 'prediction' or 'validation'.\")\n\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n\n        if method == \"prediction\":\n            if target_date &lt;= last_date:\n                raise ValueError(\n                    \"If `steps` is a date, it must be greater than the last date \"\n                    \"in the index.\"\n                )\n            span_index = pd.date_range(\n                start=last_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index) - 1\n        elif method == \"validation\":\n            first_date = pd.to_datetime(index[0])\n            if target_date &lt; first_date or target_date &gt; last_date:\n                raise ValueError(\n                    \"If `initial_train_size` is a date, it must be greater than \"\n                    \"the first date in the index and less than the last date.\"\n                )\n            span_index = pd.date_range(\n                start=first_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index)\n\n    elif isinstance(date_input, (int, np.integer)):\n        output = date_input\n\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Series | DataFrame</code> <p>pandas Series, pandas DataFrame Exogenous variables.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, list[str]]</code> <p>tuple[pd.DataFrame, list[str]]: exog_direct: pandas DataFrame     Exogenous variables transformed. exog_direct_names: list     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct(\n    exog: pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: pandas Series, pandas DataFrame\n            Exogenous variables.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[pd.DataFrame, list[str]]:\n            exog_direct: pandas DataFrame\n                Exogenous variables transformed.\n            exog_direct_names: list\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i),]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    exog_direct = pd.concat(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct) :]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>ndarray | Series | DataFrame</code> <p>numpy ndarray, pandas Series, pandas DataFrame Exogenous variables, shape(samples,). If exog is a pandas format, the direct exog names are created.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, list[str] | None]</code> <p>tuple[np.ndarray, list[str] | None]: exog_direct: numpy ndarray     Exogenous variables transformed. exog_direct_names: list, None     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray | pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[np.ndarray, list[str] | None]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: numpy ndarray, pandas Series, pandas DataFrame\n            Exogenous variables, shape(samples,). If exog is a pandas format, the\n            direct exog names are created.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[np.ndarray, list[str] | None]:\n            exog_direct: numpy ndarray\n                Exogenous variables transformed.\n            exog_direct_names: list, None\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = [exog[i : n_rows - (steps - 1 - i)] for i in range(steps)]\n    exog_direct = np.concatenate(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.get_style_repr_html","title":"<code>get_style_repr_html(is_fitted=False)</code>","text":"<p>Generate CSS style for HTML representation of the Forecaster.</p> <p>Creates a unique CSS style block with a container ID for rendering forecaster objects in Jupyter notebooks or HTML documents. The styling provides a clean, monospace display with a light gray background.</p> <p>Parameters:</p> Name Type Description Default <code>is_fitted</code> <code>bool</code> <p>Parameter to indicate if the Forecaster has been fitted. Currently not used in styling but reserved for future extensions.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, str]</code> <p>A tuple containing: - style (str): CSS style block as a string with unique container class. - unique_id (str): Unique 8-character ID for the container element.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n&gt;&gt;&gt; print(f\"Container ID: {uid}\")\nContainer ID: a1b2c3d4\n&gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\nStyle contains CSS: True\n</code></pre> <p>Using in HTML rendering:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n&gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n&gt;&gt;&gt; print(\"background-color\" in html)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def get_style_repr_html(is_fitted: bool = False) -&gt; Tuple[str, str]:\n    \"\"\"Generate CSS style for HTML representation of the Forecaster.\n\n    Creates a unique CSS style block with a container ID for rendering\n    forecaster objects in Jupyter notebooks or HTML documents. The styling\n    provides a clean, monospace display with a light gray background.\n\n    Args:\n        is_fitted: Parameter to indicate if the Forecaster has been fitted.\n            Currently not used in styling but reserved for future extensions.\n\n    Returns:\n        tuple: A tuple containing:\n            - style (str): CSS style block as a string with unique container class.\n            - unique_id (str): Unique 8-character ID for the container element.\n\n    Examples:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n        &gt;&gt;&gt; print(f\"Container ID: {uid}\")\n        Container ID: a1b2c3d4\n        &gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\n        Style contains CSS: True\n\n        Using in HTML rendering:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n        &gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n        &gt;&gt;&gt; print(\"background-color\" in html)\n        True\n    \"\"\"\n\n    unique_id = str(uuid.uuid4())[:8]\n    style = f\"\"\"\n    &lt;style&gt;\n        .container-{unique_id} {{\n            font-family: monospace;\n            background-color: #f0f0f0;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n    &lt;/style&gt;\n    \"\"\"\n    return style, unique_id\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_estimator","title":"<code>initialize_estimator(estimator=None, regressor=None)</code>","text":"<p>Helper to handle the deprecation of 'regressor' in favor of 'estimator'. Returns the valid estimator object.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <code>regressor</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None Deprecated. An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>estimator or pipeline compatible with the scikit-learn API The valid estimator object.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_estimator(\n    estimator: object | None = None, regressor: object | None = None\n) -&gt; None:\n    \"\"\"\n    Helper to handle the deprecation of 'regressor' in favor of 'estimator'.\n    Returns the valid estimator object.\n\n    Args:\n        estimator: estimator or pipeline compatible with the scikit-learn API, default None\n            An instance of a estimator or pipeline compatible with the scikit-learn API.\n        regressor: estimator or pipeline compatible with the scikit-learn API, default None\n            Deprecated. An instance of a estimator or pipeline compatible with the\n            scikit-learn API.\n\n    Returns:\n        estimator or pipeline compatible with the scikit-learn API\n            The valid estimator object.\n\n    \"\"\"\n\n    if regressor is not None:\n        warnings.warn(\n            \"The `regressor` argument is deprecated and will be removed in a future \"\n            \"version. Please use `estimator` instead.\",\n            FutureWarning,\n            stacklevel=3,  # Important: to point to the user's code\n        )\n        if estimator is not None:\n            raise ValueError(\n                \"Both `estimator` and `regressor` were provided. Use only `estimator`.\"\n            )\n        return regressor\n\n    return estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_transformer_series","title":"<code>initialize_transformer_series(forecaster_name, series_names_in_, encoding=None, transformer_series=None)</code>","text":"<p>Initialize transformer_series_ attribute for multivariate/multiseries forecasters.</p> <p>Creates a dictionary of transformers for each time series in multivariate or multiseries forecasting. Handles three cases: no transformation (None), same transformer for all series (single object), or different transformers per series (dictionary). Clones transformer objects to avoid overwriting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster using this function. Special handling is applied for 'ForecasterRecursiveMultiSeries'.</p> required <code>series_names_in_</code> <code>list[str]</code> <p>Names of the time series (levels) used during training. These will be the keys in the returned transformer dictionary.</p> required <code>encoding</code> <code>str | None</code> <p>Encoding used to identify different series. Only used for ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level' entry. Defaults to None.</p> <code>None</code> <code>transformer_series</code> <code>object | dict[str, object | None] | None</code> <p>Transformer(s) to apply to series. Can be: - None: No transformation applied - Single transformer object: Same transformer cloned for all series - Dict mapping series names to transformers: Different transformer per series Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, object | None]</code> <p>Dictionary with series names as keys and transformer objects (or None) as values. Transformers are cloned to prevent overwriting.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If transformer_series is a dict and some series_names_in_ are not present in the dict keys (those series get no transformation).</p> <p>Examples:</p> <p>No transformation:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n&gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=series,\n...     transformer_series=None\n... )\n&gt;&gt;&gt; print(result)\n{'series1': None, 'series2': None, 'series3': None}\n</code></pre> <p>Same transformer for all series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=scaler\n... )\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\nTrue\n&gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\nFalse\n</code></pre> <p>Different transformer per series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; transformers = {\n...     'series1': StandardScaler(),\n...     'series2': MinMaxScaler()\n... }\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=transformers\n... )\n&gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\nTrue\n&gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list[str],\n    encoding: str | None = None,\n    transformer_series: object | dict[str, object | None] | None = None,\n) -&gt; dict[str, object | None]:\n    \"\"\"Initialize transformer_series_ attribute for multivariate/multiseries forecasters.\n\n    Creates a dictionary of transformers for each time series in multivariate or\n    multiseries forecasting. Handles three cases: no transformation (None), same\n    transformer for all series (single object), or different transformers per series\n    (dictionary). Clones transformer objects to avoid overwriting.\n\n    Args:\n        forecaster_name: Name of the forecaster using this function. Special handling\n            is applied for 'ForecasterRecursiveMultiSeries'.\n        series_names_in_: Names of the time series (levels) used during training.\n            These will be the keys in the returned transformer dictionary.\n        encoding: Encoding used to identify different series. Only used for\n            ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level'\n            entry. Defaults to None.\n        transformer_series: Transformer(s) to apply to series. Can be:\n            - None: No transformation applied\n            - Single transformer object: Same transformer cloned for all series\n            - Dict mapping series names to transformers: Different transformer per series\n            Defaults to None.\n\n    Returns:\n        dict: Dictionary with series names as keys and transformer objects (or None)\n            as values. Transformers are cloned to prevent overwriting.\n\n    Warnings:\n        IgnoredArgumentWarning: If transformer_series is a dict and some series_names_in_\n            are not present in the dict keys (those series get no transformation).\n\n    Examples:\n        No transformation:\n        &gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n        &gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=series,\n        ...     transformer_series=None\n        ... )\n        &gt;&gt;&gt; print(result)\n        {'series1': None, 'series2': None, 'series3': None}\n\n        Same transformer for all series:\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=scaler\n        ... )\n        &gt;&gt;&gt; len(result)\n        2\n        &gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\n        True\n        &gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\n        False\n\n        Different transformer per series:\n        &gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n        &gt;&gt;&gt; transformers = {\n        ...     'series1': StandardScaler(),\n        ...     'series2': MinMaxScaler()\n        ... }\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=transformers\n        ... )\n        &gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\n        True\n        &gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\n        True\n    \"\"\"\n    from copy import deepcopy\n    from sklearn.base import clone\n    from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n\n    if forecaster_name == \"ForecasterRecursiveMultiSeries\":\n        if encoding is None:\n            series_names_in_ = [\"_unknown_level\"]\n        else:\n            series_names_in_ = series_names_in_ + [\"_unknown_level\"]\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {\n            serie: clone(transformer_series) for serie in series_names_in_\n        }\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            {\n                k: deepcopy(v)\n                for k, v in transformer_series.items()\n                if k in transformer_series_\n            }\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {\"_unknown_level\"}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\",\n                IgnoredArgumentWarning,\n            )\n\n    return transformer_series_\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.initialize_window_features","title":"<code>initialize_window_features(window_features)</code>","text":"<p>Check window_features argument input and generate the corresponding list.</p> <p>This function validates window feature objects and extracts their metadata, ensuring they have the required attributes (window_sizes, features_names) and methods (transform_batch, transform) for proper forecasting operations.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>Any</code> <p>Classes used to create window features. Can be a single object or a list of objects. Each object must have <code>window_sizes</code>, <code>features_names</code> attributes and <code>transform_batch</code>, <code>transform</code> methods.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]</code> <p>A tuple containing: - window_features (list or None): List of classes used to create window features. - window_features_names (list or None): List with all the features names of the window features. - max_size_window_features (int or None): Maximum value of the <code>window_sizes</code> attribute of all classes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>window_features</code> is an empty list.</p> <code>ValueError</code> <p>If a window feature is missing required attributes or methods.</p> <code>TypeError</code> <p>If <code>window_sizes</code> or <code>features_names</code> have incorrect types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 14\n&gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\nNumber of features: 4\n</code></pre> <p>Multiple window features:</p> <pre><code>&gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n&gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 7\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def initialize_window_features(\n    window_features: Any,\n) -&gt; Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]:\n    \"\"\"Check window_features argument input and generate the corresponding list.\n\n    This function validates window feature objects and extracts their metadata,\n    ensuring they have the required attributes (window_sizes, features_names) and\n    methods (transform_batch, transform) for proper forecasting operations.\n\n    Args:\n        window_features: Classes used to create window features. Can be a single\n            object or a list of objects. Each object must have `window_sizes`,\n            `features_names` attributes and `transform_batch`, `transform` methods.\n\n    Returns:\n        tuple: A tuple containing:\n            - window_features (list or None): List of classes used to create window features.\n            - window_features_names (list or None): List with all the features names of the window features.\n            - max_size_window_features (int or None): Maximum value of the `window_sizes` attribute of all classes.\n\n    Raises:\n        ValueError: If `window_features` is an empty list.\n        ValueError: If a window feature is missing required attributes or methods.\n        TypeError: If `window_sizes` or `features_names` have incorrect types.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 14\n        &gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\n        Number of features: 4\n\n        Multiple window features:\n        &gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n        &gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 7\n    \"\"\"\n\n    needed_atts = [\"window_sizes\", \"features_names\"]\n    needed_methods = [\"transform_batch\", \"transform\"]\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) &lt; 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n\n        max_window_sizes = []\n        window_features_names = []\n        needed_atts_set = set(needed_atts)\n        needed_methods_set = set(needed_methods)\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set(dir(wf))\n            if not needed_atts_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not needed_methods_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n\n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n\n            if isinstance(window_sizes, int):\n                if window_sizes &lt; 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws &gt;= 1 for ws in window_sizes\n                ):\n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.predict_multivariate","title":"<code>predict_multivariate(forecasters, steps_ahead, exog=None, show_progress=False)</code>","text":"<p>Generate multi-output predictions using multiple baseline forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>dict</code> <p>Dictionary of fitted forecaster instances (one per target). Keys are target names, values are the fitted forecasters (e.g., ForecasterRecursive, ForecasterEquivalentDate).</p> required <code>steps_ahead</code> <code>int</code> <p>Number of steps to forecast.</p> required <code>exog</code> <code>DataFrame</code> <p>Exogenous variables for prediction. If provided, will be passed to each forecaster's predict method.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar while predicting per target forecaster. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with predictions for all targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n&gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n&gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f1.fit(y=y1)\n&gt;&gt;&gt; f2.fit(y=y2)\n&gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n&gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n&gt;&gt;&gt; predictions\n   target1  target2\n5      6.0     12.0\n6      7.0     14.0\n</code></pre> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def predict_multivariate(\n    forecasters: dict[str, Any],\n    steps_ahead: int,\n    exog: pd.DataFrame | None = None,\n    show_progress: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multi-output predictions using multiple baseline forecasters.\n\n    Args:\n        forecasters (dict): Dictionary of fitted forecaster instances (one per target).\n            Keys are target names, values are the fitted forecasters (e.g.,\n            ForecasterRecursive, ForecasterEquivalentDate).\n        steps_ahead (int): Number of steps to forecast.\n        exog (pd.DataFrame, optional): Exogenous variables for prediction.\n            If provided, will be passed to each forecaster's predict method.\n        show_progress (bool, optional): Show progress bar while predicting\n            per target forecaster. Default: False.\n\n    Returns:\n        pd.DataFrame: DataFrame with predictions for all targets.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n        &gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n        &gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f1.fit(y=y1)\n        &gt;&gt;&gt; f2.fit(y=y2)\n        &gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n        &gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n        &gt;&gt;&gt; predictions\n           target1  target2\n        5      6.0     12.0\n        6      7.0     14.0\n    \"\"\"\n\n    if not forecasters:\n        return pd.DataFrame()\n\n    predictions = {}\n\n    target_iter = forecasters.items()\n    if show_progress and tqdm is not None:\n        target_iter = tqdm(\n            forecasters.items(),\n            desc=\"Predicting targets\",\n            unit=\"model\",\n        )\n\n    for target, forecaster in target_iter:\n        # Generate predictions for this target\n        if exog is not None:\n            pred = forecaster.predict(steps=steps_ahead, exog=exog)\n        else:\n            pred = forecaster.predict(steps=steps_ahead)\n        predictions[target] = pred\n\n    # Combine into a single DataFrame\n    return pd.concat(predictions, axis=1)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.prepare_steps_direct","title":"<code>prepare_steps_direct(max_step, steps=None)</code>","text":"<p>Prepare list of steps to be predicted in Direct Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>max_step</code> <code>int | list[int] | ndarray</code> <p>int, list, numpy ndarray Maximum number of future steps the forecaster will predict when using predict methods.</p> required <code>steps</code> <code>int | list[int] | None</code> <p>int, list, None, default None Predict n steps. The value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list   are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at   initialization.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: Steps to be predicted.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def prepare_steps_direct(\n    max_step: int | list[int] | np.ndarray, steps: int | list[int] | None = None\n) -&gt; list[int]:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Args:\n        max_step: int, list, numpy ndarray\n            Maximum number of future steps the forecaster will predict\n            when using predict methods.\n        steps: int, list, None, default None\n            Predict n steps. The value of `steps` must be less than or equal to the\n            value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list\n              are predicted.\n            - If `None`: As many steps are predicted as were defined at\n              initialization.\n\n    Returns:\n        list[int]:\n            Steps to be predicted.\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps_direct = list(range(1, steps + 1))\n    elif steps is None:\n        if isinstance(max_step, int):\n            steps_direct = list(range(1, max_step + 1))\n        else:\n            steps_direct = [int(s) for s in max_step]\n    elif isinstance(steps, list):\n        steps_direct = []\n        for step in steps:\n            if not isinstance(step, (int, np.integer)):\n                raise TypeError(\n                    f\"`steps` argument must be an int, a list of ints or `None`. \"\n                    f\"Got {type(steps)}.\"\n                )\n            steps_direct.append(int(step))\n\n    return steps_direct\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.select_n_jobs_fit_forecaster","title":"<code>select_n_jobs_fit_forecaster(forecaster_name, estimator)</code>","text":"<p>Select the number of jobs to run in parallel.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(forecaster_name, estimator):\n    \"\"\"\n    Select the number of jobs to run in parallel.\n    \"\"\"\n    import os\n\n    return os.cpu_count() or 1\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_numpy","title":"<code>transform_numpy(array, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of a numpy ndarray with a scikit-learn alike transformer, preprocessor or ColumnTransformer. The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>numpy ndarray Array to be transformed.</p> required <code>transformer</code> <code>object | None</code> <p>scikit-learn alike transformer, preprocessor, or ColumnTransformer. Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <p>fit: bool, default False     Train the transformer before applying it. inverse_transform: bool, default False     Transform back the data to the original representation. This is not available     when using transformers of class scikit-learn ColumnTransformers.</p>"},{"location":"api/forecaster/#spotforecast2_safe.forecaster.utils.transform_numpy--returns","title":"Returns","text":"<p>array_transformed : numpy ndarray     Transformed array.</p> Source code in <code>src/spotforecast2_safe/forecaster/utils.py</code> <pre><code>def transform_numpy(\n    array: np.ndarray,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer. The transformer used must\n    have the following methods: fit, transform, fit_transform and\n    inverse_transform. ColumnTransformers are not allowed since they do not\n    have inverse_transform method.\n\n    Args:\n        array: numpy ndarray\n            Array to be transformed.\n        transformer: scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n            fit_transform and inverse_transform.\n    fit: bool, default False\n        Train the transformer before applying it.\n    inverse_transform: bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n\n    if transformer is None:\n        return array\n\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"`array` argument must be a numpy ndarray. Got {type(array)}\")\n\n    original_ndim = array.ndim\n    original_shape = array.shape\n    reshaped_for_inverse = False\n\n    if original_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n        )\n        if not inverse_transform:\n            if fit:\n                array_transformed = transformer.fit_transform(array)\n            else:\n                array_transformed = transformer.transform(array)\n        else:\n            # Vectorized inverse transformation for 2D arrays with multiple columns.\n            # Reshape to single column, transform, and reshape back.\n            # This is faster than applying the transformer column by column.\n            if array.shape[1] &gt; 1:\n                array = array.reshape(-1, 1)\n                reshaped_for_inverse = True\n            array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if isinstance(array_transformed, (pd.Series, pd.DataFrame)):\n        array_transformed = array_transformed.to_numpy()\n\n    # Reshape back to original shape only if we reshaped for inverse_transform\n    if reshaped_for_inverse:\n        array_transformed = array_transformed.reshape(original_shape)\n\n    if original_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n</code></pre>"},{"location":"api/model_selection/","title":"Model Selection Module","text":"<p>Hyperparameter tuning and model selection using Bayesian, grid, and random search.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection","title":"<code>spotforecast2_safe.model_selection</code>","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.OneStepAheadFold","title":"<code>OneStepAheadFold</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds for one-step-ahead forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>initial_train_size</code> <code>int | str | Timestamp</code> <p>Number of observations used for initial training.</p> <ul> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set.</li> </ul> required <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>class OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Args:\n        initial_train_size (int | str | pd.Timestamp): Number of observations used\n            for initial training.\n\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n        window_size (int, optional): Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation (int, optional): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        return_all_indexes (bool, optional): Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose (bool, optional): Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        initial_train_size (int): Number of observations used for initial training.\n        window_size (int): Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation (int): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        return_all_indexes (bool): Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose (bool): Whether to print information about generated folds.\n    \"\"\"\n\n    def __init__(\n        self,\n        initial_train_size: int | str | pd.Timestamp,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        super().__init__(\n            initial_train_size=initial_train_size,\n            window_size=window_size,\n            differentiation=differentiation,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Initial train size = {self.initial_train_size},\\n\"\n            f\"Window size        = {self.window_size},\\n\"\n            f\"Differentiation    = {self.differentiation},\\n\"\n            f\"Return all indexes = {self.return_all_indexes},\\n\"\n            f\"Verbose            = {self.verbose}\\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html()\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def split(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n        as_pandas: bool = False,\n        externally_fitted: Any = None,\n    ) -&gt; list | pd.DataFrame:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Args:\n            X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or index to split.\n            as_pandas (bool, optional): If True, the folds are returned as a DataFrame.\n                This is useful to visualize the folds in a more interpretable way.\n                Defaults to False.\n            externally_fitted (Any, optional): This argument is not used in this class.\n                It is included for API consistency. Defaults to None.\n\n        Returns:\n            list | pd.DataFrame: A list of lists containing the indices (position) of\n            the fold. The list contains 2 lists with the following information:\n\n            - fold: fold number.\n            - [train_start, train_end]: list with the start and end positions of the\n                training set.\n            - [test_start, test_end]: list with the start and end positions of the test\n                set. These are the observations used to evaluate the forecaster.\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\n                in this fold.\n\n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'test_start',\n            'test_end', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        index = self._extract_index(X)\n\n        self.initial_train_size = date_to_index_position(\n            index=index,\n            date_input=self.initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n\n        fold = [\n            0,\n            [0, self.initial_train_size - 1],\n            [self.initial_train_size, len(X)],\n            True,\n        ]\n\n        if self.verbose:\n            self._print_info(index=index, fold=fold)\n\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        if self.return_all_indexes:\n            fold = [\n                fold[0],\n                [range(fold[1][0], fold[1][1] + 1)],\n                [range(fold[2][0], fold[2][1])],\n                fold[3],\n            ]\n        else:\n            fold = [\n                fold[0],\n                [fold[1][0], fold[1][1] + 1],\n                [fold[2][0], fold[2][1]],\n                fold[3],\n            ]\n\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(\n                    data=[[fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]],\n                    columns=[\n                        \"fold\",\n                        \"train_start\",\n                        \"train_end\",\n                        \"test_start\",\n                        \"test_end\",\n                        \"fit_forecaster\",\n                    ],\n                )\n            else:\n                fold = pd.DataFrame(\n                    data=[fold],\n                    columns=[\"fold\", \"train_index\", \"test_index\", \"fit_forecaster\"],\n                )\n\n        return fold\n\n    def _print_info(self, index: pd.Index, fold: list[list[int]]) -&gt; None:\n        \"\"\"\n        Print information about folds.\n\n        Args:\n            index (pd.Index): Index of the time series data.\n            fold (list): A list of lists containing the indices (position) of the fold.\n        \"\"\"\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        print(f\"Number of observations in train: {initial_train_size}\")\n        if self.differentiation is not None:\n            print(\n                f\"    First {differentiation} observation/s in training set \"\n                f\"are used for differentiation\"\n            )\n        print(f\"Number of observations in test: {test_length}\")\n\n        training_start = index[fold[1][0] + differentiation]\n        training_end = index[fold[1][-1]]\n        test_start = index[fold[2][0]]\n        test_end = index[fold[2][-1] - 1]\n\n        print(f\"Training : {training_start} -- {training_end} (n={initial_train_size})\")\n        print(f\"Test     : {test_start} -- {test_end} (n={test_length})\")\n        print(\"\")\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.OneStepAheadFold.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when printed.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when printed.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Initial train size = {self.initial_train_size},\\n\"\n        f\"Window size        = {self.window_size},\\n\"\n        f\"Differentiation    = {self.differentiation},\\n\"\n        f\"Return all indexes = {self.return_all_indexes},\\n\"\n        f\"Verbose            = {self.verbose}\\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.OneStepAheadFold.split","title":"<code>split(X, as_pandas=False, externally_fitted=None)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series | DataFrame | Index | dict</code> <p>Time series data or index to split.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.</p> <code>False</code> <code>externally_fitted</code> <code>Any</code> <p>This argument is not used in this class. It is included for API consistency. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list | DataFrame</code> <p>list | pd.DataFrame: A list of lists containing the indices (position) of</p> <code>list | DataFrame</code> <p>the fold. The list contains 2 lists with the following information:</p> <code>list | DataFrame</code> <ul> <li>fold: fold number.</li> </ul> <code>list | DataFrame</code> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> </ul> <code>list | DataFrame</code> <ul> <li>fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.</li> </ul> <code>list | DataFrame</code> <p>It is important to note that the returned values are the positions of the</p> <code>list | DataFrame</code> <p>observations and not the actual values of the index, so they can be used to</p> <code>list | DataFrame</code> <p>slice the data directly using iloc.</p> <code>list | DataFrame</code> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the</p> <code>list | DataFrame</code> <p>following columns: 'fold', 'train_start', 'train_end', 'test_start',</p> <code>list | DataFrame</code> <p>'test_end', 'fit_forecaster'.</p> <code>list | DataFrame</code> <p>Following the python convention, the start index is inclusive and the end</p> <code>list | DataFrame</code> <p>index is exclusive. This means that the last index is not included in the</p> <code>list | DataFrame</code> <p>slice.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n    externally_fitted: Any = None,\n) -&gt; list | pd.DataFrame:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Args:\n        X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or index to split.\n        as_pandas (bool, optional): If True, the folds are returned as a DataFrame.\n            This is useful to visualize the folds in a more interpretable way.\n            Defaults to False.\n        externally_fitted (Any, optional): This argument is not used in this class.\n            It is included for API consistency. Defaults to None.\n\n    Returns:\n        list | pd.DataFrame: A list of lists containing the indices (position) of\n        the fold. The list contains 2 lists with the following information:\n\n        - fold: fold number.\n        - [train_start, train_end]: list with the start and end positions of the\n            training set.\n        - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n        - fit_forecaster: boolean indicating whether the forecaster should be fitted\n            in this fold.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'test_start',\n        'test_end', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    index = self._extract_index(X)\n\n    self.initial_train_size = date_to_index_position(\n        index=index,\n        date_input=self.initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n\n    fold = [\n        0,\n        [0, self.initial_train_size - 1],\n        [self.initial_train_size, len(X)],\n        True,\n    ]\n\n    if self.verbose:\n        self._print_info(index=index, fold=fold)\n\n    # NOTE: +1 to prevent iloc pandas from deleting the last observation\n    if self.return_all_indexes:\n        fold = [\n            fold[0],\n            [range(fold[1][0], fold[1][1] + 1)],\n            [range(fold[2][0], fold[2][1])],\n            fold[3],\n        ]\n    else:\n        fold = [\n            fold[0],\n            [fold[1][0], fold[1][1] + 1],\n            [fold[2][0], fold[2][1]],\n            fold[3],\n        ]\n\n    if as_pandas:\n        if not self.return_all_indexes:\n            fold = pd.DataFrame(\n                data=[[fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]],\n                columns=[\n                    \"fold\",\n                    \"train_start\",\n                    \"train_end\",\n                    \"test_start\",\n                    \"test_end\",\n                    \"fit_forecaster\",\n                ],\n            )\n        else:\n            fold = pd.DataFrame(\n                data=[fold],\n                columns=[\"fold\", \"train_index\", \"test_index\", \"fit_forecaster\"],\n            )\n\n    return fold\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.TimeSeriesFold","title":"<code>TimeSeriesFold</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds.</p> <p>When used within a backtesting or hyperparameter search, the arguments 'initial_train_size', 'window_size' and 'differentiation' are not required as they are automatically set by the backtesting or hyperparameter search functions.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> required <code>initial_train_size</code> <code>int | str | Timestamp | None</code> <p>Number of observations used for initial training.</p> <ul> <li>If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</li> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set.</li> </ul> <p>Defaults to None.</p> <code>None</code> <code>fold_stride</code> <code>int | None</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <ul> <li>If <code>None</code>, it defaults to the same value as <code>steps</code>, meaning that folds   are placed back-to-back without overlap.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap and multiple forecasts will   be generated for the same observations.</li> <li>If <code>fold_stride &gt; steps</code>, gaps are left between consecutive test sets.</li> </ul> <p>Defaults to None.</p> <code>None</code> <code>window_size</code> <code>int | None</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int | None</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>refit</code> <code>bool | int</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <p>Defaults to False.</p> <code>False</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold. Defaults to True.</p> <code>True</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set. Defaults to 0.</p> <code>0</code> <code>skip_folds</code> <code>int | list[int] | None</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9. Defaults to None.</p> <code>None</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete. Defaults to True.</p> <code>True</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>steps</code> <p>Number of observations used to be predicted in each fold.</p> <code>initial_train_size</code> <p>Number of observations used for initial training. If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</p> <code>fold_stride</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <code>overlapping_folds</code> <p>Whether the folds overlap.</p> <code>window_size</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <p>Whether to print information about generated folds.</p> <p>Examples:</p> <p>Basic usage with fixed train size:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n&gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n&gt;&gt;&gt; # Create fold splitter\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=10,\n...     initial_train_size=50,\n...     refit=True,\n...     fixed_train_size=True\n... )\n&gt;&gt;&gt; # Get folds\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\nNumber of folds: 4\n</code></pre> <p>Overlapping folds with custom stride:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=30,\n...     initial_train_size=50,\n...     fold_stride=7,\n...     fixed_train_size=False\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\n</code></pre> <p>Return as pandas DataFrame:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n&gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n&gt;&gt;&gt; print(folds_df.columns.tolist())\n['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\n</code></pre> <p>Skip folds for faster evaluation:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=5,\n...     initial_train_size=50,\n...     skip_folds=2\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n</code></pre> Note <p>Returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is <code>X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</code>, the <code>initial_train_size = 3</code>, <code>window_size = 2</code>, <code>steps = 4</code>, and <code>gap = 1</code>, the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].</p> <p>The first element is the fold number, the first list <code>[0, 3]</code> indicates that the training set goes from the first to the third observation. The second list <code>[1, 3]</code> indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list <code>[3, 8]</code> indicates that the test set goes from the fourth to the eighth observation. The fourth list <code>[4, 8]</code> indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean <code>False</code> indicates that the forecaster should not be trained in this fold.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> <p>As an example, with <code>initial_train_size=50</code>, <code>steps=30</code>, and <code>fold_stride=7</code>, the first test fold will cover observations [50, 80), the second fold [57, 87), and the third fold [64, 94). This configuration produces multiple forecasts for the same observations, which is often desirable in rolling-origin evaluation.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_ts_cv.py</code> <pre><code>class TimeSeriesFold(BaseFold):\n    \"\"\"Class to split time series data into train and test folds.\n\n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Args:\n        steps: Number of observations used to be predicted in each fold.\n            This is also commonly referred to as the forecast horizon or test size.\n        initial_train_size: Number of observations used for initial training.\n\n            - If `None` or 0, the initial forecaster is not trained in the first fold.\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n\n            Defaults to None.\n        fold_stride: Number of observations that the start of the test set\n            advances between consecutive folds.\n\n            - If `None`, it defaults to the same value as `steps`, meaning that folds\n              are placed back-to-back without overlap.\n            - If `fold_stride &lt; steps`, test sets overlap and multiple forecasts will\n              be generated for the same observations.\n            - If `fold_stride &gt; steps`, gaps are left between consecutive test sets.\n\n            Defaults to None.\n        window_size: Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation: Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        refit: Whether to refit the forecaster in each fold.\n\n            - If `True`, the forecaster is refitted in each fold.\n            - If `False`, the forecaster is trained only in the first fold.\n            - If an integer, the forecaster is trained in the first fold and then refitted\n              every `refit` folds.\n\n            Defaults to False.\n        fixed_train_size: Whether the training size is fixed or increases\n            in each fold. Defaults to True.\n        gap: Number of observations between the end of the training set\n            and the start of the test set. Defaults to 0.\n        skip_folds: Number of folds to skip.\n\n            - If an integer, every 'skip_folds'-th is returned.\n            - If a list, the indexes of the folds to skip.\n\n            For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n            0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n            8, and 9. Defaults to None.\n        allow_incomplete_fold: Whether to allow the last fold to include\n            fewer observations than `steps`. If `False`, the last fold is excluded if it\n            is incomplete. Defaults to True.\n        return_all_indexes: Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose: Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        steps: Number of observations used to be predicted in each fold.\n        initial_train_size: Number of observations used for initial training.\n            If `None` or 0, the initial forecaster is not trained in the first fold.\n        fold_stride: Number of observations that the start of the test set\n            advances between consecutive folds.\n        overlapping_folds: Whether the folds overlap.\n        window_size: Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation: Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        refit: Whether to refit the forecaster in each fold.\n        fixed_train_size: Whether the training size is fixed or increases in each fold.\n        gap: Number of observations between the end of the training set and the\n            start of the test set.\n        skip_folds: Number of folds to skip.\n        allow_incomplete_fold: Whether to allow the last fold to include fewer\n            observations than `steps`.\n        return_all_indexes: Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose: Whether to print information about generated folds.\n\n    Examples:\n        Basic usage with fixed train size:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n        &gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n        &gt;&gt;&gt; # Create fold splitter\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=10,\n        ...     initial_train_size=50,\n        ...     refit=True,\n        ...     fixed_train_size=True\n        ... )\n        &gt;&gt;&gt; # Get folds\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\n        Number of folds: 4\n\n        Overlapping folds with custom stride:\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=30,\n        ...     initial_train_size=50,\n        ...     fold_stride=7,\n        ...     fixed_train_size=False\n        ... )\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\n\n        Return as pandas DataFrame:\n        &gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n        &gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n        &gt;&gt;&gt; print(folds_df.columns.tolist())\n        ['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\n\n        Skip folds for faster evaluation:\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=5,\n        ...     initial_train_size=50,\n        ...     skip_folds=2\n        ... )\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n\n    Note:\n        Returned values are the positions of the observations and not the actual values of\n        the index, so they can be used to slice the data directly using iloc. For example,\n        if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the\n        `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n        the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].\n\n        The first element is the fold number, the first list `[0, 3]` indicates that\n        the training set goes from the first to the third observation. The second\n        list `[1, 3]` indicates that the last window seen by the forecaster during\n        training goes from the second to the third observation. The third list `[3, 8]`\n        indicates that the test set goes from the fourth to the eighth observation.\n        The fourth list `[4, 8]` indicates that the test set including the gap goes\n        from the fifth to the eighth observation. The boolean `False` indicates that\n        the forecaster should not be trained in this fold.\n\n        Following the python convention, the start index is inclusive and the end index is\n        exclusive. This means that the last index is not included in the slice.\n\n        As an example, with `initial_train_size=50`, `steps=30`, and `fold_stride=7`,\n        the first test fold will cover observations [50, 80), the second fold [57, 87),\n        and the third fold [64, 94). This configuration produces multiple forecasts\n        for the same observations, which is often desirable in rolling-origin\n        evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int,\n        initial_train_size: int | str | pd.Timestamp | None = None,\n        fold_stride: int | None = None,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        refit: bool | int = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: int | list[int] | None = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        super().__init__(\n            steps=steps,\n            initial_train_size=initial_train_size,\n            fold_stride=fold_stride,\n            window_size=window_size,\n            differentiation=differentiation,\n            refit=refit,\n            fixed_train_size=fixed_train_size,\n            gap=gap,\n            skip_folds=skip_folds,\n            allow_incomplete_fold=allow_incomplete_fold,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n        self.steps = steps\n        self.fold_stride = fold_stride if fold_stride is not None else steps\n        self.overlapping_folds = self.fold_stride &lt; self.steps\n        self.refit = refit\n        self.fixed_train_size = fixed_train_size\n        self.gap = gap\n        self.skip_folds = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Information displayed when printed.\n\n        Returns:\n            String representation of the TimeSeriesFold object.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Initial train size    = {self.initial_train_size},\\n\"\n            f\"Steps                 = {self.steps},\\n\"\n            f\"Fold stride           = {self.fold_stride},\\n\"\n            f\"Overlapping folds     = {self.overlapping_folds},\\n\"\n            f\"Window size           = {self.window_size},\\n\"\n            f\"Differentiation       = {self.differentiation},\\n\"\n            f\"Refit                 = {self.refit},\\n\"\n            f\"Fixed train size      = {self.fixed_train_size},\\n\"\n            f\"Gap                   = {self.gap},\\n\"\n            f\"Skip folds            = {self.skip_folds},\\n\"\n            f\"Allow incomplete fold = {self.allow_incomplete_fold},\\n\"\n            f\"Return all indexes    = {self.return_all_indexes},\\n\"\n            f\"Verbose               = {self.verbose}\\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"HTML representation of the object.\n\n        The \"General Information\" section is expanded by default.\n\n        Returns:\n            HTML string representation for Jupyter notebooks.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html()\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Steps:&lt;/strong&gt; {self.steps}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Fold stride:&lt;/strong&gt; {self.fold_stride}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Overlapping folds:&lt;/strong&gt; {self.overlapping_folds}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Refit:&lt;/strong&gt; {self.refit}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Fixed train size:&lt;/strong&gt; {self.fixed_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Gap:&lt;/strong&gt; {self.gap}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Skip folds:&lt;/strong&gt; {self.skip_folds}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Allow incomplete fold:&lt;/strong&gt; {self.allow_incomplete_fold}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def split(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n        as_pandas: bool = False,\n    ) -&gt; list | pd.DataFrame:\n        \"\"\"Split the time series data into train and test folds.\n\n        Args:\n            X: Time series data or index to split. Can be a pandas Series, DataFrame,\n                Index, or a dictionary of Series/DataFrames.\n            as_pandas: If True, the folds are returned as a DataFrame. This is useful\n                to visualize the folds in a more interpretable way. Defaults to False.\n\n        Returns:\n            A list of lists containing the indices (position) for each fold, or a\n            DataFrame if `as_pandas=True`. Each list contains 4 lists and a boolean\n            with the following information:\n\n            - **fold**: fold number.\n            - **[train_start, train_end]**: list with the start and end positions of\n                    the training set.\n            - **[last_window_start, last_window_end]**: list with the start and end\n                    positions of the last window seen by the forecaster during training.\n                    The last window is used to generate the lags use as predictors. If\n                    `differentiation` is included, the interval is extended as many\n                    observations as the differentiation order. If the argument `window_size`\n                    is `None`, this list is empty.\n            - **[test_start, test_end]**: list with the start and end positions of\n                    the test set. These are the observations used to evaluate the forecaster.\n            - **[test_start_with_gap, test_end_with_gap]**: list with the start and\n                    end positions of the test set including the gap. The gap is the number\n                    of observations between the end of the training set and the start of\n                    the test set.\n            - **fit_forecaster**: boolean indicating whether the forecaster should be\n                    fitted in this fold.\n\n        Note:\n            The returned values are the positions of the observations and not the\n            actual values of the index, so they can be used to slice the data directly\n            using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n            'test_end_with_gap', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        window_size_as_date_offset = isinstance(\n            self.window_size, pd.tseries.offsets.DateOffset\n        )\n        if window_size_as_date_offset:\n            # Calculate the window_size in steps. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance between\n            # two dates may not be constant.\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). This is because  \"\n                    f\"the offset (forecaster.offset) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                    f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                    f\"size of `y`.\"\n                )\n\n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError(\n                    \"To use split method when `initial_train_size` is None, \"\n                    \"`window_size` must be an integer greater than 0. \"\n                    \"Although no initial training is done and all data is used to \"\n                    \"evaluate the model, the first `window_size` observations are \"\n                    \"needed to create the initial predictors. Got `window_size` = None.\"\n                )\n            if self.refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                    \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n                )\n            externally_fitted = True\n            self.initial_train_size = self.window_size  # Reset to None later\n        else:\n            if self.window_size is None:\n                warnings.warn(\n                    \"Last window cannot be calculated because `window_size` is None.\",\n                    IgnoredArgumentWarning,\n                )\n            externally_fitted = False\n\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n\n        self.initial_train_size = date_to_index_position(\n            index=index,\n            date_input=self.initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n\n        if window_size_as_date_offset:\n            if self.initial_train_size is not None:\n                if self.initial_train_size &lt; self.window_size:\n                    raise ValueError(\n                        f\"If `initial_train_size` is an integer, it must be greater than \"\n                        f\"the `window_size` of the forecaster ({self.window_size}) \"\n                        f\"and smaller than the length of the series ({len(X)}). If \"\n                        f\"it is a date, it must be within this range of the index.\"\n                    )\n\n        if self.allow_incomplete_fold:\n            # At least one observation after the gap to allow incomplete fold\n            if len(index) &lt;= self.initial_train_size + self.gap:\n                raise ValueError(\n                    f\"The time series must have more than `initial_train_size + gap` \"\n                    f\"observations to create at least one fold.\\n\"\n                    f\"    Time series length: {len(index)}\\n\"\n                    f\"    Required &gt; {self.initial_train_size + self.gap}\\n\"\n                    f\"    initial_train_size: {self.initial_train_size}\\n\"\n                    f\"    gap: {self.gap}\\n\"\n                )\n        else:\n            # At least one complete fold\n            if len(index) &lt; self.initial_train_size + self.gap + self.steps:\n                raise ValueError(\n                    f\"The time series must have at least `initial_train_size + gap + steps` \"\n                    f\"observations to create a minimum of one complete fold \"\n                    f\"(allow_incomplete_fold=False).\\n\"\n                    f\"    Time series length: {len(index)}\\n\"\n                    f\"    Required &gt;= {self.initial_train_size + self.gap + self.steps}\\n\"\n                    f\"    initial_train_size: {self.initial_train_size}\\n\"\n                    f\"    gap: {self.gap}\\n\"\n                    f\"    steps: {self.steps}\\n\"\n                )\n\n        while self.initial_train_size + (i * self.fold_stride) + self.gap &lt; len(index):\n\n            if self.refit:\n                # NOTE: If `fixed_train_size` the train size doesn't increase but\n                # moves by `fold_stride` positions in each iteration. If `False`,\n                # the train size increases by `fold_stride` in each iteration.\n                train_iloc_start = (\n                    i * (self.fold_stride) if self.fixed_train_size else 0\n                )\n                train_iloc_end = self.initial_train_size + i * (self.fold_stride)\n                test_iloc_start = train_iloc_end\n            else:\n                # NOTE: The train size doesn't increase and doesn't move.\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * (self.fold_stride)\n\n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n\n            partitions = [\n                idx[train_iloc_start:train_iloc_end],\n                (\n                    idx[last_window_iloc_start:test_iloc_start]\n                    if self.window_size is not None\n                    else []\n                ),\n                idx[test_iloc_start:test_iloc_end],\n                idx[test_iloc_start + self.gap : test_iloc_end],\n            ]\n            folds.append(partitions)\n            i += 1\n\n        # NOTE: Delete all incomplete folds at the end if not allowed\n        n_removed_folds = 0\n        if not self.allow_incomplete_fold:\n            # NOTE: While folds and the last \"test_index_with_gap\" is incomplete,\n            # calculating len of range objects\n            while folds and len(folds[-1][3]) &lt; self.steps:\n                folds.pop()\n                n_removed_folds += 1\n\n        # Replace partitions inside folds with length 0 with `None`\n        folds = [\n            [partition if len(partition) &gt; 0 else None for partition in fold]\n            for fold in folds\n        ]\n\n        # Create a flag to know whether to train the forecaster\n        if self.refit == 0:\n            self.refit = False\n\n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit):\n                fit_forecaster[i] = True\n\n        for i in range(len(folds)):\n            folds[i].insert(0, i)\n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][1] = folds[i - 1][1]\n\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(\n                    np.arange(0, len(folds)), index_to_keep, assume_unique=True\n                )\n                index_to_skip = [\n                    int(x) for x in index_to_skip\n                ]  # Required since numpy 2.0\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]\n\n        if self.verbose:\n            self._print_info(\n                index=index,\n                folds=folds,\n                externally_fitted=externally_fitted,\n                n_removed_folds=n_removed_folds,\n                index_to_skip=index_to_skip,\n            )\n\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            # NOTE: +1 to prevent iloc pandas from deleting the last observation\n            folds = [\n                [\n                    fold[0],\n                    [fold[1][0], fold[1][-1] + 1],\n                    (\n                        [fold[2][0], fold[2][-1] + 1]\n                        if self.window_size is not None\n                        else []\n                    ),\n                    [fold[3][0], fold[3][-1] + 1],\n                    [fold[4][0], fold[4][-1] + 1],\n                    fold[5],\n                ]\n                for fold in folds\n            ]\n\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][5] = False\n\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[2] = [None, None]\n\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(\n                    data=[\n                        [fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]\n                        for fold in folds\n                    ],\n                    columns=[\n                        \"fold\",\n                        \"train_start\",\n                        \"train_end\",\n                        \"last_window_start\",\n                        \"last_window_end\",\n                        \"test_start\",\n                        \"test_end\",\n                        \"test_start_with_gap\",\n                        \"test_end_with_gap\",\n                        \"fit_forecaster\",\n                    ],\n                )\n            else:\n                folds = pd.DataFrame(\n                    data=folds,\n                    columns=[\n                        \"fold\",\n                        \"train_index\",\n                        \"last_window_index\",\n                        \"test_index\",\n                        \"test_index_with_gap\",\n                        \"fit_forecaster\",\n                    ],\n                )\n\n        return folds\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        folds: list[list[int]],\n        externally_fitted: bool,\n        n_removed_folds: int,\n        index_to_skip: list[int],\n    ) -&gt; None:\n        \"\"\"Print information about folds.\n\n        Args:\n            index: Index of the time series data.\n            folds: A list of lists containing the indices (position) for each fold.\n            externally_fitted: Whether an already trained forecaster is to be used.\n            n_removed_folds: Number of folds removed.\n            index_to_skip: Number of folds skipped.\n        \"\"\"\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        if externally_fitted:\n            print(\n                f\"An already trained forecaster is to be used. Window size: \"\n                f\"{self.window_size}\"\n            )\n        else:\n            if self.differentiation is None:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size}\"\n                )\n            else:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size - self.differentiation}\"\n                )\n                print(\n                    f\"    First {self.differentiation} observation/s in training sets \"\n                    f\"are used for differentiation\"\n                )\n        print(\n            f\"Number of observations used for backtesting: \"\n            f\"{len(index) - self.initial_train_size}\"\n        )\n        print(f\"    Number of folds: {len(folds)}\")\n        print(\n            f\"    Number skipped folds: \"\n            f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n        )\n        print(f\"    Number of steps per fold: {self.steps}\")\n        if self.steps != self.fold_stride:\n            print(\n                f\"    Number of steps to the next fold (fold stride): {self.fold_stride}\"\n            )\n        print(\n            f\"    Number of steps to exclude between last observed data \"\n            f\"(last window) and predictions (gap): {self.gap}\"\n        )\n        if n_removed_folds &gt; 0:\n            print(\n                f\"    The last {n_removed_folds} fold(s) have been excluded \"\n                f\"because they were incomplete.\"\n            )\n\n        if len(folds[-1][4]) &lt; self.steps:\n            print(f\"    Last fold only includes {len(folds[-1][4])} observations.\")\n\n        print(\"\")\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        for i, fold in enumerate(folds):\n            is_fold_skipped = i in index_to_skip\n            has_training = fold[-1] if i != 0 else True\n            training_start = (\n                index[fold[1][0] + differentiation] if fold[1] is not None else None\n            )\n            training_end = index[fold[1][-1]] if fold[1] is not None else None\n            training_length = (\n                len(fold[1]) - differentiation if fold[1] is not None else 0\n            )\n            validation_start = index[fold[4][0]]\n            validation_end = index[fold[4][-1]]\n            validation_length = len(fold[4])\n\n            print(f\"Fold: {i}\")\n            if is_fold_skipped:\n                print(\"    Fold skipped\")\n            elif not externally_fitted and has_training:\n                print(\n                    f\"    Training:   {training_start} -- {training_end}  \"\n                    f\"(n={training_length})\"\n                )\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n            else:\n                print(\"    Training:   No training in this fold\")\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n\n        print(\"\")\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.TimeSeriesFold.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when printed.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the TimeSeriesFold object.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_ts_cv.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Information displayed when printed.\n\n    Returns:\n        String representation of the TimeSeriesFold object.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Initial train size    = {self.initial_train_size},\\n\"\n        f\"Steps                 = {self.steps},\\n\"\n        f\"Fold stride           = {self.fold_stride},\\n\"\n        f\"Overlapping folds     = {self.overlapping_folds},\\n\"\n        f\"Window size           = {self.window_size},\\n\"\n        f\"Differentiation       = {self.differentiation},\\n\"\n        f\"Refit                 = {self.refit},\\n\"\n        f\"Fixed train size      = {self.fixed_train_size},\\n\"\n        f\"Gap                   = {self.gap},\\n\"\n        f\"Skip folds            = {self.skip_folds},\\n\"\n        f\"Allow incomplete fold = {self.allow_incomplete_fold},\\n\"\n        f\"Return all indexes    = {self.return_all_indexes},\\n\"\n        f\"Verbose               = {self.verbose}\\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.TimeSeriesFold.split","title":"<code>split(X, as_pandas=False)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series | DataFrame | Index | dict[str, Series | DataFrame]</code> <p>Time series data or index to split. Can be a pandas Series, DataFrame, Index, or a dictionary of Series/DataFrames.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list | DataFrame</code> <p>A list of lists containing the indices (position) for each fold, or a</p> <code>list | DataFrame</code> <p>DataFrame if <code>as_pandas=True</code>. Each list contains 4 lists and a boolean</p> <code>list | DataFrame</code> <p>with the following information:</p> <code>list | DataFrame</code> <ul> <li>fold: fold number.</li> </ul> <code>list | DataFrame</code> <ul> <li>[train_start, train_end]: list with the start and end positions of     the training set.</li> </ul> <code>list | DataFrame</code> <ul> <li>[last_window_start, last_window_end]: list with the start and end     positions of the last window seen by the forecaster during training.     The last window is used to generate the lags use as predictors. If     <code>differentiation</code> is included, the interval is extended as many     observations as the differentiation order. If the argument <code>window_size</code>     is <code>None</code>, this list is empty.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start, test_end]: list with the start and end positions of     the test set. These are the observations used to evaluate the forecaster.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start_with_gap, test_end_with_gap]: list with the start and     end positions of the test set including the gap. The gap is the number     of observations between the end of the training set and the start of     the test set.</li> </ul> <code>list | DataFrame</code> <ul> <li>fit_forecaster: boolean indicating whether the forecaster should be     fitted in this fold.</li> </ul> Note <p>The returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_ts_cv.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n) -&gt; list | pd.DataFrame:\n    \"\"\"Split the time series data into train and test folds.\n\n    Args:\n        X: Time series data or index to split. Can be a pandas Series, DataFrame,\n            Index, or a dictionary of Series/DataFrames.\n        as_pandas: If True, the folds are returned as a DataFrame. This is useful\n            to visualize the folds in a more interpretable way. Defaults to False.\n\n    Returns:\n        A list of lists containing the indices (position) for each fold, or a\n        DataFrame if `as_pandas=True`. Each list contains 4 lists and a boolean\n        with the following information:\n\n        - **fold**: fold number.\n        - **[train_start, train_end]**: list with the start and end positions of\n                the training set.\n        - **[last_window_start, last_window_end]**: list with the start and end\n                positions of the last window seen by the forecaster during training.\n                The last window is used to generate the lags use as predictors. If\n                `differentiation` is included, the interval is extended as many\n                observations as the differentiation order. If the argument `window_size`\n                is `None`, this list is empty.\n        - **[test_start, test_end]**: list with the start and end positions of\n                the test set. These are the observations used to evaluate the forecaster.\n        - **[test_start_with_gap, test_end_with_gap]**: list with the start and\n                end positions of the test set including the gap. The gap is the number\n                of observations between the end of the training set and the start of\n                the test set.\n        - **fit_forecaster**: boolean indicating whether the forecaster should be\n                fitted in this fold.\n\n    Note:\n        The returned values are the positions of the observations and not the\n        actual values of the index, so they can be used to slice the data directly\n        using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n        'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n        'test_end_with_gap', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    window_size_as_date_offset = isinstance(\n        self.window_size, pd.tseries.offsets.DateOffset\n    )\n    if window_size_as_date_offset:\n        # Calculate the window_size in steps. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance between\n        # two dates may not be constant.\n        first_valid_index = X.index[-1] - self.window_size\n        try:\n            window_size_idx_start = X.index.get_loc(first_valid_index)\n            window_size_idx_end = X.index.get_loc(X.index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset (forecaster.offset) is larger than the available \"\n                f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                f\"size of `y`.\"\n            )\n\n    if self.initial_train_size is None:\n        if self.window_size is None:\n            raise ValueError(\n                \"To use split method when `initial_train_size` is None, \"\n                \"`window_size` must be an integer greater than 0. \"\n                \"Although no initial training is done and all data is used to \"\n                \"evaluate the model, the first `window_size` observations are \"\n                \"needed to create the initial predictors. Got `window_size` = None.\"\n            )\n        if self.refit:\n            raise ValueError(\n                \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n            )\n        externally_fitted = True\n        self.initial_train_size = self.window_size  # Reset to None later\n    else:\n        if self.window_size is None:\n            warnings.warn(\n                \"Last window cannot be calculated because `window_size` is None.\",\n                IgnoredArgumentWarning,\n            )\n        externally_fitted = False\n\n    index = self._extract_index(X)\n    idx = range(len(index))\n    folds = []\n    i = 0\n\n    self.initial_train_size = date_to_index_position(\n        index=index,\n        date_input=self.initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n\n    if window_size_as_date_offset:\n        if self.initial_train_size is not None:\n            if self.initial_train_size &lt; self.window_size:\n                raise ValueError(\n                    f\"If `initial_train_size` is an integer, it must be greater than \"\n                    f\"the `window_size` of the forecaster ({self.window_size}) \"\n                    f\"and smaller than the length of the series ({len(X)}). If \"\n                    f\"it is a date, it must be within this range of the index.\"\n                )\n\n    if self.allow_incomplete_fold:\n        # At least one observation after the gap to allow incomplete fold\n        if len(index) &lt;= self.initial_train_size + self.gap:\n            raise ValueError(\n                f\"The time series must have more than `initial_train_size + gap` \"\n                f\"observations to create at least one fold.\\n\"\n                f\"    Time series length: {len(index)}\\n\"\n                f\"    Required &gt; {self.initial_train_size + self.gap}\\n\"\n                f\"    initial_train_size: {self.initial_train_size}\\n\"\n                f\"    gap: {self.gap}\\n\"\n            )\n    else:\n        # At least one complete fold\n        if len(index) &lt; self.initial_train_size + self.gap + self.steps:\n            raise ValueError(\n                f\"The time series must have at least `initial_train_size + gap + steps` \"\n                f\"observations to create a minimum of one complete fold \"\n                f\"(allow_incomplete_fold=False).\\n\"\n                f\"    Time series length: {len(index)}\\n\"\n                f\"    Required &gt;= {self.initial_train_size + self.gap + self.steps}\\n\"\n                f\"    initial_train_size: {self.initial_train_size}\\n\"\n                f\"    gap: {self.gap}\\n\"\n                f\"    steps: {self.steps}\\n\"\n            )\n\n    while self.initial_train_size + (i * self.fold_stride) + self.gap &lt; len(index):\n\n        if self.refit:\n            # NOTE: If `fixed_train_size` the train size doesn't increase but\n            # moves by `fold_stride` positions in each iteration. If `False`,\n            # the train size increases by `fold_stride` in each iteration.\n            train_iloc_start = (\n                i * (self.fold_stride) if self.fixed_train_size else 0\n            )\n            train_iloc_end = self.initial_train_size + i * (self.fold_stride)\n            test_iloc_start = train_iloc_end\n        else:\n            # NOTE: The train size doesn't increase and doesn't move.\n            train_iloc_start = 0\n            train_iloc_end = self.initial_train_size\n            test_iloc_start = self.initial_train_size + i * (self.fold_stride)\n\n        if self.window_size is not None:\n            last_window_iloc_start = test_iloc_start - self.window_size\n\n        test_iloc_end = test_iloc_start + self.gap + self.steps\n\n        partitions = [\n            idx[train_iloc_start:train_iloc_end],\n            (\n                idx[last_window_iloc_start:test_iloc_start]\n                if self.window_size is not None\n                else []\n            ),\n            idx[test_iloc_start:test_iloc_end],\n            idx[test_iloc_start + self.gap : test_iloc_end],\n        ]\n        folds.append(partitions)\n        i += 1\n\n    # NOTE: Delete all incomplete folds at the end if not allowed\n    n_removed_folds = 0\n    if not self.allow_incomplete_fold:\n        # NOTE: While folds and the last \"test_index_with_gap\" is incomplete,\n        # calculating len of range objects\n        while folds and len(folds[-1][3]) &lt; self.steps:\n            folds.pop()\n            n_removed_folds += 1\n\n    # Replace partitions inside folds with length 0 with `None`\n    folds = [\n        [partition if len(partition) &gt; 0 else None for partition in fold]\n        for fold in folds\n    ]\n\n    # Create a flag to know whether to train the forecaster\n    if self.refit == 0:\n        self.refit = False\n\n    if isinstance(self.refit, bool):\n        fit_forecaster = [self.refit] * len(folds)\n        fit_forecaster[0] = True\n    else:\n        fit_forecaster = [False] * len(folds)\n        for i in range(0, len(fit_forecaster), self.refit):\n            fit_forecaster[i] = True\n\n    for i in range(len(folds)):\n        folds[i].insert(0, i)\n        folds[i].append(fit_forecaster[i])\n        if fit_forecaster[i] is False:\n            folds[i][1] = folds[i - 1][1]\n\n    index_to_skip = []\n    if self.skip_folds is not None:\n        if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n            index_to_keep = np.arange(0, len(folds), self.skip_folds)\n            index_to_skip = np.setdiff1d(\n                np.arange(0, len(folds)), index_to_keep, assume_unique=True\n            )\n            index_to_skip = [\n                int(x) for x in index_to_skip\n            ]  # Required since numpy 2.0\n        if isinstance(self.skip_folds, list):\n            index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]\n\n    if self.verbose:\n        self._print_info(\n            index=index,\n            folds=folds,\n            externally_fitted=externally_fitted,\n            n_removed_folds=n_removed_folds,\n            index_to_skip=index_to_skip,\n        )\n\n    folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n    if not self.return_all_indexes:\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        folds = [\n            [\n                fold[0],\n                [fold[1][0], fold[1][-1] + 1],\n                (\n                    [fold[2][0], fold[2][-1] + 1]\n                    if self.window_size is not None\n                    else []\n                ),\n                [fold[3][0], fold[3][-1] + 1],\n                [fold[4][0], fold[4][-1] + 1],\n                fold[5],\n            ]\n            for fold in folds\n        ]\n\n    if externally_fitted:\n        self.initial_train_size = None\n        folds[0][5] = False\n\n    if as_pandas:\n        if self.window_size is None:\n            for fold in folds:\n                fold[2] = [None, None]\n\n        if not self.return_all_indexes:\n            folds = pd.DataFrame(\n                data=[\n                    [fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]\n                    for fold in folds\n                ],\n                columns=[\n                    \"fold\",\n                    \"train_start\",\n                    \"train_end\",\n                    \"last_window_start\",\n                    \"last_window_end\",\n                    \"test_start\",\n                    \"test_end\",\n                    \"test_start_with_gap\",\n                    \"test_end_with_gap\",\n                    \"fit_forecaster\",\n                ],\n            )\n        else:\n            folds = pd.DataFrame(\n                data=folds,\n                columns=[\n                    \"fold\",\n                    \"train_index\",\n                    \"last_window_index\",\n                    \"test_index\",\n                    \"test_index_with_gap\",\n                    \"fit_forecaster\",\n                ],\n            )\n\n    return folds\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.backtesting_forecaster","title":"<code>backtesting_forecaster(forecaster, y, cv, metric, exog=None, interval=None, interval_method='bootstrapping', n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123, return_predictors=False, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False)</code>","text":"<p>Backtesting of forecaster model following the folds generated by the TimeSeriesFold class and using the metric(s) provided.</p> <p>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is set to <code>None</code> in the TimeSeriesFold class, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</p> <p>A copy of the original forecaster is created so that it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate)</code> <p>Forecaster model.</p> required <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>str | Callable | list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>str</code>: {'mean_squared_error', 'mean_absolute_error',   'mean_absolute_percentage_error', 'mean_squared_log_error',   'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code>   (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>Series | DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i]. Defaults to None.</p> <code>None</code> <code>interval</code> <code>float | list | tuple | str | object</code> <p>Specifies whether probabilistic predictions should be estimated and the method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must   be between 0 and 100 inclusive. For example, a 95% confidence interval can   be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10,   50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be   generated.</li> <li>If scipy.stats distribution object, the distribution parameters will   be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated. Defaults to None.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction intervals.</li> <li>'conformal': Employs the conformal prediction split method for   interval estimation. Defaults to 'bootstrapping'.</li> </ul> <code>'bootstrapping'</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned. Defaults to False.</p> <code>False</code> <code>n_jobs</code> <code>int | str</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function <code>skforecast.utils.select_n_jobs_backtesting</code>. Defaults to 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used for backtesting. Defaults to False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, spotforecast warnings will be suppressed during the backtesting process. See <code>spotforecast.exceptions.warn_skforecast_categories</code> for more information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <ul> <li>metric_values: Value(s) of the metric(s).</li> <li> <p>backtest_predictions: Value of predictions. The DataFrame includes   the following columns:</p> </li> <li> <p>fold: Indicates the fold number where the prediction was made.</p> </li> <li>pred: Predicted values for the corresponding series and time steps.</li> </ul> <p>If <code>interval</code> is not <code>None</code>, additional columns are included depending   on the method:</p> <ul> <li>For <code>float</code>: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> of 2 elements: Columns <code>lower_bound</code> and     <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> with multiple percentiles: One column per     percentile (e.g., <code>p_10</code>, <code>p_50</code>, <code>p_90</code>).</li> <li>For <code>'bootstrapping'</code>: One column per bootstrapping iteration     (e.g., <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n</code>).</li> <li>For <code>scipy.stats</code> distribution objects: One column for each     estimated parameter of the distribution (e.g., <code>loc</code>, <code>scale</code>).</li> </ul> <p>If <code>return_predictors</code> is <code>True</code>, one column per predictor is created.</p> <p>Depending on the relation between <code>steps</code> and <code>fold_stride</code>, the output   may include repeated indexes (if <code>fold_stride &lt; steps</code>) or gaps   (if <code>fold_stride &gt; steps</code>). See Notes below for more details.</p> Notes <p>Note on <code>fold_stride</code> vs. <code>steps</code>:</p> <ul> <li>If <code>fold_stride == steps</code>, test sets are placed back-to-back without overlap.   Each observation appears only once in the output DataFrame, so the   index is unique.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap. Multiple forecasts are   generated for the same observations and, therefore, the output   DataFrame contains repeated indexes.</li> <li>If <code>fold_stride &gt; steps</code>, there are gaps between consecutive test sets.   Some observations in the series will not have associated predictions,   so the output DataFrame has non-contiguous indexes.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(random_state=123),\n...     lags=2\n... )\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=2,\n...     initial_train_size=5,\n...     refit=False\n... )\n&gt;&gt;&gt; metric_values, backtest_predictions = backtesting_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     metric='mean_squared_error'\n... )\n&gt;&gt;&gt; metric_values\n   mean_squared_error\n0            0.201334\n&gt;&gt;&gt; backtest_predictions\n   fold  pred\n5     0  5.18\n6     0  6.10\n7     1  7.36\n8     1  8.40\n9     2  9.31\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/validation.py</code> <pre><code>def backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = \"bootstrapping\",\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n\n    A copy of the original forecaster is created so that it is not modified during\n    the process.\n\n    Args:\n        forecaster (ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate):\n            Forecaster model.\n        y (pd.Series): Training time series.\n        cv (TimeSeriesFold): TimeSeriesFold object with the information needed to\n            split the data into folds.\n        metric (str | Callable | list): Metric used to quantify the goodness of fit\n            of the model.\n\n            - If `str`: {'mean_squared_error', 'mean_absolute_error',\n              'mean_absolute_percentage_error', 'mean_squared_log_error',\n              'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n            - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n              (Optional) that returns a float.\n            - If `list`: List containing multiple strings and/or Callables.\n        exog (pd.Series | pd.DataFrame, optional): Exogenous variable/s included as\n            predictor/s. Must have the same number of observations as `y` and should\n            be aligned so that y[i] is regressed on exog[i]. Defaults to None.\n        interval (float | list | tuple | str | object, optional): Specifies whether\n            probabilistic predictions should be estimated and the method to use.\n            The following options are supported:\n\n            - If `float`, represents the nominal (expected) coverage (between 0 and 1).\n            For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n            - If `list` or `tuple`: Sequence of percentiles to compute, each value must\n              be between 0 and 100 inclusive. For example, a 95% confidence interval can\n              be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10,\n              50 and 90) as `interval = [10, 50, 90]`.\n            - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be\n              generated.\n            - If scipy.stats distribution object, the distribution parameters will\n              be estimated for each prediction.\n            - If None, no probabilistic predictions are estimated.\n            Defaults to None.\n        interval_method (str, optional): Technique used to estimate prediction\n            intervals. Available options:\n\n            - 'bootstrapping': Bootstrapping is used to generate prediction intervals.\n            - 'conformal': Employs the conformal prediction split method for\n              interval estimation.\n            Defaults to 'bootstrapping'.\n        n_boot (int, optional): Number of bootstrapping iterations to perform when\n            estimating prediction intervals. Defaults to 250.\n        use_in_sample_residuals (bool, optional): If `True`, residuals from the\n            training data are used as proxy of prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals (bool, optional): If `True`, residuals are selected\n            based on the predicted values (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state (int, optional): Seed for the random number generator to\n            ensure reproducibility. Defaults to 123.\n        return_predictors (bool, optional): If `True`, the predictors used to make\n            the predictions are also returned. Defaults to False.\n        n_jobs (int | str, optional): The number of jobs to run in parallel. If `-1`,\n            then the number of jobs is set to the number of cores. If 'auto', `n_jobs`\n            is set using the function `skforecast.utils.select_n_jobs_backtesting`.\n            Defaults to 'auto'.\n        verbose (bool, optional): Print number of folds and index of training and\n            validation sets used for backtesting. Defaults to False.\n        show_progress (bool, optional): Whether to show a progress bar.\n            Defaults to True.\n        suppress_warnings (bool, optional): If `True`, spotforecast warnings will be\n            suppressed during the backtesting process. See\n            `spotforecast.exceptions.warn_skforecast_categories` for more information.\n            Defaults to False.\n\n    Returns:\n        tuple (pd.DataFrame, pd.DataFrame):\n            - metric_values: Value(s) of the metric(s).\n            - backtest_predictions: Value of predictions. The DataFrame includes\n              the following columns:\n\n              - fold: Indicates the fold number where the prediction was made.\n              - pred: Predicted values for the corresponding series and time steps.\n\n              If `interval` is not `None`, additional columns are included depending\n              on the method:\n\n              - For `float`: Columns `lower_bound` and `upper_bound`.\n              - For `list` or `tuple` of 2 elements: Columns `lower_bound` and\n                `upper_bound`.\n              - For `list` or `tuple` with multiple percentiles: One column per\n                percentile (e.g., `p_10`, `p_50`, `p_90`).\n              - For `'bootstrapping'`: One column per bootstrapping iteration\n                (e.g., `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n`).\n              - For `scipy.stats` distribution objects: One column for each\n                estimated parameter of the distribution (e.g., `loc`, `scale`).\n\n              If `return_predictors` is `True`, one column per predictor is created.\n\n              Depending on the relation between `steps` and `fold_stride`, the output\n              may include repeated indexes (if `fold_stride &lt; steps`) or gaps\n              (if `fold_stride &gt; steps`). See Notes below for more details.\n\n    Notes:\n        Note on `fold_stride` vs. `steps`:\n\n        - If `fold_stride == steps`, test sets are placed back-to-back without overlap.\n          Each observation appears only once in the output DataFrame, so the\n          index is unique.\n        - If `fold_stride &lt; steps`, test sets overlap. Multiple forecasts are\n          generated for the same observations and, therefore, the output\n          DataFrame contains repeated indexes.\n        - If `fold_stride &gt; steps`, there are gaps between consecutive test sets.\n          Some observations in the series will not have associated predictions,\n          so the output DataFrame has non-contiguous indexes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(random_state=123),\n        ...     lags=2\n        ... )\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=2,\n        ...     initial_train_size=5,\n        ...     refit=False\n        ... )\n        &gt;&gt;&gt; metric_values, backtest_predictions = backtesting_forecaster(\n        ...     forecaster=forecaster,\n        ...     y=y,\n        ...     cv=cv,\n        ...     metric='mean_squared_error'\n        ... )\n        &gt;&gt;&gt; metric_values\n           mean_squared_error\n        0            0.201334\n        &gt;&gt;&gt; backtest_predictions\n           fold  pred\n        5     0  5.18\n        6     0  6.10\n        7     1  7.36\n        8     1  8.40\n        9     2  9.31\n    \"\"\"\n\n    forecaters_allowed = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterEquivalentDate\",\n        \"ForecasterRecursiveClassifier\",\n    ]\n\n    if type(forecaster).__name__ not in forecaters_allowed:\n        raise TypeError(\n            f\"`forecaster` must be of type {forecaters_allowed}. For all other \"\n            f\"types of forecasters use the other functions available in the \"\n            f\"`model_selection` module.\"\n        )\n\n    check_backtesting_input(\n        forecaster=forecaster,\n        cv=cv,\n        y=y,\n        metric=metric,\n        interval=interval,\n        interval_method=interval_method,\n        n_boot=n_boot,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        random_state=random_state,\n        return_predictors=return_predictors,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n    )\n\n    metric_values, backtest_predictions = _backtesting_forecaster(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        metric=metric,\n        exog=exog,\n        interval=interval,\n        interval_method=interval_method,\n        n_boot=n_boot,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        random_state=random_state,\n        return_predictors=return_predictors,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection/#bayesian-search","title":"Bayesian Search","text":""},{"location":"api/model_selection/#bayesian_search","title":"bayesian_search","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.bayesian_search","title":"<code>spotforecast2_safe.model_selection.bayesian_search</code>","text":"<p>Bayesian hyperparameter search functions for forecasters using Optuna.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.bayesian_search.bayesian_search_forecaster","title":"<code>bayesian_search_forecaster(forecaster, y, cv, search_space, metric, exog=None, n_trials=10, random_state=123, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None, kwargs_create_study={}, kwargs_study_optimize={})</code>","text":"<p>Bayesian hyperparameter optimization for a Forecaster using Optuna.</p> <p>Performs Bayesian hyperparameter search using the Optuna library for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model. Can be ForecasterRecursive, ForecasterDirect, or any compatible forecaster class.</p> required <code>y</code> <code>Series</code> <p>Training time series values. Must be a pandas Series with a datetime or numeric index.</p> required <code>cv</code> <code>TimeSeriesFold | OneStepAheadFold</code> <p>Cross-validation strategy with information needed to split the data into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.</p> required <code>search_space</code> <code>Callable</code> <p>Callable function with argument <code>trial</code> that returns a dictionary with parameter names (str) as keys and Trial objects from optuna (trial.suggest_float, trial.suggest_int, trial.suggest_categorical) as values. Can optionally include 'lags' key to search over different lag configurations.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric(s) to quantify model goodness of fit. Can be: - str: One of 'mean_squared_error', 'mean_absolute_error',   'mean_absolute_percentage_error', 'mean_squared_log_error',   'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'. - Callable: Function with arguments (y_true, y_pred) or   (y_true, y_pred, y_train) that returns a float. - list: List containing multiple strings and/or Callables.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictors. Must have the same number of observations as <code>y</code> and aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings sampled during optimization. Default is 10.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Seed for sampling reproducibility. When passing a custom sampler in kwargs_create_study, set the seed within the sampler (e.g., {'sampler': TPESampler(seed=145)}). Default is 123.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>If True, refit the forecaster using the best parameters found on the whole dataset at the end. Default is True.</p> <code>True</code> <code>n_jobs</code> <code>int | str</code> <p>Number of parallel jobs. If -1, uses all cores. If 'auto', uses spotforecast.skforecast.utils.select_n_jobs_backtesting to automatically determine the number of jobs. Default is 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>If True, print number of folds used for cross-validation. Default is False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show an Optuna progress bar during optimization. Default is True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress spotforecast warnings during hyperparameter search. Default is False.</p> <code>False</code> <code>output_file</code> <code>str | None</code> <p>Filename or full path to save results as TSV. If None, results are not saved to file. Default is None.</p> <code>None</code> <code>kwargs_create_study</code> <code>dict</code> <p>Additional keyword arguments passed to optuna.create_study(). If not specified, direction is set to 'minimize' and TPESampler(seed=123) is used. Default is {}.</p> <code>{}</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Additional keyword arguments passed to study.optimize(). Default is {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, object]</code> <p>tuple[pd.DataFrame, object]: A tuple containing: - results: DataFrame with columns 'lags', 'params', metric values,   and individual parameter columns. Sorted by the first metric. - best_trial: Best optimization result as an optuna.FrozenTrial   object containing the best parameters and metric value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If exog length doesn't match y length when return_best=True.</p> <code>TypeError</code> <p>If cv is not an instance of TimeSeriesFold or OneStepAheadFold.</p> <code>ValueError</code> <p>If metric list contains duplicate metric names.</p> Source code in <code>src/spotforecast2_safe/model_selection/bayesian_search.py</code> <pre><code>def bayesian_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    search_space: Callable,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_trials: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n    kwargs_create_study: dict = {},\n    kwargs_study_optimize: dict = {},\n) -&gt; tuple[pd.DataFrame, object]:\n    \"\"\"\n    Bayesian hyperparameter optimization for a Forecaster using Optuna.\n\n    Performs Bayesian hyperparameter search using the Optuna library for a\n    Forecaster object. Validation is done using time series backtesting with\n    the provided cross-validation strategy.\n\n    Args:\n        forecaster: Forecaster model. Can be ForecasterRecursive, ForecasterDirect,\n            or any compatible forecaster class.\n        y: Training time series values. Must be a pandas Series with a\n            datetime or numeric index.\n        cv: Cross-validation strategy with information needed to split the data\n            into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.\n        search_space: Callable function with argument `trial` that returns\n            a dictionary with parameter names (str) as keys and Trial objects\n            from optuna (trial.suggest_float, trial.suggest_int,\n            trial.suggest_categorical) as values. Can optionally include 'lags'\n            key to search over different lag configurations.\n        metric: Metric(s) to quantify model goodness of fit. Can be:\n            - str: One of 'mean_squared_error', 'mean_absolute_error',\n              'mean_absolute_percentage_error', 'mean_squared_log_error',\n              'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'.\n            - Callable: Function with arguments (y_true, y_pred) or\n              (y_true, y_pred, y_train) that returns a float.\n            - list: List containing multiple strings and/or Callables.\n        exog: Exogenous variable(s) included as predictors. Must have the\n            same number of observations as `y` and aligned so that y[i] is\n            regressed on exog[i]. Default is None.\n        n_trials: Number of parameter settings sampled during optimization.\n            Default is 10.\n        random_state: Seed for sampling reproducibility. When passing a custom\n            sampler in kwargs_create_study, set the seed within the sampler\n            (e.g., {'sampler': TPESampler(seed=145)}). Default is 123.\n        return_best: If True, refit the forecaster using the best parameters\n            found on the whole dataset at the end. Default is True.\n        n_jobs: Number of parallel jobs. If -1, uses all cores. If 'auto',\n            uses spotforecast.skforecast.utils.select_n_jobs_backtesting to\n            automatically determine the number of jobs. Default is 'auto'.\n        verbose: If True, print number of folds used for cross-validation.\n            Default is False.\n        show_progress: Whether to show an Optuna progress bar during\n            optimization. Default is True.\n        suppress_warnings: If True, suppress spotforecast warnings during\n            hyperparameter search. Default is False.\n        output_file: Filename or full path to save results as TSV. If None,\n            results are not saved to file. Default is None.\n        kwargs_create_study: Additional keyword arguments passed to\n            optuna.create_study(). If not specified, direction is set to\n            'minimize' and TPESampler(seed=123) is used. Default is {}.\n        kwargs_study_optimize: Additional keyword arguments passed to\n            study.optimize(). Default is {}.\n\n    Returns:\n        tuple[pd.DataFrame, object]: A tuple containing:\n            - results: DataFrame with columns 'lags', 'params', metric values,\n              and individual parameter columns. Sorted by the first metric.\n            - best_trial: Best optimization result as an optuna.FrozenTrial\n              object containing the best parameters and metric value.\n\n    Raises:\n        ValueError: If exog length doesn't match y length when return_best=True.\n        TypeError: If cv is not an instance of TimeSeriesFold or OneStepAheadFold.\n        ValueError: If metric list contains duplicate metric names.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            f\"`exog` must have same number of samples as `y`. \"\n            f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\"\n        )\n\n    results, best_trial = _bayesian_search_optuna(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        exog=exog,\n        search_space=search_space,\n        metric=metric,\n        n_trials=n_trials,\n        random_state=random_state,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n        kwargs_create_study=kwargs_create_study,\n        kwargs_study_optimize=kwargs_study_optimize,\n    )\n\n    return results, best_trial\n</code></pre>"},{"location":"api/model_selection/#grid-search","title":"Grid Search","text":""},{"location":"api/model_selection/#grid_search","title":"grid_search","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.grid_search","title":"<code>spotforecast2_safe.model_selection.grid_search</code>","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.grid_search.grid_search_forecaster","title":"<code>grid_search_forecaster(forecaster, y, cv, param_grid, metric, exog=None, lags_grid=None, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None)</code>","text":"<p>Exhaustive grid search over parameter values for a Forecaster.</p> Source code in <code>src/spotforecast2_safe/model_selection/grid_search.py</code> <pre><code>def grid_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_grid: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive grid search over parameter values for a Forecaster.\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        param_grid=param_grid,\n        metric=metric,\n        exog=exog,\n        lags_grid=lags_grid,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection/#random-search","title":"Random Search","text":""},{"location":"api/model_selection/#random_search","title":"random_search","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.random_search","title":"<code>spotforecast2_safe.model_selection.random_search</code>","text":"<p>Random search hyperparameter optimization for forecasters.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.random_search.random_search_forecaster","title":"<code>random_search_forecaster(forecaster, y, cv, param_distributions, metric, exog=None, lags_grid=None, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None)</code>","text":"<p>Random search over parameter distributions for a Forecaster.</p> <p>Performs random sampling of parameter settings from distributions for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy. This is more efficient than grid search when exploring large parameter spaces.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model (ForecasterRecursive or ForecasterDirect).</p> required <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold | OneStepAheadFold</code> <p>Cross-validation strategy (TimeSeriesFold or OneStepAheadFold) with information needed to split the data into folds.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameter names (str) as keys and distributions or lists of parameters to try as values. Use scipy.stats distributions for continuous parameters.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric(s) to quantify model goodness of fit. If str: 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'. If Callable: Function with arguments (y_true, y_pred, y_train) that returns a float. If list: Multiple strings and/or Callables.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictors. Must have the same number of observations as y and aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <code>lags_grid</code> <code>list[int | list[int] | ndarray[int] | range[int]] | dict[str, list[int | list[int] | ndarray[int] | range[int]]] | None</code> <p>Lists of lags to try. Can be int, lists, numpy ndarray, or range objects. If dict, keys are used as labels in results DataFrame. Default is None.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings sampled per lags configuration. Trades off runtime vs solution quality. Default is 10.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Seed for random sampling for reproducible output. Default is 123.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>If True, refit the forecaster using best parameters on the whole dataset. Default is True.</p> <code>True</code> <code>n_jobs</code> <code>int | str</code> <p>Number of jobs to run in parallel. If -1, uses all cores. If 'auto', uses select_n_jobs_backtesting. Default is 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>If True, print number of folds used for cv. Default is False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Default is True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress spotforecast warnings during hyperparameter search. Default is False.</p> <code>False</code> <code>output_file</code> <code>str | None</code> <p>Filename or full path to save results as TSV. If None, results are not saved to file. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Results for each parameter combination with columns: lags (lags</p> <code>DataFrame</code> <p>configuration), lags_label (descriptive label), params (parameters</p> <code>DataFrame</code> <p>configuration), metric (metric value), and additional columns with</p> <code>DataFrame</code> <p>param=value pairs.</p> <p>Examples:</p> <p>Basic random search with continuous parameter distributions:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from scipy.stats import uniform\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from spotforecast2.model_selection.random_search import random_search_forecaster\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(123)\n&gt;&gt;&gt; y = pd.Series(np.random.randn(50), name='y')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up forecaster and cross-validation\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; cv = TimeSeriesFold(steps=3, initial_train_size=20, refit=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define parameter distributions with scipy.stats\n&gt;&gt;&gt; param_distributions = {\n...     'estimator__alpha': uniform(0.1, 10.0)  # Uniform between 0.1 and 10.1\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run random search\n&gt;&gt;&gt; results = random_search_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     param_distributions=param_distributions,\n...     metric='mean_squared_error',\n...     n_iter=5,\n...     random_state=42,\n...     return_best=False,\n...     verbose=False,\n...     show_progress=False\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check results\n&gt;&gt;&gt; print(results.shape[0])\n5\n&gt;&gt;&gt; print('estimator__alpha' in results.columns)\nTrue\n&gt;&gt;&gt; print('mean_squared_error' in results.columns)\nTrue\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/random_search.py</code> <pre><code>def random_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_distributions: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Random search over parameter distributions for a Forecaster.\n\n    Performs random sampling of parameter settings from distributions for a\n    Forecaster object. Validation is done using time series backtesting with\n    the provided cross-validation strategy. This is more efficient than grid\n    search when exploring large parameter spaces.\n\n    Args:\n        forecaster: Forecaster model (ForecasterRecursive or ForecasterDirect).\n        y: Training time series.\n        cv: Cross-validation strategy (TimeSeriesFold or OneStepAheadFold)\n            with information needed to split the data into folds.\n        param_distributions: Dictionary with parameter names (str) as keys\n            and distributions or lists of parameters to try as values.\n            Use scipy.stats distributions for continuous parameters.\n        metric: Metric(s) to quantify model goodness of fit. If str:\n            'mean_squared_error', 'mean_absolute_error',\n            'mean_absolute_percentage_error', 'mean_squared_log_error',\n            'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'.\n            If Callable: Function with arguments (y_true, y_pred, y_train)\n            that returns a float. If list: Multiple strings and/or Callables.\n        exog: Exogenous variable(s) included as predictors. Must have the\n            same number of observations as y and aligned so that y[i] is\n            regressed on exog[i]. Default is None.\n        lags_grid: Lists of lags to try. Can be int, lists, numpy ndarray,\n            or range objects. If dict, keys are used as labels in results\n            DataFrame. Default is None.\n        n_iter: Number of parameter settings sampled per lags configuration.\n            Trades off runtime vs solution quality. Default is 10.\n        random_state: Seed for random sampling for reproducible output.\n            Default is 123.\n        return_best: If True, refit the forecaster using best parameters\n            on the whole dataset. Default is True.\n        n_jobs: Number of jobs to run in parallel. If -1, uses all cores.\n            If 'auto', uses select_n_jobs_backtesting. Default is 'auto'.\n        verbose: If True, print number of folds used for cv. Default is False.\n        show_progress: Whether to show a progress bar. Default is True.\n        suppress_warnings: If True, suppress spotforecast warnings during\n            hyperparameter search. Default is False.\n        output_file: Filename or full path to save results as TSV. If None,\n            results are not saved to file. Default is None.\n\n    Returns:\n        Results for each parameter combination with columns: lags (lags\n        configuration), lags_label (descriptive label), params (parameters\n        configuration), metric (metric value), and additional columns with\n        param=value pairs.\n\n    Examples:\n        Basic random search with continuous parameter distributions:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from scipy.stats import uniform\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n        &gt;&gt;&gt; from spotforecast2.model_selection.random_search import random_search_forecaster\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(123)\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(50), name='y')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Set up forecaster and cross-validation\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; cv = TimeSeriesFold(steps=3, initial_train_size=20, refit=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define parameter distributions with scipy.stats\n        &gt;&gt;&gt; param_distributions = {\n        ...     'estimator__alpha': uniform(0.1, 10.0)  # Uniform between 0.1 and 10.1\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run random search\n        &gt;&gt;&gt; results = random_search_forecaster(\n        ...     forecaster=forecaster,\n        ...     y=y,\n        ...     cv=cv,\n        ...     param_distributions=param_distributions,\n        ...     metric='mean_squared_error',\n        ...     n_iter=5,\n        ...     random_state=42,\n        ...     return_best=False,\n        ...     verbose=False,\n        ...     show_progress=False\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check results\n        &gt;&gt;&gt; print(results.shape[0])\n        5\n        &gt;&gt;&gt; print('estimator__alpha' in results.columns)\n        True\n        &gt;&gt;&gt; print('mean_squared_error' in results.columns)\n        True\n    \"\"\"\n\n    param_grid = list(\n        ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state)\n    )\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        param_grid=param_grid,\n        metric=metric,\n        exog=exog,\n        lags_grid=lags_grid,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection/#cross-validation-splitting","title":"Cross-Validation Splitting","text":""},{"location":"api/model_selection/#split_base","title":"split_base","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_base","title":"<code>spotforecast2_safe.model_selection.split_base</code>","text":"<p>Base class for time series cross-validation splitting.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_base.BaseFold","title":"<code>BaseFold</code>","text":"<p>Base class for all Fold classes in spotforecast. All fold classes should specify all the parameters that can be set at the class level in their <code>__init__</code>.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size. Defaults to None.</p> <code>None</code> <code>initial_train_size</code> <code>int | str | Timestamp</code> <p>Number of observations used for initial training.</p> <ul> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set. Defaults to None.</li> </ul> <code>None</code> <code>fold_stride</code> <code>int</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <ul> <li>If <code>None</code>, it defaults to the same value as <code>steps</code>, meaning that folds   are placed back-to-back without overlap.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap and multiple forecasts will   be generated for the same observations.</li> <li>If <code>fold_stride &gt; steps</code>, gaps are left between consecutive test sets. Defaults to None.</li> </ul> <code>None</code> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>refit</code> <code>bool | int</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds. Defaults to False.</li> </ul> <code>False</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold. Defaults to True.</p> <code>True</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set. Defaults to 0.</p> <code>0</code> <code>skip_folds</code> <code>int | list</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9. Defaults to None.</p> <code>None</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete. Defaults to True.</p> <code>True</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_base.py</code> <pre><code>class BaseFold:\n    \"\"\"\n    Base class for all Fold classes in spotforecast. All fold classes should specify\n    all the parameters that can be set at the class level in their ``__init__``.\n\n    Args:\n        steps (int, optional): Number of observations used to be predicted in each fold.\n            This is also commonly referred to as the forecast horizon or test size.\n            Defaults to None.\n        initial_train_size (int | str | pd.Timestamp, optional): Number of observations\n            used for initial training.\n\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n            Defaults to None.\n        fold_stride (int, optional): Number of observations that the start of the test\n            set advances between consecutive folds.\n\n            - If `None`, it defaults to the same value as `steps`, meaning that folds\n              are placed back-to-back without overlap.\n            - If `fold_stride &lt; steps`, test sets overlap and multiple forecasts will\n              be generated for the same observations.\n            - If `fold_stride &gt; steps`, gaps are left between consecutive test sets.\n            Defaults to None.\n        window_size (int, optional): Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation (int, optional): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        refit (bool | int, optional): Whether to refit the forecaster in each fold.\n\n            - If `True`, the forecaster is refitted in each fold.\n            - If `False`, the forecaster is trained only in the first fold.\n            - If an integer, the forecaster is trained in the first fold and then refitted\n              every `refit` folds.\n            Defaults to False.\n        fixed_train_size (bool, optional): Whether the training size is fixed or increases\n            in each fold. Defaults to True.\n        gap (int, optional): Number of observations between the end of the training set\n            and the start of the test set. Defaults to 0.\n        skip_folds (int | list, optional): Number of folds to skip.\n\n            - If an integer, every 'skip_folds'-th is returned.\n            - If a list, the indexes of the folds to skip.\n\n            For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n            0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n            8, and 9. Defaults to None.\n        allow_incomplete_fold (bool, optional): Whether to allow the last fold to include\n            fewer observations than `steps`. If `False`, the last fold is excluded if it\n            is incomplete. Defaults to True.\n        return_all_indexes (bool, optional): Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose (bool, optional): Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        initial_train_size (int): Number of observations used for initial training.\n        window_size (int): Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation (int): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        return_all_indexes (bool): Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose (bool): Whether to print information about generated folds.\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int | None = None,\n        initial_train_size: int | str | pd.Timestamp | None = None,\n        fold_stride: int | None = None,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        refit: bool | int = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: int | list[int] | None = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        self._validate_params(\n            cv_name=type(self).__name__,\n            steps=steps,\n            initial_train_size=initial_train_size,\n            fold_stride=fold_stride,\n            window_size=window_size,\n            differentiation=differentiation,\n            refit=refit,\n            fixed_train_size=fixed_train_size,\n            gap=gap,\n            skip_folds=skip_folds,\n            allow_incomplete_fold=allow_incomplete_fold,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n        self.initial_train_size = initial_train_size\n        self.window_size = window_size\n        self.differentiation = differentiation\n        self.return_all_indexes = return_all_indexes\n        self.verbose = verbose\n\n    def _validate_params(\n        self,\n        cv_name: str,\n        steps: int | None = None,\n        initial_train_size: int | str | pd.Timestamp | None = None,\n        fold_stride: int | None = None,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        refit: bool | int = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: int | list[int] | None = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Validate all input parameters to ensure correctness.\n        \"\"\"\n\n        if cv_name == \"TimeSeriesFold\":\n            if not isinstance(steps, (int, np.integer)) or steps &lt; 1:\n                raise ValueError(\n                    f\"`steps` must be an integer greater than 0. Got {steps}.\"\n                )\n            if not isinstance(\n                initial_train_size, (int, np.integer, str, pd.Timestamp, type(None))\n            ):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0, a date \"\n                    f\"string, a pandas Timestamp, or None. Got {initial_train_size}.\"\n                )\n            if (\n                isinstance(initial_train_size, (int, np.integer))\n                and initial_train_size &lt; 1\n            ):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0, \"\n                    f\"a date string, a pandas Timestamp, or None. Got {initial_train_size}.\"\n                )\n            if fold_stride is not None:\n                if not isinstance(fold_stride, (int, np.integer)) or fold_stride &lt; 1:\n                    raise ValueError(\n                        f\"`fold_stride` must be an integer greater than 0. Got {fold_stride}.\"\n                    )\n            if not isinstance(refit, (bool, int, np.integer)):\n                raise TypeError(\n                    f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                    f\"Got {refit}.\"\n                )\n            if (\n                isinstance(refit, (int, np.integer))\n                and not isinstance(refit, bool)\n                and refit &lt; 0\n            ):\n                raise TypeError(\n                    f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                    f\"Got {refit}.\"\n                )\n            if not isinstance(fixed_train_size, bool):\n                raise TypeError(\n                    f\"`fixed_train_size` must be a boolean: `True`, `False`. \"\n                    f\"Got {fixed_train_size}.\"\n                )\n            if not isinstance(gap, (int, np.integer)) or gap &lt; 0:\n                raise ValueError(\n                    f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n                )\n            if skip_folds is not None:\n                if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                    raise TypeError(\n                        f\"`skip_folds` must be an integer greater than 0, a list of \"\n                        f\"integers or `None`. Got {skip_folds}.\"\n                    )\n                if isinstance(skip_folds, (int, np.integer)) and skip_folds &lt; 1:\n                    raise ValueError(\n                        f\"`skip_folds` must be an integer greater than 0, a list of \"\n                        f\"integers or `None`. Got {skip_folds}.\"\n                    )\n                if isinstance(skip_folds, list) and any([x &lt; 1 for x in skip_folds]):\n                    raise ValueError(\n                        f\"`skip_folds` list must contain integers greater than or \"\n                        f\"equal to 1. The first fold is always needed to train the \"\n                        f\"forecaster. Got {skip_folds}.\"\n                    )\n            if not isinstance(allow_incomplete_fold, bool):\n                raise TypeError(\n                    f\"`allow_incomplete_fold` must be a boolean: `True`, `False`. \"\n                    f\"Got {allow_incomplete_fold}.\"\n                )\n\n        if cv_name == \"OneStepAheadFold\":\n            if not isinstance(initial_train_size, (int, np.integer, str, pd.Timestamp)):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0, a date \"\n                    f\"string, or a pandas Timestamp. Got {initial_train_size}.\"\n                )\n            if (\n                isinstance(initial_train_size, (int, np.integer))\n                and initial_train_size &lt; 1\n            ):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0, \"\n                    f\"a date string, or a pandas Timestamp. Got {initial_train_size}.\"\n                )\n\n        if (\n            not isinstance(window_size, (int, np.integer, pd.DateOffset, type(None)))\n            or isinstance(window_size, (int, np.integer))\n            and window_size &lt; 1\n        ):\n            raise ValueError(\n                f\"`window_size` must be an integer greater than 0. Got {window_size}.\"\n            )\n\n        if differentiation is not None:\n            if (\n                not isinstance(differentiation, (int, np.integer))\n                or differentiation &lt; 0\n            ):\n                raise ValueError(\n                    f\"`differentiation` must be None or an integer greater than or \"\n                    f\"equal to 0. Got {differentiation}.\"\n                )\n\n        if not isinstance(return_all_indexes, bool):\n            raise TypeError(\n                f\"`return_all_indexes` must be a boolean: `True`, `False`. \"\n                f\"Got {return_all_indexes}.\"\n            )\n\n        if not isinstance(verbose, bool):\n            raise TypeError(\n                f\"`verbose` must be a boolean: `True`, `False`. \" f\"Got {verbose}.\"\n            )\n\n    def _extract_index(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    ) -&gt; pd.Index:\n        \"\"\"\n        Extracts and returns the index from the input data X.\n\n        Args:\n            X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or\n                index to split.\n\n        Returns:\n            pd.Index: Index extracted from the input data.\n        \"\"\"\n\n        if isinstance(X, (pd.Series, pd.DataFrame)):\n            idx = X.index\n        elif isinstance(X, dict):\n            indexes_freq = set()\n            not_valid_index = []\n            min_index = []\n            max_index = []\n            for k, v in X.items():\n                if v is None:\n                    continue\n\n                idx = v.index\n                if isinstance(idx, pd.DatetimeIndex):\n                    indexes_freq.add(idx.freq)\n                elif isinstance(idx, pd.RangeIndex):\n                    indexes_freq.add(idx.step)\n                else:\n                    not_valid_index.append(k)\n\n                min_index.append(idx[0])\n                max_index.append(idx[-1])\n\n            if not_valid_index:\n                raise TypeError(\n                    f\"If `X` is a dictionary, all series must have a Pandas \"\n                    f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n                    f\"Review series: {not_valid_index}\"\n                )\n\n            if None in indexes_freq:\n                raise ValueError(\n                    \"If `X` is a dictionary, all series must have a Pandas \"\n                    \"RangeIndex or DatetimeIndex with the same step/frequency. \"\n                    \"Found series with no frequency or step.\"\n                )\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    f\"If `X` is a dictionary, all series must have a Pandas \"\n                    f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n                    f\"Found frequencies: {sorted(indexes_freq)}\"\n                )\n\n            if isinstance(idx, pd.DatetimeIndex):\n                idx = pd.date_range(\n                    start=min(min_index), end=max(max_index), freq=indexes_freq.pop()\n                )\n            else:\n                idx = pd.RangeIndex(\n                    start=min(min_index),\n                    stop=max(max_index) + 1,\n                    step=indexes_freq.pop(),\n                )\n        else:\n            idx = X\n\n        return idx\n\n    def set_params(self, params: dict) -&gt; None:\n        \"\"\"\n        Set the parameters of the Fold object. Before overwriting the current\n        parameters, the input parameters are validated to ensure correctness.\n\n        Args:\n            params (dict): Dictionary with the parameters to set.\n        \"\"\"\n\n        if not isinstance(params, dict):\n            raise TypeError(f\"`params` must be a dictionary. Got {type(params)}.\")\n\n        current_params = dict(vars(self))\n        unknown_params = set(params.keys()) - set(current_params.keys())\n        if unknown_params:\n            warnings.warn(\n                f\"Unknown parameters: {unknown_params}. They have been ignored.\",\n                IgnoredArgumentWarning,\n            )\n\n        filtered_params = {k: v for k, v in params.items() if k in current_params}\n        updated_params = {\n            \"cv_name\": type(self).__name__,\n            **current_params,\n            **filtered_params,\n        }\n\n        self._validate_params(**updated_params)\n        for key, value in updated_params.items():\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_base.BaseFold.set_params","title":"<code>set_params(params)</code>","text":"<p>Set the parameters of the Fold object. Before overwriting the current parameters, the input parameters are validated to ensure correctness.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dictionary with the parameters to set.</p> required Source code in <code>src/spotforecast2_safe/model_selection/split_base.py</code> <pre><code>def set_params(self, params: dict) -&gt; None:\n    \"\"\"\n    Set the parameters of the Fold object. Before overwriting the current\n    parameters, the input parameters are validated to ensure correctness.\n\n    Args:\n        params (dict): Dictionary with the parameters to set.\n    \"\"\"\n\n    if not isinstance(params, dict):\n        raise TypeError(f\"`params` must be a dictionary. Got {type(params)}.\")\n\n    current_params = dict(vars(self))\n    unknown_params = set(params.keys()) - set(current_params.keys())\n    if unknown_params:\n        warnings.warn(\n            f\"Unknown parameters: {unknown_params}. They have been ignored.\",\n            IgnoredArgumentWarning,\n        )\n\n    filtered_params = {k: v for k, v in params.items() if k in current_params}\n    updated_params = {\n        \"cv_name\": type(self).__name__,\n        **current_params,\n        **filtered_params,\n    }\n\n    self._validate_params(**updated_params)\n    for key, value in updated_params.items():\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/model_selection/#split_one_step","title":"split_one_step","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_one_step","title":"<code>spotforecast2_safe.model_selection.split_one_step</code>","text":"<p>One step ahead cross-validation splitting.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_one_step.OneStepAheadFold","title":"<code>OneStepAheadFold</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds for one-step-ahead forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>initial_train_size</code> <code>int | str | Timestamp</code> <p>Number of observations used for initial training.</p> <ul> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set.</li> </ul> required <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>class OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Args:\n        initial_train_size (int | str | pd.Timestamp): Number of observations used\n            for initial training.\n\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n        window_size (int, optional): Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation (int, optional): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        return_all_indexes (bool, optional): Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose (bool, optional): Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        initial_train_size (int): Number of observations used for initial training.\n        window_size (int): Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation (int): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        return_all_indexes (bool): Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose (bool): Whether to print information about generated folds.\n    \"\"\"\n\n    def __init__(\n        self,\n        initial_train_size: int | str | pd.Timestamp,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        super().__init__(\n            initial_train_size=initial_train_size,\n            window_size=window_size,\n            differentiation=differentiation,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Initial train size = {self.initial_train_size},\\n\"\n            f\"Window size        = {self.window_size},\\n\"\n            f\"Differentiation    = {self.differentiation},\\n\"\n            f\"Return all indexes = {self.return_all_indexes},\\n\"\n            f\"Verbose            = {self.verbose}\\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html()\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def split(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n        as_pandas: bool = False,\n        externally_fitted: Any = None,\n    ) -&gt; list | pd.DataFrame:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Args:\n            X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or index to split.\n            as_pandas (bool, optional): If True, the folds are returned as a DataFrame.\n                This is useful to visualize the folds in a more interpretable way.\n                Defaults to False.\n            externally_fitted (Any, optional): This argument is not used in this class.\n                It is included for API consistency. Defaults to None.\n\n        Returns:\n            list | pd.DataFrame: A list of lists containing the indices (position) of\n            the fold. The list contains 2 lists with the following information:\n\n            - fold: fold number.\n            - [train_start, train_end]: list with the start and end positions of the\n                training set.\n            - [test_start, test_end]: list with the start and end positions of the test\n                set. These are the observations used to evaluate the forecaster.\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\n                in this fold.\n\n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'test_start',\n            'test_end', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        index = self._extract_index(X)\n\n        self.initial_train_size = date_to_index_position(\n            index=index,\n            date_input=self.initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n\n        fold = [\n            0,\n            [0, self.initial_train_size - 1],\n            [self.initial_train_size, len(X)],\n            True,\n        ]\n\n        if self.verbose:\n            self._print_info(index=index, fold=fold)\n\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        if self.return_all_indexes:\n            fold = [\n                fold[0],\n                [range(fold[1][0], fold[1][1] + 1)],\n                [range(fold[2][0], fold[2][1])],\n                fold[3],\n            ]\n        else:\n            fold = [\n                fold[0],\n                [fold[1][0], fold[1][1] + 1],\n                [fold[2][0], fold[2][1]],\n                fold[3],\n            ]\n\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(\n                    data=[[fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]],\n                    columns=[\n                        \"fold\",\n                        \"train_start\",\n                        \"train_end\",\n                        \"test_start\",\n                        \"test_end\",\n                        \"fit_forecaster\",\n                    ],\n                )\n            else:\n                fold = pd.DataFrame(\n                    data=[fold],\n                    columns=[\"fold\", \"train_index\", \"test_index\", \"fit_forecaster\"],\n                )\n\n        return fold\n\n    def _print_info(self, index: pd.Index, fold: list[list[int]]) -&gt; None:\n        \"\"\"\n        Print information about folds.\n\n        Args:\n            index (pd.Index): Index of the time series data.\n            fold (list): A list of lists containing the indices (position) of the fold.\n        \"\"\"\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        print(f\"Number of observations in train: {initial_train_size}\")\n        if self.differentiation is not None:\n            print(\n                f\"    First {differentiation} observation/s in training set \"\n                f\"are used for differentiation\"\n            )\n        print(f\"Number of observations in test: {test_length}\")\n\n        training_start = index[fold[1][0] + differentiation]\n        training_end = index[fold[1][-1]]\n        test_start = index[fold[2][0]]\n        test_end = index[fold[2][-1] - 1]\n\n        print(f\"Training : {training_start} -- {training_end} (n={initial_train_size})\")\n        print(f\"Test     : {test_start} -- {test_end} (n={test_length})\")\n        print(\"\")\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_one_step.OneStepAheadFold.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when printed.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when printed.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Initial train size = {self.initial_train_size},\\n\"\n        f\"Window size        = {self.window_size},\\n\"\n        f\"Differentiation    = {self.differentiation},\\n\"\n        f\"Return all indexes = {self.return_all_indexes},\\n\"\n        f\"Verbose            = {self.verbose}\\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_one_step.OneStepAheadFold.split","title":"<code>split(X, as_pandas=False, externally_fitted=None)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series | DataFrame | Index | dict</code> <p>Time series data or index to split.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.</p> <code>False</code> <code>externally_fitted</code> <code>Any</code> <p>This argument is not used in this class. It is included for API consistency. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list | DataFrame</code> <p>list | pd.DataFrame: A list of lists containing the indices (position) of</p> <code>list | DataFrame</code> <p>the fold. The list contains 2 lists with the following information:</p> <code>list | DataFrame</code> <ul> <li>fold: fold number.</li> </ul> <code>list | DataFrame</code> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> </ul> <code>list | DataFrame</code> <ul> <li>fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.</li> </ul> <code>list | DataFrame</code> <p>It is important to note that the returned values are the positions of the</p> <code>list | DataFrame</code> <p>observations and not the actual values of the index, so they can be used to</p> <code>list | DataFrame</code> <p>slice the data directly using iloc.</p> <code>list | DataFrame</code> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the</p> <code>list | DataFrame</code> <p>following columns: 'fold', 'train_start', 'train_end', 'test_start',</p> <code>list | DataFrame</code> <p>'test_end', 'fit_forecaster'.</p> <code>list | DataFrame</code> <p>Following the python convention, the start index is inclusive and the end</p> <code>list | DataFrame</code> <p>index is exclusive. This means that the last index is not included in the</p> <code>list | DataFrame</code> <p>slice.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n    externally_fitted: Any = None,\n) -&gt; list | pd.DataFrame:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Args:\n        X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or index to split.\n        as_pandas (bool, optional): If True, the folds are returned as a DataFrame.\n            This is useful to visualize the folds in a more interpretable way.\n            Defaults to False.\n        externally_fitted (Any, optional): This argument is not used in this class.\n            It is included for API consistency. Defaults to None.\n\n    Returns:\n        list | pd.DataFrame: A list of lists containing the indices (position) of\n        the fold. The list contains 2 lists with the following information:\n\n        - fold: fold number.\n        - [train_start, train_end]: list with the start and end positions of the\n            training set.\n        - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n        - fit_forecaster: boolean indicating whether the forecaster should be fitted\n            in this fold.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'test_start',\n        'test_end', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    index = self._extract_index(X)\n\n    self.initial_train_size = date_to_index_position(\n        index=index,\n        date_input=self.initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n\n    fold = [\n        0,\n        [0, self.initial_train_size - 1],\n        [self.initial_train_size, len(X)],\n        True,\n    ]\n\n    if self.verbose:\n        self._print_info(index=index, fold=fold)\n\n    # NOTE: +1 to prevent iloc pandas from deleting the last observation\n    if self.return_all_indexes:\n        fold = [\n            fold[0],\n            [range(fold[1][0], fold[1][1] + 1)],\n            [range(fold[2][0], fold[2][1])],\n            fold[3],\n        ]\n    else:\n        fold = [\n            fold[0],\n            [fold[1][0], fold[1][1] + 1],\n            [fold[2][0], fold[2][1]],\n            fold[3],\n        ]\n\n    if as_pandas:\n        if not self.return_all_indexes:\n            fold = pd.DataFrame(\n                data=[[fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]],\n                columns=[\n                    \"fold\",\n                    \"train_start\",\n                    \"train_end\",\n                    \"test_start\",\n                    \"test_end\",\n                    \"fit_forecaster\",\n                ],\n            )\n        else:\n            fold = pd.DataFrame(\n                data=[fold],\n                columns=[\"fold\", \"train_index\", \"test_index\", \"fit_forecaster\"],\n            )\n\n    return fold\n</code></pre>"},{"location":"api/model_selection/#split_ts_cv","title":"split_ts_cv","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_ts_cv","title":"<code>spotforecast2_safe.model_selection.split_ts_cv</code>","text":"<p>Time series cross-validation splitting.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_ts_cv.TimeSeriesFold","title":"<code>TimeSeriesFold</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds.</p> <p>When used within a backtesting or hyperparameter search, the arguments 'initial_train_size', 'window_size' and 'differentiation' are not required as they are automatically set by the backtesting or hyperparameter search functions.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> required <code>initial_train_size</code> <code>int | str | Timestamp | None</code> <p>Number of observations used for initial training.</p> <ul> <li>If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</li> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set.</li> </ul> <p>Defaults to None.</p> <code>None</code> <code>fold_stride</code> <code>int | None</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <ul> <li>If <code>None</code>, it defaults to the same value as <code>steps</code>, meaning that folds   are placed back-to-back without overlap.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap and multiple forecasts will   be generated for the same observations.</li> <li>If <code>fold_stride &gt; steps</code>, gaps are left between consecutive test sets.</li> </ul> <p>Defaults to None.</p> <code>None</code> <code>window_size</code> <code>int | None</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int | None</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>refit</code> <code>bool | int</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <p>Defaults to False.</p> <code>False</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold. Defaults to True.</p> <code>True</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set. Defaults to 0.</p> <code>0</code> <code>skip_folds</code> <code>int | list[int] | None</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9. Defaults to None.</p> <code>None</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete. Defaults to True.</p> <code>True</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>steps</code> <p>Number of observations used to be predicted in each fold.</p> <code>initial_train_size</code> <p>Number of observations used for initial training. If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</p> <code>fold_stride</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <code>overlapping_folds</code> <p>Whether the folds overlap.</p> <code>window_size</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <p>Whether to print information about generated folds.</p> <p>Examples:</p> <p>Basic usage with fixed train size:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n&gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n&gt;&gt;&gt; # Create fold splitter\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=10,\n...     initial_train_size=50,\n...     refit=True,\n...     fixed_train_size=True\n... )\n&gt;&gt;&gt; # Get folds\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\nNumber of folds: 4\n</code></pre> <p>Overlapping folds with custom stride:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=30,\n...     initial_train_size=50,\n...     fold_stride=7,\n...     fixed_train_size=False\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\n</code></pre> <p>Return as pandas DataFrame:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n&gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n&gt;&gt;&gt; print(folds_df.columns.tolist())\n['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\n</code></pre> <p>Skip folds for faster evaluation:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=5,\n...     initial_train_size=50,\n...     skip_folds=2\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n</code></pre> Note <p>Returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is <code>X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</code>, the <code>initial_train_size = 3</code>, <code>window_size = 2</code>, <code>steps = 4</code>, and <code>gap = 1</code>, the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].</p> <p>The first element is the fold number, the first list <code>[0, 3]</code> indicates that the training set goes from the first to the third observation. The second list <code>[1, 3]</code> indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list <code>[3, 8]</code> indicates that the test set goes from the fourth to the eighth observation. The fourth list <code>[4, 8]</code> indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean <code>False</code> indicates that the forecaster should not be trained in this fold.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> <p>As an example, with <code>initial_train_size=50</code>, <code>steps=30</code>, and <code>fold_stride=7</code>, the first test fold will cover observations [50, 80), the second fold [57, 87), and the third fold [64, 94). This configuration produces multiple forecasts for the same observations, which is often desirable in rolling-origin evaluation.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_ts_cv.py</code> <pre><code>class TimeSeriesFold(BaseFold):\n    \"\"\"Class to split time series data into train and test folds.\n\n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Args:\n        steps: Number of observations used to be predicted in each fold.\n            This is also commonly referred to as the forecast horizon or test size.\n        initial_train_size: Number of observations used for initial training.\n\n            - If `None` or 0, the initial forecaster is not trained in the first fold.\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n\n            Defaults to None.\n        fold_stride: Number of observations that the start of the test set\n            advances between consecutive folds.\n\n            - If `None`, it defaults to the same value as `steps`, meaning that folds\n              are placed back-to-back without overlap.\n            - If `fold_stride &lt; steps`, test sets overlap and multiple forecasts will\n              be generated for the same observations.\n            - If `fold_stride &gt; steps`, gaps are left between consecutive test sets.\n\n            Defaults to None.\n        window_size: Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation: Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        refit: Whether to refit the forecaster in each fold.\n\n            - If `True`, the forecaster is refitted in each fold.\n            - If `False`, the forecaster is trained only in the first fold.\n            - If an integer, the forecaster is trained in the first fold and then refitted\n              every `refit` folds.\n\n            Defaults to False.\n        fixed_train_size: Whether the training size is fixed or increases\n            in each fold. Defaults to True.\n        gap: Number of observations between the end of the training set\n            and the start of the test set. Defaults to 0.\n        skip_folds: Number of folds to skip.\n\n            - If an integer, every 'skip_folds'-th is returned.\n            - If a list, the indexes of the folds to skip.\n\n            For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n            0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n            8, and 9. Defaults to None.\n        allow_incomplete_fold: Whether to allow the last fold to include\n            fewer observations than `steps`. If `False`, the last fold is excluded if it\n            is incomplete. Defaults to True.\n        return_all_indexes: Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose: Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        steps: Number of observations used to be predicted in each fold.\n        initial_train_size: Number of observations used for initial training.\n            If `None` or 0, the initial forecaster is not trained in the first fold.\n        fold_stride: Number of observations that the start of the test set\n            advances between consecutive folds.\n        overlapping_folds: Whether the folds overlap.\n        window_size: Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation: Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        refit: Whether to refit the forecaster in each fold.\n        fixed_train_size: Whether the training size is fixed or increases in each fold.\n        gap: Number of observations between the end of the training set and the\n            start of the test set.\n        skip_folds: Number of folds to skip.\n        allow_incomplete_fold: Whether to allow the last fold to include fewer\n            observations than `steps`.\n        return_all_indexes: Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose: Whether to print information about generated folds.\n\n    Examples:\n        Basic usage with fixed train size:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n        &gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n        &gt;&gt;&gt; # Create fold splitter\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=10,\n        ...     initial_train_size=50,\n        ...     refit=True,\n        ...     fixed_train_size=True\n        ... )\n        &gt;&gt;&gt; # Get folds\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\n        Number of folds: 4\n\n        Overlapping folds with custom stride:\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=30,\n        ...     initial_train_size=50,\n        ...     fold_stride=7,\n        ...     fixed_train_size=False\n        ... )\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\n\n        Return as pandas DataFrame:\n        &gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n        &gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n        &gt;&gt;&gt; print(folds_df.columns.tolist())\n        ['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\n\n        Skip folds for faster evaluation:\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=5,\n        ...     initial_train_size=50,\n        ...     skip_folds=2\n        ... )\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n\n    Note:\n        Returned values are the positions of the observations and not the actual values of\n        the index, so they can be used to slice the data directly using iloc. For example,\n        if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the\n        `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n        the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].\n\n        The first element is the fold number, the first list `[0, 3]` indicates that\n        the training set goes from the first to the third observation. The second\n        list `[1, 3]` indicates that the last window seen by the forecaster during\n        training goes from the second to the third observation. The third list `[3, 8]`\n        indicates that the test set goes from the fourth to the eighth observation.\n        The fourth list `[4, 8]` indicates that the test set including the gap goes\n        from the fifth to the eighth observation. The boolean `False` indicates that\n        the forecaster should not be trained in this fold.\n\n        Following the python convention, the start index is inclusive and the end index is\n        exclusive. This means that the last index is not included in the slice.\n\n        As an example, with `initial_train_size=50`, `steps=30`, and `fold_stride=7`,\n        the first test fold will cover observations [50, 80), the second fold [57, 87),\n        and the third fold [64, 94). This configuration produces multiple forecasts\n        for the same observations, which is often desirable in rolling-origin\n        evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int,\n        initial_train_size: int | str | pd.Timestamp | None = None,\n        fold_stride: int | None = None,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        refit: bool | int = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: int | list[int] | None = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        super().__init__(\n            steps=steps,\n            initial_train_size=initial_train_size,\n            fold_stride=fold_stride,\n            window_size=window_size,\n            differentiation=differentiation,\n            refit=refit,\n            fixed_train_size=fixed_train_size,\n            gap=gap,\n            skip_folds=skip_folds,\n            allow_incomplete_fold=allow_incomplete_fold,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n        self.steps = steps\n        self.fold_stride = fold_stride if fold_stride is not None else steps\n        self.overlapping_folds = self.fold_stride &lt; self.steps\n        self.refit = refit\n        self.fixed_train_size = fixed_train_size\n        self.gap = gap\n        self.skip_folds = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Information displayed when printed.\n\n        Returns:\n            String representation of the TimeSeriesFold object.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Initial train size    = {self.initial_train_size},\\n\"\n            f\"Steps                 = {self.steps},\\n\"\n            f\"Fold stride           = {self.fold_stride},\\n\"\n            f\"Overlapping folds     = {self.overlapping_folds},\\n\"\n            f\"Window size           = {self.window_size},\\n\"\n            f\"Differentiation       = {self.differentiation},\\n\"\n            f\"Refit                 = {self.refit},\\n\"\n            f\"Fixed train size      = {self.fixed_train_size},\\n\"\n            f\"Gap                   = {self.gap},\\n\"\n            f\"Skip folds            = {self.skip_folds},\\n\"\n            f\"Allow incomplete fold = {self.allow_incomplete_fold},\\n\"\n            f\"Return all indexes    = {self.return_all_indexes},\\n\"\n            f\"Verbose               = {self.verbose}\\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"HTML representation of the object.\n\n        The \"General Information\" section is expanded by default.\n\n        Returns:\n            HTML string representation for Jupyter notebooks.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html()\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Steps:&lt;/strong&gt; {self.steps}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Fold stride:&lt;/strong&gt; {self.fold_stride}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Overlapping folds:&lt;/strong&gt; {self.overlapping_folds}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Refit:&lt;/strong&gt; {self.refit}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Fixed train size:&lt;/strong&gt; {self.fixed_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Gap:&lt;/strong&gt; {self.gap}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Skip folds:&lt;/strong&gt; {self.skip_folds}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Allow incomplete fold:&lt;/strong&gt; {self.allow_incomplete_fold}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def split(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n        as_pandas: bool = False,\n    ) -&gt; list | pd.DataFrame:\n        \"\"\"Split the time series data into train and test folds.\n\n        Args:\n            X: Time series data or index to split. Can be a pandas Series, DataFrame,\n                Index, or a dictionary of Series/DataFrames.\n            as_pandas: If True, the folds are returned as a DataFrame. This is useful\n                to visualize the folds in a more interpretable way. Defaults to False.\n\n        Returns:\n            A list of lists containing the indices (position) for each fold, or a\n            DataFrame if `as_pandas=True`. Each list contains 4 lists and a boolean\n            with the following information:\n\n            - **fold**: fold number.\n            - **[train_start, train_end]**: list with the start and end positions of\n                    the training set.\n            - **[last_window_start, last_window_end]**: list with the start and end\n                    positions of the last window seen by the forecaster during training.\n                    The last window is used to generate the lags use as predictors. If\n                    `differentiation` is included, the interval is extended as many\n                    observations as the differentiation order. If the argument `window_size`\n                    is `None`, this list is empty.\n            - **[test_start, test_end]**: list with the start and end positions of\n                    the test set. These are the observations used to evaluate the forecaster.\n            - **[test_start_with_gap, test_end_with_gap]**: list with the start and\n                    end positions of the test set including the gap. The gap is the number\n                    of observations between the end of the training set and the start of\n                    the test set.\n            - **fit_forecaster**: boolean indicating whether the forecaster should be\n                    fitted in this fold.\n\n        Note:\n            The returned values are the positions of the observations and not the\n            actual values of the index, so they can be used to slice the data directly\n            using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n            'test_end_with_gap', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        window_size_as_date_offset = isinstance(\n            self.window_size, pd.tseries.offsets.DateOffset\n        )\n        if window_size_as_date_offset:\n            # Calculate the window_size in steps. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance between\n            # two dates may not be constant.\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). This is because  \"\n                    f\"the offset (forecaster.offset) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                    f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                    f\"size of `y`.\"\n                )\n\n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError(\n                    \"To use split method when `initial_train_size` is None, \"\n                    \"`window_size` must be an integer greater than 0. \"\n                    \"Although no initial training is done and all data is used to \"\n                    \"evaluate the model, the first `window_size` observations are \"\n                    \"needed to create the initial predictors. Got `window_size` = None.\"\n                )\n            if self.refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                    \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n                )\n            externally_fitted = True\n            self.initial_train_size = self.window_size  # Reset to None later\n        else:\n            if self.window_size is None:\n                warnings.warn(\n                    \"Last window cannot be calculated because `window_size` is None.\",\n                    IgnoredArgumentWarning,\n                )\n            externally_fitted = False\n\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n\n        self.initial_train_size = date_to_index_position(\n            index=index,\n            date_input=self.initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n\n        if window_size_as_date_offset:\n            if self.initial_train_size is not None:\n                if self.initial_train_size &lt; self.window_size:\n                    raise ValueError(\n                        f\"If `initial_train_size` is an integer, it must be greater than \"\n                        f\"the `window_size` of the forecaster ({self.window_size}) \"\n                        f\"and smaller than the length of the series ({len(X)}). If \"\n                        f\"it is a date, it must be within this range of the index.\"\n                    )\n\n        if self.allow_incomplete_fold:\n            # At least one observation after the gap to allow incomplete fold\n            if len(index) &lt;= self.initial_train_size + self.gap:\n                raise ValueError(\n                    f\"The time series must have more than `initial_train_size + gap` \"\n                    f\"observations to create at least one fold.\\n\"\n                    f\"    Time series length: {len(index)}\\n\"\n                    f\"    Required &gt; {self.initial_train_size + self.gap}\\n\"\n                    f\"    initial_train_size: {self.initial_train_size}\\n\"\n                    f\"    gap: {self.gap}\\n\"\n                )\n        else:\n            # At least one complete fold\n            if len(index) &lt; self.initial_train_size + self.gap + self.steps:\n                raise ValueError(\n                    f\"The time series must have at least `initial_train_size + gap + steps` \"\n                    f\"observations to create a minimum of one complete fold \"\n                    f\"(allow_incomplete_fold=False).\\n\"\n                    f\"    Time series length: {len(index)}\\n\"\n                    f\"    Required &gt;= {self.initial_train_size + self.gap + self.steps}\\n\"\n                    f\"    initial_train_size: {self.initial_train_size}\\n\"\n                    f\"    gap: {self.gap}\\n\"\n                    f\"    steps: {self.steps}\\n\"\n                )\n\n        while self.initial_train_size + (i * self.fold_stride) + self.gap &lt; len(index):\n\n            if self.refit:\n                # NOTE: If `fixed_train_size` the train size doesn't increase but\n                # moves by `fold_stride` positions in each iteration. If `False`,\n                # the train size increases by `fold_stride` in each iteration.\n                train_iloc_start = (\n                    i * (self.fold_stride) if self.fixed_train_size else 0\n                )\n                train_iloc_end = self.initial_train_size + i * (self.fold_stride)\n                test_iloc_start = train_iloc_end\n            else:\n                # NOTE: The train size doesn't increase and doesn't move.\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * (self.fold_stride)\n\n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n\n            partitions = [\n                idx[train_iloc_start:train_iloc_end],\n                (\n                    idx[last_window_iloc_start:test_iloc_start]\n                    if self.window_size is not None\n                    else []\n                ),\n                idx[test_iloc_start:test_iloc_end],\n                idx[test_iloc_start + self.gap : test_iloc_end],\n            ]\n            folds.append(partitions)\n            i += 1\n\n        # NOTE: Delete all incomplete folds at the end if not allowed\n        n_removed_folds = 0\n        if not self.allow_incomplete_fold:\n            # NOTE: While folds and the last \"test_index_with_gap\" is incomplete,\n            # calculating len of range objects\n            while folds and len(folds[-1][3]) &lt; self.steps:\n                folds.pop()\n                n_removed_folds += 1\n\n        # Replace partitions inside folds with length 0 with `None`\n        folds = [\n            [partition if len(partition) &gt; 0 else None for partition in fold]\n            for fold in folds\n        ]\n\n        # Create a flag to know whether to train the forecaster\n        if self.refit == 0:\n            self.refit = False\n\n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit):\n                fit_forecaster[i] = True\n\n        for i in range(len(folds)):\n            folds[i].insert(0, i)\n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][1] = folds[i - 1][1]\n\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(\n                    np.arange(0, len(folds)), index_to_keep, assume_unique=True\n                )\n                index_to_skip = [\n                    int(x) for x in index_to_skip\n                ]  # Required since numpy 2.0\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]\n\n        if self.verbose:\n            self._print_info(\n                index=index,\n                folds=folds,\n                externally_fitted=externally_fitted,\n                n_removed_folds=n_removed_folds,\n                index_to_skip=index_to_skip,\n            )\n\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            # NOTE: +1 to prevent iloc pandas from deleting the last observation\n            folds = [\n                [\n                    fold[0],\n                    [fold[1][0], fold[1][-1] + 1],\n                    (\n                        [fold[2][0], fold[2][-1] + 1]\n                        if self.window_size is not None\n                        else []\n                    ),\n                    [fold[3][0], fold[3][-1] + 1],\n                    [fold[4][0], fold[4][-1] + 1],\n                    fold[5],\n                ]\n                for fold in folds\n            ]\n\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][5] = False\n\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[2] = [None, None]\n\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(\n                    data=[\n                        [fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]\n                        for fold in folds\n                    ],\n                    columns=[\n                        \"fold\",\n                        \"train_start\",\n                        \"train_end\",\n                        \"last_window_start\",\n                        \"last_window_end\",\n                        \"test_start\",\n                        \"test_end\",\n                        \"test_start_with_gap\",\n                        \"test_end_with_gap\",\n                        \"fit_forecaster\",\n                    ],\n                )\n            else:\n                folds = pd.DataFrame(\n                    data=folds,\n                    columns=[\n                        \"fold\",\n                        \"train_index\",\n                        \"last_window_index\",\n                        \"test_index\",\n                        \"test_index_with_gap\",\n                        \"fit_forecaster\",\n                    ],\n                )\n\n        return folds\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        folds: list[list[int]],\n        externally_fitted: bool,\n        n_removed_folds: int,\n        index_to_skip: list[int],\n    ) -&gt; None:\n        \"\"\"Print information about folds.\n\n        Args:\n            index: Index of the time series data.\n            folds: A list of lists containing the indices (position) for each fold.\n            externally_fitted: Whether an already trained forecaster is to be used.\n            n_removed_folds: Number of folds removed.\n            index_to_skip: Number of folds skipped.\n        \"\"\"\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        if externally_fitted:\n            print(\n                f\"An already trained forecaster is to be used. Window size: \"\n                f\"{self.window_size}\"\n            )\n        else:\n            if self.differentiation is None:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size}\"\n                )\n            else:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size - self.differentiation}\"\n                )\n                print(\n                    f\"    First {self.differentiation} observation/s in training sets \"\n                    f\"are used for differentiation\"\n                )\n        print(\n            f\"Number of observations used for backtesting: \"\n            f\"{len(index) - self.initial_train_size}\"\n        )\n        print(f\"    Number of folds: {len(folds)}\")\n        print(\n            f\"    Number skipped folds: \"\n            f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n        )\n        print(f\"    Number of steps per fold: {self.steps}\")\n        if self.steps != self.fold_stride:\n            print(\n                f\"    Number of steps to the next fold (fold stride): {self.fold_stride}\"\n            )\n        print(\n            f\"    Number of steps to exclude between last observed data \"\n            f\"(last window) and predictions (gap): {self.gap}\"\n        )\n        if n_removed_folds &gt; 0:\n            print(\n                f\"    The last {n_removed_folds} fold(s) have been excluded \"\n                f\"because they were incomplete.\"\n            )\n\n        if len(folds[-1][4]) &lt; self.steps:\n            print(f\"    Last fold only includes {len(folds[-1][4])} observations.\")\n\n        print(\"\")\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        for i, fold in enumerate(folds):\n            is_fold_skipped = i in index_to_skip\n            has_training = fold[-1] if i != 0 else True\n            training_start = (\n                index[fold[1][0] + differentiation] if fold[1] is not None else None\n            )\n            training_end = index[fold[1][-1]] if fold[1] is not None else None\n            training_length = (\n                len(fold[1]) - differentiation if fold[1] is not None else 0\n            )\n            validation_start = index[fold[4][0]]\n            validation_end = index[fold[4][-1]]\n            validation_length = len(fold[4])\n\n            print(f\"Fold: {i}\")\n            if is_fold_skipped:\n                print(\"    Fold skipped\")\n            elif not externally_fitted and has_training:\n                print(\n                    f\"    Training:   {training_start} -- {training_end}  \"\n                    f\"(n={training_length})\"\n                )\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n            else:\n                print(\"    Training:   No training in this fold\")\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n\n        print(\"\")\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_ts_cv.TimeSeriesFold.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when printed.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the TimeSeriesFold object.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_ts_cv.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Information displayed when printed.\n\n    Returns:\n        String representation of the TimeSeriesFold object.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Initial train size    = {self.initial_train_size},\\n\"\n        f\"Steps                 = {self.steps},\\n\"\n        f\"Fold stride           = {self.fold_stride},\\n\"\n        f\"Overlapping folds     = {self.overlapping_folds},\\n\"\n        f\"Window size           = {self.window_size},\\n\"\n        f\"Differentiation       = {self.differentiation},\\n\"\n        f\"Refit                 = {self.refit},\\n\"\n        f\"Fixed train size      = {self.fixed_train_size},\\n\"\n        f\"Gap                   = {self.gap},\\n\"\n        f\"Skip folds            = {self.skip_folds},\\n\"\n        f\"Allow incomplete fold = {self.allow_incomplete_fold},\\n\"\n        f\"Return all indexes    = {self.return_all_indexes},\\n\"\n        f\"Verbose               = {self.verbose}\\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.split_ts_cv.TimeSeriesFold.split","title":"<code>split(X, as_pandas=False)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series | DataFrame | Index | dict[str, Series | DataFrame]</code> <p>Time series data or index to split. Can be a pandas Series, DataFrame, Index, or a dictionary of Series/DataFrames.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list | DataFrame</code> <p>A list of lists containing the indices (position) for each fold, or a</p> <code>list | DataFrame</code> <p>DataFrame if <code>as_pandas=True</code>. Each list contains 4 lists and a boolean</p> <code>list | DataFrame</code> <p>with the following information:</p> <code>list | DataFrame</code> <ul> <li>fold: fold number.</li> </ul> <code>list | DataFrame</code> <ul> <li>[train_start, train_end]: list with the start and end positions of     the training set.</li> </ul> <code>list | DataFrame</code> <ul> <li>[last_window_start, last_window_end]: list with the start and end     positions of the last window seen by the forecaster during training.     The last window is used to generate the lags use as predictors. If     <code>differentiation</code> is included, the interval is extended as many     observations as the differentiation order. If the argument <code>window_size</code>     is <code>None</code>, this list is empty.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start, test_end]: list with the start and end positions of     the test set. These are the observations used to evaluate the forecaster.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start_with_gap, test_end_with_gap]: list with the start and     end positions of the test set including the gap. The gap is the number     of observations between the end of the training set and the start of     the test set.</li> </ul> <code>list | DataFrame</code> <ul> <li>fit_forecaster: boolean indicating whether the forecaster should be     fitted in this fold.</li> </ul> Note <p>The returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>src/spotforecast2_safe/model_selection/split_ts_cv.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n) -&gt; list | pd.DataFrame:\n    \"\"\"Split the time series data into train and test folds.\n\n    Args:\n        X: Time series data or index to split. Can be a pandas Series, DataFrame,\n            Index, or a dictionary of Series/DataFrames.\n        as_pandas: If True, the folds are returned as a DataFrame. This is useful\n            to visualize the folds in a more interpretable way. Defaults to False.\n\n    Returns:\n        A list of lists containing the indices (position) for each fold, or a\n        DataFrame if `as_pandas=True`. Each list contains 4 lists and a boolean\n        with the following information:\n\n        - **fold**: fold number.\n        - **[train_start, train_end]**: list with the start and end positions of\n                the training set.\n        - **[last_window_start, last_window_end]**: list with the start and end\n                positions of the last window seen by the forecaster during training.\n                The last window is used to generate the lags use as predictors. If\n                `differentiation` is included, the interval is extended as many\n                observations as the differentiation order. If the argument `window_size`\n                is `None`, this list is empty.\n        - **[test_start, test_end]**: list with the start and end positions of\n                the test set. These are the observations used to evaluate the forecaster.\n        - **[test_start_with_gap, test_end_with_gap]**: list with the start and\n                end positions of the test set including the gap. The gap is the number\n                of observations between the end of the training set and the start of\n                the test set.\n        - **fit_forecaster**: boolean indicating whether the forecaster should be\n                fitted in this fold.\n\n    Note:\n        The returned values are the positions of the observations and not the\n        actual values of the index, so they can be used to slice the data directly\n        using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n        'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n        'test_end_with_gap', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    window_size_as_date_offset = isinstance(\n        self.window_size, pd.tseries.offsets.DateOffset\n    )\n    if window_size_as_date_offset:\n        # Calculate the window_size in steps. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance between\n        # two dates may not be constant.\n        first_valid_index = X.index[-1] - self.window_size\n        try:\n            window_size_idx_start = X.index.get_loc(first_valid_index)\n            window_size_idx_end = X.index.get_loc(X.index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset (forecaster.offset) is larger than the available \"\n                f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                f\"size of `y`.\"\n            )\n\n    if self.initial_train_size is None:\n        if self.window_size is None:\n            raise ValueError(\n                \"To use split method when `initial_train_size` is None, \"\n                \"`window_size` must be an integer greater than 0. \"\n                \"Although no initial training is done and all data is used to \"\n                \"evaluate the model, the first `window_size` observations are \"\n                \"needed to create the initial predictors. Got `window_size` = None.\"\n            )\n        if self.refit:\n            raise ValueError(\n                \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n            )\n        externally_fitted = True\n        self.initial_train_size = self.window_size  # Reset to None later\n    else:\n        if self.window_size is None:\n            warnings.warn(\n                \"Last window cannot be calculated because `window_size` is None.\",\n                IgnoredArgumentWarning,\n            )\n        externally_fitted = False\n\n    index = self._extract_index(X)\n    idx = range(len(index))\n    folds = []\n    i = 0\n\n    self.initial_train_size = date_to_index_position(\n        index=index,\n        date_input=self.initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n\n    if window_size_as_date_offset:\n        if self.initial_train_size is not None:\n            if self.initial_train_size &lt; self.window_size:\n                raise ValueError(\n                    f\"If `initial_train_size` is an integer, it must be greater than \"\n                    f\"the `window_size` of the forecaster ({self.window_size}) \"\n                    f\"and smaller than the length of the series ({len(X)}). If \"\n                    f\"it is a date, it must be within this range of the index.\"\n                )\n\n    if self.allow_incomplete_fold:\n        # At least one observation after the gap to allow incomplete fold\n        if len(index) &lt;= self.initial_train_size + self.gap:\n            raise ValueError(\n                f\"The time series must have more than `initial_train_size + gap` \"\n                f\"observations to create at least one fold.\\n\"\n                f\"    Time series length: {len(index)}\\n\"\n                f\"    Required &gt; {self.initial_train_size + self.gap}\\n\"\n                f\"    initial_train_size: {self.initial_train_size}\\n\"\n                f\"    gap: {self.gap}\\n\"\n            )\n    else:\n        # At least one complete fold\n        if len(index) &lt; self.initial_train_size + self.gap + self.steps:\n            raise ValueError(\n                f\"The time series must have at least `initial_train_size + gap + steps` \"\n                f\"observations to create a minimum of one complete fold \"\n                f\"(allow_incomplete_fold=False).\\n\"\n                f\"    Time series length: {len(index)}\\n\"\n                f\"    Required &gt;= {self.initial_train_size + self.gap + self.steps}\\n\"\n                f\"    initial_train_size: {self.initial_train_size}\\n\"\n                f\"    gap: {self.gap}\\n\"\n                f\"    steps: {self.steps}\\n\"\n            )\n\n    while self.initial_train_size + (i * self.fold_stride) + self.gap &lt; len(index):\n\n        if self.refit:\n            # NOTE: If `fixed_train_size` the train size doesn't increase but\n            # moves by `fold_stride` positions in each iteration. If `False`,\n            # the train size increases by `fold_stride` in each iteration.\n            train_iloc_start = (\n                i * (self.fold_stride) if self.fixed_train_size else 0\n            )\n            train_iloc_end = self.initial_train_size + i * (self.fold_stride)\n            test_iloc_start = train_iloc_end\n        else:\n            # NOTE: The train size doesn't increase and doesn't move.\n            train_iloc_start = 0\n            train_iloc_end = self.initial_train_size\n            test_iloc_start = self.initial_train_size + i * (self.fold_stride)\n\n        if self.window_size is not None:\n            last_window_iloc_start = test_iloc_start - self.window_size\n\n        test_iloc_end = test_iloc_start + self.gap + self.steps\n\n        partitions = [\n            idx[train_iloc_start:train_iloc_end],\n            (\n                idx[last_window_iloc_start:test_iloc_start]\n                if self.window_size is not None\n                else []\n            ),\n            idx[test_iloc_start:test_iloc_end],\n            idx[test_iloc_start + self.gap : test_iloc_end],\n        ]\n        folds.append(partitions)\n        i += 1\n\n    # NOTE: Delete all incomplete folds at the end if not allowed\n    n_removed_folds = 0\n    if not self.allow_incomplete_fold:\n        # NOTE: While folds and the last \"test_index_with_gap\" is incomplete,\n        # calculating len of range objects\n        while folds and len(folds[-1][3]) &lt; self.steps:\n            folds.pop()\n            n_removed_folds += 1\n\n    # Replace partitions inside folds with length 0 with `None`\n    folds = [\n        [partition if len(partition) &gt; 0 else None for partition in fold]\n        for fold in folds\n    ]\n\n    # Create a flag to know whether to train the forecaster\n    if self.refit == 0:\n        self.refit = False\n\n    if isinstance(self.refit, bool):\n        fit_forecaster = [self.refit] * len(folds)\n        fit_forecaster[0] = True\n    else:\n        fit_forecaster = [False] * len(folds)\n        for i in range(0, len(fit_forecaster), self.refit):\n            fit_forecaster[i] = True\n\n    for i in range(len(folds)):\n        folds[i].insert(0, i)\n        folds[i].append(fit_forecaster[i])\n        if fit_forecaster[i] is False:\n            folds[i][1] = folds[i - 1][1]\n\n    index_to_skip = []\n    if self.skip_folds is not None:\n        if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n            index_to_keep = np.arange(0, len(folds), self.skip_folds)\n            index_to_skip = np.setdiff1d(\n                np.arange(0, len(folds)), index_to_keep, assume_unique=True\n            )\n            index_to_skip = [\n                int(x) for x in index_to_skip\n            ]  # Required since numpy 2.0\n        if isinstance(self.skip_folds, list):\n            index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]\n\n    if self.verbose:\n        self._print_info(\n            index=index,\n            folds=folds,\n            externally_fitted=externally_fitted,\n            n_removed_folds=n_removed_folds,\n            index_to_skip=index_to_skip,\n        )\n\n    folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n    if not self.return_all_indexes:\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        folds = [\n            [\n                fold[0],\n                [fold[1][0], fold[1][-1] + 1],\n                (\n                    [fold[2][0], fold[2][-1] + 1]\n                    if self.window_size is not None\n                    else []\n                ),\n                [fold[3][0], fold[3][-1] + 1],\n                [fold[4][0], fold[4][-1] + 1],\n                fold[5],\n            ]\n            for fold in folds\n        ]\n\n    if externally_fitted:\n        self.initial_train_size = None\n        folds[0][5] = False\n\n    if as_pandas:\n        if self.window_size is None:\n            for fold in folds:\n                fold[2] = [None, None]\n\n        if not self.return_all_indexes:\n            folds = pd.DataFrame(\n                data=[\n                    [fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]\n                    for fold in folds\n                ],\n                columns=[\n                    \"fold\",\n                    \"train_start\",\n                    \"train_end\",\n                    \"last_window_start\",\n                    \"last_window_end\",\n                    \"test_start\",\n                    \"test_end\",\n                    \"test_start_with_gap\",\n                    \"test_end_with_gap\",\n                    \"fit_forecaster\",\n                ],\n            )\n        else:\n            folds = pd.DataFrame(\n                data=folds,\n                columns=[\n                    \"fold\",\n                    \"train_index\",\n                    \"last_window_index\",\n                    \"test_index\",\n                    \"test_index_with_gap\",\n                    \"fit_forecaster\",\n                ],\n            )\n\n    return folds\n</code></pre>"},{"location":"api/model_selection/#utilities-and-metrics","title":"Utilities and Metrics","text":""},{"location":"api/model_selection/#utils_common","title":"utils_common","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_common","title":"<code>spotforecast2_safe.model_selection.utils_common</code>","text":"<p>Common validation and initialization utilities for model selection.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_common.OneStepAheadValidationWarning","title":"<code>OneStepAheadValidationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used when validation is performed with one-step-ahead predictions.</p> Source code in <code>src/spotforecast2_safe/model_selection/utils_common.py</code> <pre><code>class OneStepAheadValidationWarning(UserWarning):\n    \"\"\"\n    Warning used when validation is performed with one-step-ahead predictions.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_common.check_backtesting_input","title":"<code>check_backtesting_input(forecaster, cv, metric, add_aggregated_metric=True, y=None, series=None, exog=None, interval=None, interval_method='bootstrapping', alpha=None, n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123, return_predictors=False, freeze_params=True, n_jobs='auto', show_progress=True, suppress_warnings=False)</code>","text":"<p>This is a helper function to check most inputs of backtesting functions in modules <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model.</p> required <code>cv</code> <code>object</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>add_aggregated_metric</code> <code>bool</code> <p>If <code>True</code>, the aggregated metrics (average, weighted average and pooling) over all levels are also returned (only multiseries).</p> <code>True</code> <code>y</code> <code>Series | None</code> <p>Training time series for uni-series forecasters.</p> <code>None</code> <code>series</code> <code>DataFrame | dict[str, Series | DataFrame]</code> <p>Training time series for multi-series forecasters.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | dict[str, Series | DataFrame] | None</code> <p>Exogenous variables.</p> <code>None</code> <code>interval</code> <code>float | list[float] | tuple[float] | str | object | None</code> <p>Specifies whether probabilistic predictions should be estimated and the method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must be between 0 and 100 inclusive. For example, a 95% confidence interval can be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10, 50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be generated.</li> <li>If scipy.stats distribution object, the distribution parameters will be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction intervals.</li> <li>'conformal': Employs the conformal prediction split method for interval estimation.</li> </ul> <code>'bootstrapping'</code> <code>alpha</code> <code>float | None</code> <p>The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals.  If <code>False</code>, out_sample_residuals are used if they are already stored inside the forecaster.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned.</p> <code>False</code> <code>n_jobs</code> <code>int | str</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function select_n_jobs_fit_forecaster.</p> <code>'auto'</code> <code>freeze_params</code> <code>bool</code> <p>Determines whether to freeze the model parameters after the first fit for estimators that perform automatic model selection.</p> <ul> <li>If <code>True</code>, the model parameters found during the first fit (e.g., order and seasonal_order for Arima, or smoothing parameters for Ets) are reused in all subsequent refits. This avoids re-running the automatic selection procedure in each fold and reduces runtime.</li> <li>If <code>False</code>, automatic model selection is performed independently in each refit, allowing parameters to adapt across folds. This increases runtime and adds a <code>params</code> column to the output with the parameters selected per fold.</li> </ul> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, spotforecast warnings will be suppressed during the backtesting process.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_backtesting_input\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=3,\n...     initial_train_size=5,\n...     gap=0,\n...     refit=False,\n...     fixed_train_size=False,\n...     allow_incomplete_fold=True\n... )\n&gt;&gt;&gt; check_backtesting_input(\n...     forecaster=forecaster,\n...     cv=cv,\n...     metric=mean_squared_error,\n...     y=y\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/utils_common.py</code> <pre><code>def check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: str | Callable | list[str | Callable],\n    add_aggregated_metric: bool = True,\n    y: pd.Series | None = None,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame] = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = \"bootstrapping\",\n    alpha: float | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    freeze_params: bool = True,\n    n_jobs: int | str = \"auto\",\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in\n    modules `model_selection`.\n\n    Args:\n        forecaster: Forecaster model.\n        cv: TimeSeriesFold object with the information needed to split the data into folds.\n        metric: Metric used to quantify the goodness of fit of the model.\n        add_aggregated_metric: If `True`, the aggregated metrics (average, weighted average and pooling)\n            over all levels are also returned (only multiseries).\n        y: Training time series for uni-series forecasters.\n        series: Training time series for multi-series forecasters.\n        exog: Exogenous variables.\n        interval: Specifies whether probabilistic predictions should be estimated and the\n            method to use. The following options are supported:\n\n            - If `float`, represents the nominal (expected) coverage (between 0 and 1).\n            For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n            - If `list` or `tuple`: Sequence of percentiles to compute, each value must\n            be between 0 and 100 inclusive. For example, a 95% confidence interval can\n            be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10,\n            50 and 90) as `interval = [10, 50, 90]`.\n            - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be generated.\n            - If scipy.stats distribution object, the distribution parameters will\n            be estimated for each prediction.\n            - If None, no probabilistic predictions are estimated.\n        interval_method: Technique used to estimate prediction intervals. Available options:\n\n            - 'bootstrapping': Bootstrapping is used to generate prediction\n            intervals.\n            - 'conformal': Employs the conformal prediction split method for\n            interval estimation.\n        alpha: The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        n_boot: Number of bootstrapping iterations to perform when estimating prediction\n            intervals.\n        use_in_sample_residuals: If `True`, residuals from the training data are used as proxy of prediction\n            error to create prediction intervals.  If `False`, out_sample_residuals\n            are used if they are already stored inside the forecaster.\n        use_binned_residuals: If `True`, residuals are selected based on the predicted values\n            (binned selection).\n            If `False`, residuals are selected randomly.\n        random_state: Seed for the random number generator to ensure reproducibility.\n        return_predictors: If `True`, the predictors used to make the predictions are also returned.\n        n_jobs: The number of jobs to run in parallel. If `-1`, then the number of jobs is\n            set to the number of cores. If 'auto', `n_jobs` is set using the function\n            select_n_jobs_fit_forecaster.\n        freeze_params: Determines whether to freeze the model parameters after the first fit\n            for estimators that perform automatic model selection.\n\n            - If `True`, the model parameters found during the first fit (e.g., order\n            and seasonal_order for Arima, or smoothing parameters for Ets) are reused\n            in all subsequent refits. This avoids re-running the automatic selection\n            procedure in each fold and reduces runtime.\n            - If `False`, automatic model selection is performed independently in each\n            refit, allowing parameters to adapt across folds. This increases runtime\n            and adds a `params` column to the output with the parameters selected per\n            fold.\n        show_progress: Whether to show a progress bar.\n        suppress_warnings: If `True`, spotforecast warnings will be suppressed during the backtesting\n            process.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_backtesting_input\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=3,\n        ...     initial_train_size=5,\n        ...     gap=0,\n        ...     refit=False,\n        ...     fixed_train_size=False,\n        ...     allow_incomplete_fold=True\n        ... )\n        &gt;&gt;&gt; check_backtesting_input(\n        ...     forecaster=forecaster,\n        ...     cv=cv,\n        ...     metric=mean_squared_error,\n        ...     y=y\n        ... )\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a 'TimeSeriesFold' object. Got '{cv_name}'.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterStats\",\n        \"ForecasterEquivalentDate\",\n        \"ForecasterRecursiveClassifier\",\n    ]\n    forecasters_direct = [\n        \"ForecasterDirect\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_no_dict = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\"ForecasterRecursiveMultiSeries\"]\n    # NOTE: ForecasterStats has interval but not with bootstrapping or conformal\n    forecasters_boot_conformal = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_return_predictors = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRecursiveClassifier\",\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = \"y\"\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi_no_dict:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = \"series\"\n        data_length = len(series)\n\n    elif forecaster_name in forecasters_multi_dict:\n\n        # NOTE: Checks are not need as they are done in the function\n        # `check_preprocess_series` that is used before `check_backtesting_input`\n        # in the backtesting function.\n\n        data_name = \"series\"\n        data_length = max([len(series[serie]) for serie in series])\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            # NOTE: Checks are not need as they are done in the function\n            # `check_preprocess_exog_multiseries` that is used before\n            # `check_backtesting_input` in the backtesting function.\n            pass\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, \"differentiation\"):\n        if forecaster.differentiation_max != cv.differentiation:\n            if forecaster_name == \"ForecasterRecursiveMultiSeries\" and isinstance(\n                forecaster.differentiation, dict\n            ):\n                raise ValueError(\n                    f\"When using a dict as `differentiation` in ForecasterRecursiveMultiSeries, \"\n                    f\"the `differentiation` included in the cv ({cv.differentiation}) must be \"\n                    f\"the same as the maximum `differentiation` included in the forecaster \"\n                    f\"({forecaster.differentiation_max}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n            else:\n                raise ValueError(\n                    f\"The differentiation included in the forecaster \"\n                    f\"({forecaster.differentiation_max}) differs from the differentiation \"\n                    f\"included in the cv ({cv.differentiation}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        # NOTE: Checks when initial_train_size is not None cannot be done here\n        # because the forecaster is not fitted yet and we don't know the\n        # window_size since pd.DateOffset is not a fixed window size.\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}) or \"\n                f\"a date within this range of the index.\"\n            )\n    elif initial_train_size is not None:\n        if forecaster_name in forecasters_uni:\n            index = cv._extract_index(y)\n        else:\n            index = cv._extract_index(series)\n\n        initial_train_size = date_to_index_position(\n            index=index,\n            date_input=initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n        if (\n            initial_train_size &lt; forecaster.window_size\n            or initial_train_size &gt;= data_length\n        ):\n            raise ValueError(\n                f\"If `initial_train_size` is an integer, it must be greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}). If \"\n                f\"it is a date, it must be within this range of the index.\"\n            )\n        if allow_incomplete_fold:\n            # At least one observation after the gap to allow incomplete fold\n            if data_length &lt;= initial_train_size + gap:\n                raise ValueError(\n                    f\"`{data_name}` must have more than `initial_train_size + gap` \"\n                    f\"observations to create at least one fold.\\n\"\n                    f\"    Time series length: {data_length}\\n\"\n                    f\"    Required &gt; {initial_train_size + gap}\\n\"\n                    f\"    initial_train_size: {initial_train_size}\\n\"\n                    f\"    gap: {gap}\\n\"\n                )\n        else:\n            # At least one complete fold\n            if data_length &lt; initial_train_size + gap + steps:\n                raise ValueError(\n                    f\"`{data_name}` must have at least `initial_train_size + gap + steps` \"\n                    f\"observations to create a minimum of one complete fold \"\n                    f\"(allow_incomplete_fold=False).\\n\"\n                    f\"    Time series length: {data_length}\\n\"\n                    f\"    Required &gt;= {initial_train_size + gap + steps}\\n\"\n                    f\"    initial_train_size: {initial_train_size}\\n\"\n                    f\"    gap: {gap}\\n\"\n                    f\"    steps: {steps}\\n\"\n                )\n    else:\n        if forecaster_name in [\"ForecasterStats\", \"ForecasterEquivalentDate\"]:\n            raise ValueError(\n                f\"When using {forecaster_name}, `initial_train_size` must be an \"\n                f\"integer smaller than the length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == \"ForecasterStats\" and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterStats. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot &lt; 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state &lt; 0:\n        raise TypeError(\n            f\"`random_state` must be an integer greater than 0. Got {random_state}.\"\n        )\n    if not isinstance(return_predictors, bool):\n        raise TypeError(\"`return_predictors` must be a boolean: `True`, `False`.\")\n    if not isinstance(freeze_params, bool):\n        raise TypeError(\"`freeze_params` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != \"auto\":\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n\n        if forecaster_name in forecasters_boot_conformal:\n\n            if interval_method == \"conformal\":\n                if not isinstance(interval, (float, list, tuple)):\n                    raise TypeError(\n                        f\"When `interval_method` is 'conformal', `interval` must \"\n                        f\"be a float or a list/tuple defining a symmetric interval. \"\n                        f\"Got {type(interval)}.\"\n                    )\n            elif interval_method == \"bootstrapping\":\n                if not isinstance(interval, (float, list, tuple, str)) and (\n                    not hasattr(interval, \"_pdf\")\n                    or not callable(getattr(interval, \"fit\", None))\n                ):\n                    raise TypeError(\n                        f\"When `interval_method` is 'bootstrapping', `interval` \"\n                        f\"must be a float, a list or tuple of floats, a \"\n                        f\"scipy.stats distribution object (with methods `_pdf` and \"\n                        f\"`fit`) or the string 'bootstrapping'. Got {type(interval)}.\"\n                    )\n                if isinstance(interval, (list, tuple)):\n                    for i in interval:\n                        if not isinstance(i, (int, float)):\n                            raise TypeError(\n                                f\"`interval` must be a list or tuple of floats. \"\n                                f\"Got {type(i)} in {interval}.\"\n                            )\n                    if len(interval) == 2:\n                        check_interval(interval=interval)\n                    else:\n                        for q in interval:\n                            if (q &lt; 0.0) or (q &gt; 100.0):\n                                raise ValueError(\n                                    \"When `interval` is a list or tuple, all values must be \"\n                                    \"between 0 and 100 inclusive.\"\n                                )\n                elif isinstance(interval, str):\n                    if interval != \"bootstrapping\":\n                        raise ValueError(\n                            f\"When `interval` is a string, it must be 'bootstrapping'.\"\n                            f\"Got {interval}.\"\n                        )\n            else:\n                raise ValueError(\n                    f\"`interval_method` must be 'bootstrapping' or 'conformal'. \"\n                    f\"Got {interval_method}.\"\n                )\n        else:\n            if forecaster_name == \"ForecasterRecursiveClassifier\":\n                raise ValueError(\n                    f\"`interval` is not supported for {forecaster_name}. Class \"\n                    f\"probabilities are returned by default during backtesting, \"\n                    f\"set `interval=None`.\"\n                )\n            check_interval(interval=interval, alpha=alpha)\n\n    if return_predictors and forecaster_name not in forecasters_return_predictors:\n        raise ValueError(\n            f\"`return_predictors` is only allowed for forecasters of type \"\n            f\"{forecasters_return_predictors}. Got {forecaster_name}.\"\n        )\n\n    if forecaster_name in forecasters_direct and forecaster.max_step &lt; steps + gap:\n        raise ValueError(\n            f\"When using a {forecaster_name}, the combination of steps \"\n            f\"+ gap ({steps + gap}) cannot be greater than the `steps` parameter \"\n            f\"declared when the forecaster is initialized ({forecaster.max_step}).\"\n        )\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_common.check_one_step_ahead_input","title":"<code>check_one_step_ahead_input(forecaster, cv, metric, y=None, series=None, exog=None, show_progress=True, suppress_warnings=False)</code>","text":"<p>This is a helper function to check most inputs of hyperparameter tuning functions in modules <code>model_selection</code> when using a <code>OneStepAheadFold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model.</p> required <code>cv</code> <code>object</code> <p>OneStepAheadFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>y</code> <code>Series | None</code> <p>Training time series for uni-series forecasters.</p> <code>None</code> <code>series</code> <code>DataFrame | dict[str, Series | DataFrame]</code> <p>Training time series for multi-series forecasters.</p> <code>None</code> <code>exog</code> <code>Series | DataFrame | dict[str, Series | DataFrame] | None</code> <p>Exogenous variables.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, spotforecast warnings will be suppressed during the hyperparameter search.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_one_step_ahead_input\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import OneStepAheadFold\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; cv = OneStepAheadFold(\n...     initial_train_size=5,\n...     return_all_predictions=False\n... )\n&gt;&gt;&gt; check_one_step_ahead_input(\n...     forecaster=forecaster,\n...     cv=cv,\n...     metric=mean_squared_error,\n...     y=y\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/utils_common.py</code> <pre><code>def check_one_step_ahead_input(\n    forecaster: object,\n    cv: object,\n    metric: str | Callable | list[str | Callable],\n    y: pd.Series | None = None,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame] = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; None:\n    \"\"\"\n    This is a helper function to check most inputs of hyperparameter tuning\n    functions in modules `model_selection` when using a `OneStepAheadFold`.\n\n    Args:\n        forecaster: Forecaster model.\n        cv: OneStepAheadFold object with the information needed to split the data into folds.\n        metric: Metric used to quantify the goodness of fit of the model.\n        y: Training time series for uni-series forecasters.\n        series: Training time series for multi-series forecasters.\n        exog: Exogenous variables.\n        show_progress: Whether to show a progress bar.\n        suppress_warnings: If `True`, spotforecast warnings will be suppressed during the hyperparameter\n            search.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_one_step_ahead_input\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.model_selection import OneStepAheadFold\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n        &gt;&gt;&gt; cv = OneStepAheadFold(\n        ...     initial_train_size=5,\n        ...     return_all_predictions=False\n        ... )\n        &gt;&gt;&gt; check_one_step_ahead_input(\n        ...     forecaster=forecaster,\n        ...     cv=cv,\n        ...     metric=mean_squared_error,\n        ...     y=y\n        ... )\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"OneStepAheadFold\":\n        raise TypeError(f\"`cv` must be a 'OneStepAheadFold' object. Got '{cv_name}'.\")\n\n    initial_train_size = cv.initial_train_size\n\n    forecasters_one_step_ahead = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterRecursiveClassifier\",\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n    ]\n    if forecaster_name not in forecasters_one_step_ahead:\n        raise TypeError(\n            f\"Only forecasters of type {forecasters_one_step_ahead} are allowed \"\n            f\"when using `cv` of type `OneStepAheadFold`. Got {forecaster_name}.\"\n        )\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterRecursiveClassifier\",\n    ]\n    forecasters_multi_no_dict = [\n        \"ForecasterDirectMultiVariate\",\n    ]\n    forecasters_multi_dict = [\"ForecasterRecursiveMultiSeries\"]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(f\"`y` must be a pandas Series. Got {type(y)}\")\n        data_name = \"y\"\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi_no_dict:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}\")\n        data_name = \"series\"\n        data_length = len(series)\n\n    elif forecaster_name in forecasters_multi_dict:\n\n        # NOTE: Checks are not need as they are done in the function\n        # `check_preprocess_series` that is used before `check_one_step_ahead_input`\n        # in the backtesting function.\n\n        data_name = \"series\"\n        data_length = max([len(series[serie]) for serie in series])\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            # NOTE: Checks are not need as they are done in the function\n            # `check_preprocess_exog_multiseries` that is used before\n            # `check_backtesting_input` in the backtesting function.\n            pass\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, \"differentiation\"):\n        if forecaster.differentiation_max != cv.differentiation:\n            if forecaster_name == \"ForecasterRecursiveMultiSeries\" and isinstance(\n                forecaster.differentiation, dict\n            ):\n                raise ValueError(\n                    f\"When using a dict as `differentiation` in ForecasterRecursiveMultiSeries, \"\n                    f\"the `differentiation` included in the cv ({cv.differentiation}) must be \"\n                    f\"the same as the maximum `differentiation` included in the forecaster \"\n                    f\"({forecaster.differentiation_max}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n            else:\n                raise ValueError(\n                    f\"The differentiation included in the forecaster \"\n                    f\"({forecaster.differentiation_max}) differs from the differentiation \"\n                    f\"included in the cv ({cv.differentiation}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name in forecasters_uni:\n        index = cv._extract_index(y)\n    else:\n        index = cv._extract_index(series)\n\n    initial_train_size = date_to_index_position(\n        index=index,\n        date_input=initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n    if initial_train_size &lt; forecaster.window_size or initial_train_size &gt;= data_length:\n        raise ValueError(\n            f\"If `initial_train_size` is an integer, it must be greater than \"\n            f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n            f\"and smaller than the length of `{data_name}` ({data_length}). If \"\n            f\"it is a date, it must be within this range of the index.\"\n        )\n\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n\n    if not suppress_warnings:\n        warnings.warn(\n            \"One-step-ahead predictions are used for faster model comparison, but they \"\n            \"may not fully represent multi-step prediction performance. It is recommended \"\n            \"to backtest the final model for a more accurate multi-step performance \"\n            \"estimate.\",\n            OneStepAheadValidationWarning,\n        )\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_common.initialize_lags_grid","title":"<code>initialize_lags_grid(forecaster, lags_grid=None)</code>","text":"<p>Initialize lags grid and lags label for model selection.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model. ForecasterRecursive, ForecasterDirect, ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.</p> required <code>lags_grid</code> <code>list[int | list[int] | ndarray[int] | range[int]] | dict[str, list[int | list[int] | ndarray[int] | range[int]]] | None</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range objects. If <code>dict</code>, the keys are used as labels in the <code>results</code> DataFrame, and the values are used as the lists of lags to try.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict[str, int], str]</code> <p>(lags_grid, lags_label) - lags_grid (dict): Dictionary with lags configuration for each iteration. - lags_label (str): Label for lags representation in the results object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import initialize_lags_grid\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; lags_grid = [2, 4]\n&gt;&gt;&gt; lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n&gt;&gt;&gt; print(lags_grid)\n{'2': 2, '4': 4}\n&gt;&gt;&gt; print(lags_label)\nvalues\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/utils_common.py</code> <pre><code>def initialize_lags_grid(\n    forecaster: object,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n) -&gt; tuple[dict[str, int], str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection.\n\n    Args:\n        forecaster: Forecaster model. ForecasterRecursive, ForecasterDirect,\n            ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n        lags_grid: Lists of lags to try, containing int, lists, numpy ndarray, or range\n            objects. If `dict`, the keys are used as labels in the `results`\n            DataFrame, and the values are used as the lists of lags to try.\n\n    Returns:\n        tuple: (lags_grid, lags_label)\n            - lags_grid (dict): Dictionary with lags configuration for each iteration.\n            - lags_label (str): Label for lags representation in the results object.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.model_selection.utils_common import initialize_lags_grid\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n        &gt;&gt;&gt; lags_grid = [2, 4]\n        &gt;&gt;&gt; lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n        &gt;&gt;&gt; print(lags_grid)\n        {'2': 2, '4': 4}\n        &gt;&gt;&gt; print(lags_label)\n        values\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            f\"`lags_grid` argument must be a list, dict or None. \"\n            f\"Got {type(lags_grid)}.\"\n        )\n\n    lags_label = \"values\"\n    if isinstance(lags_grid, list):\n        lags_grid = {f\"{lags}\": lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f\"{lags}\": lags}\n    else:\n        lags_label = \"keys\"\n\n    return lags_grid, lags_label\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_common.select_n_jobs_backtesting","title":"<code>select_n_jobs_backtesting(forecaster, refit)</code>","text":"<p>Select the optimal number of jobs to use in the backtesting process. This selection is based on heuristics and is not guaranteed to be optimal.</p> <p>The number of jobs is chosen as follows:</p> <ul> <li>If <code>refit</code> is an integer, then <code>n_jobs = 1</code>. This is because parallelization doesn't work with intermittent refit.</li> <li>If forecaster is 'ForecasterRecursive' and estimator is a linear estimator, then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterRecursive' and estimator is not a linear estimator then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate' and <code>refit = True</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate' and <code>refit = False</code>, then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterRecursiveMultiSeries', then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterStats' or 'ForecasterEquivalentDate', then <code>n_jobs = 1</code>.</li> <li>If estimator is a <code>LGBMRegressor(n_jobs=1)</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If estimator is a <code>LGBMRegressor</code> with internal n_jobs != 1, then <code>n_jobs = 1</code>. This is because <code>lightgbm</code> is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model.</p> required <code>refit</code> <code>bool | int</code> <p>If the forecaster is refitted during the backtesting process.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of jobs to run in parallel.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import select_n_jobs_backtesting\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; select_n_jobs_backtesting(forecaster, refit=True)\n1\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/utils_common.py</code> <pre><code>def select_n_jobs_backtesting(forecaster: object, refit: bool | int) -&gt; int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't\n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and estimator is a linear estimator,\n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and estimator is not a linear\n    estimator then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterStats' or 'ForecasterEquivalentDate',\n    then `n_jobs = 1`.\n    - If estimator is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If estimator is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Args:\n        forecaster: Forecaster model.\n        refit: If the forecaster is refitted during the backtesting process.\n\n    Returns:\n        int: The number of jobs to run in parallel.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.model_selection.utils_common import select_n_jobs_backtesting\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n        &gt;&gt;&gt; select_n_jobs_backtesting(forecaster, refit=True)\n        1\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if forecaster_name == \"ForecasterStats\":\n        n_jobs = 1\n        return n_jobs\n\n    if isinstance(forecaster.estimator, Pipeline):\n        estimator = forecaster.estimator[-1]\n    else:\n        estimator = forecaster.estimator\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in {\"ForecasterRecursive\", \"ForecasterRecursiveClassifier\"}:\n            if isinstance(estimator, (LinearModel, LinearClassifierMixin)):\n                n_jobs = 1\n            elif type(estimator).__name__ in {\"LGBMRegressor\", \"LGBMClassifier\"}:\n                n_jobs = cpu_count() - 1 if estimator.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in {\"ForecasterDirect\", \"ForecasterDirectMultiVariate\"}:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in {\"ForecasterRecursiveMultiSeries\"}:\n            if type(estimator).__name__ == \"LGBMRegressor\":\n                n_jobs = cpu_count() - 1 if estimator.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in {\"ForecasterEquivalentDate\"}:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"api/model_selection/#utils_metrics","title":"utils_metrics","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.utils_metrics","title":"<code>spotforecast2_safe.model_selection.utils_metrics</code>","text":"<p>Metrics calculation utilities for model selection.</p>"},{"location":"api/model_selection/#validation","title":"validation","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.validation","title":"<code>spotforecast2_safe.model_selection.validation</code>","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.validation.backtesting_forecaster","title":"<code>backtesting_forecaster(forecaster, y, cv, metric, exog=None, interval=None, interval_method='bootstrapping', n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123, return_predictors=False, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False)</code>","text":"<p>Backtesting of forecaster model following the folds generated by the TimeSeriesFold class and using the metric(s) provided.</p> <p>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is set to <code>None</code> in the TimeSeriesFold class, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</p> <p>A copy of the original forecaster is created so that it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate)</code> <p>Forecaster model.</p> required <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>str | Callable | list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>str</code>: {'mean_squared_error', 'mean_absolute_error',   'mean_absolute_percentage_error', 'mean_squared_log_error',   'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code>   (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>Series | DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i]. Defaults to None.</p> <code>None</code> <code>interval</code> <code>float | list | tuple | str | object</code> <p>Specifies whether probabilistic predictions should be estimated and the method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must   be between 0 and 100 inclusive. For example, a 95% confidence interval can   be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10,   50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be   generated.</li> <li>If scipy.stats distribution object, the distribution parameters will   be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated. Defaults to None.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction intervals.</li> <li>'conformal': Employs the conformal prediction split method for   interval estimation. Defaults to 'bootstrapping'.</li> </ul> <code>'bootstrapping'</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned. Defaults to False.</p> <code>False</code> <code>n_jobs</code> <code>int | str</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function <code>skforecast.utils.select_n_jobs_backtesting</code>. Defaults to 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used for backtesting. Defaults to False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, spotforecast warnings will be suppressed during the backtesting process. See <code>spotforecast.exceptions.warn_skforecast_categories</code> for more information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <ul> <li>metric_values: Value(s) of the metric(s).</li> <li> <p>backtest_predictions: Value of predictions. The DataFrame includes   the following columns:</p> </li> <li> <p>fold: Indicates the fold number where the prediction was made.</p> </li> <li>pred: Predicted values for the corresponding series and time steps.</li> </ul> <p>If <code>interval</code> is not <code>None</code>, additional columns are included depending   on the method:</p> <ul> <li>For <code>float</code>: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> of 2 elements: Columns <code>lower_bound</code> and     <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> with multiple percentiles: One column per     percentile (e.g., <code>p_10</code>, <code>p_50</code>, <code>p_90</code>).</li> <li>For <code>'bootstrapping'</code>: One column per bootstrapping iteration     (e.g., <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n</code>).</li> <li>For <code>scipy.stats</code> distribution objects: One column for each     estimated parameter of the distribution (e.g., <code>loc</code>, <code>scale</code>).</li> </ul> <p>If <code>return_predictors</code> is <code>True</code>, one column per predictor is created.</p> <p>Depending on the relation between <code>steps</code> and <code>fold_stride</code>, the output   may include repeated indexes (if <code>fold_stride &lt; steps</code>) or gaps   (if <code>fold_stride &gt; steps</code>). See Notes below for more details.</p> Notes <p>Note on <code>fold_stride</code> vs. <code>steps</code>:</p> <ul> <li>If <code>fold_stride == steps</code>, test sets are placed back-to-back without overlap.   Each observation appears only once in the output DataFrame, so the   index is unique.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap. Multiple forecasts are   generated for the same observations and, therefore, the output   DataFrame contains repeated indexes.</li> <li>If <code>fold_stride &gt; steps</code>, there are gaps between consecutive test sets.   Some observations in the series will not have associated predictions,   so the output DataFrame has non-contiguous indexes.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(random_state=123),\n...     lags=2\n... )\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=2,\n...     initial_train_size=5,\n...     refit=False\n... )\n&gt;&gt;&gt; metric_values, backtest_predictions = backtesting_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     metric='mean_squared_error'\n... )\n&gt;&gt;&gt; metric_values\n   mean_squared_error\n0            0.201334\n&gt;&gt;&gt; backtest_predictions\n   fold  pred\n5     0  5.18\n6     0  6.10\n7     1  7.36\n8     1  8.40\n9     2  9.31\n</code></pre> Source code in <code>src/spotforecast2_safe/model_selection/validation.py</code> <pre><code>def backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = \"bootstrapping\",\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n\n    A copy of the original forecaster is created so that it is not modified during\n    the process.\n\n    Args:\n        forecaster (ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate):\n            Forecaster model.\n        y (pd.Series): Training time series.\n        cv (TimeSeriesFold): TimeSeriesFold object with the information needed to\n            split the data into folds.\n        metric (str | Callable | list): Metric used to quantify the goodness of fit\n            of the model.\n\n            - If `str`: {'mean_squared_error', 'mean_absolute_error',\n              'mean_absolute_percentage_error', 'mean_squared_log_error',\n              'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n            - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n              (Optional) that returns a float.\n            - If `list`: List containing multiple strings and/or Callables.\n        exog (pd.Series | pd.DataFrame, optional): Exogenous variable/s included as\n            predictor/s. Must have the same number of observations as `y` and should\n            be aligned so that y[i] is regressed on exog[i]. Defaults to None.\n        interval (float | list | tuple | str | object, optional): Specifies whether\n            probabilistic predictions should be estimated and the method to use.\n            The following options are supported:\n\n            - If `float`, represents the nominal (expected) coverage (between 0 and 1).\n            For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n            - If `list` or `tuple`: Sequence of percentiles to compute, each value must\n              be between 0 and 100 inclusive. For example, a 95% confidence interval can\n              be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10,\n              50 and 90) as `interval = [10, 50, 90]`.\n            - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be\n              generated.\n            - If scipy.stats distribution object, the distribution parameters will\n              be estimated for each prediction.\n            - If None, no probabilistic predictions are estimated.\n            Defaults to None.\n        interval_method (str, optional): Technique used to estimate prediction\n            intervals. Available options:\n\n            - 'bootstrapping': Bootstrapping is used to generate prediction intervals.\n            - 'conformal': Employs the conformal prediction split method for\n              interval estimation.\n            Defaults to 'bootstrapping'.\n        n_boot (int, optional): Number of bootstrapping iterations to perform when\n            estimating prediction intervals. Defaults to 250.\n        use_in_sample_residuals (bool, optional): If `True`, residuals from the\n            training data are used as proxy of prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals (bool, optional): If `True`, residuals are selected\n            based on the predicted values (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state (int, optional): Seed for the random number generator to\n            ensure reproducibility. Defaults to 123.\n        return_predictors (bool, optional): If `True`, the predictors used to make\n            the predictions are also returned. Defaults to False.\n        n_jobs (int | str, optional): The number of jobs to run in parallel. If `-1`,\n            then the number of jobs is set to the number of cores. If 'auto', `n_jobs`\n            is set using the function `skforecast.utils.select_n_jobs_backtesting`.\n            Defaults to 'auto'.\n        verbose (bool, optional): Print number of folds and index of training and\n            validation sets used for backtesting. Defaults to False.\n        show_progress (bool, optional): Whether to show a progress bar.\n            Defaults to True.\n        suppress_warnings (bool, optional): If `True`, spotforecast warnings will be\n            suppressed during the backtesting process. See\n            `spotforecast.exceptions.warn_skforecast_categories` for more information.\n            Defaults to False.\n\n    Returns:\n        tuple (pd.DataFrame, pd.DataFrame):\n            - metric_values: Value(s) of the metric(s).\n            - backtest_predictions: Value of predictions. The DataFrame includes\n              the following columns:\n\n              - fold: Indicates the fold number where the prediction was made.\n              - pred: Predicted values for the corresponding series and time steps.\n\n              If `interval` is not `None`, additional columns are included depending\n              on the method:\n\n              - For `float`: Columns `lower_bound` and `upper_bound`.\n              - For `list` or `tuple` of 2 elements: Columns `lower_bound` and\n                `upper_bound`.\n              - For `list` or `tuple` with multiple percentiles: One column per\n                percentile (e.g., `p_10`, `p_50`, `p_90`).\n              - For `'bootstrapping'`: One column per bootstrapping iteration\n                (e.g., `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n`).\n              - For `scipy.stats` distribution objects: One column for each\n                estimated parameter of the distribution (e.g., `loc`, `scale`).\n\n              If `return_predictors` is `True`, one column per predictor is created.\n\n              Depending on the relation between `steps` and `fold_stride`, the output\n              may include repeated indexes (if `fold_stride &lt; steps`) or gaps\n              (if `fold_stride &gt; steps`). See Notes below for more details.\n\n    Notes:\n        Note on `fold_stride` vs. `steps`:\n\n        - If `fold_stride == steps`, test sets are placed back-to-back without overlap.\n          Each observation appears only once in the output DataFrame, so the\n          index is unique.\n        - If `fold_stride &lt; steps`, test sets overlap. Multiple forecasts are\n          generated for the same observations and, therefore, the output\n          DataFrame contains repeated indexes.\n        - If `fold_stride &gt; steps`, there are gaps between consecutive test sets.\n          Some observations in the series will not have associated predictions,\n          so the output DataFrame has non-contiguous indexes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotforecast2.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(random_state=123),\n        ...     lags=2\n        ... )\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=2,\n        ...     initial_train_size=5,\n        ...     refit=False\n        ... )\n        &gt;&gt;&gt; metric_values, backtest_predictions = backtesting_forecaster(\n        ...     forecaster=forecaster,\n        ...     y=y,\n        ...     cv=cv,\n        ...     metric='mean_squared_error'\n        ... )\n        &gt;&gt;&gt; metric_values\n           mean_squared_error\n        0            0.201334\n        &gt;&gt;&gt; backtest_predictions\n           fold  pred\n        5     0  5.18\n        6     0  6.10\n        7     1  7.36\n        8     1  8.40\n        9     2  9.31\n    \"\"\"\n\n    forecaters_allowed = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterEquivalentDate\",\n        \"ForecasterRecursiveClassifier\",\n    ]\n\n    if type(forecaster).__name__ not in forecaters_allowed:\n        raise TypeError(\n            f\"`forecaster` must be of type {forecaters_allowed}. For all other \"\n            f\"types of forecasters use the other functions available in the \"\n            f\"`model_selection` module.\"\n        )\n\n    check_backtesting_input(\n        forecaster=forecaster,\n        cv=cv,\n        y=y,\n        metric=metric,\n        interval=interval,\n        interval_method=interval_method,\n        n_boot=n_boot,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        random_state=random_state,\n        return_predictors=return_predictors,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n    )\n\n    metric_values, backtest_predictions = _backtesting_forecaster(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        metric=metric,\n        exog=exog,\n        interval=interval,\n        interval_method=interval_method,\n        n_boot=n_boot,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        random_state=random_state,\n        return_predictors=return_predictors,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing Module","text":"<p>Tools for data preprocessing, cleaning, and transformation.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing","title":"<code>spotforecast2_safe.preprocessing</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner","title":"<code>QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures","title":"<code>RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        For each statistic and window size combination, computes the rolling\n        statistic across the input time series. The output contains NaN values\n        for the initial positions where the window cannot be fully computed.\n\n        Args:\n            X: Time series data as 1D numpy array or array-like.\n\n        Returns:\n            np.ndarray: Array of shape (len(X), len(features_names)) containing\n                the computed rolling statistics. Each column corresponds to a\n                feature in features_names. Early positions contain NaN values\n                before the window is fully populated.\n        \"\"\"\n        # Assume X is 1D array\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n\n        idx_feature = 0\n        for ws in self.window_sizes:\n            for func in self.stats_funcs:\n                # Naive rolling window loop - can be optimized or use pandas rolling\n                # Using pandas for simplicity and speed if X is convertible\n                series = pd.Series(X)\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n                idx_feature += 1\n\n        return output\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n\n        Transforms a pandas Series into a DataFrame of rolling statistics while\n        preserving the original index. Useful for maintaining time alignment\n        with the input data.\n\n        Args:\n            X: Time series data as pandas Series. The index is preserved in output.\n\n        Returns:\n            pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n                columns are feature names and index matches the input Series.\n                Contains NaN values at the beginning where windows are incomplete.\n\n        Note:\n            This method is preferred over transform() when working with time-indexed\n            data, as it preserves the temporal index and is compatible with\n            forecasting workflows.\n        \"\"\"\n        values = X.to_numpy()\n        transformed = self.transform(values)\n        return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>For each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D numpy array or array-like.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    For each statistic and window size combination, computes the rolling\n    statistic across the input time series. The output contains NaN values\n    for the initial positions where the window cannot be fully computed.\n\n    Args:\n        X: Time series data as 1D numpy array or array-like.\n\n    Returns:\n        np.ndarray: Array of shape (len(X), len(features_names)) containing\n            the computed rolling statistics. Each column corresponds to a\n            feature in features_names. Early positions contain NaN values\n            before the window is fully populated.\n    \"\"\"\n    # Assume X is 1D array\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n\n    idx_feature = 0\n    for ws in self.window_sizes:\n        for func in self.stats_funcs:\n            # Naive rolling window loop - can be optimized or use pandas rolling\n            # Using pandas for simplicity and speed if X is convertible\n            series = pd.Series(X)\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n            idx_feature += 1\n\n    return output\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> <p>Transforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series</code> <p>Time series data as pandas Series. The index is preserved in output.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.</p> Note <p>This method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n\n    Transforms a pandas Series into a DataFrame of rolling statistics while\n    preserving the original index. Useful for maintaining time alignment\n    with the input data.\n\n    Args:\n        X: Time series data as pandas Series. The index is preserved in output.\n\n    Returns:\n        pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n            columns are feature names and index matches the input Series.\n            Contains NaN values at the beginning where windows are incomplete.\n\n    Note:\n        This method is preferred over transform() when working with time-indexed\n        data, as it preserves the temporal index and is compatible with\n        forecasting workflows.\n    \"\"\"\n    values = X.to_numpy()\n    transformed = self.transform(values)\n    return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction","title":"<code>WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.agg_and_resample_data","title":"<code>agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.basic_ts_checks","title":"<code>basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_holidays","title":"<code>curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_weather","title":"<code>curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.custom_weights","title":"<code>custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_missing_weights","title":"<code>get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.get_start_end","title":"<code>get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split_abs_train_val_test","title":"<code>split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split_rel_train_val_test","title":"<code>split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_test = 0\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.visualize_outliers_hist","title":"<code>visualize_outliers_hist(data, data_original, columns=None, contamination=0.01, random_state=1234, figsize=(10, 5), bins=50, **kwargs)</code>","text":"<p>Visualize outliers in DataFrame using stacked histograms.</p> <p>Creates a histogram for each specified column, displaying both regular data and detected outliers in different colors. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height). Default: (10, 5).</p> <code>(10, 5)</code> <code>bins</code> <code>int</code> <p>Number of histogram bins. Default: 50.</p> <code>50</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays matplotlib figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If matplotlib is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_hist\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... })\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_hist(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     figsize=(12, 5),\n...     alpha=0.7\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in DataFrame using stacked histograms.\n\n    Creates a histogram for each specified column, displaying both regular data\n    and detected outliers in different colors. Uses IsolationForest for outlier\n    detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        figsize: Figure size as (width, height). Default: (10, 5).\n        bins: Number of histogram bins. Default: 50.\n        **kwargs: Additional keyword arguments passed to plt.hist() (e.g., color,\n            alpha, edgecolor, etc.).\n\n    Returns:\n        None. Displays matplotlib figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If matplotlib is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_hist\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... })\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_hist(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     figsize=(12, 5),\n        ...     alpha=0.7\n        ... )\n    \"\"\"\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        # Get inliers (non-NaN values in cleaned data)\n        inliers = data[col].dropna()\n\n        # Get outlier values\n        outlier_vals = outliers[col]\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Create figure\n        plt.figure(figsize=figsize)\n        plt.hist(\n            [inliers, outlier_vals],\n            bins=bins,\n            stacked=True,\n            color=[\"lightgrey\", \"red\"],\n            label=[\"Regular Data\", \"Outliers\"],\n            **kwargs,\n        )\n        plt.grid(True, alpha=0.3)\n        plt.title(f\"{col} Distribution with Outliers ({pct_outliers:.2f}%)\")\n        plt.xlabel(\"Value\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.visualize_outliers_plotly_scatter","title":"<code>visualize_outliers_plotly_scatter(data, data_original, columns=None, contamination=0.01, random_state=1234, **kwargs)</code>","text":"<p>Visualize outliers in time series using Plotly scatter plots.</p> <p>Creates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in time series using Plotly scatter plots.\n\n    Creates an interactive time series plot for each specified column, showing\n    regular data as a line and detected outliers as scatter points. Uses\n    IsolationForest for outlier detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        **kwargs: Additional keyword arguments passed to go.Figure.update_layout()\n            (e.g., template, height, etc.).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... }, index=dates)\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_plotly_scatter(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     template='plotly_white'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add regular data as line\n        fig.add_trace(\n            go.Scatter(\n                x=data.index,\n                y=data[col],\n                mode=\"lines\",\n                name=\"Regular Data\",\n                line=dict(color=\"lightgrey\"),\n            )\n        )\n\n        # Add outliers as scatter points\n        outlier_vals = outliers[col]\n        if not outlier_vals.empty:\n            fig.add_trace(\n                go.Scatter(\n                    x=outlier_vals.index,\n                    y=outlier_vals,\n                    mode=\"markers\",\n                    name=\"Outliers\",\n                    marker=dict(color=\"red\", size=8, symbol=\"x\"),\n                )\n            )\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Update layout with custom kwargs\n        layout_kwargs = {\n            \"title\": f\"{col} Time Series with Outliers ({pct_outliers:.2f}%)\",\n            \"xaxis_title\": \"Time\",\n            \"yaxis_title\": \"Value\",\n            \"template\": \"plotly_white\",\n            \"legend\": dict(\n                orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1\n            ),\n        }\n        layout_kwargs.update(kwargs)\n        fig.update_layout(**layout_kwargs)\n        fig.show()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.visualize_ts_comparison","title":"<code>visualize_ts_comparison(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, show_mean=False, **kwargs)</code>","text":"<p>Visualize time series with optional statistical overlays.</p> <p>Similar to visualize_ts_plotly but adds options for statistical overlays like mean values across all datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to column names. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. Default: None.</p> <code>None</code> <code>show_mean</code> <code>bool</code> <p>If True, overlay the mean of all datasets. Default: False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for go.Scatter().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes is empty.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; df1 = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100)\n... }, index=dates1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df2 = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 100)\n... }, index=dates2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare with mean overlay\n&gt;&gt;&gt; visualize_ts_comparison(\n...     {'Dataset1': df1, 'Dataset2': df2},\n...     show_mean=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize time series with optional statistical overlays.\n\n    Similar to visualize_ts_plotly but adds options for statistical overlays\n    like mean values across all datasets.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to column names. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. Default: None.\n        show_mean: If True, overlay the mean of all datasets. Default: False.\n        **kwargs: Additional keyword arguments for go.Scatter().\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes is empty.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df1 = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100)\n        ... }, index=dates1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df2 = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 100)\n        ... }, index=dates2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compare with mean overlay\n        &gt;&gt;&gt; visualize_ts_comparison(\n        ...     {'Dataset1': df1, 'Dataset2': df2},\n        ...     show_mean=True\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # First visualize normally\n    visualize_ts_plotly(\n        dataframes,\n        columns=columns,\n        title_suffix=title_suffix,\n        figsize=figsize,\n        template=template,\n        colors=colors,\n        **kwargs,\n    )\n\n    # If show_mean, create additional mean plot\n    if show_mean:\n        # Determine columns to plot\n        all_columns = set()\n        for df in dataframes.values():\n            all_columns.update(df.columns)\n\n        columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n        for col in columns_to_plot:\n            fig = go.Figure()\n\n            # Add individual traces\n            if colors is None:\n                default_colors = [\n                    \"#1f77b4\",\n                    \"#ff7f0e\",\n                    \"#2ca02c\",\n                    \"#d62728\",\n                    \"#9467bd\",\n                ]\n                colors_dict = {\n                    name: default_colors[i % len(default_colors)]\n                    for i, name in enumerate(dataframes.keys())\n                }\n            else:\n                colors_dict = colors\n\n            for dataset_name, df in dataframes.items():\n                fig.add_trace(\n                    go.Scatter(\n                        x=df.index,\n                        y=df[col],\n                        mode=\"lines\",\n                        name=dataset_name,\n                        line=dict(color=colors_dict[dataset_name], width=1),\n                        opacity=0.5,\n                        **kwargs,\n                    )\n                )\n\n            # Calculate and add mean\n            # Align all dataframes by index and compute mean\n            aligned_dfs = [\n                dataframes[name][[col]].rename(columns={col: name})\n                for name in dataframes.keys()\n            ]\n            combined = pd.concat(aligned_dfs, axis=1)\n            mean_values = combined.mean(axis=1)\n\n            fig.add_trace(\n                go.Scatter(\n                    x=mean_values.index,\n                    y=mean_values,\n                    mode=\"lines\",\n                    name=\"Mean\",\n                    line=dict(color=\"black\", width=3, dash=\"dash\"),\n                )\n            )\n\n            title = f\"{col} (with mean){title_suffix}\"\n\n            fig.update_layout(\n                title=title,\n                xaxis_title=\"Time\",\n                yaxis_title=col,\n                width=figsize[0],\n                height=figsize[1],\n                template=template,\n                legend=dict(\n                    orientation=\"h\",\n                    yanchor=\"bottom\",\n                    y=1.02,\n                    xanchor=\"right\",\n                    x=1,\n                ),\n                hovermode=\"x unified\",\n            )\n\n            fig.show()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.visualize_ts_plotly","title":"<code>visualize_ts_plotly(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, **kwargs)</code>","text":"<p>Visualize multiple time series datasets interactively with Plotly.</p> <p>Creates interactive Plotly scatter plots for specified columns across multiple datasets (e.g., train, validation, test splits). Each dataset is displayed as a separate line with a unique color and name in the legend.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames with datetime index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to the column name in the title. Useful for adding units or descriptions. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template name for styling. Options include 'plotly_white', 'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. If None, uses Plotly default colors. Example: {'Train': 'blue', 'Validation': 'orange'}. Default: None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Scatter() (e.g., mode='lines+markers', line=dict(dash='dash')).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes dict is empty, contains no columns, or if specified columns don't exist in all dataframes.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <code>TypeError</code> <p>If dataframes parameter is not a dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n&gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_train = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100),\n...     'humidity': np.random.normal(60, 10, 100)\n... }, index=dates_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_val = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 50),\n...     'humidity': np.random.normal(55, 10, 50)\n... }, index=dates_val)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_test = pd.DataFrame({\n...     'temperature': np.random.normal(25, 5, 30),\n...     'humidity': np.random.normal(50, 10, 30)\n... }, index=dates_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize all datasets\n&gt;&gt;&gt; dataframes = {\n...     'Train': data_train,\n...     'Validation': data_val,\n...     'Test': data_test\n... }\n&gt;&gt;&gt; visualize_ts_plotly(dataframes)\n</code></pre> <p>Single dataset example:</p> <pre><code>&gt;&gt;&gt; # Visualize single dataset\n&gt;&gt;&gt; dataframes = {'Data': data_train}\n&gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n</code></pre> <p>Custom styling:</p> <pre><code>&gt;&gt;&gt; visualize_ts_plotly(\n...     dataframes,\n...     columns=['temperature'],\n...     template='plotly_dark',\n...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n...     mode='lines+markers'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize multiple time series datasets interactively with Plotly.\n\n    Creates interactive Plotly scatter plots for specified columns across multiple\n    datasets (e.g., train, validation, test splits). Each dataset is displayed as\n    a separate line with a unique color and name in the legend.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames with datetime\n            index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to the column name in the title. Useful for\n            adding units or descriptions. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template name for styling. Options include 'plotly_white',\n            'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. If None, uses Plotly\n            default colors. Example: {'Train': 'blue', 'Validation': 'orange'}.\n            Default: None.\n        **kwargs: Additional keyword arguments passed to go.Scatter() (e.g.,\n            mode='lines+markers', line=dict(dash='dash')).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes dict is empty, contains no columns, or if\n            specified columns don't exist in all dataframes.\n        ImportError: If plotly is not installed.\n        TypeError: If dataframes parameter is not a dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n        &gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_train = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100),\n        ...     'humidity': np.random.normal(60, 10, 100)\n        ... }, index=dates_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_val = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 50),\n        ...     'humidity': np.random.normal(55, 10, 50)\n        ... }, index=dates_val)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_test = pd.DataFrame({\n        ...     'temperature': np.random.normal(25, 5, 30),\n        ...     'humidity': np.random.normal(50, 10, 30)\n        ... }, index=dates_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize all datasets\n        &gt;&gt;&gt; dataframes = {\n        ...     'Train': data_train,\n        ...     'Validation': data_val,\n        ...     'Test': data_test\n        ... }\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes)\n\n        Single dataset example:\n\n        &gt;&gt;&gt; # Visualize single dataset\n        &gt;&gt;&gt; dataframes = {'Data': data_train}\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n\n        Custom styling:\n\n        &gt;&gt;&gt; visualize_ts_plotly(\n        ...     dataframes,\n        ...     columns=['temperature'],\n        ...     template='plotly_dark',\n        ...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n        ...     mode='lines+markers'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not isinstance(dataframes, dict):\n        raise TypeError(\"dataframes parameter must be a dictionary\")\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # Validate all dataframes have data\n    for name, df in dataframes.items():\n        if df.empty:\n            raise ValueError(f\"DataFrame '{name}' is empty\")\n        if len(df.columns) == 0:\n            raise ValueError(f\"DataFrame '{name}' contains no columns\")\n\n    # Determine columns to plot\n    all_columns = set()\n    for df in dataframes.values():\n        all_columns.update(df.columns)\n\n    if not all_columns:\n        raise ValueError(\"No columns found in any dataframe\")\n\n    columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n    # Validate columns exist in all dataframes\n    for col in columns_to_plot:\n        for name, df in dataframes.items():\n            if col not in df.columns:\n                raise ValueError(f\"Column '{col}' not found in dataframe '{name}'\")\n\n    # Default colors if not provided\n    if colors is None:\n        # Use a set of distinct colors\n        default_colors = [\n            \"#1f77b4\",  # blue\n            \"#ff7f0e\",  # orange\n            \"#2ca02c\",  # green\n            \"#d62728\",  # red\n            \"#9467bd\",  # purple\n            \"#8c564b\",  # brown\n            \"#e377c2\",  # pink\n            \"#7f7f7f\",  # gray\n            \"#bcbd22\",  # olive\n            \"#17becf\",  # cyan\n        ]\n        colors = {\n            name: default_colors[i % len(default_colors)]\n            for i, name in enumerate(dataframes.keys())\n        }\n\n    # Create figures for each column\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add trace for each dataset\n        for dataset_name, df in dataframes.items():\n            fig.add_trace(\n                go.Scatter(\n                    x=df.index,\n                    y=df[col],\n                    mode=\"lines\",\n                    name=dataset_name,\n                    line=dict(color=colors[dataset_name]),\n                    **kwargs,\n                )\n            )\n\n        # Create title\n        title = col\n        if title_suffix:\n            title = f\"{col} {title_suffix}\"\n\n        # Update layout\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"Time\",\n            yaxis_title=col,\n            width=figsize[0],\n            height=figsize[1],\n            template=template,\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.02,\n                xanchor=\"right\",\n                x=1,\n            ),\n            hovermode=\"x unified\",\n        )\n\n        fig.show()\n</code></pre>"},{"location":"api/preprocessing/#data-curation","title":"Data Curation","text":""},{"location":"api/preprocessing/#curate_data","title":"curate_data","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data","title":"<code>spotforecast2_safe.preprocessing.curate_data</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.agg_and_resample_data","title":"<code>agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.basic_ts_checks","title":"<code>basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.curate_holidays","title":"<code>curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.curate_weather","title":"<code>curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.curate_data.get_start_end","title":"<code>get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#imputation","title":"Imputation","text":""},{"location":"api/preprocessing/#imputation_1","title":"imputation","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation","title":"<code>spotforecast2_safe.preprocessing.imputation</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction","title":"<code>WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.custom_weights","title":"<code>custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.imputation.get_missing_weights","title":"<code>get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#outlier-detection-and-handling","title":"Outlier Detection and Handling","text":""},{"location":"api/preprocessing/#outlier","title":"outlier","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier","title":"<code>spotforecast2_safe.preprocessing.outlier</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.visualize_outliers_hist","title":"<code>visualize_outliers_hist(data, data_original, columns=None, contamination=0.01, random_state=1234, figsize=(10, 5), bins=50, **kwargs)</code>","text":"<p>Visualize outliers in DataFrame using stacked histograms.</p> <p>Creates a histogram for each specified column, displaying both regular data and detected outliers in different colors. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height). Default: (10, 5).</p> <code>(10, 5)</code> <code>bins</code> <code>int</code> <p>Number of histogram bins. Default: 50.</p> <code>50</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays matplotlib figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If matplotlib is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_hist\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... })\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_hist(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     figsize=(12, 5),\n...     alpha=0.7\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in DataFrame using stacked histograms.\n\n    Creates a histogram for each specified column, displaying both regular data\n    and detected outliers in different colors. Uses IsolationForest for outlier\n    detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        figsize: Figure size as (width, height). Default: (10, 5).\n        bins: Number of histogram bins. Default: 50.\n        **kwargs: Additional keyword arguments passed to plt.hist() (e.g., color,\n            alpha, edgecolor, etc.).\n\n    Returns:\n        None. Displays matplotlib figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If matplotlib is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_hist\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... })\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_hist(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     figsize=(12, 5),\n        ...     alpha=0.7\n        ... )\n    \"\"\"\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        # Get inliers (non-NaN values in cleaned data)\n        inliers = data[col].dropna()\n\n        # Get outlier values\n        outlier_vals = outliers[col]\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Create figure\n        plt.figure(figsize=figsize)\n        plt.hist(\n            [inliers, outlier_vals],\n            bins=bins,\n            stacked=True,\n            color=[\"lightgrey\", \"red\"],\n            label=[\"Regular Data\", \"Outliers\"],\n            **kwargs,\n        )\n        plt.grid(True, alpha=0.3)\n        plt.title(f\"{col} Distribution with Outliers ({pct_outliers:.2f}%)\")\n        plt.xlabel(\"Value\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.visualize_outliers_plotly_scatter","title":"<code>visualize_outliers_plotly_scatter(data, data_original, columns=None, contamination=0.01, random_state=1234, **kwargs)</code>","text":"<p>Visualize outliers in time series using Plotly scatter plots.</p> <p>Creates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in time series using Plotly scatter plots.\n\n    Creates an interactive time series plot for each specified column, showing\n    regular data as a line and detected outliers as scatter points. Uses\n    IsolationForest for outlier detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        **kwargs: Additional keyword arguments passed to go.Figure.update_layout()\n            (e.g., template, height, etc.).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... }, index=dates)\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_plotly_scatter(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     template='plotly_white'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add regular data as line\n        fig.add_trace(\n            go.Scatter(\n                x=data.index,\n                y=data[col],\n                mode=\"lines\",\n                name=\"Regular Data\",\n                line=dict(color=\"lightgrey\"),\n            )\n        )\n\n        # Add outliers as scatter points\n        outlier_vals = outliers[col]\n        if not outlier_vals.empty:\n            fig.add_trace(\n                go.Scatter(\n                    x=outlier_vals.index,\n                    y=outlier_vals,\n                    mode=\"markers\",\n                    name=\"Outliers\",\n                    marker=dict(color=\"red\", size=8, symbol=\"x\"),\n                )\n            )\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Update layout with custom kwargs\n        layout_kwargs = {\n            \"title\": f\"{col} Time Series with Outliers ({pct_outliers:.2f}%)\",\n            \"xaxis_title\": \"Time\",\n            \"yaxis_title\": \"Value\",\n            \"template\": \"plotly_white\",\n            \"legend\": dict(\n                orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1\n            ),\n        }\n        layout_kwargs.update(kwargs)\n        fig.update_layout(**layout_kwargs)\n        fig.show()\n</code></pre>"},{"location":"api/preprocessing/#time-series-splitting","title":"Time Series Splitting","text":""},{"location":"api/preprocessing/#split","title":"split","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split","title":"<code>spotforecast2_safe.preprocessing.split</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split.split_abs_train_val_test","title":"<code>split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.split.split_rel_train_val_test","title":"<code>split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_test = 0\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#rolling-features","title":"Rolling Features","text":""},{"location":"api/preprocessing/#_rolling","title":"_rolling","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling","title":"<code>spotforecast2_safe.preprocessing._rolling</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures","title":"<code>RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        For each statistic and window size combination, computes the rolling\n        statistic across the input time series. The output contains NaN values\n        for the initial positions where the window cannot be fully computed.\n\n        Args:\n            X: Time series data as 1D numpy array or array-like.\n\n        Returns:\n            np.ndarray: Array of shape (len(X), len(features_names)) containing\n                the computed rolling statistics. Each column corresponds to a\n                feature in features_names. Early positions contain NaN values\n                before the window is fully populated.\n        \"\"\"\n        # Assume X is 1D array\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n\n        idx_feature = 0\n        for ws in self.window_sizes:\n            for func in self.stats_funcs:\n                # Naive rolling window loop - can be optimized or use pandas rolling\n                # Using pandas for simplicity and speed if X is convertible\n                series = pd.Series(X)\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n                idx_feature += 1\n\n        return output\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n\n        Transforms a pandas Series into a DataFrame of rolling statistics while\n        preserving the original index. Useful for maintaining time alignment\n        with the input data.\n\n        Args:\n            X: Time series data as pandas Series. The index is preserved in output.\n\n        Returns:\n            pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n                columns are feature names and index matches the input Series.\n                Contains NaN values at the beginning where windows are incomplete.\n\n        Note:\n            This method is preferred over transform() when working with time-indexed\n            data, as it preserves the temporal index and is compatible with\n            forecasting workflows.\n        \"\"\"\n        values = X.to_numpy()\n        transformed = self.transform(values)\n        return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>For each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D numpy array or array-like.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    For each statistic and window size combination, computes the rolling\n    statistic across the input time series. The output contains NaN values\n    for the initial positions where the window cannot be fully computed.\n\n    Args:\n        X: Time series data as 1D numpy array or array-like.\n\n    Returns:\n        np.ndarray: Array of shape (len(X), len(features_names)) containing\n            the computed rolling statistics. Each column corresponds to a\n            feature in features_names. Early positions contain NaN values\n            before the window is fully populated.\n    \"\"\"\n    # Assume X is 1D array\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n\n    idx_feature = 0\n    for ws in self.window_sizes:\n        for func in self.stats_funcs:\n            # Naive rolling window loop - can be optimized or use pandas rolling\n            # Using pandas for simplicity and speed if X is convertible\n            series = pd.Series(X)\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n            idx_feature += 1\n\n    return output\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._rolling.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> <p>Transforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series</code> <p>Time series data as pandas Series. The index is preserved in output.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.</p> Note <p>This method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n\n    Transforms a pandas Series into a DataFrame of rolling statistics while\n    preserving the original index. Useful for maintaining time alignment\n    with the input data.\n\n    Args:\n        X: Time series data as pandas Series. The index is preserved in output.\n\n    Returns:\n        pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n            columns are feature names and index matches the input Series.\n            Contains NaN values at the beginning where windows are incomplete.\n\n    Note:\n        This method is preferred over transform() when working with time-indexed\n        data, as it preserves the temporal index and is compatible with\n        forecasting workflows.\n    \"\"\"\n    values = X.to_numpy()\n    transformed = self.transform(values)\n    return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#differencing","title":"Differencing","text":""},{"location":"api/preprocessing/#_differentiator","title":"_differentiator","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator","title":"<code>spotforecast2_safe.preprocessing._differentiator</code>","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._differentiator.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#binning","title":"Binning","text":""},{"location":"api/preprocessing/#_binner","title":"_binner","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner","title":"<code>spotforecast2_safe.preprocessing._binner</code>","text":"<p>QuantileBinner class for binning data into quantile-based bins.</p> <p>This module contains the QuantileBinner class which bins data into quantile-based bins using numpy.percentile with optimized performance using numpy.searchsorted.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner","title":"<code>QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing._binner.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/processing/","title":"Processing Module","text":"<p>End-to-end forecasting pipelines and prediction aggregation.</p>"},{"location":"api/processing/#spotforecast2_safe.processing","title":"<code>spotforecast2_safe.processing</code>","text":"<p>Processing module for end-to-end forecasting pipelines.</p>"},{"location":"api/processing/#n-to-n-prediction","title":"N-to-N Prediction","text":""},{"location":"api/processing/#n2n_predict","title":"n2n_predict","text":""},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict","title":"<code>spotforecast2_safe.processing.n2n_predict</code>","text":"<p>End-to-end baseline forecasting using equivalent date method.</p> <p>This module provides a complete forecasting pipeline using the ForecasterEquivalentDate baseline model. It handles data preparation, outlier detection, imputation, model training, and prediction in a single integrated function.</p> <p>Model persistence follows scikit-learn conventions using joblib for efficient serialization and deserialization of trained forecasters.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.processing.n2n_predict import n2n_predict\n&gt;&gt;&gt; predictions = n2n_predict(forecast_horizon=24, verbose=True)\n</code></pre> <p>Using cached models:</p> <pre><code>&gt;&gt;&gt; # Load existing models if available, or train new ones\n&gt;&gt;&gt; predictions = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre>"},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict.n2n_predict","title":"<code>n2n_predict(data=None, columns=None, forecast_horizon=24, contamination=0.01, window_size=72, force_train=True, model_dir=None, verbose=True, show_progress=True)</code>","text":"<p>End-to-end baseline forecasting using equivalent date method.</p> <p>This function implements a complete forecasting pipeline that: 1. Loads and validates target data 2. Detects and removes outliers 3. Imputes missing values 4. Splits into train/validation/test sets 5. Trains or loads equivalent date forecasters 6. Generates multi-step ahead predictions</p> <p>Models are persisted to disk following scikit-learn conventions using joblib. By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with target time series data. If None, fetches data automatically. Default: None.</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>List of target columns to forecast. If None, uses all available columns. Default: None.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to forecast ahead. Default: 24.</p> <code>24</code> <code>contamination</code> <code>float</code> <p>Contamination parameter for outlier detection. Default: 0.01.</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Rolling window size for gap detection. Default: 72.</p> <code>72</code> <code>force_train</code> <code>bool</code> <p>Force retraining of all models, ignoring cached models. Default: True.</p> <code>True</code> <code>model_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory for saving/loading trained models. If None, uses cache directory from get_cache_home(). Default: None (uses ~/spotforecast2_cache/forecasters).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: True.</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar during training and prediction. Default: True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>Dict</code> <ul> <li>predictions: DataFrame with forecast values for each target variable.</li> </ul> <code>Tuple[DataFrame, Dict]</code> <ul> <li>forecasters: Dictionary of trained ForecasterEquivalentDate objects keyed by target.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data validation fails or required data cannot be retrieved.</p> <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>OSError</code> <p>If models cannot be saved to disk.</p> <p>Examples:</p> <p>Basic usage with automatic model caching:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n&gt;&gt;&gt; print(predictions.shape)\n(24, 11)\n</code></pre> <p>Load cached models (if available):</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./saved_models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./saved_models\",\n...     verbose=True\n... )\n</code></pre> <p>With specific target columns:</p> <pre><code>&gt;&gt;&gt; predictions, forecasters = n2n_predict(\n...     columns=[\"power\", \"energy\"],\n...     forecast_horizon=48,\n...     force_train=False,\n...     verbose=True\n... )\n</code></pre> Notes <ul> <li>Trained models are saved to disk using joblib for fast reuse.</li> <li>When force_train=False, existing models are loaded and prediction   proceeds without retraining. This significantly speeds up prediction   for repeated calls with the same configuration.</li> <li>The model_dir directory is created automatically if it doesn't exist.</li> <li>Default model_dir uses get_cache_home() which respects the   SPOTFORECAST2_CACHE environment variable.</li> </ul> Performance Notes <ul> <li>First run: Full training (~2-5 minutes depending on data size)</li> <li>Subsequent runs (force_train=False): Model loading only (~1-2 seconds)</li> <li>Force retrain (force_train=True): Full training again (~2-5 minutes)</li> </ul> Source code in <code>src/spotforecast2_safe/processing/n2n_predict.py</code> <pre><code>def n2n_predict(\n    data: Optional[pd.DataFrame] = None,\n    columns: Optional[List[str]] = None,\n    forecast_horizon: int = 24,\n    contamination: float = 0.01,\n    window_size: int = 72,\n    force_train: bool = True,\n    model_dir: Optional[Union[str, Path]] = None,\n    verbose: bool = True,\n    show_progress: bool = True,\n) -&gt; Tuple[pd.DataFrame, Dict]:\n    \"\"\"End-to-end baseline forecasting using equivalent date method.\n\n    This function implements a complete forecasting pipeline that:\n    1. Loads and validates target data\n    2. Detects and removes outliers\n    3. Imputes missing values\n    4. Splits into train/validation/test sets\n    5. Trains or loads equivalent date forecasters\n    6. Generates multi-step ahead predictions\n\n    Models are persisted to disk following scikit-learn conventions using joblib.\n    By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.\n\n    Args:\n        data: Optional DataFrame with target time series data. If None, fetches data automatically.\n            Default: None.\n        columns: List of target columns to forecast. If None, uses all available columns.\n            Default: None.\n        forecast_horizon: Number of time steps to forecast ahead. Default: 24.\n        contamination: Contamination parameter for outlier detection. Default: 0.01.\n        window_size: Rolling window size for gap detection. Default: 72.\n        force_train: Force retraining of all models, ignoring cached models.\n            Default: True.\n        model_dir: Directory for saving/loading trained models. If None, uses cache directory from get_cache_home(). Default: None (uses ~/spotforecast2_cache/forecasters).\n        verbose: Print progress messages. Default: True.\n        show_progress: Show progress bar during training and prediction. Default: True.\n\n    Returns:\n        Tuple containing:\n        - predictions: DataFrame with forecast values for each target variable.\n        - forecasters: Dictionary of trained ForecasterEquivalentDate objects keyed by target.\n\n    Raises:\n        ValueError: If data validation fails or required data cannot be retrieved.\n        ImportError: If required dependencies are not installed.\n        OSError: If models cannot be saved to disk.\n\n    Examples:\n        Basic usage with automatic model caching:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(predictions.shape)\n        (24, 11)\n\n        Load cached models (if available):\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     force_train=False,\n        ...     model_dir=\"./saved_models\",\n        ...     verbose=True\n        ... )\n\n        Force retraining and update cache:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     forecast_horizon=24,\n        ...     force_train=True,\n        ...     model_dir=\"./saved_models\",\n        ...     verbose=True\n        ... )\n\n        With specific target columns:\n\n        &gt;&gt;&gt; predictions, forecasters = n2n_predict(\n        ...     columns=[\"power\", \"energy\"],\n        ...     forecast_horizon=48,\n        ...     force_train=False,\n        ...     verbose=True\n        ... )\n\n    Notes:\n        - Trained models are saved to disk using joblib for fast reuse.\n        - When force_train=False, existing models are loaded and prediction\n          proceeds without retraining. This significantly speeds up prediction\n          for repeated calls with the same configuration.\n        - The model_dir directory is created automatically if it doesn't exist.\n        - Default model_dir uses get_cache_home() which respects the\n          SPOTFORECAST2_CACHE environment variable.\n\n    Performance Notes:\n        - First run: Full training (~2-5 minutes depending on data size)\n        - Subsequent runs (force_train=False): Model loading only (~1-2 seconds)\n        - Force retrain (force_train=True): Full training again (~2-5 minutes)\n    \"\"\"\n    if columns is not None:\n        TARGET = columns\n    else:\n        TARGET = None\n\n    if verbose:\n        print(\"--- Starting n2n_predict ---\")\n\n    # Set default model_dir if not provided\n    if model_dir is None:\n        from spotforecast2_safe.data.fetch_data import get_cache_home\n\n        model_dir = get_cache_home() / \"forecasters\"\n\n    # Handle data input - fetch_data handles both CSV and DataFrame\n    if data is not None:\n        if verbose:\n            print(\"Using provided dataframe...\")\n        data = fetch_data(dataframe=data, columns=TARGET)\n    else:\n        if verbose:\n            print(\"Fetching data from CSV...\")\n        data = fetch_data(columns=TARGET)\n\n    START, END, COV_START, COV_END = get_start_end(\n        data=data,\n        forecast_horizon=forecast_horizon,\n        verbose=verbose,\n    )\n\n    basic_ts_checks(data, verbose=verbose)\n\n    data = agg_and_resample_data(data, verbose=verbose)\n\n    # --- Outlier Handling ---\n    if verbose:\n        print(\"Handling outliers...\")\n\n    # data_old = data.copy() # kept in notebook, maybe useful for debugging but not used logic-wise here\n    data, outliers = mark_outliers(\n        data, contamination=contamination, random_state=1234, verbose=verbose\n    )\n\n    # --- Missing Data (Imputation) ---\n    if verbose:\n        print(\"Imputing missing data...\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    if verbose:\n        n_missing = len(missing_indices)\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n\n    data = data.ffill()\n    data = data.bfill()\n\n    # --- Train, Val, Test Split ---\n    if verbose:\n        print(\"Splitting data...\")\n    data_train, data_val, data_test = split_rel_train_val_test(\n        data, perc_train=0.8, perc_val=0.2, verbose=verbose\n    )\n\n    # --- Model Fit ---\n    if verbose:\n        print(\"Fitting models...\")\n\n    end_validation = pd.concat([data_train, data_val]).index[-1]\n\n    baseline_forecasters = {}\n    targets_to_train = list(data.columns)\n\n    # Attempt to load cached models if force_train=False\n    if not force_train and _model_directory_exists(model_dir):\n        if verbose:\n            print(\"  Attempting to load cached models...\")\n        cached_forecasters, missing_targets = _load_forecasters(\n            target_columns=list(data.columns),\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n        baseline_forecasters.update(cached_forecasters)\n        targets_to_train = missing_targets\n\n        if len(cached_forecasters) == len(data.columns):\n            if verbose:\n                print(f\"  \u2713 All {len(data.columns)} forecasters loaded from cache\")\n        elif len(cached_forecasters) &gt; 0:\n            if verbose:\n                print(\n                    f\"  \u2713 Loaded {len(cached_forecasters)} forecasters, \"\n                    f\"will train {len(targets_to_train)} new ones\"\n                )\n\n    # Train missing or forced models\n    if len(targets_to_train) &gt; 0:\n        if force_train and len(baseline_forecasters) &gt; 0:\n            if verbose:\n                print(f\"  Force retraining all {len(data.columns)} forecasters...\")\n            targets_to_train = list(data.columns)\n            baseline_forecasters.clear()\n\n        target_iter = targets_to_train\n        if show_progress and tqdm is not None:\n            target_iter = tqdm(\n                targets_to_train,\n                desc=\"Training forecasters\",\n                unit=\"model\",\n            )\n\n        for target in target_iter:\n            forecaster = ForecasterEquivalentDate(\n                offset=pd.DateOffset(days=1), n_offsets=1\n            )\n\n            forecaster.fit(y=data.loc[:end_validation, target])\n\n            baseline_forecasters[target] = forecaster\n\n        # Save newly trained models to disk\n        if verbose:\n            print(f\"  Saving {len(targets_to_train)} trained forecasters to disk...\")\n        _save_forecasters(\n            forecasters={t: baseline_forecasters[t] for t in targets_to_train},\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n\n    if verbose:\n        print(f\"  \u2713 Total forecasters available: {len(baseline_forecasters)}\")\n\n    # --- Predict ---\n    if verbose:\n        print(\"Generating predictions...\")\n\n    predictions = predict_multivariate(\n        baseline_forecasters,\n        steps_ahead=forecast_horizon,\n        show_progress=show_progress,\n    )\n\n    return predictions, baseline_forecasters\n</code></pre>"},{"location":"api/processing/#n-to-n-prediction-with-covariates","title":"N-to-N Prediction with Covariates","text":""},{"location":"api/processing/#n2n_predict_with_covariates","title":"n2n_predict_with_covariates","text":""},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict_with_covariates","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates</code>","text":"<p>End-to-end recursive forecasting with exogenous covariates.</p> <p>This module provides a complete pipeline for time series forecasting using recursive forecasters with exogenous variables (weather, holidays, calendar features). It handles data preparation, feature engineering, model training, and prediction in a single integrated function.</p> <p>Model persistence follows scikit-learn conventions using joblib for efficient serialization and deserialization of trained forecasters.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.processing.n2n_predict_with_covariates import (\n...     n2n_predict_with_covariates\n... )\n&gt;&gt;&gt; predictions = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n</code></pre> <p>With custom parameters:</p> <pre><code>&gt;&gt;&gt; predictions = n2n_predict_with_covariates(\n...     forecast_horizon=48,\n...     contamination=0.02,\n...     window_size=100,\n...     lags=48,\n...     train_ratio=0.75,\n...     verbose=True\n... )\n</code></pre> <p>Using cached models:</p> <pre><code>&gt;&gt;&gt; # Load existing models if available, or train new ones\n&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./models\",\n...     verbose=True\n... )\n</code></pre>"},{"location":"api/processing/#spotforecast2_safe.processing.n2n_predict_with_covariates.n2n_predict_with_covariates","title":"<code>n2n_predict_with_covariates(data=None, forecast_horizon=24, contamination=0.01, window_size=72, lags=24, train_ratio=0.8, latitude=51.5136, longitude=7.4653, timezone='UTC', country_code='DE', state='NW', estimator=None, include_weather_windows=False, include_holiday_features=False, include_poly_features=False, force_train=True, model_dir=None, verbose=True, show_progress=False)</code>","text":"<p>End-to-end recursive forecasting with exogenous covariates.</p> <p>This function implements a complete forecasting pipeline that: 1. Loads and validates target data 2. Detects and removes outliers 3. Imputes missing values with weighted gaps 4. Creates exogenous features (weather, holidays, calendar, day/night) 5. Performs feature engineering (cyclical encoding, interactions) 6. Merges target and exogenous data 7. Splits into train/validation/test sets 8. Trains or loads recursive forecasters with sample weighting 9. Generates multi-step ahead predictions</p> <p>Models are persisted to disk following scikit-learn conventions using joblib. By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with target time series data. If None, fetches data automatically. Default: None.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to forecast ahead. Default: 24.</p> <code>24</code> <code>contamination</code> <code>float</code> <p>Contamination parameter for outlier detection. Default: 0.01.</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Rolling window size for gap detection. Default: 72.</p> <code>72</code> <code>lags</code> <code>int</code> <p>Number of lags for recursive forecaster. Default: 24.</p> <code>24</code> <code>train_ratio</code> <code>float</code> <p>Fraction of data for training. Default: 0.8.</p> <code>0.8</code> <code>latitude</code> <code>float</code> <p>Location latitude. Default: 51.5136 (Dortmund).</p> <code>51.5136</code> <code>longitude</code> <code>float</code> <p>Location longitude. Default: 7.4653 (Dortmund).</p> <code>7.4653</code> <code>timezone</code> <code>str</code> <p>Timezone for data. Default: \"UTC\".</p> <code>'UTC'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays. Default: \"DE\".</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays. Default: \"NW\".</p> <code>'NW'</code> <code>estimator</code> <code>Optional[object]</code> <p>Base estimator for recursive forecaster. If None, uses LGBMRegressor. Default: None.</p> <code>None</code> <code>include_weather_windows</code> <code>bool</code> <p>Include weather window features. Default: False.</p> <code>False</code> <code>include_holiday_features</code> <code>bool</code> <p>Include holiday features. Default: False.</p> <code>False</code> <code>include_poly_features</code> <code>bool</code> <p>Include polynomial interaction features. Default: False.</p> <code>False</code> <code>force_train</code> <code>bool</code> <p>Force retraining of all models, ignoring cached models. Default: True.</p> <code>True</code> <code>model_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory for saving/loading trained models. If None, uses the spotforecast2 cache directory (~/spotforecast2_cache by default, or SPOTFORECAST2_CACHE environment variable). Default: None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: True.</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar during training. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>Dict</code> <ul> <li>predictions: DataFrame with forecast values for each target variable.</li> </ul> <code>Dict</code> <ul> <li>metadata: Dictionary with forecast metadata (index, shapes, etc.).</li> </ul> <code>Tuple[DataFrame, Dict, Dict]</code> <ul> <li>forecasters: Dictionary of trained ForecasterRecursive objects keyed by target.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data validation fails or required data cannot be retrieved.</p> <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>OSError</code> <p>If models cannot be saved to disk.</p> <p>Examples:</p> <p>Basic usage with automatic model caching:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     verbose=True\n... )\n&gt;&gt;&gt; print(predictions.shape)\n(24, 11)\n</code></pre> <p>Load cached models (if available):</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=False,\n...     model_dir=\"./saved_models\"\n... )\n</code></pre> <p>Force retraining and update cache:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=24,\n...     force_train=True,\n...     model_dir=\"./saved_models\"\n... )\n</code></pre> <p>Custom location and features:</p> <pre><code>&gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n...     forecast_horizon=48,\n...     latitude=52.5200,  # Berlin\n...     longitude=13.4050,\n...     lags=48,\n...     include_poly_features=True,\n...     force_train=False,\n...     verbose=True\n... )\n</code></pre> Notes <ul> <li>The function uses cached weather data when available.</li> <li>Missing values are handled via forward/backward fill with downweighting   observations near gaps.</li> <li>Sample weights are passed to the forecaster to penalize observations   near missing data.</li> <li>Train/validation splits are temporal (80/20 by default).</li> <li>All features are cast to float32 for memory efficiency.</li> <li>Trained models are saved to disk using joblib for fast reuse.</li> <li>When force_train=False, existing models are loaded and prediction   proceeds without retraining. This significantly speeds up prediction   for repeated calls with the same configuration.</li> <li>The model_dir directory is created automatically if it doesn't exist.</li> <li>By default, models are cached in ~/spotforecast2_cache, which can be   customized via the SPOTFORECAST2_CACHE environment variable.</li> </ul> Performance Notes <ul> <li>First run: Full training</li> <li>Subsequent runs (force_train=False): Model loading only</li> <li>Force retrain (force_train=True): Full training again</li> </ul> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def n2n_predict_with_covariates(\n    data: Optional[pd.DataFrame] = None,\n    forecast_horizon: int = 24,\n    contamination: float = 0.01,\n    window_size: int = 72,\n    lags: int = 24,\n    train_ratio: float = 0.8,\n    latitude: float = 51.5136,\n    longitude: float = 7.4653,\n    timezone: str = \"UTC\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n    estimator: Optional[object] = None,\n    include_weather_windows: bool = False,\n    include_holiday_features: bool = False,\n    include_poly_features: bool = False,\n    force_train: bool = True,\n    model_dir: Optional[Union[str, Path]] = None,\n    verbose: bool = True,\n    show_progress: bool = False,\n) -&gt; Tuple[pd.DataFrame, Dict, Dict]:\n    \"\"\"End-to-end recursive forecasting with exogenous covariates.\n\n    This function implements a complete forecasting pipeline that:\n    1. Loads and validates target data\n    2. Detects and removes outliers\n    3. Imputes missing values with weighted gaps\n    4. Creates exogenous features (weather, holidays, calendar, day/night)\n    5. Performs feature engineering (cyclical encoding, interactions)\n    6. Merges target and exogenous data\n    7. Splits into train/validation/test sets\n    8. Trains or loads recursive forecasters with sample weighting\n    9. Generates multi-step ahead predictions\n\n    Models are persisted to disk following scikit-learn conventions using joblib.\n    By default, models are retrained (force_train=True). Set force_train=False to reuse existing cached models.\n\n    Args:\n        data: Optional DataFrame with target time series data. If None, fetches data automatically.\n            Default: None.\n        forecast_horizon: Number of time steps to forecast ahead. Default: 24.\n        contamination: Contamination parameter for outlier detection. Default: 0.01.\n        window_size: Rolling window size for gap detection. Default: 72.\n        lags: Number of lags for recursive forecaster. Default: 24.\n        train_ratio: Fraction of data for training. Default: 0.8.\n        latitude: Location latitude. Default: 51.5136 (Dortmund).\n        longitude: Location longitude. Default: 7.4653 (Dortmund).\n        timezone: Timezone for data. Default: \"UTC\".\n        country_code: Country code for holidays. Default: \"DE\".\n        state: State code for holidays. Default: \"NW\".\n        estimator: Base estimator for recursive forecaster.\n            If None, uses LGBMRegressor. Default: None.\n        include_weather_windows: Include weather window features. Default: False.\n        include_holiday_features: Include holiday features. Default: False.\n        include_poly_features: Include polynomial interaction features. Default: False.\n        force_train: Force retraining of all models, ignoring cached models.\n            Default: True.\n        model_dir: Directory for saving/loading trained models. If None, uses the\n            spotforecast2 cache directory (~/spotforecast2_cache by default, or\n            SPOTFORECAST2_CACHE environment variable). Default: None.\n        verbose: Print progress messages. Default: True.\n        show_progress: Show progress bar during training. Default: False.\n\n    Returns:\n        Tuple containing:\n        - predictions: DataFrame with forecast values for each target variable.\n        - metadata: Dictionary with forecast metadata (index, shapes, etc.).\n        - forecasters: Dictionary of trained ForecasterRecursive objects keyed by target.\n\n    Raises:\n        ValueError: If data validation fails or required data cannot be retrieved.\n        ImportError: If required dependencies are not installed.\n        OSError: If models cannot be saved to disk.\n\n    Examples:\n        Basic usage with automatic model caching:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(predictions.shape)\n        (24, 11)\n\n        Load cached models (if available):\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     force_train=False,\n        ...     model_dir=\"./saved_models\"\n        ... )\n\n        Force retraining and update cache:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=24,\n        ...     force_train=True,\n        ...     model_dir=\"./saved_models\"\n        ... )\n\n        Custom location and features:\n\n        &gt;&gt;&gt; predictions, metadata, forecasters = n2n_predict_with_covariates(\n        ...     forecast_horizon=48,\n        ...     latitude=52.5200,  # Berlin\n        ...     longitude=13.4050,\n        ...     lags=48,\n        ...     include_poly_features=True,\n        ...     force_train=False,\n        ...     verbose=True\n        ... )\n\n    Notes:\n        - The function uses cached weather data when available.\n        - Missing values are handled via forward/backward fill with downweighting\n          observations near gaps.\n        - Sample weights are passed to the forecaster to penalize observations\n          near missing data.\n        - Train/validation splits are temporal (80/20 by default).\n        - All features are cast to float32 for memory efficiency.\n        - Trained models are saved to disk using joblib for fast reuse.\n        - When force_train=False, existing models are loaded and prediction\n          proceeds without retraining. This significantly speeds up prediction\n          for repeated calls with the same configuration.\n        - The model_dir directory is created automatically if it doesn't exist.\n        - By default, models are cached in ~/spotforecast2_cache, which can be\n          customized via the SPOTFORECAST2_CACHE environment variable.\n\n    Performance Notes:\n        - First run: Full training\n        - Subsequent runs (force_train=False): Model loading only\n        - Force retrain (force_train=True): Full training again\n    \"\"\"\n    # Set default model_dir if not provided\n    if model_dir is None:\n        from spotforecast2_safe.data.fetch_data import get_cache_home\n\n        model_dir = get_cache_home() / \"forecasters\"\n\n    if verbose:\n        print(\"=\" * 80)\n        print(\"N2N Recursive Forecasting with Exogenous Covariates\")\n        print(\"=\" * 80)\n\n    # ========================================================================\n    # 1. DATA PREPARATION\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[1/9] Loading and preparing target data...\")\n\n    # Handle data input - fetch_data handles both CSV and DataFrame\n    if data is None:\n        if verbose:\n            print(\"  Fetching data from CSV...\")\n        data = fetch_data(timezone=timezone)\n    else:\n        if verbose:\n            print(\"  Using provided dataframe...\")\n        data = fetch_data(dataframe=data, timezone=timezone)\n\n    target_columns = data.columns.tolist()\n\n    if verbose:\n        print(f\"  Target variables: {target_columns}\")\n\n    start, end, cov_start, cov_end = get_start_end(\n        data=data,\n        forecast_horizon=forecast_horizon,\n        verbose=verbose,\n    )\n\n    basic_ts_checks(data, verbose=verbose)\n    data = agg_and_resample_data(data, verbose=verbose)\n\n    # ========================================================================\n    # 2. OUTLIER DETECTION AND REMOVAL\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[2/9] Detecting and marking outliers...\")\n\n    data, outliers = mark_outliers(\n        data,\n        contamination=contamination,\n        random_state=1234,\n        verbose=verbose,\n    )\n\n    # ========================================================================\n    # 3. MISSING VALUE IMPUTATION WITH WEIGHTING\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[3/9] Processing missing values and creating sample weights...\")\n\n    imputed_data, missing_mask = get_missing_weights(\n        data, window_size=window_size, verbose=verbose\n    )\n\n    # Create weight function for forecaster\n    # Invert missing_mask: True (missing) -&gt; 0 (weight), False (valid) -&gt; 1 (weight)\n    weights_series = (~missing_mask).astype(float)\n\n    # Use WeightFunction class which is picklable (unlike local functions with closures)\n    from spotforecast2_safe.preprocessing import WeightFunction\n\n    weight_func = WeightFunction(weights_series)\n\n    # Model persistence enabled: WeightFunction instances can be pickled\n    use_model_persistence = True\n\n    # ========================================================================\n    # 4. EXOGENOUS FEATURES ENGINEERING\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[4/9] Creating exogenous features...\")\n\n    # Location for day/night features\n    location = LocationInfo(\n        latitude=latitude,\n        longitude=longitude,\n        timezone=timezone,\n    )\n\n    # Holidays\n    holiday_features = _get_holiday_features(\n        data=imputed_data,\n        start=start,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        tz=timezone,\n        freq=\"h\",\n        country_code=country_code,\n        state=state,\n    )\n\n    # Weather\n    weather_features, weather_aligned = _get_weather_features(\n        data=imputed_data,\n        start=start,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        latitude=latitude,\n        longitude=longitude,\n        timezone=timezone,\n        freq=\"h\",\n        verbose=verbose,\n    )\n\n    # Calendar\n    calendar_features = _get_calendar_features(\n        start=start,\n        cov_end=cov_end,\n        freq=\"h\",\n        timezone=timezone,\n    )\n\n    # Day/night\n    sun_light_features = _get_day_night_features(\n        start=start,\n        cov_end=cov_end,\n        location=location,\n        freq=\"h\",\n        timezone=timezone,\n    )\n\n    # ========================================================================\n    # 5. COMBINE EXOGENOUS FEATURES\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[5/9] Combining and encoding exogenous features...\")\n\n    exogenous_features = pd.concat(\n        [\n            calendar_features,\n            sun_light_features,\n            weather_features,\n            holiday_features,\n        ],\n        axis=1,\n    )\n\n    assert (\n        sum(exogenous_features.isnull().sum()) == 0\n    ), \"Missing values in exogenous features\"\n\n    # Apply cyclical encoding\n    exogenous_features = _apply_cyclical_encoding(\n        data=exogenous_features,\n        drop_original=False,\n    )\n\n    # Create interactions\n    exogenous_features = _create_interaction_features(\n        exogenous_features=exogenous_features,\n        weather_aligned=weather_aligned,\n    )\n\n    # ========================================================================\n    # 6. SELECT EXOGENOUS FEATURES\n    # ========================================================================\n\n    exog_features = _select_exogenous_features(\n        exogenous_features=exogenous_features,\n        weather_aligned=weather_aligned,\n        include_weather_windows=include_weather_windows,\n        include_holiday_features=include_holiday_features,\n        include_poly_features=include_poly_features,\n    )\n\n    if verbose:\n        print(f\"  Selected {len(exog_features)} exogenous features\")\n\n    # ========================================================================\n    # 7. MERGE DATA AND COVARIATES\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[6/9] Merging target and exogenous data...\")\n\n    data_with_exog, exo_tmp, exo_pred = _merge_data_and_covariates(\n        data=imputed_data,\n        exogenous_features=exogenous_features,\n        target_columns=target_columns,\n        exog_features=exog_features,\n        start=start,\n        end=end,\n        cov_end=cov_end,\n        forecast_horizon=forecast_horizon,\n        cast_dtype=\"float32\",\n    )\n\n    if verbose:\n        print(f\"  Merged data shape: {data_with_exog.shape}\")\n        print(f\"  Exogenous prediction shape: {exo_pred.shape}\")\n\n    # ========================================================================\n    # 8. TRAIN/VALIDATION/TEST SPLIT\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[7/9] Splitting data into train/validation/test...\")\n\n    perc_val = 1.0 - train_ratio\n    data_train, data_val, data_test = split_rel_train_val_test(\n        data_with_exog,\n        perc_train=train_ratio,\n        perc_val=perc_val,\n        verbose=verbose,\n    )\n\n    # ========================================================================\n    # 9. MODEL TRAINING OR LOADING\n    # ========================================================================\n\n    if verbose:\n        print(\n            \"\\n[8/9] Loading or training recursive forecasters with exogenous variables...\"\n        )\n\n    if estimator is None:\n        estimator = LGBMRegressor(random_state=1234, verbose=-1)\n\n    window_features = RollingFeatures(stats=[\"mean\"], window_sizes=window_size)\n    end_validation = pd.concat([data_train, data_val]).index[-1]\n\n    # Attempt to load cached models if force_train=False and persistence is enabled\n    recursive_forecasters = {}\n    targets_to_train = target_columns\n\n    if use_model_persistence and not force_train and _model_directory_exists(model_dir):\n        if verbose:\n            print(\"  Attempting to load cached models...\")\n        cached_forecasters, missing_targets = _load_forecasters(\n            target_columns=target_columns,\n            model_dir=model_dir,\n            verbose=verbose,\n        )\n        recursive_forecasters.update(cached_forecasters)\n        targets_to_train = missing_targets\n\n        if len(cached_forecasters) == len(target_columns):\n            if verbose:\n                print(f\"  \u2713 All {len(target_columns)} forecasters loaded from cache\")\n        elif len(cached_forecasters) &gt; 0:\n            if verbose:\n                print(\n                    f\"  \u2713 Loaded {len(cached_forecasters)} forecasters, \"\n                    f\"will train {len(targets_to_train)} new ones\"\n                )\n\n    # Train missing or forced models\n    if len(targets_to_train) &gt; 0:\n        if force_train and len(recursive_forecasters) &gt; 0:\n            if verbose:\n                print(f\"  Force retraining all {len(target_columns)} forecasters...\")\n            targets_to_train = target_columns\n            recursive_forecasters.clear()\n\n        target_iter = targets_to_train\n        if show_progress and tqdm is not None:\n            target_iter = tqdm(\n                targets_to_train,\n                desc=\"Training forecasters\",\n                unit=\"model\",\n            )\n\n        for target in target_iter:\n            if verbose:\n                print(f\"  Training forecaster for {target}...\")\n\n            forecaster = ForecasterRecursive(\n                estimator=estimator,\n                lags=lags,\n                window_features=window_features,\n                weight_func=weight_func,\n            )\n\n            forecaster.fit(\n                y=data_with_exog[target].loc[:end_validation].squeeze(),\n                exog=data_with_exog[exog_features].loc[:end_validation],\n            )\n\n            recursive_forecasters[target] = forecaster\n\n            if verbose:\n                print(f\"    \u2713 Forecaster trained for {target}\")\n\n        # Save newly trained models to disk (only if persistence is enabled)\n        if use_model_persistence:\n            if verbose:\n                print(\n                    f\"  Saving {len(targets_to_train)} trained forecasters to disk...\"\n                )\n            _save_forecasters(\n                forecasters={t: recursive_forecasters[t] for t in targets_to_train},\n                model_dir=model_dir,\n                verbose=verbose,\n            )\n        else:\n            if verbose:\n                print(\"  \u26a0 Model persistence disabled (weight_func cannot be pickled)\")\n\n    if verbose:\n        print(f\"  \u2713 Total forecasters available: {len(recursive_forecasters)}\")\n\n    # ========================================================================\n    # 10. PREDICTION\n    # ========================================================================\n\n    if verbose:\n        print(\"\\n[9/9] Generating predictions...\")\n\n    exo_pred_subset = exo_pred[exog_features]\n\n    predictions = predict_multivariate(\n        recursive_forecasters,\n        steps_ahead=forecast_horizon,\n        exog=exo_pred_subset,\n        show_progress=show_progress,\n    )\n\n    if verbose:\n        print(f\"  Predictions shape: {predictions.shape}\")\n        print(\"\\n\" + \"=\" * 80)\n        print(\"Forecasting completed successfully!\")\n        print(\"=\" * 80)\n\n    # ========================================================================\n    # COMPILE METADATA\n    # ========================================================================\n\n    metadata = {\n        \"forecast_horizon\": forecast_horizon,\n        \"target_columns\": target_columns,\n        \"exog_features\": exog_features,\n        \"n_exog_features\": len(exog_features),\n        \"train_size\": len(data_train),\n        \"val_size\": len(data_val),\n        \"test_size\": len(data_test),\n        \"data_shape_original\": data.shape,\n        \"data_shape_merged\": data_with_exog.shape,\n        \"training_end\": end_validation,\n        \"prediction_start\": exo_pred.index[0],\n        \"prediction_end\": exo_pred.index[-1],\n        \"lags\": lags,\n        \"window_size\": window_size,\n        \"contamination\": contamination,\n        \"n_outliers\": (\n            outliers.sum() if isinstance(outliers, pd.Series) else len(outliers)\n        ),\n    }\n\n    return predictions, metadata, recursive_forecasters\n</code></pre>"},{"location":"api/processing/#aggregate-and-prediction-functions","title":"Aggregate and Prediction Functions","text":""},{"location":"api/processing/#agg_predict","title":"agg_predict","text":""},{"location":"api/processing/#spotforecast2_safe.processing.agg_predict","title":"<code>spotforecast2_safe.processing.agg_predict</code>","text":""},{"location":"api/processing/#spotforecast2_safe.processing.agg_predict.agg_predict","title":"<code>agg_predict(predictions, weights=None)</code>","text":"<p>Aggregates multiple prediction columns into a single combined prediction series.</p> <p>The combination is a weighted sum of the prediction columns. If no weights are provided, a default weighting scheme based on specific predefined columns is used.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>DataFrame containing the prediction columns.</p> required <code>weights</code> <code>Optional[Union[Dict[str, float], List[float], ndarray]]</code> <p>Dictionary mapping column names to their weights, or a list/array of weights corresponding to the order of columns in <code>predictions</code>. If None, defaults to summing all columns (weight=1.0 for each column).</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A Series containing the aggregated values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a column specified in weights (or default weights) is missing from predictions.</p> <code>ValueError</code> <p>If weights is a list/array and its length does not match the number of columns in predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.processing import agg_predict\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n&gt;&gt;&gt; agg_predict(df, weights={\"A\": 1.0, \"B\": -1.0})\n0   -2.0\n1   -2.0\ndtype: float64\n&gt;&gt;&gt; agg_predict(df, weights=[0.5, 2.0])\n0    6.5\n1    9.0\ndtype: float64\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/agg_predict.py</code> <pre><code>def agg_predict(\n    predictions: pd.DataFrame,\n    weights: Optional[Union[Dict[str, float], List[float], np.ndarray]] = None,\n) -&gt; pd.Series:\n    \"\"\"Aggregates multiple prediction columns into a single combined prediction series.\n\n    The combination is a weighted sum of the prediction columns. If no weights are provided,\n    a default weighting scheme based on specific predefined columns is used.\n\n    Args:\n        predictions (pd.DataFrame): DataFrame containing the prediction columns.\n        weights (Optional[Union[Dict[str, float], List[float], np.ndarray]]):\n            Dictionary mapping column names to their weights, or a list/array of weights\n            corresponding to the order of columns in `predictions`.\n            If None, defaults to summing all columns (weight=1.0 for each column).\n\n    Returns:\n        pd.Series: A Series containing the aggregated values.\n\n    Raises:\n        ValueError: If a column specified in weights (or default weights) is missing from predictions.\n        ValueError: If weights is a list/array and its length does not match the number of columns in predictions.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.processing import agg_predict\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        &gt;&gt;&gt; agg_predict(df, weights={\"A\": 1.0, \"B\": -1.0})\n        0   -2.0\n        1   -2.0\n        dtype: float64\n        &gt;&gt;&gt; agg_predict(df, weights=[0.5, 2.0])\n        0    6.5\n        1    9.0\n        dtype: float64\n    \"\"\"\n    if weights is None:\n        # Default to summing all columns\n        weights = {col: 1.0 for col in predictions.columns}\n\n    if isinstance(weights, (list, np.ndarray)):\n        if len(weights) != len(predictions.columns):\n            raise ValueError(\n                f\"Length of weights ({len(weights)}) does not match number of columns in predictions ({len(predictions.columns)})\"\n            )\n        # Convert to dictionary using column order\n        weights = dict(zip(predictions.columns, weights))\n\n    combined = pd.Series(0.0, index=predictions.index)\n\n    missing_cols = [col for col in weights.keys() if col not in predictions.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing columns in predictions dataframe: {missing_cols}\")\n\n    for col, weight in weights.items():\n        combined += predictions[col] * weight\n\n    return combined\n</code></pre>"},{"location":"api/stats/","title":"Statistics Module","text":"<p>Statistical utilities for time series analysis.</p>"},{"location":"api/stats/#spotforecast2_safe.stats","title":"<code>spotforecast2_safe.stats</code>","text":"<p>Statistical functions for time series analysis.</p>"},{"location":"api/stats/#spotforecast2_safe.stats.calculate_lag_autocorrelation","title":"<code>calculate_lag_autocorrelation(data, n_lags=50, last_n_samples=None, sort_by='partial_autocorrelation_abs', acf_kwargs={}, pacf_kwargs={})</code>","text":"<p>Calculate autocorrelation and partial autocorrelation for a time series.</p> <p>This is a wrapper around statsmodels.acf and statsmodels.pacf.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>Time series to calculate autocorrelation. If a DataFrame is provided, it must have exactly one column.</p> required <code>n_lags</code> <code>int</code> <p>Number of lags to calculate autocorrelation. Default is 50.</p> <code>50</code> <code>last_n_samples</code> <code>int | None</code> <p>Number of most recent samples to use. If None, use the entire series. Note that partial correlations can only be computed for lags up to 50% of the sample size. For example, if the series has 10 samples, n_lags must be less than or equal to 5. This parameter is useful to speed up calculations when the series is very long. Default is None.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Sort results by lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs or autocorrelation. Default is partial_autocorrelation_abs.</p> <code>'partial_autocorrelation_abs'</code> <code>acf_kwargs</code> <code>dict[str, object]</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.acf. Default is {}.</p> <code>{}</code> <code>pacf_kwargs</code> <code>dict[str, object]</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.pacf. Default is {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs, autocorrelation.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame with a single column.</p> <code>ValueError</code> <p>If data is a DataFrame with more than one column.</p> <code>TypeError</code> <p>If n_lags is not a positive integer.</p> <code>TypeError</code> <p>If last_n_samples is not None and not a positive integer.</p> <code>ValueError</code> <p>If sort_by is not one of the valid options.</p> <p>Examples:</p> <p>Calculate autocorrelation for a simple Series:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.head()\n   lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n0    1                     0.999998                 0.999998             1.000000         1.000000\n1    2                     0.000002                -0.000002             0.645497         0.645497\n2    3                     0.000002                 0.000002             0.298549         0.298549\n3    4                     0.000001                -0.000001             0.068719         0.068719\n</code></pre> <p>Calculate autocorrelation using only the last 8 samples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=3,\n...     last_n_samples=8\n... )\n&gt;&gt;&gt; result.shape\n(3, 5)\n</code></pre> <p>Calculate autocorrelation from a DataFrame with a single column:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.shape\n(4, 5)\n</code></pre> <p>Sort results by autocorrelation in descending order:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=4,\n...     sort_by='autocorrelation'\n... )\n&gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n   lag  autocorrelation\n0    1         1.000000\n1    2         0.645497\n2    3         0.298549\n3    4         0.068719\n</code></pre> Source code in <code>src/spotforecast2_safe/stats/autocorrelation.py</code> <pre><code>def calculate_lag_autocorrelation(\n    data: pd.Series | pd.DataFrame,\n    n_lags: int = 50,\n    last_n_samples: int | None = None,\n    sort_by: str = \"partial_autocorrelation_abs\",\n    acf_kwargs: dict[str, object] = {},\n    pacf_kwargs: dict[str, object] = {},\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate autocorrelation and partial autocorrelation for a time series.\n\n    This is a wrapper around statsmodels.acf and statsmodels.pacf.\n\n    Args:\n        data: Time series to calculate autocorrelation. If a DataFrame is provided,\n            it must have exactly one column.\n        n_lags: Number of lags to calculate autocorrelation. Default is 50.\n        last_n_samples: Number of most recent samples to use. If None, use the entire\n            series. Note that partial correlations can only be computed for lags up to\n            50% of the sample size. For example, if the series has 10 samples,\n            n_lags must be less than or equal to 5. This parameter is useful\n            to speed up calculations when the series is very long. Default is None.\n        sort_by: Sort results by lag, partial_autocorrelation_abs,\n            partial_autocorrelation, autocorrelation_abs or autocorrelation.\n            Default is partial_autocorrelation_abs.\n        acf_kwargs: Optional arguments to pass to statsmodels.tsa.stattools.acf.\n            Default is {}.\n        pacf_kwargs: Optional arguments to pass to statsmodels.tsa.stattools.pacf.\n            Default is {}.\n\n    Returns:\n        DataFrame with columns: lag, partial_autocorrelation_abs,\n            partial_autocorrelation, autocorrelation_abs, autocorrelation.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame with a single column.\n        ValueError: If data is a DataFrame with more than one column.\n        TypeError: If n_lags is not a positive integer.\n        TypeError: If last_n_samples is not None and not a positive integer.\n        ValueError: If sort_by is not one of the valid options.\n\n    Examples:\n        Calculate autocorrelation for a simple Series:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n        &gt;&gt;&gt; result.head()\n           lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n        0    1                     0.999998                 0.999998             1.000000         1.000000\n        1    2                     0.000002                -0.000002             0.645497         0.645497\n        2    3                     0.000002                 0.000002             0.298549         0.298549\n        3    4                     0.000001                -0.000001             0.068719         0.068719\n\n        Calculate autocorrelation using only the last 8 samples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(\n        ...     data=data,\n        ...     n_lags=3,\n        ...     last_n_samples=8\n        ... )\n        &gt;&gt;&gt; result.shape\n        (3, 5)\n\n        Calculate autocorrelation from a DataFrame with a single column:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n        &gt;&gt;&gt; result.shape\n        (4, 5)\n\n        Sort results by autocorrelation in descending order:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(\n        ...     data=data,\n        ...     n_lags=4,\n        ...     sort_by='autocorrelation'\n        ... )\n        &gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n           lag  autocorrelation\n        0    1         1.000000\n        1    2         0.645497\n        2    3         0.298549\n        3    4         0.068719\n\n    \"\"\"\n    check_optional_dependency(\"statsmodels\")\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`data` must be a pandas Series or a DataFrame with a single column. \"\n            f\"Got {type(data)}.\"\n        )\n    if isinstance(data, pd.DataFrame) and data.shape[1] != 1:\n        raise ValueError(\n            f\"If `data` is a DataFrame, it must have exactly one column. \"\n            f\"Got {data.shape[1]} columns.\"\n        )\n    if not isinstance(n_lags, int) or n_lags &lt;= 0:\n        raise TypeError(f\"`n_lags` must be a positive integer. Got {n_lags}.\")\n\n    if last_n_samples is not None:\n        if not isinstance(last_n_samples, int) or last_n_samples &lt;= 0:\n            raise TypeError(\n                f\"`last_n_samples` must be a positive integer. Got {last_n_samples}.\"\n            )\n        data = data.iloc[-last_n_samples:]\n\n    if sort_by not in [\n        \"lag\",\n        \"partial_autocorrelation_abs\",\n        \"partial_autocorrelation\",\n        \"autocorrelation_abs\",\n        \"autocorrelation\",\n    ]:\n        raise ValueError(\n            \"`sort_by` must be 'lag', 'partial_autocorrelation_abs', 'partial_autocorrelation', \"\n            \"'autocorrelation_abs' or 'autocorrelation'.\"\n        )\n\n    series = data.iloc[:, 0] if isinstance(data, pd.DataFrame) else data\n    if series.nunique() &lt;= 1:\n        acf_values = np.full(n_lags + 1, np.nan)\n        acf_values[0] = 1.0\n        pacf_values = np.zeros(n_lags + 1)\n        pacf_values[0] = 1.0\n    else:\n        pacf_values = pacf(data, nlags=n_lags, **pacf_kwargs)\n        acf_values = acf(data, nlags=n_lags, **acf_kwargs)\n\n    results = pd.DataFrame(\n        {\n            \"lag\": range(n_lags + 1),\n            \"partial_autocorrelation_abs\": np.abs(pacf_values),\n            \"partial_autocorrelation\": pacf_values,\n            \"autocorrelation_abs\": np.abs(acf_values),\n            \"autocorrelation\": acf_values,\n        }\n    ).iloc[1:]\n\n    if sort_by == \"lag\":\n        results = results.sort_values(by=sort_by, ascending=True).reset_index(drop=True)\n    else:\n        results = results.sort_values(by=sort_by, ascending=False).reset_index(\n            drop=True\n        )\n\n    return results\n</code></pre>"},{"location":"api/stats/#autocorrelation-analysis","title":"Autocorrelation Analysis","text":""},{"location":"api/stats/#autocorrelation","title":"autocorrelation","text":""},{"location":"api/stats/#spotforecast2_safe.stats.autocorrelation","title":"<code>spotforecast2_safe.stats.autocorrelation</code>","text":""},{"location":"api/stats/#spotforecast2_safe.stats.autocorrelation.calculate_lag_autocorrelation","title":"<code>calculate_lag_autocorrelation(data, n_lags=50, last_n_samples=None, sort_by='partial_autocorrelation_abs', acf_kwargs={}, pacf_kwargs={})</code>","text":"<p>Calculate autocorrelation and partial autocorrelation for a time series.</p> <p>This is a wrapper around statsmodels.acf and statsmodels.pacf.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>Time series to calculate autocorrelation. If a DataFrame is provided, it must have exactly one column.</p> required <code>n_lags</code> <code>int</code> <p>Number of lags to calculate autocorrelation. Default is 50.</p> <code>50</code> <code>last_n_samples</code> <code>int | None</code> <p>Number of most recent samples to use. If None, use the entire series. Note that partial correlations can only be computed for lags up to 50% of the sample size. For example, if the series has 10 samples, n_lags must be less than or equal to 5. This parameter is useful to speed up calculations when the series is very long. Default is None.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Sort results by lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs or autocorrelation. Default is partial_autocorrelation_abs.</p> <code>'partial_autocorrelation_abs'</code> <code>acf_kwargs</code> <code>dict[str, object]</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.acf. Default is {}.</p> <code>{}</code> <code>pacf_kwargs</code> <code>dict[str, object]</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.pacf. Default is {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs, autocorrelation.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame with a single column.</p> <code>ValueError</code> <p>If data is a DataFrame with more than one column.</p> <code>TypeError</code> <p>If n_lags is not a positive integer.</p> <code>TypeError</code> <p>If last_n_samples is not None and not a positive integer.</p> <code>ValueError</code> <p>If sort_by is not one of the valid options.</p> <p>Examples:</p> <p>Calculate autocorrelation for a simple Series:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.head()\n   lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n0    1                     0.999998                 0.999998             1.000000         1.000000\n1    2                     0.000002                -0.000002             0.645497         0.645497\n2    3                     0.000002                 0.000002             0.298549         0.298549\n3    4                     0.000001                -0.000001             0.068719         0.068719\n</code></pre> <p>Calculate autocorrelation using only the last 8 samples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=3,\n...     last_n_samples=8\n... )\n&gt;&gt;&gt; result.shape\n(3, 5)\n</code></pre> <p>Calculate autocorrelation from a DataFrame with a single column:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.shape\n(4, 5)\n</code></pre> <p>Sort results by autocorrelation in descending order:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=4,\n...     sort_by='autocorrelation'\n... )\n&gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n   lag  autocorrelation\n0    1         1.000000\n1    2         0.645497\n2    3         0.298549\n3    4         0.068719\n</code></pre> Source code in <code>src/spotforecast2_safe/stats/autocorrelation.py</code> <pre><code>def calculate_lag_autocorrelation(\n    data: pd.Series | pd.DataFrame,\n    n_lags: int = 50,\n    last_n_samples: int | None = None,\n    sort_by: str = \"partial_autocorrelation_abs\",\n    acf_kwargs: dict[str, object] = {},\n    pacf_kwargs: dict[str, object] = {},\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate autocorrelation and partial autocorrelation for a time series.\n\n    This is a wrapper around statsmodels.acf and statsmodels.pacf.\n\n    Args:\n        data: Time series to calculate autocorrelation. If a DataFrame is provided,\n            it must have exactly one column.\n        n_lags: Number of lags to calculate autocorrelation. Default is 50.\n        last_n_samples: Number of most recent samples to use. If None, use the entire\n            series. Note that partial correlations can only be computed for lags up to\n            50% of the sample size. For example, if the series has 10 samples,\n            n_lags must be less than or equal to 5. This parameter is useful\n            to speed up calculations when the series is very long. Default is None.\n        sort_by: Sort results by lag, partial_autocorrelation_abs,\n            partial_autocorrelation, autocorrelation_abs or autocorrelation.\n            Default is partial_autocorrelation_abs.\n        acf_kwargs: Optional arguments to pass to statsmodels.tsa.stattools.acf.\n            Default is {}.\n        pacf_kwargs: Optional arguments to pass to statsmodels.tsa.stattools.pacf.\n            Default is {}.\n\n    Returns:\n        DataFrame with columns: lag, partial_autocorrelation_abs,\n            partial_autocorrelation, autocorrelation_abs, autocorrelation.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame with a single column.\n        ValueError: If data is a DataFrame with more than one column.\n        TypeError: If n_lags is not a positive integer.\n        TypeError: If last_n_samples is not None and not a positive integer.\n        ValueError: If sort_by is not one of the valid options.\n\n    Examples:\n        Calculate autocorrelation for a simple Series:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n        &gt;&gt;&gt; result.head()\n           lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n        0    1                     0.999998                 0.999998             1.000000         1.000000\n        1    2                     0.000002                -0.000002             0.645497         0.645497\n        2    3                     0.000002                 0.000002             0.298549         0.298549\n        3    4                     0.000001                -0.000001             0.068719         0.068719\n\n        Calculate autocorrelation using only the last 8 samples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(\n        ...     data=data,\n        ...     n_lags=3,\n        ...     last_n_samples=8\n        ... )\n        &gt;&gt;&gt; result.shape\n        (3, 5)\n\n        Calculate autocorrelation from a DataFrame with a single column:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n        &gt;&gt;&gt; result.shape\n        (4, 5)\n\n        Sort results by autocorrelation in descending order:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(\n        ...     data=data,\n        ...     n_lags=4,\n        ...     sort_by='autocorrelation'\n        ... )\n        &gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n           lag  autocorrelation\n        0    1         1.000000\n        1    2         0.645497\n        2    3         0.298549\n        3    4         0.068719\n\n    \"\"\"\n    check_optional_dependency(\"statsmodels\")\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`data` must be a pandas Series or a DataFrame with a single column. \"\n            f\"Got {type(data)}.\"\n        )\n    if isinstance(data, pd.DataFrame) and data.shape[1] != 1:\n        raise ValueError(\n            f\"If `data` is a DataFrame, it must have exactly one column. \"\n            f\"Got {data.shape[1]} columns.\"\n        )\n    if not isinstance(n_lags, int) or n_lags &lt;= 0:\n        raise TypeError(f\"`n_lags` must be a positive integer. Got {n_lags}.\")\n\n    if last_n_samples is not None:\n        if not isinstance(last_n_samples, int) or last_n_samples &lt;= 0:\n            raise TypeError(\n                f\"`last_n_samples` must be a positive integer. Got {last_n_samples}.\"\n            )\n        data = data.iloc[-last_n_samples:]\n\n    if sort_by not in [\n        \"lag\",\n        \"partial_autocorrelation_abs\",\n        \"partial_autocorrelation\",\n        \"autocorrelation_abs\",\n        \"autocorrelation\",\n    ]:\n        raise ValueError(\n            \"`sort_by` must be 'lag', 'partial_autocorrelation_abs', 'partial_autocorrelation', \"\n            \"'autocorrelation_abs' or 'autocorrelation'.\"\n        )\n\n    series = data.iloc[:, 0] if isinstance(data, pd.DataFrame) else data\n    if series.nunique() &lt;= 1:\n        acf_values = np.full(n_lags + 1, np.nan)\n        acf_values[0] = 1.0\n        pacf_values = np.zeros(n_lags + 1)\n        pacf_values[0] = 1.0\n    else:\n        pacf_values = pacf(data, nlags=n_lags, **pacf_kwargs)\n        acf_values = acf(data, nlags=n_lags, **acf_kwargs)\n\n    results = pd.DataFrame(\n        {\n            \"lag\": range(n_lags + 1),\n            \"partial_autocorrelation_abs\": np.abs(pacf_values),\n            \"partial_autocorrelation\": pacf_values,\n            \"autocorrelation_abs\": np.abs(acf_values),\n            \"autocorrelation\": acf_values,\n        }\n    ).iloc[1:]\n\n    if sort_by == \"lag\":\n        results = results.sort_values(by=sort_by, ascending=True).reset_index(drop=True)\n    else:\n        results = results.sort_values(by=sort_by, ascending=False).reset_index(\n            drop=True\n        )\n\n    return results\n</code></pre>"},{"location":"api/utils/","title":"Utils Module","text":"<p>Utility functions and helpers.</p>"},{"location":"api/utils/#spotforecast2_safe.utils","title":"<code>spotforecast2_safe.utils</code>","text":"<p>Utility functions for spotforecast.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.create_holiday_df","title":"<code>create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#utc-conversion","title":"UTC Conversion","text":""},{"location":"api/utils/#convert_to_utc","title":"convert_to_utc","text":""},{"location":"api/utils/#spotforecast2_safe.utils.convert_to_utc","title":"<code>spotforecast2_safe.utils.convert_to_utc</code>","text":"<p>Utility functions for timezone conversion.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.convert_to_utc.convert_to_utc","title":"<code>convert_to_utc(df, timezone)</code>","text":"<p>Convert DataFrame index timezone to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Optional timezone string. Required if index has no timezone.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with UTC timezone index.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If index is not DatetimeIndex or has no timezone and timezone is None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.convert_to_utc import convert_to_utc\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]}, index=pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n&gt;&gt;&gt; convert_to_utc(df, \"Europe/Berlin\")\n           value\n2022-01-01 00:00:00+01:00\n2022-01-02 00:00:00+01:00\n2022-01-03 00:00:00+01:00\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/convert_to_utc.py</code> <pre><code>def convert_to_utc(df: pd.DataFrame, timezone: Optional[str]) -&gt; pd.DataFrame:\n    \"\"\"Convert DataFrame index timezone to UTC.\n\n    Args:\n        df: DataFrame with DatetimeIndex.\n        timezone: Optional timezone string. Required if index has no timezone.\n\n    Returns:\n        DataFrame with UTC timezone index.\n\n    Raises:\n        ValueError: If index is not DatetimeIndex or has no timezone and\n            timezone is None.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.convert_to_utc import convert_to_utc\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": [1, 2, 3]}, index=pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n        &gt;&gt;&gt; convert_to_utc(df, \"Europe/Berlin\")\n                   value\n        2022-01-01 00:00:00+01:00\n        2022-01-02 00:00:00+01:00\n        2022-01-03 00:00:00+01:00\n    \"\"\"\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\n            \"No DatetimeIndex found. Please specify the time column via 'index_col'\"\n        )\n    if df.index.tz is None:\n        if timezone is not None:\n            df.index = df.index.tz_localize(timezone)\n        else:\n            raise ValueError(\n                \"Index has no timezone information. Please provide a timezone.\"\n            )\n\n    df.index = df.index.tz_convert(\"UTC\")\n\n    return df\n</code></pre>"},{"location":"api/utils/#data-transformation","title":"Data Transformation","text":""},{"location":"api/utils/#data_transform","title":"data_transform","text":""},{"location":"api/utils/#spotforecast2_safe.utils.data_transform","title":"<code>spotforecast2_safe.utils.data_transform</code>","text":"<p>Data transformation utilities for time series forecasting.</p> <p>This module provides functions for normalizing and transforming data formats.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.data_transform.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>src/spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#forecaster-configuration","title":"Forecaster Configuration","text":""},{"location":"api/utils/#forecaster_config","title":"forecaster_config","text":""},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config","title":"<code>spotforecast2_safe.utils.forecaster_config</code>","text":"<p>Forecaster configuration utilities.</p> <p>This module provides functions for initializing and validating forecaster configuration parameters like lags and weights.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.forecaster_config.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2_safe.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#holiday-generation","title":"Holiday Generation","text":""},{"location":"api/utils/#generate_holiday","title":"generate_holiday","text":""},{"location":"api/utils/#spotforecast2_safe.utils.generate_holiday","title":"<code>spotforecast2_safe.utils.generate_holiday</code>","text":"<p>Utilities for generating holiday dataframe as covariate.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.generate_holiday.create_holiday_df","title":"<code>create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/utils/#validation-utilities","title":"Validation Utilities","text":""},{"location":"api/utils/#validation","title":"validation","text":""},{"location":"api/utils/#spotforecast2_safe.utils.validation","title":"<code>spotforecast2_safe.utils.validation</code>","text":"<p>Validation utilities for time series forecasting.</p> <p>This module provides validation functions for time series data and exogenous variables.</p>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2_safe.utils.validation.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2_safe/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/weather/","title":"Weather Module","text":"<p>Weather data utilities and integration.</p>"},{"location":"api/weather/#spotforecast2_safe.weather","title":"<code>spotforecast2_safe.weather</code>","text":"<p>Weather data utilities for spotforecast2.</p>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#weather-client","title":"Weather Client","text":""},{"location":"api/weather/#weather_client","title":"weather_client","text":""},{"location":"api/weather/#spotforecast2_safe.weather.weather_client","title":"<code>spotforecast2_safe.weather.weather_client</code>","text":"<p>Weather data fetching and processing using Open-Meteo API.</p>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherService","title":"<code>WeatherService</code>","text":"<p>               Bases: <code>WeatherClient</code></p> <p>High-level service for weather data generation.</p> <p>Extends WeatherClient with caching, hybrid fetching (archive+forecast), and fallback strategies.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherService(WeatherClient):\n    \"\"\"High-level service for weather data generation.\n\n    Extends WeatherClient with caching, hybrid fetching (archive+forecast),\n    and fallback strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        latitude: float,\n        longitude: float,\n        cache_path: Optional[Path] = None,\n        use_forecast: bool = True,\n    ):\n        super().__init__(latitude, longitude)\n        self.cache_path = cache_path\n        self.use_forecast = use_forecast\n\n    def get_dataframe(\n        self,\n        start: Union[str, pd.Timestamp],\n        end: Union[str, pd.Timestamp],\n        timezone: str = \"UTC\",\n        freq: str = \"h\",\n        fallback_on_failure: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n        Refactored from spotpredict.create_weather_df.\n        \"\"\"\n        start_ts = pd.Timestamp(start)\n        end_ts = pd.Timestamp(end)\n\n        # Localize if naive\n        if start_ts.tz is None:\n            start_ts = start_ts.tz_localize(timezone)\n        if end_ts.tz is None:\n            end_ts = end_ts.tz_localize(timezone)\n\n        # Convert to UTC for consistency\n        start_utc = start_ts.tz_convert(\"UTC\")\n        end_utc = end_ts.tz_convert(\"UTC\")\n\n        # 1. Try Cache\n        cached_df = self._load_cache()\n        if cached_df is not None:\n            if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n                self.logger.info(\"Using full cached data.\")\n                return self._finalize_df(\n                    cached_df.loc[start_utc:end_utc], freq, timezone\n                )\n\n        # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n        # (The original logic did partial fills, but full fetch is safer and\n        # simpler for now unless specifically improved).\n        # Actually, strict refactor implies keeping logic. Let's keep it simple:\n        # fetch what's needed.\n\n        try:\n            df = self._fetch_hybrid(start_ts, end_ts, timezone)\n        except Exception as e:\n            self.logger.warning(f\"Fetch failed: {e}\")\n            if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n                df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n            else:\n                raise\n\n        # 3. Merge with cache and save\n        if cached_df is not None:\n            df = pd.concat([cached_df, df])\n            df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n        if self.cache_path:\n            self._save_cache(df)\n\n        # 4. Return slice\n        return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n\n    def _fetch_hybrid(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch from Archive and/or Forecast based on date.\"\"\"\n        now = pd.Timestamp.now(tz=start.tz)\n        archive_cutoff = now - pd.Timedelta(days=5)\n\n        dfs = []\n\n        # Archive part\n        if start &lt; archive_cutoff:\n            arch_end = min(end, archive_cutoff)\n            try:\n                dfs.append(self.fetch_archive(start, arch_end, timezone))\n            except Exception as e:\n                self.logger.warning(f\"Archive fetch warning: {e}\")\n\n        # Forecast part\n        if end &gt; now and self.use_forecast:\n            days = (end - now).days + 2\n            days = min(max(1, days), 16)\n            try:\n                df_fore = self.fetch_forecast(days, timezone)\n                # Filter forecast to needed range to avoid overlap issues\n                dfs.append(df_fore)\n            except Exception as e:\n                self.logger.warning(f\"Forecast fetch warning: {e}\")\n\n        if not dfs:\n            raise ValueError(\"Could not fetch data from Archive or Forecast.\")\n\n        full_df = pd.concat(dfs)\n        full_df = full_df[~full_df.index.duplicated(keep=\"first\")].sort_index()\n\n        # Ensure UTC index\n        if full_df.index.tz is None:\n            full_df.index = full_df.index.tz_localize(timezone)\n        full_df.index = full_df.index.tz_convert(\"UTC\")\n\n        return full_df\n\n    def _create_fallback(\n        self,\n        start: pd.Timestamp,\n        end: pd.Timestamp,\n        source_df: pd.DataFrame,\n        timezone: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Repeat last 24h of data.\"\"\"\n        last_24 = source_df.tail(24)\n        hours = int((end - start).total_seconds() / 3600) + 1\n        repeats = (hours // 24) + 1\n\n        new_data = pd.concat([last_24] * repeats, ignore_index=True)\n        new_data = new_data.iloc[:hours]\n\n        idx = pd.date_range(start, periods=hours, freq=\"h\", tz=\"UTC\")\n        new_data.index = idx\n        return new_data\n\n    def _load_cache(self) -&gt; Optional[pd.DataFrame]:\n        if not self.cache_path or not self.cache_path.exists():\n            return None\n        try:\n            df = pd.read_parquet(self.cache_path)\n            if df.index.tz is None:\n                df.index = df.index.tz_localize(\"UTC\")\n            return df\n        except Exception:\n            return None\n\n    def _save_cache(self, df: pd.DataFrame):\n        if self.cache_path:\n            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n            df.to_parquet(self.cache_path)\n\n    def _finalize_df(self, df: pd.DataFrame, freq: str, timezone: str) -&gt; pd.DataFrame:\n        \"\"\"Resample and localize.\"\"\"\n        # Resample\n        if freq != \"h\":  # Assuming API returns hourly\n            df = df.resample(freq).ffill()  # Forward fill for weather is reasonable\n\n        # Fill gaps\n        df = df.ffill().bfill()\n\n        # Convert to requested timezone if needed (though we keep internal UTC mostly)\n        # User requested specific tz output usually?\n        # Original code returned normalized DF. Let's ensure frequency matches exactly.\n\n        return df\n</code></pre>"},{"location":"api/weather/#spotforecast2_safe.weather.weather_client.WeatherService.get_dataframe","title":"<code>get_dataframe(start, end, timezone='UTC', freq='h', fallback_on_failure=True)</code>","text":"<p>Get weather DataFrame for a specified range using best available methods.</p> <p>Refactored from spotpredict.create_weather_df.</p> Source code in <code>src/spotforecast2_safe/weather/weather_client.py</code> <pre><code>def get_dataframe(\n    self,\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n    Refactored from spotpredict.create_weather_df.\n    \"\"\"\n    start_ts = pd.Timestamp(start)\n    end_ts = pd.Timestamp(end)\n\n    # Localize if naive\n    if start_ts.tz is None:\n        start_ts = start_ts.tz_localize(timezone)\n    if end_ts.tz is None:\n        end_ts = end_ts.tz_localize(timezone)\n\n    # Convert to UTC for consistency\n    start_utc = start_ts.tz_convert(\"UTC\")\n    end_utc = end_ts.tz_convert(\"UTC\")\n\n    # 1. Try Cache\n    cached_df = self._load_cache()\n    if cached_df is not None:\n        if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n            self.logger.info(\"Using full cached data.\")\n            return self._finalize_df(\n                cached_df.loc[start_utc:end_utc], freq, timezone\n            )\n\n    # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n    # (The original logic did partial fills, but full fetch is safer and\n    # simpler for now unless specifically improved).\n    # Actually, strict refactor implies keeping logic. Let's keep it simple:\n    # fetch what's needed.\n\n    try:\n        df = self._fetch_hybrid(start_ts, end_ts, timezone)\n    except Exception as e:\n        self.logger.warning(f\"Fetch failed: {e}\")\n        if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n            df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n        else:\n            raise\n\n    # 3. Merge with cache and save\n    if cached_df is not None:\n        df = pd.concat([cached_df, df])\n        df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n    if self.cache_path:\n        self._save_cache(df)\n\n    # 4. Return slice\n    return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n</code></pre>"},{"location":"preprocessing/outliers/","title":"Outlier Detection and Handling","text":"<p>Guide for identifying and handling outliers in time series data.</p>"},{"location":"preprocessing/outliers/#overview","title":"Overview","text":"<p>Outlier detection is crucial for time series forecasting as extreme values can distort model training and predictions. This module provides robust outlier detection and marking capabilities.</p>"},{"location":"preprocessing/outliers/#key-functions","title":"Key Functions","text":""},{"location":"preprocessing/outliers/#mark-outliers","title":"Mark Outliers","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>spotforecast2_safe.preprocessing.outlier.mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"preprocessing/outliers/#visualize-outliers","title":"Visualize Outliers","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.visualize_outliers_plotly_scatter","title":"<code>spotforecast2_safe.preprocessing.outlier.visualize_outliers_plotly_scatter(data, data_original, columns=None, contamination=0.01, random_state=1234, **kwargs)</code>","text":"<p>Visualize outliers in time series using Plotly scatter plots.</p> <p>Creates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in time series using Plotly scatter plots.\n\n    Creates an interactive time series plot for each specified column, showing\n    regular data as a line and detected outliers as scatter points. Uses\n    IsolationForest for outlier detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        **kwargs: Additional keyword arguments passed to go.Figure.update_layout()\n            (e.g., template, height, etc.).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... }, index=dates)\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_plotly_scatter(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     template='plotly_white'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add regular data as line\n        fig.add_trace(\n            go.Scatter(\n                x=data.index,\n                y=data[col],\n                mode=\"lines\",\n                name=\"Regular Data\",\n                line=dict(color=\"lightgrey\"),\n            )\n        )\n\n        # Add outliers as scatter points\n        outlier_vals = outliers[col]\n        if not outlier_vals.empty:\n            fig.add_trace(\n                go.Scatter(\n                    x=outlier_vals.index,\n                    y=outlier_vals,\n                    mode=\"markers\",\n                    name=\"Outliers\",\n                    marker=dict(color=\"red\", size=8, symbol=\"x\"),\n                )\n            )\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Update layout with custom kwargs\n        layout_kwargs = {\n            \"title\": f\"{col} Time Series with Outliers ({pct_outliers:.2f}%)\",\n            \"xaxis_title\": \"Time\",\n            \"yaxis_title\": \"Value\",\n            \"template\": \"plotly_white\",\n            \"legend\": dict(\n                orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1\n            ),\n        }\n        layout_kwargs.update(kwargs)\n        fig.update_layout(**layout_kwargs)\n        fig.show()\n</code></pre>"},{"location":"preprocessing/outliers/#outlier-module","title":"Outlier Module","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier","title":"<code>spotforecast2_safe.preprocessing.outlier</code>","text":""},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.visualize_outliers_hist","title":"<code>visualize_outliers_hist(data, data_original, columns=None, contamination=0.01, random_state=1234, figsize=(10, 5), bins=50, **kwargs)</code>","text":"<p>Visualize outliers in DataFrame using stacked histograms.</p> <p>Creates a histogram for each specified column, displaying both regular data and detected outliers in different colors. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height). Default: (10, 5).</p> <code>(10, 5)</code> <code>bins</code> <code>int</code> <p>Number of histogram bins. Default: 50.</p> <code>50</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays matplotlib figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If matplotlib is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_hist\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... })\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_hist(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     figsize=(12, 5),\n...     alpha=0.7\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in DataFrame using stacked histograms.\n\n    Creates a histogram for each specified column, displaying both regular data\n    and detected outliers in different colors. Uses IsolationForest for outlier\n    detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        figsize: Figure size as (width, height). Default: (10, 5).\n        bins: Number of histogram bins. Default: 50.\n        **kwargs: Additional keyword arguments passed to plt.hist() (e.g., color,\n            alpha, edgecolor, etc.).\n\n    Returns:\n        None. Displays matplotlib figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If matplotlib is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_hist\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... })\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_hist(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     figsize=(12, 5),\n        ...     alpha=0.7\n        ... )\n    \"\"\"\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        # Get inliers (non-NaN values in cleaned data)\n        inliers = data[col].dropna()\n\n        # Get outlier values\n        outlier_vals = outliers[col]\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Create figure\n        plt.figure(figsize=figsize)\n        plt.hist(\n            [inliers, outlier_vals],\n            bins=bins,\n            stacked=True,\n            color=[\"lightgrey\", \"red\"],\n            label=[\"Regular Data\", \"Outliers\"],\n            **kwargs,\n        )\n        plt.grid(True, alpha=0.3)\n        plt.title(f\"{col} Distribution with Outliers ({pct_outliers:.2f}%)\")\n        plt.xlabel(\"Value\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"preprocessing/outliers/#spotforecast2_safe.preprocessing.outlier.visualize_outliers_plotly_scatter","title":"<code>visualize_outliers_plotly_scatter(data, data_original, columns=None, contamination=0.01, random_state=1234, **kwargs)</code>","text":"<p>Visualize outliers in time series using Plotly scatter plots.</p> <p>Creates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in time series using Plotly scatter plots.\n\n    Creates an interactive time series plot for each specified column, showing\n    regular data as a line and detected outliers as scatter points. Uses\n    IsolationForest for outlier detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        **kwargs: Additional keyword arguments passed to go.Figure.update_layout()\n            (e.g., template, height, etc.).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier import visualize_outliers_plotly_scatter\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... }, index=dates)\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_plotly_scatter(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     template='plotly_white'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add regular data as line\n        fig.add_trace(\n            go.Scatter(\n                x=data.index,\n                y=data[col],\n                mode=\"lines\",\n                name=\"Regular Data\",\n                line=dict(color=\"lightgrey\"),\n            )\n        )\n\n        # Add outliers as scatter points\n        outlier_vals = outliers[col]\n        if not outlier_vals.empty:\n            fig.add_trace(\n                go.Scatter(\n                    x=outlier_vals.index,\n                    y=outlier_vals,\n                    mode=\"markers\",\n                    name=\"Outliers\",\n                    marker=dict(color=\"red\", size=8, symbol=\"x\"),\n                )\n            )\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Update layout with custom kwargs\n        layout_kwargs = {\n            \"title\": f\"{col} Time Series with Outliers ({pct_outliers:.2f}%)\",\n            \"xaxis_title\": \"Time\",\n            \"yaxis_title\": \"Value\",\n            \"template\": \"plotly_white\",\n            \"legend\": dict(\n                orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1\n            ),\n        }\n        layout_kwargs.update(kwargs)\n        fig.update_layout(**layout_kwargs)\n        fig.show()\n</code></pre>"},{"location":"preprocessing/outliers/#examples","title":"Examples","text":"<pre><code>import pandas as pd\nfrom spotforecast2_safe.preprocessing.outlier import mark_outliers\n\n# Create sample time series data\ndata = pd.DataFrame({\n    'value': [1, 2, 100, 4, 5, 6, 7, 8, 9, 10],  # 100 is an outlier\n})\n\n# Mark outliers\nresult_data, outlier_mask = mark_outliers(\n    data,\n    contamination=0.1,  # Expect 10% contamination\n    columns=['value']\n)\n\nprint(f\"Outliers marked: {outlier_mask.sum()} records\")\n</code></pre>"},{"location":"preprocessing/outliers/#detection-methods","title":"Detection Methods","text":"<p>This module uses isolation forest and other statistical methods to detect: - Sudden spikes or drops - Seasonal anomalies - Drift in baseline values - Sudden shifts in variance</p>"},{"location":"preprocessing/time_series_visualization/","title":"Time Series Visualization","text":"<p>Interactive visualization tools for time series data analysis and comparison.</p>"},{"location":"preprocessing/time_series_visualization/#overview","title":"Overview","text":"<p>This module provides Plotly-based interactive visualizations for: - Single and multiple time series - Train/validation/test splits - Model comparison - Temporal patterns and trends</p>"},{"location":"preprocessing/time_series_visualization/#visualization-functions","title":"Visualization Functions","text":""},{"location":"preprocessing/time_series_visualization/#visualize-time-series-plotly","title":"Visualize Time Series (Plotly)","text":""},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.visualize_ts_plotly","title":"<code>spotforecast2_safe.preprocessing.time_series_visualization.visualize_ts_plotly(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, **kwargs)</code>","text":"<p>Visualize multiple time series datasets interactively with Plotly.</p> <p>Creates interactive Plotly scatter plots for specified columns across multiple datasets (e.g., train, validation, test splits). Each dataset is displayed as a separate line with a unique color and name in the legend.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames with datetime index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to the column name in the title. Useful for adding units or descriptions. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template name for styling. Options include 'plotly_white', 'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. If None, uses Plotly default colors. Example: {'Train': 'blue', 'Validation': 'orange'}. Default: None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Scatter() (e.g., mode='lines+markers', line=dict(dash='dash')).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes dict is empty, contains no columns, or if specified columns don't exist in all dataframes.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <code>TypeError</code> <p>If dataframes parameter is not a dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n&gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_train = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100),\n...     'humidity': np.random.normal(60, 10, 100)\n... }, index=dates_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_val = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 50),\n...     'humidity': np.random.normal(55, 10, 50)\n... }, index=dates_val)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_test = pd.DataFrame({\n...     'temperature': np.random.normal(25, 5, 30),\n...     'humidity': np.random.normal(50, 10, 30)\n... }, index=dates_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize all datasets\n&gt;&gt;&gt; dataframes = {\n...     'Train': data_train,\n...     'Validation': data_val,\n...     'Test': data_test\n... }\n&gt;&gt;&gt; visualize_ts_plotly(dataframes)\n</code></pre> <p>Single dataset example:</p> <pre><code>&gt;&gt;&gt; # Visualize single dataset\n&gt;&gt;&gt; dataframes = {'Data': data_train}\n&gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n</code></pre> <p>Custom styling:</p> <pre><code>&gt;&gt;&gt; visualize_ts_plotly(\n...     dataframes,\n...     columns=['temperature'],\n...     template='plotly_dark',\n...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n...     mode='lines+markers'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize multiple time series datasets interactively with Plotly.\n\n    Creates interactive Plotly scatter plots for specified columns across multiple\n    datasets (e.g., train, validation, test splits). Each dataset is displayed as\n    a separate line with a unique color and name in the legend.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames with datetime\n            index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to the column name in the title. Useful for\n            adding units or descriptions. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template name for styling. Options include 'plotly_white',\n            'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. If None, uses Plotly\n            default colors. Example: {'Train': 'blue', 'Validation': 'orange'}.\n            Default: None.\n        **kwargs: Additional keyword arguments passed to go.Scatter() (e.g.,\n            mode='lines+markers', line=dict(dash='dash')).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes dict is empty, contains no columns, or if\n            specified columns don't exist in all dataframes.\n        ImportError: If plotly is not installed.\n        TypeError: If dataframes parameter is not a dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n        &gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_train = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100),\n        ...     'humidity': np.random.normal(60, 10, 100)\n        ... }, index=dates_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_val = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 50),\n        ...     'humidity': np.random.normal(55, 10, 50)\n        ... }, index=dates_val)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_test = pd.DataFrame({\n        ...     'temperature': np.random.normal(25, 5, 30),\n        ...     'humidity': np.random.normal(50, 10, 30)\n        ... }, index=dates_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize all datasets\n        &gt;&gt;&gt; dataframes = {\n        ...     'Train': data_train,\n        ...     'Validation': data_val,\n        ...     'Test': data_test\n        ... }\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes)\n\n        Single dataset example:\n\n        &gt;&gt;&gt; # Visualize single dataset\n        &gt;&gt;&gt; dataframes = {'Data': data_train}\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n\n        Custom styling:\n\n        &gt;&gt;&gt; visualize_ts_plotly(\n        ...     dataframes,\n        ...     columns=['temperature'],\n        ...     template='plotly_dark',\n        ...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n        ...     mode='lines+markers'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not isinstance(dataframes, dict):\n        raise TypeError(\"dataframes parameter must be a dictionary\")\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # Validate all dataframes have data\n    for name, df in dataframes.items():\n        if df.empty:\n            raise ValueError(f\"DataFrame '{name}' is empty\")\n        if len(df.columns) == 0:\n            raise ValueError(f\"DataFrame '{name}' contains no columns\")\n\n    # Determine columns to plot\n    all_columns = set()\n    for df in dataframes.values():\n        all_columns.update(df.columns)\n\n    if not all_columns:\n        raise ValueError(\"No columns found in any dataframe\")\n\n    columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n    # Validate columns exist in all dataframes\n    for col in columns_to_plot:\n        for name, df in dataframes.items():\n            if col not in df.columns:\n                raise ValueError(f\"Column '{col}' not found in dataframe '{name}'\")\n\n    # Default colors if not provided\n    if colors is None:\n        # Use a set of distinct colors\n        default_colors = [\n            \"#1f77b4\",  # blue\n            \"#ff7f0e\",  # orange\n            \"#2ca02c\",  # green\n            \"#d62728\",  # red\n            \"#9467bd\",  # purple\n            \"#8c564b\",  # brown\n            \"#e377c2\",  # pink\n            \"#7f7f7f\",  # gray\n            \"#bcbd22\",  # olive\n            \"#17becf\",  # cyan\n        ]\n        colors = {\n            name: default_colors[i % len(default_colors)]\n            for i, name in enumerate(dataframes.keys())\n        }\n\n    # Create figures for each column\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add trace for each dataset\n        for dataset_name, df in dataframes.items():\n            fig.add_trace(\n                go.Scatter(\n                    x=df.index,\n                    y=df[col],\n                    mode=\"lines\",\n                    name=dataset_name,\n                    line=dict(color=colors[dataset_name]),\n                    **kwargs,\n                )\n            )\n\n        # Create title\n        title = col\n        if title_suffix:\n            title = f\"{col} {title_suffix}\"\n\n        # Update layout\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"Time\",\n            yaxis_title=col,\n            width=figsize[0],\n            height=figsize[1],\n            template=template,\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.02,\n                xanchor=\"right\",\n                x=1,\n            ),\n            hovermode=\"x unified\",\n        )\n\n        fig.show()\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#compare-time-series","title":"Compare Time Series","text":""},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.visualize_ts_comparison","title":"<code>spotforecast2_safe.preprocessing.time_series_visualization.visualize_ts_comparison(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, show_mean=False, **kwargs)</code>","text":"<p>Visualize time series with optional statistical overlays.</p> <p>Similar to visualize_ts_plotly but adds options for statistical overlays like mean values across all datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to column names. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. Default: None.</p> <code>None</code> <code>show_mean</code> <code>bool</code> <p>If True, overlay the mean of all datasets. Default: False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for go.Scatter().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes is empty.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; df1 = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100)\n... }, index=dates1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df2 = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 100)\n... }, index=dates2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare with mean overlay\n&gt;&gt;&gt; visualize_ts_comparison(\n...     {'Dataset1': df1, 'Dataset2': df2},\n...     show_mean=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize time series with optional statistical overlays.\n\n    Similar to visualize_ts_plotly but adds options for statistical overlays\n    like mean values across all datasets.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to column names. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. Default: None.\n        show_mean: If True, overlay the mean of all datasets. Default: False.\n        **kwargs: Additional keyword arguments for go.Scatter().\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes is empty.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df1 = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100)\n        ... }, index=dates1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df2 = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 100)\n        ... }, index=dates2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compare with mean overlay\n        &gt;&gt;&gt; visualize_ts_comparison(\n        ...     {'Dataset1': df1, 'Dataset2': df2},\n        ...     show_mean=True\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # First visualize normally\n    visualize_ts_plotly(\n        dataframes,\n        columns=columns,\n        title_suffix=title_suffix,\n        figsize=figsize,\n        template=template,\n        colors=colors,\n        **kwargs,\n    )\n\n    # If show_mean, create additional mean plot\n    if show_mean:\n        # Determine columns to plot\n        all_columns = set()\n        for df in dataframes.values():\n            all_columns.update(df.columns)\n\n        columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n        for col in columns_to_plot:\n            fig = go.Figure()\n\n            # Add individual traces\n            if colors is None:\n                default_colors = [\n                    \"#1f77b4\",\n                    \"#ff7f0e\",\n                    \"#2ca02c\",\n                    \"#d62728\",\n                    \"#9467bd\",\n                ]\n                colors_dict = {\n                    name: default_colors[i % len(default_colors)]\n                    for i, name in enumerate(dataframes.keys())\n                }\n            else:\n                colors_dict = colors\n\n            for dataset_name, df in dataframes.items():\n                fig.add_trace(\n                    go.Scatter(\n                        x=df.index,\n                        y=df[col],\n                        mode=\"lines\",\n                        name=dataset_name,\n                        line=dict(color=colors_dict[dataset_name], width=1),\n                        opacity=0.5,\n                        **kwargs,\n                    )\n                )\n\n            # Calculate and add mean\n            # Align all dataframes by index and compute mean\n            aligned_dfs = [\n                dataframes[name][[col]].rename(columns={col: name})\n                for name in dataframes.keys()\n            ]\n            combined = pd.concat(aligned_dfs, axis=1)\n            mean_values = combined.mean(axis=1)\n\n            fig.add_trace(\n                go.Scatter(\n                    x=mean_values.index,\n                    y=mean_values,\n                    mode=\"lines\",\n                    name=\"Mean\",\n                    line=dict(color=\"black\", width=3, dash=\"dash\"),\n                )\n            )\n\n            title = f\"{col} (with mean){title_suffix}\"\n\n            fig.update_layout(\n                title=title,\n                xaxis_title=\"Time\",\n                yaxis_title=col,\n                width=figsize[0],\n                height=figsize[1],\n                template=template,\n                legend=dict(\n                    orientation=\"h\",\n                    yanchor=\"bottom\",\n                    y=1.02,\n                    xanchor=\"right\",\n                    x=1,\n                ),\n                hovermode=\"x unified\",\n            )\n\n            fig.show()\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#time-series-visualization-module","title":"Time Series Visualization Module","text":""},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization","title":"<code>spotforecast2_safe.preprocessing.time_series_visualization</code>","text":"<p>Time series visualization.</p>"},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.plot_forecast","title":"<code>plot_forecast(model, X, y, cv_results=None, title='Forecast', figsize=None, show=True, nrows=None, ncols=1, sharex=True)</code>","text":"<p>Plot model forecast against actuals and display CV metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Fitted scikit-learn model.</p> required <code>X</code> <code>DataFrame</code> <p>Feature matrix (e.g., test set).</p> required <code>y</code> <code>Union[Series, DataFrame]</code> <p>Target series or DataFrame (e.g., test set).</p> required <code>cv_results</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary of cross-validation results from <code>evaluate()</code> or <code>sklearn.model_selection.cross_validate()</code>.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"Forecast\".</p> <code>'Forecast'</code> <code>figsize</code> <code>Optional[tuple]</code> <p>Figure dimensions.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>nrows</code> <code>Optional[int]</code> <p>Number of rows for subplots (multivariate).</p> <code>None</code> <code>ncols</code> <code>int</code> <p>Number of columns for subplots (multivariate).</p> <code>1</code> <code>sharex</code> <code>bool</code> <p>Whether to share x-axis for subplots. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The matplotlib Figure object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_forecast\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n&gt;&gt;&gt; X = pd.DataFrame({\"feat\": np.arange(10)}, index=dates)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; model = LinearRegression().fit(X, y)\n&gt;&gt;&gt; # Plot forecast\n&gt;&gt;&gt; fig = plot_forecast(model, X, y, show=False)\n&gt;&gt;&gt; plt.close(fig)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def plot_forecast(\n    model: Any,\n    X: pd.DataFrame,\n    y: Union[pd.Series, pd.DataFrame],\n    cv_results: Optional[Dict[str, Any]] = None,\n    title: str = \"Forecast\",\n    figsize: Optional[tuple] = None,\n    show: bool = True,\n    nrows: Optional[int] = None,\n    ncols: int = 1,\n    sharex: bool = True,\n) -&gt; plt.Figure:\n    \"\"\"Plot model forecast against actuals and display CV metrics.\n\n    Args:\n        model: Fitted scikit-learn model.\n        X: Feature matrix (e.g., test set).\n        y: Target series or DataFrame (e.g., test set).\n        cv_results: Optional dictionary of cross-validation results from\n            `evaluate()` or `sklearn.model_selection.cross_validate()`.\n        title: Title of the plot. Defaults to \"Forecast\".\n        figsize: Figure dimensions.\n        show: Whether to display the plot. Defaults to True.\n        nrows: Number of rows for subplots (multivariate).\n        ncols: Number of columns for subplots (multivariate).\n        sharex: Whether to share x-axis for subplots. Defaults to True.\n\n    Returns:\n        plt.Figure: The matplotlib Figure object.\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_forecast\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n        &gt;&gt;&gt; X = pd.DataFrame({\"feat\": np.arange(10)}, index=dates)\n        &gt;&gt;&gt; y = pd.Series(np.arange(10), index=dates)\n        &gt;&gt;&gt; model = LinearRegression().fit(X, y)\n        &gt;&gt;&gt; # Plot forecast\n        &gt;&gt;&gt; fig = plot_forecast(model, X, y, show=False)\n        &gt;&gt;&gt; plt.close(fig)\n    \"\"\"\n    # 1. Generate predictions/forecast\n    # Assume model is already fitted\n    y_pred = model.predict(X)\n\n    # 2. Format title with metrics if available\n    if cv_results:\n        metrics_str = []\n        if \"test_neg_mean_absolute_error\" in cv_results:\n            mae = -cv_results[\"test_neg_mean_absolute_error\"]\n            metrics_str.append(f\"MAE: {np.mean(mae):.3f} (\u00b1{np.std(mae):.3f})\")\n        if \"test_neg_root_mean_squared_error\" in cv_results:\n            rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n            metrics_str.append(f\"RMSE: {np.mean(rmse):.3f} (\u00b1{np.std(rmse):.3f})\")\n\n        if metrics_str:\n            title += \"\\n\" + \" | \".join(metrics_str)\n\n    # 3. Plot\n    predictions = {\"Forecast\": y_pred}\n    return plot_predictions(\n        y,\n        predictions,\n        slice_seq=None,\n        title=title,\n        figsize=figsize,\n        show=show,\n        nrows=nrows,\n        ncols=ncols,\n        sharex=sharex,\n    )\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.plot_predictions","title":"<code>plot_predictions(y_true, predictions, slice_seq=None, title='Predictions vs Actuals', figsize=None, show=True, nrows=None, ncols=1, sharex=True)</code>","text":"<p>Plot actual values against one or more prediction series.</p> <p>Allows visualizing model performance by overlaying predictions on top of actual data. Supports slicing to focus on a specific time range (e.g., the recent test set). Handles both univariate and multivariate targets by creating subplots for multiple targets.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[Series, DataFrame]</code> <p>Series or DataFrame containing the actual target values.</p> required <code>predictions</code> <code>Dict[str, Union[Series, DataFrame, ndarray]]</code> <p>Dictionary where keys are labels (e.g., model names) and values are the corresponding predictions. If arrays are provided, they must have the same length as the sliced <code>y_true</code>.</p> required <code>slice_seq</code> <code>Optional[slice]</code> <p>Optional slice object to select a subset of the data. If None, the entire series is plotted. Example: <code>slice(-96, None)</code> to select the last 96 points.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"Predictions vs Actuals\".</p> <code>'Predictions vs Actuals'</code> <code>figsize</code> <code>Optional[tuple]</code> <p>Tuple defining figure width and height. If None, automatically calculated based on number of subplots.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>nrows</code> <code>Optional[int]</code> <p>Number of rows for subplots (multivariate). Defaults to n_targets.</p> <code>None</code> <code>ncols</code> <code>int</code> <p>Number of columns for subplots (multivariate). Defaults to 1.</p> <code>1</code> <code>sharex</code> <code>bool</code> <p>Whether to share x-axis for subplots. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The matplotlib Figure object containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_predictions\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n&gt;&gt;&gt; y_true = pd.Series(np.arange(10), index=dates, name=\"Target\")\n&gt;&gt;&gt; predictions = {\"Model A\": y_true + 0.5}\n&gt;&gt;&gt; # Plot predictions\n&gt;&gt;&gt; fig = plot_predictions(y_true, predictions, show=False)\n&gt;&gt;&gt; plt.close(fig)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def plot_predictions(\n    y_true: Union[pd.Series, pd.DataFrame],\n    predictions: Dict[str, Union[pd.Series, pd.DataFrame, np.ndarray]],\n    slice_seq: Optional[slice] = None,\n    title: str = \"Predictions vs Actuals\",\n    figsize: Optional[tuple] = None,\n    show: bool = True,\n    nrows: Optional[int] = None,\n    ncols: int = 1,\n    sharex: bool = True,\n) -&gt; plt.Figure:\n    \"\"\"Plot actual values against one or more prediction series.\n\n    Allows visualizing model performance by overlaying predictions on top of\n    actual data. Supports slicing to focus on a specific time range (e.g.,\n    the recent test set). Handles both univariate and multivariate targets\n    by creating subplots for multiple targets.\n\n    Args:\n        y_true: Series or DataFrame containing the actual target values.\n        predictions: Dictionary where keys are labels (e.g., model names) and\n            values are the corresponding predictions.\n            If arrays are provided, they must have the same length as the\n            sliced `y_true`.\n        slice_seq: Optional slice object to select a subset of the data.\n            If None, the entire series is plotted.\n            Example: `slice(-96, None)` to select the last 96 points.\n        title: Title of the plot. Defaults to \"Predictions vs Actuals\".\n        figsize: Tuple defining figure width and height. If None, automatically\n            calculated based on number of subplots.\n        show: Whether to display the plot. Defaults to True.\n        nrows: Number of rows for subplots (multivariate). Defaults to n_targets.\n        ncols: Number of columns for subplots (multivariate). Defaults to 1.\n        sharex: Whether to share x-axis for subplots. Defaults to True.\n\n    Returns:\n        plt.Figure: The matplotlib Figure object containing the plot.\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_predictions\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n        &gt;&gt;&gt; y_true = pd.Series(np.arange(10), index=dates, name=\"Target\")\n        &gt;&gt;&gt; predictions = {\"Model A\": y_true + 0.5}\n        &gt;&gt;&gt; # Plot predictions\n        &gt;&gt;&gt; fig = plot_predictions(y_true, predictions, show=False)\n        &gt;&gt;&gt; plt.close(fig)\n    \"\"\"\n    if slice_seq is None:\n        slice_seq = slice(None)\n\n    # Handle y_true slicing\n    y_plot = y_true.iloc[slice_seq]\n\n    # Determine dimensions\n    if isinstance(y_plot, pd.Series):\n        targets = [y_plot.name] if y_plot.name else [\"Target\"]\n        # Convert to DataFrame for consistent interface\n        y_plot = y_plot.to_frame(name=targets[0])\n    else:\n        targets = y_plot.columns.tolist()\n\n    n_targets = len(targets)\n\n    # Setup layout\n    if nrows is None:\n        nrows = n_targets\n\n    # Check if nrows * ncols covers all targets\n    if nrows * ncols &lt; n_targets:\n        # Auto-adjust if invalid\n        nrows = (n_targets + ncols - 1) // ncols\n\n    if figsize is None:\n        figsize = (12, 4 * nrows)\n\n    fig, axes = plt.subplots(\n        nrows=nrows,\n        ncols=ncols,\n        figsize=figsize,\n        sharex=sharex,\n        squeeze=False,  # Ensure axes is always 2D array\n    )\n    fig.suptitle(title)\n\n    # Flatten axes for iteration\n    axes_flat = axes.flatten()\n\n    for i, target in enumerate(targets):\n        if i &gt;= len(axes_flat):\n            break\n\n        ax = axes_flat[i]\n\n        # Plot Actuals\n        target_actuals = y_plot[target]\n        ax.plot(\n            target_actuals.index,\n            target_actuals.values,\n            \"x-\",\n            alpha=0.5,\n            label=\"Actual\",\n            color=\"black\",\n            linewidth=2,\n        )\n\n        # Plot Predictions\n        for label, y_pred in predictions.items():\n            if isinstance(y_pred, pd.DataFrame):\n                # Try specific column logic\n                if target in y_pred.columns:\n                    pred_part = y_pred[target]\n                elif len(y_pred.columns) == n_targets:\n                    # Assume aligned order? Risky but fallback\n                    pred_part = y_pred.iloc[:, i]\n                else:\n                    continue  # Warning?\n\n            elif isinstance(y_pred, np.ndarray):\n                # If array, check dimensions\n                if y_pred.ndim &gt; 1 and y_pred.shape[1] == n_targets:\n                    pred_part = y_pred[:, i]\n                elif y_pred.ndim == 1 and n_targets == 1:\n                    pred_part = y_pred\n                else:\n                    continue  # Mismatch\n\n            elif isinstance(y_pred, pd.Series):\n                if n_targets == 1:\n                    pred_part = y_pred\n                else:\n                    continue  # Mismatch?\n\n            else:\n                continue\n\n            # Process slice/alignment for pred_part\n            # Logic borrowed from previous:\n            # If length matches full y_true, slice it.\n            # If length matches y_plot (sliced), use as is.\n\n            full_len = len(y_true)\n            sliced_len = len(y_plot)\n\n            vals_to_plot = None\n\n            # Simple heuristic\n            if isinstance(pred_part, (pd.Series, pd.DataFrame)):\n                vals_to_plot = pred_part.values\n            else:\n                vals_to_plot = pred_part\n\n            if len(vals_to_plot) == full_len:\n                vals_to_plot = vals_to_plot[slice_seq]\n            elif len(vals_to_plot) != sliced_len:\n                # Length mismatch warning?\n                pass\n\n            ax.plot(target_actuals.index, vals_to_plot, \"x-\", label=label, alpha=0.8)\n\n        ax.set_title(target)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        if not sharex:\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n\n    # Hide unused subplots\n    for j in range(i + 1, len(axes_flat)):\n        axes_flat[j].axis(\"off\")\n\n    if sharex:\n        # Rotate labels for bottom row axes\n        for ax in axes[-1, :]:\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n\n    plt.tight_layout()\n\n    if show:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.plot_seasonality","title":"<code>plot_seasonality(data, target, figsize=(8, 5), show=True, logscale=False)</code>","text":"<p>Plot seasonal patterns (annual, weekly, daily) for a given target.</p> <p>Creates a 2x2 grid of plots: 1. Distribution by month (boxplot + median). 2. Distribution by week day (boxplot + median). 3. Distribution by hour of day (boxplot + median). 4. Mean target value by day of week and hour.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the time series data. Must have a DatetimeIndex or an index convertible to datetime.</p> required <code>target</code> <code>str</code> <p>Name of the column to plot.</p> required <code>figsize</code> <code>tuple[int, int]</code> <p>Figure dimensions (width, height). Defaults to (8, 5).</p> <code>(8, 5)</code> <code>show</code> <code>bool</code> <p>Whether to display the plot immediately. Defaults to True.</p> <code>True</code> <code>logscale</code> <code>Union[bool, list[bool]]</code> <p>Whether to use a log scale for the y-axis. Can be a single boolean (applies to all 4 plots) or a list of 4 booleans (applies to each plot individually). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The matplotlib Figure object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_seasonality\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=1000, freq=\"h\")\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": range(1, 1001)}, index=dates)\n&gt;&gt;&gt; # Plot seasonality with log scale for all plots\n&gt;&gt;&gt; fig = plot_seasonality(data=df, target=\"value\", logscale=True, show=False)\n&gt;&gt;&gt; plt.close(fig)\n&gt;&gt;&gt; # Plot seasonality with log scale for the first plot only\n&gt;&gt;&gt; fig = plot_seasonality(\n...     data=df,\n...     target=\"value\",\n...     logscale=[True, False, False, False],\n...     show=False\n... )\n&gt;&gt;&gt; plt.close(fig)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def plot_seasonality(\n    data: pd.DataFrame,\n    target: str,\n    figsize: tuple[int, int] = (8, 5),\n    show: bool = True,\n    logscale: Union[bool, list[bool]] = False,\n) -&gt; plt.Figure:\n    \"\"\"Plot seasonal patterns (annual, weekly, daily) for a given target.\n\n    Creates a 2x2 grid of plots:\n    1. Distribution by month (boxplot + median).\n    2. Distribution by week day (boxplot + median).\n    3. Distribution by hour of day (boxplot + median).\n    4. Mean target value by day of week and hour.\n\n    Args:\n        data: DataFrame containing the time series data. Must have a DatetimeIndex\n            or an index convertible to datetime.\n        target: Name of the column to plot.\n        figsize: Figure dimensions (width, height). Defaults to (8, 5).\n        show: Whether to display the plot immediately. Defaults to True.\n        logscale: Whether to use a log scale for the y-axis.\n            Can be a single boolean (applies to all 4 plots) or a list of 4\n            booleans (applies to each plot individually). Defaults to False.\n\n    Returns:\n        plt.Figure: The matplotlib Figure object.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_seasonality\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=1000, freq=\"h\")\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": range(1, 1001)}, index=dates)\n        &gt;&gt;&gt; # Plot seasonality with log scale for all plots\n        &gt;&gt;&gt; fig = plot_seasonality(data=df, target=\"value\", logscale=True, show=False)\n        &gt;&gt;&gt; plt.close(fig)\n        &gt;&gt;&gt; # Plot seasonality with log scale for the first plot only\n        &gt;&gt;&gt; fig = plot_seasonality(\n        ...     data=df,\n        ...     target=\"value\",\n        ...     logscale=[True, False, False, False],\n        ...     show=False\n        ... )\n        &gt;&gt;&gt; plt.close(fig)\n    \"\"\"\n    # Work on a copy to avoid modifying the original dataframe with localized features\n    df = data.copy()\n\n    # Create temporal features\n    df[\"month\"] = df.index.month\n    df[\"week_day\"] = df.index.day_of_week + 1\n    df[\"hour_day\"] = df.index.hour + 1\n\n    # Handle logscale\n    if isinstance(logscale, bool):\n        logscales = [logscale] * 4\n        sharey = True\n    else:\n        if len(logscale) != 4:\n            raise ValueError(\"logscale list must have length 4.\")\n        logscales = logscale\n        # If different scales are used, we should not share y-axis\n        sharey = len(set(logscales)) == 1\n\n    fig, axs = plt.subplots(2, 2, figsize=figsize, sharex=False, sharey=sharey)\n    axs = axs.ravel()\n\n    # 1. Distribution by month\n    df.boxplot(\n        column=target, by=\"month\", ax=axs[0], flierprops={\"markersize\": 3, \"alpha\": 0.3}\n    )\n    df.groupby(\"month\")[target].median().plot(style=\"o-\", linewidth=0.8, ax=axs[0])\n    axs[0].set_ylabel(target)\n    axs[0].set_title(f\"{target} distribution by month\", fontsize=9)\n\n    # 2. Distribution by week day\n    df.boxplot(\n        column=target,\n        by=\"week_day\",\n        ax=axs[1],\n        flierprops={\"markersize\": 3, \"alpha\": 0.3},\n    )\n    df.groupby(\"week_day\")[target].median().plot(style=\"o-\", linewidth=0.8, ax=axs[1])\n    axs[1].set_ylabel(target)\n    axs[1].set_title(f\"{target} distribution by week day\", fontsize=9)\n\n    # 3. Distribution by the hour of the day\n    df.boxplot(\n        column=target,\n        by=\"hour_day\",\n        ax=axs[2],\n        flierprops={\"markersize\": 3, \"alpha\": 0.3},\n    )\n    df.groupby(\"hour_day\")[target].median().plot(style=\"o-\", linewidth=0.8, ax=axs[2])\n    axs[2].set_ylabel(target)\n    axs[2].set_title(f\"{target} distribution by the hour of the day\", fontsize=9)\n\n    # 4. Distribution by week day and hour of the day\n    mean_day_hour = df.groupby([\"week_day\", \"hour_day\"])[target].mean()\n    mean_day_hour.plot(ax=axs[3])\n    axs[3].set(\n        title=f\"Mean {target} during week\",\n        xticks=[i * 24 for i in range(7)],\n        xticklabels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n        xlabel=\"Day and hour\",\n        ylabel=f\"Number of {target}\",\n    )\n    axs[3].grid(True)\n    axs[3].title.set_size(10)\n\n    # Apply logscale\n    for i, ax in enumerate(axs):\n        if logscales[i]:\n            ax.set_yscale(\"log\")\n\n    fig.suptitle(f\"Seasonality plots: {target}\", fontsize=12)\n    fig.tight_layout()\n\n    if show:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.plot_zoomed_timeseries","title":"<code>plot_zoomed_timeseries(data, target, zoom, title=None, figsize=(8, 4), show=True)</code>","text":"<p>Plot a time series with a zoomed-in focus area.</p> <p>Creates a two-panel plot: 1. Top panel: Full time series with the zoom area highlighted. 2. Bottom panel: Zoomed-in view of the specified time range.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the time series data. Must have a DatetimeIndex or an index convertible to datetime.</p> required <code>target</code> <code>str</code> <p>Name of the column to plot.</p> required <code>zoom</code> <code>tuple[str, str]</code> <p>Tuple of (start_date, end_date) strings defining the zoom range.</p> required <code>title</code> <code>Optional[str]</code> <p>Optional title for the plot. If None, defaults to target name.</p> <code>None</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure dimensions (width, height). Defaults to (8, 4).</p> <code>(8, 4)</code> <code>show</code> <code>bool</code> <p>Whether to display the plot immediately. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The matplotlib Figure object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_zoomed_timeseries\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"h\")\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": range(100)}, index=dates)\n&gt;&gt;&gt; # Plot with zoom\n&gt;&gt;&gt; fig = plot_zoomed_timeseries(\n...     data=df,\n...     target=\"value\",\n...     zoom=(\"2023-01-02 00:00\", \"2023-01-03 00:00\"),\n...     show=False\n... )\n&gt;&gt;&gt; plt.close(fig)\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def plot_zoomed_timeseries(\n    data: pd.DataFrame,\n    target: str,\n    zoom: tuple[str, str],\n    title: Optional[str] = None,\n    figsize: tuple[int, int] = (8, 4),\n    show: bool = True,\n) -&gt; plt.Figure:\n    \"\"\"Plot a time series with a zoomed-in focus area.\n\n    Creates a two-panel plot:\n    1. Top panel: Full time series with the zoom area highlighted.\n    2. Bottom panel: Zoomed-in view of the specified time range.\n\n    Args:\n        data: DataFrame containing the time series data. Must have a DatetimeIndex\n            or an index convertible to datetime.\n        target: Name of the column to plot.\n        zoom: Tuple of (start_date, end_date) strings defining the zoom range.\n        title: Optional title for the plot. If None, defaults to target name.\n        figsize: Figure dimensions (width, height). Defaults to (8, 4).\n        show: Whether to display the plot immediately. Defaults to True.\n\n    Returns:\n        plt.Figure: The matplotlib Figure object.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_zoomed_timeseries\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"h\")\n        &gt;&gt;&gt; df = pd.DataFrame({\"value\": range(100)}, index=dates)\n        &gt;&gt;&gt; # Plot with zoom\n        &gt;&gt;&gt; fig = plot_zoomed_timeseries(\n        ...     data=df,\n        ...     target=\"value\",\n        ...     zoom=(\"2023-01-02 00:00\", \"2023-01-03 00:00\"),\n        ...     show=False\n        ... )\n        &gt;&gt;&gt; plt.close(fig)\n    \"\"\"\n    if title is None:\n        title = target\n\n    fig, axs = plt.subplots(\n        2, 1, figsize=figsize, gridspec_kw={\"height_ratios\": [1, 2]}\n    )\n\n    # Top plot: Full series with highlighted zoom area\n    data[target].plot(ax=axs[0], color=\"black\", alpha=0.5)\n    axs[0].axvspan(zoom[0], zoom[1], color=\"blue\", alpha=0.7)\n    axs[0].set_title(f\"{title}\")\n    axs[0].set_xlabel(\"\")\n    axs[0].grid(True)\n\n    # Bottom plot: Zoomed view\n    data.loc[zoom[0] : zoom[1], target].plot(ax=axs[1], color=\"blue\")\n    axs[1].set_title(f\"Zoom: {zoom[0]} to {zoom[1]}\", fontsize=10)\n    axs[1].grid(True)\n\n    plt.tight_layout()\n\n    if show:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.visualize_ts_comparison","title":"<code>visualize_ts_comparison(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, show_mean=False, **kwargs)</code>","text":"<p>Visualize time series with optional statistical overlays.</p> <p>Similar to visualize_ts_plotly but adds options for statistical overlays like mean values across all datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to column names. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. Default: None.</p> <code>None</code> <code>show_mean</code> <code>bool</code> <p>If True, overlay the mean of all datasets. Default: False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for go.Scatter().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes is empty.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; df1 = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100)\n... }, index=dates1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df2 = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 100)\n... }, index=dates2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare with mean overlay\n&gt;&gt;&gt; visualize_ts_comparison(\n...     {'Dataset1': df1, 'Dataset2': df2},\n...     show_mean=True\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize time series with optional statistical overlays.\n\n    Similar to visualize_ts_plotly but adds options for statistical overlays\n    like mean values across all datasets.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to column names. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. Default: None.\n        show_mean: If True, overlay the mean of all datasets. Default: False.\n        **kwargs: Additional keyword arguments for go.Scatter().\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes is empty.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df1 = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100)\n        ... }, index=dates1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df2 = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 100)\n        ... }, index=dates2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compare with mean overlay\n        &gt;&gt;&gt; visualize_ts_comparison(\n        ...     {'Dataset1': df1, 'Dataset2': df2},\n        ...     show_mean=True\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # First visualize normally\n    visualize_ts_plotly(\n        dataframes,\n        columns=columns,\n        title_suffix=title_suffix,\n        figsize=figsize,\n        template=template,\n        colors=colors,\n        **kwargs,\n    )\n\n    # If show_mean, create additional mean plot\n    if show_mean:\n        # Determine columns to plot\n        all_columns = set()\n        for df in dataframes.values():\n            all_columns.update(df.columns)\n\n        columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n        for col in columns_to_plot:\n            fig = go.Figure()\n\n            # Add individual traces\n            if colors is None:\n                default_colors = [\n                    \"#1f77b4\",\n                    \"#ff7f0e\",\n                    \"#2ca02c\",\n                    \"#d62728\",\n                    \"#9467bd\",\n                ]\n                colors_dict = {\n                    name: default_colors[i % len(default_colors)]\n                    for i, name in enumerate(dataframes.keys())\n                }\n            else:\n                colors_dict = colors\n\n            for dataset_name, df in dataframes.items():\n                fig.add_trace(\n                    go.Scatter(\n                        x=df.index,\n                        y=df[col],\n                        mode=\"lines\",\n                        name=dataset_name,\n                        line=dict(color=colors_dict[dataset_name], width=1),\n                        opacity=0.5,\n                        **kwargs,\n                    )\n                )\n\n            # Calculate and add mean\n            # Align all dataframes by index and compute mean\n            aligned_dfs = [\n                dataframes[name][[col]].rename(columns={col: name})\n                for name in dataframes.keys()\n            ]\n            combined = pd.concat(aligned_dfs, axis=1)\n            mean_values = combined.mean(axis=1)\n\n            fig.add_trace(\n                go.Scatter(\n                    x=mean_values.index,\n                    y=mean_values,\n                    mode=\"lines\",\n                    name=\"Mean\",\n                    line=dict(color=\"black\", width=3, dash=\"dash\"),\n                )\n            )\n\n            title = f\"{col} (with mean){title_suffix}\"\n\n            fig.update_layout(\n                title=title,\n                xaxis_title=\"Time\",\n                yaxis_title=col,\n                width=figsize[0],\n                height=figsize[1],\n                template=template,\n                legend=dict(\n                    orientation=\"h\",\n                    yanchor=\"bottom\",\n                    y=1.02,\n                    xanchor=\"right\",\n                    x=1,\n                ),\n                hovermode=\"x unified\",\n            )\n\n            fig.show()\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#spotforecast2_safe.preprocessing.time_series_visualization.visualize_ts_plotly","title":"<code>visualize_ts_plotly(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, **kwargs)</code>","text":"<p>Visualize multiple time series datasets interactively with Plotly.</p> <p>Creates interactive Plotly scatter plots for specified columns across multiple datasets (e.g., train, validation, test splits). Each dataset is displayed as a separate line with a unique color and name in the legend.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames with datetime index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to the column name in the title. Useful for adding units or descriptions. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template name for styling. Options include 'plotly_white', 'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. If None, uses Plotly default colors. Example: {'Train': 'blue', 'Validation': 'orange'}. Default: None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Scatter() (e.g., mode='lines+markers', line=dict(dash='dash')).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes dict is empty, contains no columns, or if specified columns don't exist in all dataframes.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <code>TypeError</code> <p>If dataframes parameter is not a dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n&gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_train = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100),\n...     'humidity': np.random.normal(60, 10, 100)\n... }, index=dates_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_val = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 50),\n...     'humidity': np.random.normal(55, 10, 50)\n... }, index=dates_val)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_test = pd.DataFrame({\n...     'temperature': np.random.normal(25, 5, 30),\n...     'humidity': np.random.normal(50, 10, 30)\n... }, index=dates_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize all datasets\n&gt;&gt;&gt; dataframes = {\n...     'Train': data_train,\n...     'Validation': data_val,\n...     'Test': data_test\n... }\n&gt;&gt;&gt; visualize_ts_plotly(dataframes)\n</code></pre> <p>Single dataset example:</p> <pre><code>&gt;&gt;&gt; # Visualize single dataset\n&gt;&gt;&gt; dataframes = {'Data': data_train}\n&gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n</code></pre> <p>Custom styling:</p> <pre><code>&gt;&gt;&gt; visualize_ts_plotly(\n...     dataframes,\n...     columns=['temperature'],\n...     template='plotly_dark',\n...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n...     mode='lines+markers'\n... )\n</code></pre> Source code in <code>src/spotforecast2_safe/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize multiple time series datasets interactively with Plotly.\n\n    Creates interactive Plotly scatter plots for specified columns across multiple\n    datasets (e.g., train, validation, test splits). Each dataset is displayed as\n    a separate line with a unique color and name in the legend.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames with datetime\n            index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to the column name in the title. Useful for\n            adding units or descriptions. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template name for styling. Options include 'plotly_white',\n            'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. If None, uses Plotly\n            default colors. Example: {'Train': 'blue', 'Validation': 'orange'}.\n            Default: None.\n        **kwargs: Additional keyword arguments passed to go.Scatter() (e.g.,\n            mode='lines+markers', line=dict(dash='dash')).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes dict is empty, contains no columns, or if\n            specified columns don't exist in all dataframes.\n        ImportError: If plotly is not installed.\n        TypeError: If dataframes parameter is not a dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n        &gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_train = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100),\n        ...     'humidity': np.random.normal(60, 10, 100)\n        ... }, index=dates_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_val = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 50),\n        ...     'humidity': np.random.normal(55, 10, 50)\n        ... }, index=dates_val)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_test = pd.DataFrame({\n        ...     'temperature': np.random.normal(25, 5, 30),\n        ...     'humidity': np.random.normal(50, 10, 30)\n        ... }, index=dates_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize all datasets\n        &gt;&gt;&gt; dataframes = {\n        ...     'Train': data_train,\n        ...     'Validation': data_val,\n        ...     'Test': data_test\n        ... }\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes)\n\n        Single dataset example:\n\n        &gt;&gt;&gt; # Visualize single dataset\n        &gt;&gt;&gt; dataframes = {'Data': data_train}\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n\n        Custom styling:\n\n        &gt;&gt;&gt; visualize_ts_plotly(\n        ...     dataframes,\n        ...     columns=['temperature'],\n        ...     template='plotly_dark',\n        ...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n        ...     mode='lines+markers'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not isinstance(dataframes, dict):\n        raise TypeError(\"dataframes parameter must be a dictionary\")\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # Validate all dataframes have data\n    for name, df in dataframes.items():\n        if df.empty:\n            raise ValueError(f\"DataFrame '{name}' is empty\")\n        if len(df.columns) == 0:\n            raise ValueError(f\"DataFrame '{name}' contains no columns\")\n\n    # Determine columns to plot\n    all_columns = set()\n    for df in dataframes.values():\n        all_columns.update(df.columns)\n\n    if not all_columns:\n        raise ValueError(\"No columns found in any dataframe\")\n\n    columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n    # Validate columns exist in all dataframes\n    for col in columns_to_plot:\n        for name, df in dataframes.items():\n            if col not in df.columns:\n                raise ValueError(f\"Column '{col}' not found in dataframe '{name}'\")\n\n    # Default colors if not provided\n    if colors is None:\n        # Use a set of distinct colors\n        default_colors = [\n            \"#1f77b4\",  # blue\n            \"#ff7f0e\",  # orange\n            \"#2ca02c\",  # green\n            \"#d62728\",  # red\n            \"#9467bd\",  # purple\n            \"#8c564b\",  # brown\n            \"#e377c2\",  # pink\n            \"#7f7f7f\",  # gray\n            \"#bcbd22\",  # olive\n            \"#17becf\",  # cyan\n        ]\n        colors = {\n            name: default_colors[i % len(default_colors)]\n            for i, name in enumerate(dataframes.keys())\n        }\n\n    # Create figures for each column\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add trace for each dataset\n        for dataset_name, df in dataframes.items():\n            fig.add_trace(\n                go.Scatter(\n                    x=df.index,\n                    y=df[col],\n                    mode=\"lines\",\n                    name=dataset_name,\n                    line=dict(color=colors[dataset_name]),\n                    **kwargs,\n                )\n            )\n\n        # Create title\n        title = col\n        if title_suffix:\n            title = f\"{col} {title_suffix}\"\n\n        # Update layout\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"Time\",\n            yaxis_title=col,\n            width=figsize[0],\n            height=figsize[1],\n            template=template,\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.02,\n                xanchor=\"right\",\n                x=1,\n            ),\n            hovermode=\"x unified\",\n        )\n\n        fig.show()\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#examples","title":"Examples","text":""},{"location":"preprocessing/time_series_visualization/#basic-visualization","title":"Basic Visualization","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample time series\ndates = pd.date_range('2024-01-01', periods=100, freq='h')\ndata_train = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates)\n\n# Visualize\nvisualize_ts_plotly({'Train': data_train})\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#multi-dataset-comparison","title":"Multi-Dataset Comparison","text":"<pre><code>from spotforecast2_safe.preprocessing.time_series_visualization import visualize_ts_comparison\n\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_comparison(\n    dataframes,\n    show_mean=True,\n    title_suffix='[\u00b0C]'\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#features","title":"Features","text":"<ul> <li>Interactive Exploration: Zoom, pan, and hover for detailed insights</li> <li>Multiple Datasets: Compare multiple time series side-by-side</li> <li>Customization: Control colors, templates, and figure sizes</li> <li>Export Support: Save visualizations as PNG or HTML</li> <li>Responsive Design: Works on desktop and mobile displays</li> </ul>"},{"location":"preprocessing/time_series_visualization/#requirements","title":"Requirements","text":"<p>Requires <code>plotly&gt;=6.5.2</code> for visualization functionality.</p>"},{"location":"processing/model_persistence/","title":"Model Persistence","text":"<p>Guide for saving and loading trained forecasting models.</p>"},{"location":"processing/model_persistence/#overview","title":"Overview","text":"<p>The model persistence functionality enables you to: - Save trained forecasters to disk - Load previously trained models - Manage model caching - Handle batch model operations</p>"},{"location":"processing/model_persistence/#functions","title":"Functions","text":""},{"location":"processing/model_persistence/#saving-models","title":"Saving Models","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._save_forecasters","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._save_forecasters(forecasters, model_dir, verbose=False)</code>","text":"<p>Save trained forecasters to disk using joblib.</p> <p>Follows scikit-learn persistence conventions using joblib for efficient serialization of sklearn-compatible estimators.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>Dict[str, object]</code> <p>Dictionary mapping target names to trained ForecasterRecursive objects.</p> required <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory to save models. Created if it doesn't exist.</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Path]</code> <p>Dict[str, Path]: Dictionary mapping target names to saved model filepaths.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If models cannot be written to disk.</p> <code>TypeError</code> <p>If forecasters contain non-serializable objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forecasters = {\"power\": forecaster_obj}\n&gt;&gt;&gt; paths = _save_forecasters(forecasters, \"./models\", verbose=True)\n&gt;&gt;&gt; print(paths[\"power\"])\nmodels/forecaster_power.joblib\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _save_forecasters(\n    forecasters: Dict[str, object],\n    model_dir: Union[str, Path],\n    verbose: bool = False,\n) -&gt; Dict[str, Path]:\n    \"\"\"Save trained forecasters to disk using joblib.\n\n    Follows scikit-learn persistence conventions using joblib for efficient\n    serialization of sklearn-compatible estimators.\n\n    Args:\n        forecasters: Dictionary mapping target names to trained ForecasterRecursive objects.\n        model_dir: Directory to save models. Created if it doesn't exist.\n        verbose: Print progress messages. Default: False.\n\n    Returns:\n        Dict[str, Path]: Dictionary mapping target names to saved model filepaths.\n\n    Raises:\n        OSError: If models cannot be written to disk.\n        TypeError: If forecasters contain non-serializable objects.\n\n    Examples:\n        &gt;&gt;&gt; forecasters = {\"power\": forecaster_obj}\n        &gt;&gt;&gt; paths = _save_forecasters(forecasters, \"./models\", verbose=True)\n        &gt;&gt;&gt; print(paths[\"power\"])\n        models/forecaster_power.joblib\n    \"\"\"\n    model_path = _ensure_model_dir(model_dir)\n    saved_paths = {}\n\n    for target, forecaster in forecasters.items():\n        filepath = _get_model_filepath(model_path, target)\n        try:\n            dump(forecaster, filepath, compress=3)\n            saved_paths[target] = filepath\n            if verbose:\n                print(f\"  \u2713 Saved forecaster for {target} to {filepath}\")\n        except Exception as e:\n            raise OSError(f\"Failed to save model for {target}: {e}\")\n\n    return saved_paths\n</code></pre>"},{"location":"processing/model_persistence/#loading-models","title":"Loading Models","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._load_forecasters","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._load_forecasters(target_columns, model_dir, verbose=False)</code>","text":"<p>Load trained forecasters from disk using joblib.</p> <p>Attempts to load all forecasters for given targets. Missing models are indicated in the return value for selective retraining.</p> <p>Parameters:</p> Name Type Description Default <code>target_columns</code> <code>List[str]</code> <p>List of target variable names to load.</p> required <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory containing saved models.</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, object]</code> <p>Tuple[Dict[str, object], List[str]]:</p> <code>List[str]</code> <ul> <li>forecasters: Dictionary of successfully loaded ForecasterRecursive objects.</li> </ul> <code>Tuple[Dict[str, object], List[str]]</code> <ul> <li>missing_targets: List of target names without saved models.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forecasters, missing = _load_forecasters(\n...     [\"power\", \"energy\"],\n...     \"./models\",\n...     verbose=True\n... )\n&gt;&gt;&gt; print(missing)\n['energy']\n</code></pre> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _load_forecasters(\n    target_columns: List[str],\n    model_dir: Union[str, Path],\n    verbose: bool = False,\n) -&gt; Tuple[Dict[str, object], List[str]]:\n    \"\"\"Load trained forecasters from disk using joblib.\n\n    Attempts to load all forecasters for given targets. Missing models\n    are indicated in the return value for selective retraining.\n\n    Args:\n        target_columns: List of target variable names to load.\n        model_dir: Directory containing saved models.\n        verbose: Print progress messages. Default: False.\n\n    Returns:\n        Tuple[Dict[str, object], List[str]]:\n        - forecasters: Dictionary of successfully loaded ForecasterRecursive objects.\n        - missing_targets: List of target names without saved models.\n\n    Examples:\n        &gt;&gt;&gt; forecasters, missing = _load_forecasters(\n        ...     [\"power\", \"energy\"],\n        ...     \"./models\",\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(missing)\n        ['energy']\n    \"\"\"\n    model_path = Path(model_dir)\n    forecasters = {}\n    missing_targets = []\n\n    for target in target_columns:\n        filepath = _get_model_filepath(model_path, target)\n        if filepath.exists():\n            try:\n                forecasters[target] = load(filepath)\n                if verbose:\n                    print(f\"  \u2713 Loaded forecaster for {target} from {filepath}\")\n            except Exception as e:\n                if verbose:\n                    print(f\"  \u2717 Failed to load {target}: {e}\")\n                missing_targets.append(target)\n        else:\n            missing_targets.append(target)\n\n    return forecasters, missing_targets\n</code></pre>"},{"location":"processing/model_persistence/#model-directory-management","title":"Model Directory Management","text":""},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._ensure_model_dir","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._ensure_model_dir(model_dir)</code>","text":"<p>Ensure model directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory path for model storage.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Validated Path object.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If directory cannot be created.</p> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _ensure_model_dir(model_dir: Union[str, Path]) -&gt; Path:\n    \"\"\"Ensure model directory exists.\n\n    Args:\n        model_dir: Directory path for model storage.\n\n    Returns:\n        Path: Validated Path object.\n\n    Raises:\n        OSError: If directory cannot be created.\n    \"\"\"\n    model_path = Path(model_dir)\n    model_path.mkdir(parents=True, exist_ok=True)\n    return model_path\n</code></pre>"},{"location":"processing/model_persistence/#spotforecast2_safe.processing.n2n_predict_with_covariates._model_directory_exists","title":"<code>spotforecast2_safe.processing.n2n_predict_with_covariates._model_directory_exists(model_dir)</code>","text":"<p>Check if model directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>Directory path to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if directory exists, False otherwise.</p> Source code in <code>src/spotforecast2_safe/processing/n2n_predict_with_covariates.py</code> <pre><code>def _model_directory_exists(model_dir: Union[str, Path]) -&gt; bool:\n    \"\"\"Check if model directory exists.\n\n    Args:\n        model_dir: Directory path to check.\n\n    Returns:\n        bool: True if directory exists, False otherwise.\n    \"\"\"\n    return Path(model_dir).exists()\n</code></pre>"},{"location":"processing/model_persistence/#examples","title":"Examples","text":"<pre><code>from spotforecast2_safe.processing.n2n_predict_with_covariates import (\n    _save_forecasters,\n    _load_forecasters,\n)\n\n# Save trained models\ntrained_forecasters = {...}  # Your trained forecasters\n_save_forecasters(trained_forecasters, model_dir=\"models/\")\n\n# Load previously trained models\nloaded_forecasters = _load_forecasters(model_dir=\"models/\")\n</code></pre>"}]}